{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sklearn_MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "plvG4_aa36uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lINPgm3v2I0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,KFold,ShuffleSplit,cross_val_score,StratifiedShuffleSplit\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAQNso0o2eRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"drive/My Drive/Thesis/Wildfire.csv\")\n",
        "X = data.loc[:,['NDVI','LST','Burned_Area','SM','SUSM']].values.astype('float32')\n",
        "nor_X = preprocessing.normalize(X)\n",
        "Y = data.loc[:,['Class']].values.astype('int')\n",
        "labelencoder_y_1 = LabelEncoder()\n",
        "Y = labelencoder_y_1.fit_transform(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHQUhWMn8kMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample figsize in inches\n",
        "fig, ax = plt.subplots(figsize=(20,10))         \n",
        "# Imbalanced DataFrame Correlation\n",
        "corr = data.corr()\n",
        "sns.heatmap(corr, cmap='YlGnBu', annot_kws={'size':30}, ax=ax)\n",
        "ax.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nADx9diXOGhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1080)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OLefgbOsnjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.value_counts(y_test).plot.bar()\n",
        "plt.title('class histogram')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzW4N308AeJV",
        "colab_type": "code",
        "outputId": "f70bb905-7bc1-424b-a714-ae8de87420f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "kfold = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=12)\n",
        "training_accuracy = []\n",
        "testing_accuracy = []\n",
        "epochs=10\n",
        "for train, test in kfold.split(X_train, y_train):\n",
        "\n",
        "  clf = MLPClassifier(solver='sgd',learning_rate='adaptive',momentum=0.9, activation='relu',alpha=5, batch_size='auto',verbose=True,n_iter_no_change = epochs)\n",
        "  clf.fit(X[train],Y[train])\n",
        "  model = clf\n",
        "  \n",
        "  #training accuracy\n",
        "  y_tr_pred = clf.predict(X[train])\n",
        "  results_tr = cross_val_score(model, X[train] ,y_tr_pred, cv = kfold,verbose=1)\n",
        "  training_accuracy.append(results_tr.max()*100.0)\n",
        "  \n",
        "  #testing Accuracy\n",
        "  y_te_pred = clf.predict(X_test)\n",
        "  results = cross_val_score(model, X_test ,y_te_pred, cv = kfold,verbose=1)\n",
        "  testing_accuracy.append(results.max()*100.0)\n",
        "  \n",
        "  print(\"Training Accuracy (Shuffle Split) : %.3f%% (%.3f%%)\" % (results_tr.max()*100.0, results_tr.std()*100.0))\n",
        "  print(\"Prediction Accuracy (Shuffle Split) : %.3f%% (%.3f%%)\" % (results.max()*100.0, results.std()*100.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.95918017\n",
            "Iteration 2, loss = 0.41412313\n",
            "Iteration 3, loss = 0.38473055\n",
            "Iteration 4, loss = 0.36406503\n",
            "Iteration 5, loss = 0.34863988\n",
            "Iteration 6, loss = 0.33818369\n",
            "Iteration 7, loss = 0.33019605\n",
            "Iteration 8, loss = 0.32472213\n",
            "Iteration 9, loss = 0.32070491\n",
            "Iteration 10, loss = 0.31707818\n",
            "Iteration 11, loss = 0.31499817\n",
            "Iteration 12, loss = 0.31384742\n",
            "Iteration 13, loss = 0.31222949\n",
            "Iteration 14, loss = 0.31272689\n",
            "Iteration 15, loss = 0.31062250\n",
            "Iteration 16, loss = 0.31132395\n",
            "Iteration 17, loss = 0.31030705\n",
            "Iteration 18, loss = 0.31026238\n",
            "Iteration 19, loss = 0.30978468\n",
            "Iteration 20, loss = 0.30953694\n",
            "Iteration 21, loss = 0.30951026\n",
            "Iteration 22, loss = 0.30978915\n",
            "Iteration 23, loss = 0.30925891\n",
            "Iteration 24, loss = 0.30944570\n",
            "Iteration 25, loss = 0.30964592\n",
            "Iteration 26, loss = 0.30936845\n",
            "Iteration 27, loss = 0.30918822\n",
            "Iteration 28, loss = 0.30940547\n",
            "Iteration 29, loss = 0.30892203\n",
            "Iteration 30, loss = 0.32184831\n",
            "Iteration 31, loss = 0.31845508\n",
            "Iteration 32, loss = 0.31505622\n",
            "Iteration 33, loss = 0.31278387\n",
            "Iteration 34, loss = 0.31113731\n",
            "Iteration 35, loss = 0.31052464\n",
            "Iteration 36, loss = 0.30929890\n",
            "Iteration 37, loss = 0.30887135\n",
            "Iteration 38, loss = 0.30915790\n",
            "Iteration 39, loss = 0.30813726\n",
            "Iteration 40, loss = 0.30815884\n",
            "Iteration 41, loss = 0.30789809\n",
            "Iteration 42, loss = 0.30712643\n",
            "Iteration 43, loss = 0.30840101\n",
            "Iteration 44, loss = 0.30756883\n",
            "Iteration 45, loss = 0.30775327\n",
            "Iteration 46, loss = 0.30710567\n",
            "Iteration 47, loss = 0.30781982\n",
            "Iteration 48, loss = 0.30763103\n",
            "Iteration 49, loss = 0.30729934\n",
            "Iteration 50, loss = 0.30746402\n",
            "Iteration 51, loss = 0.31191623\n",
            "Iteration 52, loss = 0.32073662\n",
            "Iteration 53, loss = 0.31710616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.31407085\n",
            "Iteration 55, loss = 0.31314581\n",
            "Iteration 56, loss = 0.31217354\n",
            "Iteration 57, loss = 0.31137748\n",
            "Iteration 58, loss = 0.31068705\n",
            "Iteration 59, loss = 0.31014789\n",
            "Iteration 60, loss = 0.30956247\n",
            "Iteration 61, loss = 0.30904437\n",
            "Iteration 62, loss = 0.30851368\n",
            "Iteration 63, loss = 0.30788419\n",
            "Iteration 64, loss = 0.30754777\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 65, loss = 0.30645235\n",
            "Iteration 66, loss = 0.30627148\n",
            "Iteration 67, loss = 0.30607575\n",
            "Iteration 68, loss = 0.30601600\n",
            "Iteration 69, loss = 0.30589486\n",
            "Iteration 70, loss = 0.30582786\n",
            "Iteration 71, loss = 0.30568758\n",
            "Iteration 72, loss = 0.30560358\n",
            "Iteration 73, loss = 0.30548273\n",
            "Iteration 74, loss = 0.30543909\n",
            "Iteration 75, loss = 0.30524633\n",
            "Iteration 76, loss = 0.30513331\n",
            "Iteration 77, loss = 0.30501928\n",
            "Iteration 78, loss = 0.30491046\n",
            "Iteration 79, loss = 0.30485250\n",
            "Iteration 80, loss = 0.30472456\n",
            "Iteration 81, loss = 0.30461830\n",
            "Iteration 82, loss = 0.30465925\n",
            "Iteration 83, loss = 0.30430368\n",
            "Iteration 84, loss = 0.30432793\n",
            "Iteration 85, loss = 0.30411364\n",
            "Iteration 86, loss = 0.30417627\n",
            "Iteration 87, loss = 0.30388836\n",
            "Iteration 88, loss = 0.30375925\n",
            "Iteration 89, loss = 0.30382010\n",
            "Iteration 90, loss = 0.30355500\n",
            "Iteration 91, loss = 0.30352723\n",
            "Iteration 92, loss = 0.30342994\n",
            "Iteration 93, loss = 0.30325602\n",
            "Iteration 94, loss = 0.30319932\n",
            "Iteration 95, loss = 0.30317358\n",
            "Iteration 96, loss = 0.30291290\n",
            "Iteration 97, loss = 0.30275010\n",
            "Iteration 98, loss = 0.30262599\n",
            "Iteration 99, loss = 0.30254596\n",
            "Iteration 100, loss = 0.30239021\n",
            "Iteration 101, loss = 0.30225427\n",
            "Iteration 102, loss = 0.30211245\n",
            "Iteration 103, loss = 0.30203750\n",
            "Iteration 104, loss = 0.30193738\n",
            "Iteration 105, loss = 0.30190823\n",
            "Iteration 106, loss = 0.30167397\n",
            "Iteration 107, loss = 0.30150533\n",
            "Iteration 108, loss = 0.30132654\n",
            "Iteration 109, loss = 0.30119765\n",
            "Iteration 110, loss = 0.30113649\n",
            "Iteration 111, loss = 0.30100636\n",
            "Iteration 112, loss = 0.30090359\n",
            "Iteration 113, loss = 0.30075079\n",
            "Iteration 114, loss = 0.30047592\n",
            "Iteration 115, loss = 0.30038795\n",
            "Iteration 116, loss = 0.30036709\n",
            "Iteration 117, loss = 0.30014939\n",
            "Iteration 118, loss = 0.29996144\n",
            "Iteration 119, loss = 0.29983278\n",
            "Iteration 120, loss = 0.29972678\n",
            "Iteration 121, loss = 0.29954380\n",
            "Iteration 122, loss = 0.29946535\n",
            "Iteration 123, loss = 0.29913667\n",
            "Iteration 124, loss = 0.29903989\n",
            "Iteration 125, loss = 0.29879329\n",
            "Iteration 126, loss = 0.29880443\n",
            "Iteration 127, loss = 0.29865372\n",
            "Iteration 128, loss = 0.29828201\n",
            "Iteration 129, loss = 0.29830255\n",
            "Iteration 130, loss = 0.29806090\n",
            "Iteration 131, loss = 0.29796755\n",
            "Iteration 132, loss = 0.29782952\n",
            "Iteration 133, loss = 0.29753575\n",
            "Iteration 134, loss = 0.29738250\n",
            "Iteration 135, loss = 0.29720504\n",
            "Iteration 136, loss = 0.29709962\n",
            "Iteration 137, loss = 0.29684636\n",
            "Iteration 138, loss = 0.29667482\n",
            "Iteration 139, loss = 0.29647278\n",
            "Iteration 140, loss = 0.29629138\n",
            "Iteration 141, loss = 0.29604151\n",
            "Iteration 142, loss = 0.29600556\n",
            "Iteration 143, loss = 0.29572106\n",
            "Iteration 144, loss = 0.29555624\n",
            "Iteration 145, loss = 0.29538666\n",
            "Iteration 146, loss = 0.29522656\n",
            "Iteration 147, loss = 0.29492195\n",
            "Iteration 148, loss = 0.29474176\n",
            "Iteration 149, loss = 0.29465632\n",
            "Iteration 150, loss = 0.29446253\n",
            "Iteration 151, loss = 0.29402762\n",
            "Iteration 152, loss = 0.29380180\n",
            "Iteration 153, loss = 0.29381878\n",
            "Iteration 154, loss = 0.29350427\n",
            "Iteration 155, loss = 0.29323184\n",
            "Iteration 156, loss = 0.29294323\n",
            "Iteration 157, loss = 0.29278285\n",
            "Iteration 158, loss = 0.29256295\n",
            "Iteration 159, loss = 0.29239516\n",
            "Iteration 160, loss = 0.29201733\n",
            "Iteration 161, loss = 0.29196749\n",
            "Iteration 162, loss = 0.29171534\n",
            "Iteration 163, loss = 0.29151184\n",
            "Iteration 164, loss = 0.29131147\n",
            "Iteration 165, loss = 0.29098642\n",
            "Iteration 166, loss = 0.29076369\n",
            "Iteration 167, loss = 0.29048440\n",
            "Iteration 168, loss = 0.29030769\n",
            "Iteration 169, loss = 0.29009304\n",
            "Iteration 170, loss = 0.28971837\n",
            "Iteration 171, loss = 0.28966210\n",
            "Iteration 172, loss = 0.28949463\n",
            "Iteration 173, loss = 0.28904091\n",
            "Iteration 174, loss = 0.28884414\n",
            "Iteration 175, loss = 0.28846896\n",
            "Iteration 176, loss = 0.28849486\n",
            "Iteration 177, loss = 0.28812343\n",
            "Iteration 178, loss = 0.28778363\n",
            "Iteration 179, loss = 0.28765551\n",
            "Iteration 180, loss = 0.28723286\n",
            "Iteration 181, loss = 0.28708529\n",
            "Iteration 182, loss = 0.28671208\n",
            "Iteration 183, loss = 0.28665774\n",
            "Iteration 184, loss = 0.28631318\n",
            "Iteration 185, loss = 0.28598624\n",
            "Iteration 186, loss = 0.28576985\n",
            "Iteration 187, loss = 0.28554673\n",
            "Iteration 188, loss = 0.28527017\n",
            "Iteration 189, loss = 0.28486937\n",
            "Iteration 190, loss = 0.28464627\n",
            "Iteration 191, loss = 0.28442360\n",
            "Iteration 192, loss = 0.28414354\n",
            "Iteration 193, loss = 0.28385403\n",
            "Iteration 194, loss = 0.28368232\n",
            "Iteration 195, loss = 0.28322875\n",
            "Iteration 196, loss = 0.28295116\n",
            "Iteration 197, loss = 0.28286278\n",
            "Iteration 198, loss = 0.28230613\n",
            "Iteration 199, loss = 0.28213787\n",
            "Iteration 200, loss = 0.28186857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.35401642\n",
            "Iteration 2, loss = 0.26021198\n",
            "Iteration 3, loss = 0.19806223\n",
            "Iteration 4, loss = 0.15075650\n",
            "Iteration 5, loss = 0.11474940\n",
            "Iteration 6, loss = 0.08734233\n",
            "Iteration 7, loss = 0.06648125\n",
            "Iteration 8, loss = 0.05060268\n",
            "Iteration 9, loss = 0.03851660\n",
            "Iteration 10, loss = 0.02931719\n",
            "Iteration 11, loss = 0.02231499\n",
            "Iteration 12, loss = 0.01698521\n",
            "Iteration 13, loss = 0.01292842\n",
            "Iteration 14, loss = 0.00984056\n",
            "Iteration 15, loss = 0.00749021\n",
            "Iteration 16, loss = 0.00570123\n",
            "Iteration 17, loss = 0.00433953\n",
            "Iteration 18, loss = 0.00330317\n",
            "Iteration 19, loss = 0.00251848\n",
            "Iteration 20, loss = 0.00196939\n",
            "Iteration 21, loss = 0.00165080\n",
            "Iteration 22, loss = 0.00144353\n",
            "Iteration 23, loss = 0.00129399\n",
            "Iteration 24, loss = 0.00118496\n",
            "Iteration 25, loss = 0.00110494\n",
            "Iteration 26, loss = 0.00104580\n",
            "Iteration 27, loss = 0.00100185\n",
            "Iteration 28, loss = 0.00096902\n",
            "Iteration 29, loss = 0.00094439\n",
            "Iteration 30, loss = 0.00092585\n",
            "Iteration 31, loss = 0.00091186\n",
            "Iteration 32, loss = 0.00090126\n",
            "Iteration 33, loss = 0.00089324\n",
            "Iteration 34, loss = 0.00088716\n",
            "Iteration 35, loss = 0.00088252\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.00088021\n",
            "Iteration 37, loss = 0.00087954\n",
            "Iteration 38, loss = 0.00087892\n",
            "Iteration 39, loss = 0.00087832\n",
            "Iteration 40, loss = 0.00087776\n",
            "Iteration 41, loss = 0.00087723\n",
            "Iteration 42, loss = 0.00087672\n",
            "Iteration 43, loss = 0.00087624\n",
            "Iteration 44, loss = 0.00087579\n",
            "Iteration 45, loss = 0.00087535\n",
            "Iteration 46, loss = 0.00087495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.00087470\n",
            "Iteration 48, loss = 0.00087463\n",
            "Iteration 49, loss = 0.00087455\n",
            "Iteration 50, loss = 0.00087448\n",
            "Iteration 51, loss = 0.00087440\n",
            "Iteration 52, loss = 0.00087433\n",
            "Iteration 53, loss = 0.00087426\n",
            "Iteration 54, loss = 0.00087419\n",
            "Iteration 55, loss = 0.00087411\n",
            "Iteration 56, loss = 0.00087404\n",
            "Iteration 57, loss = 0.00087397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 58, loss = 0.00087393\n",
            "Iteration 59, loss = 0.00087392\n",
            "Iteration 60, loss = 0.00087390\n",
            "Iteration 61, loss = 0.00087389\n",
            "Iteration 62, loss = 0.00087388\n",
            "Iteration 63, loss = 0.00087386\n",
            "Iteration 64, loss = 0.00087385\n",
            "Iteration 65, loss = 0.00087384\n",
            "Iteration 66, loss = 0.00087382\n",
            "Iteration 67, loss = 0.00087381\n",
            "Iteration 68, loss = 0.00087380\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 69, loss = 0.00087379\n",
            "Iteration 70, loss = 0.00087378\n",
            "Iteration 71, loss = 0.00087378\n",
            "Iteration 72, loss = 0.00087378\n",
            "Iteration 73, loss = 0.00087378\n",
            "Iteration 74, loss = 0.00087377\n",
            "Iteration 75, loss = 0.00087377\n",
            "Iteration 76, loss = 0.00087377\n",
            "Iteration 77, loss = 0.00087377\n",
            "Iteration 78, loss = 0.00087376\n",
            "Iteration 79, loss = 0.00087376\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 80, loss = 0.00087376\n",
            "Iteration 81, loss = 0.00087376\n",
            "Iteration 82, loss = 0.00087376\n",
            "Iteration 83, loss = 0.00087376\n",
            "Iteration 84, loss = 0.00087376\n",
            "Iteration 85, loss = 0.00087376\n",
            "Iteration 86, loss = 0.00087376\n",
            "Iteration 87, loss = 0.00087376\n",
            "Iteration 88, loss = 0.00087375\n",
            "Iteration 89, loss = 0.00087375\n",
            "Iteration 90, loss = 0.00087375\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12122682\n",
            "Iteration 2, loss = 0.09228106\n",
            "Iteration 3, loss = 0.07024041\n",
            "Iteration 4, loss = 0.05346466\n",
            "Iteration 5, loss = 0.04070540\n",
            "Iteration 6, loss = 0.03101618\n",
            "Iteration 7, loss = 0.02365593\n",
            "Iteration 8, loss = 0.01806702\n",
            "Iteration 9, loss = 0.01382840\n",
            "Iteration 10, loss = 0.01061974\n",
            "Iteration 11, loss = 0.00819687\n",
            "Iteration 12, loss = 0.00637304\n",
            "Iteration 13, loss = 0.00500479\n",
            "Iteration 14, loss = 0.00398242\n",
            "Iteration 15, loss = 0.00321968\n",
            "Iteration 16, loss = 0.00264929\n",
            "Iteration 17, loss = 0.00222190\n",
            "Iteration 18, loss = 0.00190083\n",
            "Iteration 19, loss = 0.00165901\n",
            "Iteration 20, loss = 0.00147643\n",
            "Iteration 21, loss = 0.00133831\n",
            "Iteration 22, loss = 0.00123365\n",
            "Iteration 23, loss = 0.00115426\n",
            "Iteration 24, loss = 0.00109398\n",
            "Iteration 25, loss = 0.00104818\n",
            "Iteration 26, loss = 0.00101337\n",
            "Iteration 27, loss = 0.00098689\n",
            "Iteration 28, loss = 0.00096674\n",
            "Iteration 29, loss = 0.00095140\n",
            "Iteration 30, loss = 0.00093971\n",
            "Iteration 31, loss = 0.00093081\n",
            "Iteration 32, loss = 0.00092402\n",
            "Iteration 33, loss = 0.00091884\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091626\n",
            "Iteration 35, loss = 0.00091551\n",
            "Iteration 36, loss = 0.00091481\n",
            "Iteration 37, loss = 0.00091414\n",
            "Iteration 38, loss = 0.00091351\n",
            "Iteration 39, loss = 0.00091292\n",
            "Iteration 40, loss = 0.00091235\n",
            "Iteration 41, loss = 0.00091181\n",
            "Iteration 42, loss = 0.00091130\n",
            "Iteration 43, loss = 0.00091082\n",
            "Iteration 44, loss = 0.00091036\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00091009\n",
            "Iteration 46, loss = 0.00091001\n",
            "Iteration 47, loss = 0.00090992\n",
            "Iteration 48, loss = 0.00090984\n",
            "Iteration 49, loss = 0.00090975\n",
            "Iteration 50, loss = 0.00090967\n",
            "Iteration 51, loss = 0.00090959\n",
            "Iteration 52, loss = 0.00090951\n",
            "Iteration 53, loss = 0.00090943\n",
            "Iteration 54, loss = 0.00090935\n",
            "Iteration 55, loss = 0.00090927\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090923\n",
            "Iteration 57, loss = 0.00090921\n",
            "Iteration 58, loss = 0.00090920\n",
            "Iteration 59, loss = 0.00090918\n",
            "Iteration 60, loss = 0.00090916\n",
            "Iteration 61, loss = 0.00090915\n",
            "Iteration 62, loss = 0.00090913\n",
            "Iteration 63, loss = 0.00090912\n",
            "Iteration 64, loss = 0.00090910\n",
            "Iteration 65, loss = 0.00090909\n",
            "Iteration 66, loss = 0.00090907\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090906\n",
            "Iteration 68, loss = 0.00090906\n",
            "Iteration 69, loss = 0.00090906\n",
            "Iteration 70, loss = 0.00090905\n",
            "Iteration 71, loss = 0.00090905\n",
            "Iteration 72, loss = 0.00090905\n",
            "Iteration 73, loss = 0.00090905\n",
            "Iteration 74, loss = 0.00090904\n",
            "Iteration 75, loss = 0.00090904\n",
            "Iteration 76, loss = 0.00090904\n",
            "Iteration 77, loss = 0.00090903\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090903\n",
            "Iteration 79, loss = 0.00090903\n",
            "Iteration 80, loss = 0.00090903\n",
            "Iteration 81, loss = 0.00090903\n",
            "Iteration 82, loss = 0.00090903\n",
            "Iteration 83, loss = 0.00090903\n",
            "Iteration 84, loss = 0.00090903\n",
            "Iteration 85, loss = 0.00090903\n",
            "Iteration 86, loss = 0.00090903\n",
            "Iteration 87, loss = 0.00090903\n",
            "Iteration 88, loss = 0.00090903\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23663868\n",
            "Iteration 3, loss = 0.18011924\n",
            "Iteration 4, loss = 0.13709906\n",
            "Iteration 5, loss = 0.10435394\n",
            "Iteration 6, loss = 0.07942976\n",
            "Iteration 7, loss = 0.06045853\n",
            "Iteration 8, loss = 0.04601845\n",
            "Iteration 9, loss = 0.03502727\n",
            "Iteration 10, loss = 0.02666126\n",
            "Iteration 11, loss = 0.02029341\n",
            "Iteration 12, loss = 0.01544648\n",
            "Iteration 13, loss = 0.01175720\n",
            "Iteration 14, loss = 0.00894908\n",
            "Iteration 15, loss = 0.00681165\n",
            "Iteration 16, loss = 0.00518474\n",
            "Iteration 17, loss = 0.00394650\n",
            "Iteration 18, loss = 0.00300806\n",
            "Iteration 19, loss = 0.00234338\n",
            "Iteration 20, loss = 0.00194431\n",
            "Iteration 21, loss = 0.00167634\n",
            "Iteration 22, loss = 0.00147753\n",
            "Iteration 23, loss = 0.00132965\n",
            "Iteration 24, loss = 0.00121921\n",
            "Iteration 25, loss = 0.00113641\n",
            "Iteration 26, loss = 0.00107413\n",
            "Iteration 27, loss = 0.00102715\n",
            "Iteration 28, loss = 0.00099164\n",
            "Iteration 29, loss = 0.00096474\n",
            "Iteration 30, loss = 0.00094435\n",
            "Iteration 31, loss = 0.00092886\n",
            "Iteration 32, loss = 0.00091709\n",
            "Iteration 33, loss = 0.00090814\n",
            "Iteration 34, loss = 0.00090132\n",
            "Iteration 35, loss = 0.00089612\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.00089351\n",
            "Iteration 37, loss = 0.00089277\n",
            "Iteration 38, loss = 0.00089206\n",
            "Iteration 39, loss = 0.00089139\n",
            "Iteration 40, loss = 0.00089076\n",
            "Iteration 41, loss = 0.00089016\n",
            "Iteration 42, loss = 0.00088959\n",
            "Iteration 43, loss = 0.00088905\n",
            "Iteration 44, loss = 0.00088853\n",
            "Iteration 45, loss = 0.00088805\n",
            "Iteration 46, loss = 0.00088759\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.00088731\n",
            "Iteration 48, loss = 0.00088723\n",
            "Iteration 49, loss = 0.00088714\n",
            "Iteration 50, loss = 0.00088706\n",
            "Iteration 51, loss = 0.00088697\n",
            "Iteration 52, loss = 0.00088689\n",
            "Iteration 53, loss = 0.00088681\n",
            "Iteration 54, loss = 0.00088673\n",
            "Iteration 55, loss = 0.00088665\n",
            "Iteration 56, loss = 0.00088657\n",
            "Iteration 57, loss = 0.00088649\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 58, loss = 0.00088644\n",
            "Iteration 59, loss = 0.00088642\n",
            "Iteration 60, loss = 0.00088641\n",
            "Iteration 61, loss = 0.00088639\n",
            "Iteration 62, loss = 0.00088638\n",
            "Iteration 63, loss = 0.00088636\n",
            "Iteration 64, loss = 0.00088635\n",
            "Iteration 65, loss = 0.00088633\n",
            "Iteration 66, loss = 0.00088632\n",
            "Iteration 67, loss = 0.00088630\n",
            "Iteration 68, loss = 0.00088629\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 69, loss = 0.00088628\n",
            "Iteration 70, loss = 0.00088627\n",
            "Iteration 71, loss = 0.00088627\n",
            "Iteration 72, loss = 0.00088627\n",
            "Iteration 73, loss = 0.00088626\n",
            "Iteration 74, loss = 0.00088626\n",
            "Iteration 75, loss = 0.00088626\n",
            "Iteration 76, loss = 0.00088626\n",
            "Iteration 77, loss = 0.00088625\n",
            "Iteration 78, loss = 0.00088625\n",
            "Iteration 79, loss = 0.00088625\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 80, loss = 0.00088624\n",
            "Iteration 81, loss = 0.00088624\n",
            "Iteration 82, loss = 0.00088624\n",
            "Iteration 83, loss = 0.00088624\n",
            "Iteration 84, loss = 0.00088624\n",
            "Iteration 85, loss = 0.00088624\n",
            "Iteration 86, loss = 0.00088624\n",
            "Iteration 87, loss = 0.00088624\n",
            "Iteration 88, loss = 0.00088624\n",
            "Iteration 89, loss = 0.00088624\n",
            "Iteration 90, loss = 0.00088624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.22997377\n",
            "Iteration 3, loss = 0.17504620\n",
            "Iteration 4, loss = 0.13323768\n",
            "Iteration 5, loss = 0.10141482\n",
            "Iteration 6, loss = 0.07719262\n",
            "Iteration 7, loss = 0.05875572\n",
            "Iteration 8, loss = 0.04472234\n",
            "Iteration 9, loss = 0.03404074\n",
            "Iteration 10, loss = 0.02591035\n",
            "Iteration 11, loss = 0.01972185\n",
            "Iteration 12, loss = 0.01501143\n",
            "Iteration 13, loss = 0.01142606\n",
            "Iteration 14, loss = 0.00869703\n",
            "Iteration 15, loss = 0.00661980\n",
            "Iteration 16, loss = 0.00503871\n",
            "Iteration 17, loss = 0.00383538\n",
            "Iteration 18, loss = 0.00292479\n",
            "Iteration 19, loss = 0.00229065\n",
            "Iteration 20, loss = 0.00191212\n",
            "Iteration 21, loss = 0.00165432\n",
            "Iteration 22, loss = 0.00146507\n",
            "Iteration 23, loss = 0.00132508\n",
            "Iteration 24, loss = 0.00122111\n",
            "Iteration 25, loss = 0.00114364\n",
            "Iteration 26, loss = 0.00108570\n",
            "Iteration 27, loss = 0.00104219\n",
            "Iteration 28, loss = 0.00100942\n",
            "Iteration 29, loss = 0.00098467\n",
            "Iteration 30, loss = 0.00096594\n",
            "Iteration 31, loss = 0.00095174\n",
            "Iteration 32, loss = 0.00094097\n",
            "Iteration 33, loss = 0.00093277\n",
            "Iteration 34, loss = 0.00092649\n",
            "Iteration 35, loss = 0.00092166\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.00091926\n",
            "Iteration 37, loss = 0.00091857\n",
            "Iteration 38, loss = 0.00091792\n",
            "Iteration 39, loss = 0.00091731\n",
            "Iteration 40, loss = 0.00091673\n",
            "Iteration 41, loss = 0.00091618\n",
            "Iteration 42, loss = 0.00091566\n",
            "Iteration 43, loss = 0.00091517\n",
            "Iteration 44, loss = 0.00091470\n",
            "Iteration 45, loss = 0.00091426\n",
            "Iteration 46, loss = 0.00091384\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.00091359\n",
            "Iteration 48, loss = 0.00091352\n",
            "Iteration 49, loss = 0.00091344\n",
            "Iteration 50, loss = 0.00091336\n",
            "Iteration 51, loss = 0.00091329\n",
            "Iteration 52, loss = 0.00091321\n",
            "Iteration 53, loss = 0.00091314\n",
            "Iteration 54, loss = 0.00091306\n",
            "Iteration 55, loss = 0.00091299\n",
            "Iteration 56, loss = 0.00091292\n",
            "Iteration 57, loss = 0.00091285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 58, loss = 0.00091281\n",
            "Iteration 59, loss = 0.00091279\n",
            "Iteration 60, loss = 0.00091278\n",
            "Iteration 61, loss = 0.00091276\n",
            "Iteration 62, loss = 0.00091275\n",
            "Iteration 63, loss = 0.00091274\n",
            "Iteration 64, loss = 0.00091272\n",
            "Iteration 65, loss = 0.00091271\n",
            "Iteration 66, loss = 0.00091269\n",
            "Iteration 67, loss = 0.00091268\n",
            "Iteration 68, loss = 0.00091267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 69, loss = 0.00091266\n",
            "Iteration 70, loss = 0.00091266\n",
            "Iteration 71, loss = 0.00091265\n",
            "Iteration 72, loss = 0.00091265\n",
            "Iteration 73, loss = 0.00091265\n",
            "Iteration 74, loss = 0.00091264\n",
            "Iteration 75, loss = 0.00091264\n",
            "Iteration 76, loss = 0.00091264\n",
            "Iteration 77, loss = 0.00091264\n",
            "Iteration 78, loss = 0.00091263\n",
            "Iteration 79, loss = 0.00091263\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 80, loss = 0.00091263\n",
            "Iteration 81, loss = 0.00091263\n",
            "Iteration 82, loss = 0.00091263\n",
            "Iteration 83, loss = 0.00091263\n",
            "Iteration 84, loss = 0.00091263\n",
            "Iteration 85, loss = 0.00091263\n",
            "Iteration 86, loss = 0.00091263\n",
            "Iteration 87, loss = 0.00091263\n",
            "Iteration 88, loss = 0.00091262\n",
            "Iteration 89, loss = 0.00091262\n",
            "Iteration 90, loss = 0.00091262\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12709680\n",
            "Iteration 2, loss = 0.09676072\n",
            "Iteration 3, loss = 0.07366467\n",
            "Iteration 4, loss = 0.05608896\n",
            "Iteration 5, loss = 0.04271610\n",
            "Iteration 6, loss = 0.03254349\n",
            "Iteration 7, loss = 0.02480826\n",
            "Iteration 8, loss = 0.01893009\n",
            "Iteration 9, loss = 0.01446752\n",
            "Iteration 10, loss = 0.01108482\n",
            "Iteration 11, loss = 0.00852680\n",
            "Iteration 12, loss = 0.00659874\n",
            "Iteration 13, loss = 0.00515120\n",
            "Iteration 14, loss = 0.00406914\n",
            "Iteration 15, loss = 0.00326313\n",
            "Iteration 16, loss = 0.00266360\n",
            "Iteration 17, loss = 0.00221766\n",
            "Iteration 18, loss = 0.00188577\n",
            "Iteration 19, loss = 0.00163744\n",
            "Iteration 20, loss = 0.00145086\n",
            "Iteration 21, loss = 0.00131020\n",
            "Iteration 22, loss = 0.00120392\n",
            "Iteration 23, loss = 0.00112346\n",
            "Iteration 24, loss = 0.00106244\n",
            "Iteration 25, loss = 0.00101612\n",
            "Iteration 26, loss = 0.00098092\n",
            "Iteration 27, loss = 0.00095416\n",
            "Iteration 28, loss = 0.00093380\n",
            "Iteration 29, loss = 0.00091830\n",
            "Iteration 30, loss = 0.00090650\n",
            "Iteration 31, loss = 0.00089751\n",
            "Iteration 32, loss = 0.00089066\n",
            "Iteration 33, loss = 0.00088543\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00088281\n",
            "Iteration 35, loss = 0.00088206\n",
            "Iteration 36, loss = 0.00088135\n",
            "Iteration 37, loss = 0.00088068\n",
            "Iteration 38, loss = 0.00088004\n",
            "Iteration 39, loss = 0.00087943\n",
            "Iteration 40, loss = 0.00087886\n",
            "Iteration 41, loss = 0.00087832\n",
            "Iteration 42, loss = 0.00087780\n",
            "Iteration 43, loss = 0.00087731\n",
            "Iteration 44, loss = 0.00087685\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00087658\n",
            "Iteration 46, loss = 0.00087649\n",
            "Iteration 47, loss = 0.00087640\n",
            "Iteration 48, loss = 0.00087632\n",
            "Iteration 49, loss = 0.00087623\n",
            "Iteration 50, loss = 0.00087615\n",
            "Iteration 51, loss = 0.00087607\n",
            "Iteration 52, loss = 0.00087599\n",
            "Iteration 53, loss = 0.00087590\n",
            "Iteration 54, loss = 0.00087582\n",
            "Iteration 55, loss = 0.00087575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00087570\n",
            "Iteration 57, loss = 0.00087568\n",
            "Iteration 58, loss = 0.00087567\n",
            "Iteration 59, loss = 0.00087565\n",
            "Iteration 60, loss = 0.00087564\n",
            "Iteration 61, loss = 0.00087562\n",
            "Iteration 62, loss = 0.00087560\n",
            "Iteration 63, loss = 0.00087559\n",
            "Iteration 64, loss = 0.00087557\n",
            "Iteration 65, loss = 0.00087556\n",
            "Iteration 66, loss = 0.00087554\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00087553\n",
            "Iteration 68, loss = 0.00087553\n",
            "Iteration 69, loss = 0.00087553\n",
            "Iteration 70, loss = 0.00087552\n",
            "Iteration 71, loss = 0.00087552\n",
            "Iteration 72, loss = 0.00087552\n",
            "Iteration 73, loss = 0.00087552\n",
            "Iteration 74, loss = 0.00087551\n",
            "Iteration 75, loss = 0.00087551\n",
            "Iteration 76, loss = 0.00087551\n",
            "Iteration 77, loss = 0.00087550\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00087550\n",
            "Iteration 79, loss = 0.00087550\n",
            "Iteration 80, loss = 0.00087550\n",
            "Iteration 81, loss = 0.00087550\n",
            "Iteration 82, loss = 0.00087550\n",
            "Iteration 83, loss = 0.00087550\n",
            "Iteration 84, loss = 0.00087550\n",
            "Iteration 85, loss = 0.00087550\n",
            "Iteration 86, loss = 0.00087550\n",
            "Iteration 87, loss = 0.00087550\n",
            "Iteration 88, loss = 0.00087550\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11997195\n",
            "Iteration 2, loss = 0.09133041\n",
            "Iteration 3, loss = 0.06952883\n",
            "Iteration 4, loss = 0.05294030\n",
            "Iteration 5, loss = 0.04032079\n",
            "Iteration 6, loss = 0.03072433\n",
            "Iteration 7, loss = 0.02343100\n",
            "Iteration 8, loss = 0.01789289\n",
            "Iteration 9, loss = 0.01369325\n",
            "Iteration 10, loss = 0.01051570\n",
            "Iteration 11, loss = 0.00811818\n",
            "Iteration 12, loss = 0.00631491\n",
            "Iteration 13, loss = 0.00496237\n",
            "Iteration 14, loss = 0.00395032\n",
            "Iteration 15, loss = 0.00319582\n",
            "Iteration 16, loss = 0.00263262\n",
            "Iteration 17, loss = 0.00221069\n",
            "Iteration 18, loss = 0.00189363\n",
            "Iteration 19, loss = 0.00165478\n",
            "Iteration 20, loss = 0.00147434\n",
            "Iteration 21, loss = 0.00133777\n",
            "Iteration 22, loss = 0.00123426\n",
            "Iteration 23, loss = 0.00115572\n",
            "Iteration 24, loss = 0.00109607\n",
            "Iteration 25, loss = 0.00105074\n",
            "Iteration 26, loss = 0.00101628\n",
            "Iteration 27, loss = 0.00099006\n",
            "Iteration 28, loss = 0.00097009\n",
            "Iteration 29, loss = 0.00095488\n",
            "Iteration 30, loss = 0.00094329\n",
            "Iteration 31, loss = 0.00093444\n",
            "Iteration 32, loss = 0.00092771\n",
            "Iteration 33, loss = 0.00092258\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00092001\n",
            "Iteration 35, loss = 0.00091928\n",
            "Iteration 36, loss = 0.00091858\n",
            "Iteration 37, loss = 0.00091792\n",
            "Iteration 38, loss = 0.00091729\n",
            "Iteration 39, loss = 0.00091670\n",
            "Iteration 40, loss = 0.00091614\n",
            "Iteration 41, loss = 0.00091560\n",
            "Iteration 42, loss = 0.00091510\n",
            "Iteration 43, loss = 0.00091462\n",
            "Iteration 44, loss = 0.00091416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00091389\n",
            "Iteration 46, loss = 0.00091381\n",
            "Iteration 47, loss = 0.00091372\n",
            "Iteration 48, loss = 0.00091364\n",
            "Iteration 49, loss = 0.00091356\n",
            "Iteration 50, loss = 0.00091348\n",
            "Iteration 51, loss = 0.00091339\n",
            "Iteration 52, loss = 0.00091331\n",
            "Iteration 53, loss = 0.00091324\n",
            "Iteration 54, loss = 0.00091316\n",
            "Iteration 55, loss = 0.00091308\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00091303\n",
            "Iteration 57, loss = 0.00091302\n",
            "Iteration 58, loss = 0.00091300\n",
            "Iteration 59, loss = 0.00091299\n",
            "Iteration 60, loss = 0.00091297\n",
            "Iteration 61, loss = 0.00091296\n",
            "Iteration 62, loss = 0.00091294\n",
            "Iteration 63, loss = 0.00091293\n",
            "Iteration 64, loss = 0.00091291\n",
            "Iteration 65, loss = 0.00091290\n",
            "Iteration 66, loss = 0.00091288\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00091287\n",
            "Iteration 68, loss = 0.00091287\n",
            "Iteration 69, loss = 0.00091287\n",
            "Iteration 70, loss = 0.00091286\n",
            "Iteration 71, loss = 0.00091286\n",
            "Iteration 72, loss = 0.00091286\n",
            "Iteration 73, loss = 0.00091285\n",
            "Iteration 74, loss = 0.00091285\n",
            "Iteration 75, loss = 0.00091285\n",
            "Iteration 76, loss = 0.00091284\n",
            "Iteration 77, loss = 0.00091284\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00091284\n",
            "Iteration 79, loss = 0.00091284\n",
            "Iteration 80, loss = 0.00091284\n",
            "Iteration 81, loss = 0.00091284\n",
            "Iteration 82, loss = 0.00091284\n",
            "Iteration 83, loss = 0.00091284\n",
            "Iteration 84, loss = 0.00091284\n",
            "Iteration 85, loss = 0.00091284\n",
            "Iteration 86, loss = 0.00091284\n",
            "Iteration 87, loss = 0.00091283\n",
            "Iteration 88, loss = 0.00091283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13281978\n",
            "Iteration 2, loss = 0.10110599\n",
            "Iteration 3, loss = 0.07695925\n",
            "Iteration 4, loss = 0.05858981\n",
            "Iteration 5, loss = 0.04461959\n",
            "Iteration 6, loss = 0.03399522\n",
            "Iteration 7, loss = 0.02591951\n",
            "Iteration 8, loss = 0.01978617\n",
            "Iteration 9, loss = 0.01513355\n",
            "Iteration 10, loss = 0.01160960\n",
            "Iteration 11, loss = 0.00894672\n",
            "Iteration 12, loss = 0.00694097\n",
            "Iteration 13, loss = 0.00543805\n",
            "Iteration 14, loss = 0.00431372\n",
            "Iteration 15, loss = 0.00347190\n",
            "Iteration 16, loss = 0.00284078\n",
            "Iteration 17, loss = 0.00236664\n",
            "Iteration 18, loss = 0.00200987\n",
            "Iteration 19, loss = 0.00174078\n",
            "Iteration 20, loss = 0.00153740\n",
            "Iteration 21, loss = 0.00138353\n",
            "Iteration 22, loss = 0.00126701\n",
            "Iteration 23, loss = 0.00117867\n",
            "Iteration 24, loss = 0.00111163\n",
            "Iteration 25, loss = 0.00106070\n",
            "Iteration 26, loss = 0.00102199\n",
            "Iteration 27, loss = 0.00099255\n",
            "Iteration 28, loss = 0.00097016\n",
            "Iteration 29, loss = 0.00095311\n",
            "Iteration 30, loss = 0.00094013\n",
            "Iteration 31, loss = 0.00093024\n",
            "Iteration 32, loss = 0.00092270\n",
            "Iteration 33, loss = 0.00091695\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091407\n",
            "Iteration 35, loss = 0.00091325\n",
            "Iteration 36, loss = 0.00091247\n",
            "Iteration 37, loss = 0.00091173\n",
            "Iteration 38, loss = 0.00091102\n",
            "Iteration 39, loss = 0.00091036\n",
            "Iteration 40, loss = 0.00090973\n",
            "Iteration 41, loss = 0.00090913\n",
            "Iteration 42, loss = 0.00090856\n",
            "Iteration 43, loss = 0.00090803\n",
            "Iteration 44, loss = 0.00090752\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090722\n",
            "Iteration 46, loss = 0.00090712\n",
            "Iteration 47, loss = 0.00090703\n",
            "Iteration 48, loss = 0.00090693\n",
            "Iteration 49, loss = 0.00090684\n",
            "Iteration 50, loss = 0.00090675\n",
            "Iteration 51, loss = 0.00090666\n",
            "Iteration 52, loss = 0.00090657\n",
            "Iteration 53, loss = 0.00090648\n",
            "Iteration 54, loss = 0.00090640\n",
            "Iteration 55, loss = 0.00090631\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090626\n",
            "Iteration 57, loss = 0.00090624\n",
            "Iteration 58, loss = 0.00090622\n",
            "Iteration 59, loss = 0.00090620\n",
            "Iteration 60, loss = 0.00090619\n",
            "Iteration 61, loss = 0.00090617\n",
            "Iteration 62, loss = 0.00090615\n",
            "Iteration 63, loss = 0.00090614\n",
            "Iteration 64, loss = 0.00090612\n",
            "Iteration 65, loss = 0.00090610\n",
            "Iteration 66, loss = 0.00090609\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090608\n",
            "Iteration 68, loss = 0.00090607\n",
            "Iteration 69, loss = 0.00090607\n",
            "Iteration 70, loss = 0.00090607\n",
            "Iteration 71, loss = 0.00090606\n",
            "Iteration 72, loss = 0.00090606\n",
            "Iteration 73, loss = 0.00090606\n",
            "Iteration 74, loss = 0.00090605\n",
            "Iteration 75, loss = 0.00090605\n",
            "Iteration 76, loss = 0.00090605\n",
            "Iteration 77, loss = 0.00090604\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090604\n",
            "Iteration 79, loss = 0.00090604\n",
            "Iteration 80, loss = 0.00090604\n",
            "Iteration 81, loss = 0.00090604\n",
            "Iteration 82, loss = 0.00090604\n",
            "Iteration 83, loss = 0.00090604\n",
            "Iteration 84, loss = 0.00090604\n",
            "Iteration 85, loss = 0.00090604\n",
            "Iteration 86, loss = 0.00090604\n",
            "Iteration 87, loss = 0.00090603\n",
            "Iteration 88, loss = 0.00090603\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.21054804\n",
            "Iteration 2, loss = 0.15615324\n",
            "Iteration 3, loss = 0.11885717\n",
            "Iteration 4, loss = 0.09046899\n",
            "Iteration 5, loss = 0.06886113\n",
            "Iteration 6, loss = 0.05241414\n",
            "Iteration 7, loss = 0.03989540\n",
            "Iteration 8, loss = 0.03036668\n",
            "Iteration 9, loss = 0.02311381\n",
            "Iteration 10, loss = 0.01759325\n",
            "Iteration 11, loss = 0.01339123\n",
            "Iteration 12, loss = 0.01019283\n",
            "Iteration 13, loss = 0.00775835\n",
            "Iteration 14, loss = 0.00590532\n",
            "Iteration 15, loss = 0.00449488\n",
            "Iteration 16, loss = 0.00342186\n",
            "Iteration 17, loss = 0.00262047\n",
            "Iteration 18, loss = 0.00210535\n",
            "Iteration 19, loss = 0.00180157\n",
            "Iteration 20, loss = 0.00158232\n",
            "Iteration 21, loss = 0.00141725\n",
            "Iteration 22, loss = 0.00129305\n",
            "Iteration 23, loss = 0.00119945\n",
            "Iteration 24, loss = 0.00112873\n",
            "Iteration 25, loss = 0.00107519\n",
            "Iteration 26, loss = 0.00103463\n",
            "Iteration 27, loss = 0.00100388\n",
            "Iteration 28, loss = 0.00098054\n",
            "Iteration 29, loss = 0.00096281\n",
            "Iteration 30, loss = 0.00094932\n",
            "Iteration 31, loss = 0.00093905\n",
            "Iteration 32, loss = 0.00093123\n",
            "Iteration 33, loss = 0.00092527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00092229\n",
            "Iteration 35, loss = 0.00092143\n",
            "Iteration 36, loss = 0.00092062\n",
            "Iteration 37, loss = 0.00091986\n",
            "Iteration 38, loss = 0.00091913\n",
            "Iteration 39, loss = 0.00091844\n",
            "Iteration 40, loss = 0.00091779\n",
            "Iteration 41, loss = 0.00091717\n",
            "Iteration 42, loss = 0.00091659\n",
            "Iteration 43, loss = 0.00091603\n",
            "Iteration 44, loss = 0.00091551\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00091520\n",
            "Iteration 46, loss = 0.00091510\n",
            "Iteration 47, loss = 0.00091500\n",
            "Iteration 48, loss = 0.00091490\n",
            "Iteration 49, loss = 0.00091481\n",
            "Iteration 50, loss = 0.00091471\n",
            "Iteration 51, loss = 0.00091462\n",
            "Iteration 52, loss = 0.00091453\n",
            "Iteration 53, loss = 0.00091444\n",
            "Iteration 54, loss = 0.00091435\n",
            "Iteration 55, loss = 0.00091426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00091420\n",
            "Iteration 57, loss = 0.00091418\n",
            "Iteration 58, loss = 0.00091417\n",
            "Iteration 59, loss = 0.00091415\n",
            "Iteration 60, loss = 0.00091413\n",
            "Iteration 61, loss = 0.00091411\n",
            "Iteration 62, loss = 0.00091410\n",
            "Iteration 63, loss = 0.00091408\n",
            "Iteration 64, loss = 0.00091406\n",
            "Iteration 65, loss = 0.00091404\n",
            "Iteration 66, loss = 0.00091403\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00091402\n",
            "Iteration 68, loss = 0.00091401\n",
            "Iteration 69, loss = 0.00091401\n",
            "Iteration 70, loss = 0.00091401\n",
            "Iteration 71, loss = 0.00091400\n",
            "Iteration 72, loss = 0.00091400\n",
            "Iteration 73, loss = 0.00091400\n",
            "Iteration 74, loss = 0.00091399\n",
            "Iteration 75, loss = 0.00091399\n",
            "Iteration 76, loss = 0.00091399\n",
            "Iteration 77, loss = 0.00091398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00091398\n",
            "Iteration 79, loss = 0.00091398\n",
            "Iteration 80, loss = 0.00091398\n",
            "Iteration 81, loss = 0.00091398\n",
            "Iteration 82, loss = 0.00091398\n",
            "Iteration 83, loss = 0.00091398\n",
            "Iteration 84, loss = 0.00091398\n",
            "Iteration 85, loss = 0.00091398\n",
            "Iteration 86, loss = 0.00091397\n",
            "Iteration 87, loss = 0.00091397\n",
            "Iteration 88, loss = 0.00091397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13296445\n",
            "Iteration 2, loss = 0.10121778\n",
            "Iteration 3, loss = 0.07705338\n",
            "Iteration 4, loss = 0.05866947\n",
            "Iteration 5, loss = 0.04468287\n",
            "Iteration 6, loss = 0.03404426\n",
            "Iteration 7, loss = 0.02595555\n",
            "Iteration 8, loss = 0.01980966\n",
            "Iteration 9, loss = 0.01514485\n",
            "Iteration 10, loss = 0.01160995\n",
            "Iteration 11, loss = 0.00893785\n",
            "Iteration 12, loss = 0.00692445\n",
            "Iteration 13, loss = 0.00541333\n",
            "Iteration 14, loss = 0.00428350\n",
            "Iteration 15, loss = 0.00344049\n",
            "Iteration 16, loss = 0.00281191\n",
            "Iteration 17, loss = 0.00234242\n",
            "Iteration 18, loss = 0.00199061\n",
            "Iteration 19, loss = 0.00172602\n",
            "Iteration 20, loss = 0.00152642\n",
            "Iteration 21, loss = 0.00137552\n",
            "Iteration 22, loss = 0.00126124\n",
            "Iteration 23, loss = 0.00117459\n",
            "Iteration 24, loss = 0.00110881\n",
            "Iteration 25, loss = 0.00105882\n",
            "Iteration 26, loss = 0.00102077\n",
            "Iteration 27, loss = 0.00099182\n",
            "Iteration 28, loss = 0.00096983\n",
            "Iteration 29, loss = 0.00095310\n",
            "Iteration 30, loss = 0.00094037\n",
            "Iteration 31, loss = 0.00093068\n",
            "Iteration 32, loss = 0.00092330\n",
            "Iteration 33, loss = 0.00091768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091487\n",
            "Iteration 35, loss = 0.00091406\n",
            "Iteration 36, loss = 0.00091330\n",
            "Iteration 37, loss = 0.00091258\n",
            "Iteration 38, loss = 0.00091189\n",
            "Iteration 39, loss = 0.00091124\n",
            "Iteration 40, loss = 0.00091063\n",
            "Iteration 41, loss = 0.00091005\n",
            "Iteration 42, loss = 0.00090949\n",
            "Iteration 43, loss = 0.00090897\n",
            "Iteration 44, loss = 0.00090847\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090818\n",
            "Iteration 46, loss = 0.00090808\n",
            "Iteration 47, loss = 0.00090799\n",
            "Iteration 48, loss = 0.00090790\n",
            "Iteration 49, loss = 0.00090781\n",
            "Iteration 50, loss = 0.00090772\n",
            "Iteration 51, loss = 0.00090763\n",
            "Iteration 52, loss = 0.00090755\n",
            "Iteration 53, loss = 0.00090746\n",
            "Iteration 54, loss = 0.00090737\n",
            "Iteration 55, loss = 0.00090729\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090724\n",
            "Iteration 57, loss = 0.00090722\n",
            "Iteration 58, loss = 0.00090720\n",
            "Iteration 59, loss = 0.00090719\n",
            "Iteration 60, loss = 0.00090717\n",
            "Iteration 61, loss = 0.00090715\n",
            "Iteration 62, loss = 0.00090714\n",
            "Iteration 63, loss = 0.00090712\n",
            "Iteration 64, loss = 0.00090711\n",
            "Iteration 65, loss = 0.00090709\n",
            "Iteration 66, loss = 0.00090707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090706\n",
            "Iteration 68, loss = 0.00090706\n",
            "Iteration 69, loss = 0.00090706\n",
            "Iteration 70, loss = 0.00090705\n",
            "Iteration 71, loss = 0.00090705\n",
            "Iteration 72, loss = 0.00090705\n",
            "Iteration 73, loss = 0.00090704\n",
            "Iteration 74, loss = 0.00090704\n",
            "Iteration 75, loss = 0.00090704\n",
            "Iteration 76, loss = 0.00090703\n",
            "Iteration 77, loss = 0.00090703\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090703\n",
            "Iteration 79, loss = 0.00090703\n",
            "Iteration 80, loss = 0.00090703\n",
            "Iteration 81, loss = 0.00090703\n",
            "Iteration 82, loss = 0.00090703\n",
            "Iteration 83, loss = 0.00090702\n",
            "Iteration 84, loss = 0.00090702\n",
            "Iteration 85, loss = 0.00090702\n",
            "Iteration 86, loss = 0.00090702\n",
            "Iteration 87, loss = 0.00090702\n",
            "Iteration 88, loss = 0.00090702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12317703\n",
            "Iteration 2, loss = 0.09377268\n",
            "Iteration 3, loss = 0.07139582\n",
            "Iteration 4, loss = 0.05437089\n",
            "Iteration 5, loss = 0.04141967\n",
            "Iteration 6, loss = 0.03157083\n",
            "Iteration 7, loss = 0.02408549\n",
            "Iteration 8, loss = 0.01840156\n",
            "Iteration 9, loss = 0.01409146\n",
            "Iteration 10, loss = 0.01082966\n",
            "Iteration 11, loss = 0.00836782\n",
            "Iteration 12, loss = 0.00651546\n",
            "Iteration 13, loss = 0.00512504\n",
            "Iteration 14, loss = 0.00408272\n",
            "Iteration 15, loss = 0.00330137\n",
            "Iteration 16, loss = 0.00271495\n",
            "Iteration 17, loss = 0.00227399\n",
            "Iteration 18, loss = 0.00194173\n",
            "Iteration 19, loss = 0.00169089\n",
            "Iteration 20, loss = 0.00150115\n",
            "Iteration 21, loss = 0.00135735\n",
            "Iteration 22, loss = 0.00124822\n",
            "Iteration 23, loss = 0.00116534\n",
            "Iteration 24, loss = 0.00110236\n",
            "Iteration 25, loss = 0.00105447\n",
            "Iteration 26, loss = 0.00101804\n",
            "Iteration 27, loss = 0.00099032\n",
            "Iteration 28, loss = 0.00096922\n",
            "Iteration 29, loss = 0.00095315\n",
            "Iteration 30, loss = 0.00094091\n",
            "Iteration 31, loss = 0.00093158\n",
            "Iteration 32, loss = 0.00092447\n",
            "Iteration 33, loss = 0.00091905\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091634\n",
            "Iteration 35, loss = 0.00091556\n",
            "Iteration 36, loss = 0.00091482\n",
            "Iteration 37, loss = 0.00091413\n",
            "Iteration 38, loss = 0.00091347\n",
            "Iteration 39, loss = 0.00091284\n",
            "Iteration 40, loss = 0.00091225\n",
            "Iteration 41, loss = 0.00091168\n",
            "Iteration 42, loss = 0.00091115\n",
            "Iteration 43, loss = 0.00091064\n",
            "Iteration 44, loss = 0.00091016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090988\n",
            "Iteration 46, loss = 0.00090979\n",
            "Iteration 47, loss = 0.00090970\n",
            "Iteration 48, loss = 0.00090961\n",
            "Iteration 49, loss = 0.00090953\n",
            "Iteration 50, loss = 0.00090944\n",
            "Iteration 51, loss = 0.00090936\n",
            "Iteration 52, loss = 0.00090927\n",
            "Iteration 53, loss = 0.00090919\n",
            "Iteration 54, loss = 0.00090911\n",
            "Iteration 55, loss = 0.00090902\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090897\n",
            "Iteration 57, loss = 0.00090896\n",
            "Iteration 58, loss = 0.00090894\n",
            "Iteration 59, loss = 0.00090893\n",
            "Iteration 60, loss = 0.00090891\n",
            "Iteration 61, loss = 0.00090889\n",
            "Iteration 62, loss = 0.00090888\n",
            "Iteration 63, loss = 0.00090886\n",
            "Iteration 64, loss = 0.00090885\n",
            "Iteration 65, loss = 0.00090883\n",
            "Iteration 66, loss = 0.00090881\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090880\n",
            "Iteration 68, loss = 0.00090880\n",
            "Iteration 69, loss = 0.00090880\n",
            "Iteration 70, loss = 0.00090879\n",
            "Iteration 71, loss = 0.00090879\n",
            "Iteration 72, loss = 0.00090879\n",
            "Iteration 73, loss = 0.00090878\n",
            "Iteration 74, loss = 0.00090878\n",
            "Iteration 75, loss = 0.00090878\n",
            "Iteration 76, loss = 0.00090878\n",
            "Iteration 77, loss = 0.00090877\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090877\n",
            "Iteration 79, loss = 0.00090877\n",
            "Iteration 80, loss = 0.00090877\n",
            "Iteration 81, loss = 0.00090877\n",
            "Iteration 82, loss = 0.00090877\n",
            "Iteration 83, loss = 0.00090877\n",
            "Iteration 84, loss = 0.00090877\n",
            "Iteration 85, loss = 0.00090877\n",
            "Iteration 86, loss = 0.00090877\n",
            "Iteration 87, loss = 0.00090876\n",
            "Iteration 88, loss = 0.00090876\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 20.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.29456628\n",
            "Iteration 3, loss = 0.27185423\n",
            "Iteration 4, loss = 0.25089335\n",
            "Iteration 5, loss = 0.23154863\n",
            "Iteration 6, loss = 0.21369546\n",
            "Iteration 7, loss = 0.19721882\n",
            "Iteration 8, loss = 0.18201258\n",
            "Iteration 9, loss = 0.16797880\n",
            "Iteration 10, loss = 0.15502707\n",
            "Iteration 11, loss = 0.14307396\n",
            "Iteration 12, loss = 0.13204248\n",
            "Iteration 13, loss = 0.12186156\n",
            "Iteration 14, loss = 0.11246563\n",
            "Iteration 15, loss = 0.10379415\n",
            "Iteration 16, loss = 0.09579127\n",
            "Iteration 17, loss = 0.08840544\n",
            "Iteration 18, loss = 0.08158909\n",
            "Iteration 19, loss = 0.07529829\n",
            "Iteration 20, loss = 0.06949254\n",
            "Iteration 21, loss = 0.06413443\n",
            "Iteration 22, loss = 0.05918945\n",
            "Iteration 23, loss = 0.05462575\n",
            "Iteration 24, loss = 0.05041392\n",
            "Iteration 25, loss = 0.04652684\n",
            "Iteration 26, loss = 0.04293946\n",
            "Iteration 27, loss = 0.03962868\n",
            "Iteration 28, loss = 0.03657318\n",
            "Iteration 29, loss = 0.03375326\n",
            "Iteration 30, loss = 0.03115077\n",
            "Iteration 31, loss = 0.02874894\n",
            "Iteration 32, loss = 0.02653230\n",
            "Iteration 33, loss = 0.02448657\n",
            "Iteration 34, loss = 0.02259858\n",
            "Iteration 35, loss = 0.02085615\n",
            "Iteration 36, loss = 0.01924807\n",
            "Iteration 37, loss = 0.01776398\n",
            "Iteration 38, loss = 0.01639432\n",
            "Iteration 39, loss = 0.01513026\n",
            "Iteration 40, loss = 0.01396367\n",
            "Iteration 41, loss = 0.01288702\n",
            "Iteration 42, loss = 0.01189339\n",
            "Iteration 43, loss = 0.01097637\n",
            "Iteration 44, loss = 0.01013005\n",
            "Iteration 45, loss = 0.00934899\n",
            "Iteration 46, loss = 0.00862815\n",
            "Iteration 47, loss = 0.00796289\n",
            "Iteration 48, loss = 0.00734893\n",
            "Iteration 49, loss = 0.00678230\n",
            "Iteration 50, loss = 0.00625936\n",
            "Iteration 51, loss = 0.00577675\n",
            "Iteration 52, loss = 0.00533134\n",
            "Iteration 53, loss = 0.00492028\n",
            "Iteration 54, loss = 0.00454091\n",
            "Iteration 55, loss = 0.00419082\n",
            "Iteration 56, loss = 0.00386778\n",
            "Iteration 57, loss = 0.00356989\n",
            "Iteration 58, loss = 0.00329564\n",
            "Iteration 59, loss = 0.00304436\n",
            "Iteration 60, loss = 0.00281672\n",
            "Iteration 61, loss = 0.00261515\n",
            "Iteration 62, loss = 0.00244247\n",
            "Iteration 63, loss = 0.00229823\n",
            "Iteration 64, loss = 0.00217674\n",
            "Iteration 65, loss = 0.00207076\n",
            "Iteration 66, loss = 0.00197528\n",
            "Iteration 67, loss = 0.00188789\n",
            "Iteration 68, loss = 0.00180797\n",
            "Iteration 69, loss = 0.00173484\n",
            "Iteration 70, loss = 0.00166790\n",
            "Iteration 71, loss = 0.00160659\n",
            "Iteration 72, loss = 0.00155042\n",
            "Iteration 73, loss = 0.00149894\n",
            "Iteration 74, loss = 0.00145174\n",
            "Iteration 75, loss = 0.00140845\n",
            "Iteration 76, loss = 0.00136872\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00134488\n",
            "Iteration 78, loss = 0.00133766\n",
            "Iteration 79, loss = 0.00133066\n",
            "Iteration 80, loss = 0.00132377\n",
            "Iteration 81, loss = 0.00131700\n",
            "Iteration 82, loss = 0.00131034\n",
            "Iteration 83, loss = 0.00130380\n",
            "Iteration 84, loss = 0.00129737\n",
            "Iteration 85, loss = 0.00129104\n",
            "Iteration 86, loss = 0.00128482\n",
            "Iteration 87, loss = 0.00127870\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00127484\n",
            "Iteration 89, loss = 0.00127362\n",
            "Iteration 90, loss = 0.00127243\n",
            "Iteration 91, loss = 0.00127124\n",
            "Iteration 92, loss = 0.00127006\n",
            "Iteration 93, loss = 0.00126887\n",
            "Iteration 94, loss = 0.00126770\n",
            "Iteration 95, loss = 0.00126652\n",
            "Iteration 96, loss = 0.00126535\n",
            "Iteration 97, loss = 0.00126419\n",
            "Iteration 98, loss = 0.00126303\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00126229\n",
            "Iteration 100, loss = 0.00126205\n",
            "Iteration 101, loss = 0.00126182\n",
            "Iteration 102, loss = 0.00126159\n",
            "Iteration 103, loss = 0.00126136\n",
            "Iteration 104, loss = 0.00126113\n",
            "Iteration 105, loss = 0.00126090\n",
            "Iteration 106, loss = 0.00126067\n",
            "Iteration 107, loss = 0.00126044\n",
            "Iteration 108, loss = 0.00126021\n",
            "Iteration 109, loss = 0.00125998\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00125983\n",
            "Iteration 111, loss = 0.00125978\n",
            "Iteration 112, loss = 0.00125974\n",
            "Iteration 113, loss = 0.00125969\n",
            "Iteration 114, loss = 0.00125965\n",
            "Iteration 115, loss = 0.00125960\n",
            "Iteration 116, loss = 0.00125955\n",
            "Iteration 117, loss = 0.00125951\n",
            "Iteration 118, loss = 0.00125946\n",
            "Iteration 119, loss = 0.00125942\n",
            "Iteration 120, loss = 0.00125937\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00125934\n",
            "Iteration 122, loss = 0.00125933\n",
            "Iteration 123, loss = 0.00125932\n",
            "Iteration 124, loss = 0.00125931\n",
            "Iteration 125, loss = 0.00125930\n",
            "Iteration 126, loss = 0.00125929\n",
            "Iteration 127, loss = 0.00125929\n",
            "Iteration 128, loss = 0.00125928\n",
            "Iteration 129, loss = 0.00125927\n",
            "Iteration 130, loss = 0.00125926\n",
            "Iteration 131, loss = 0.00125925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12844215\n",
            "Iteration 2, loss = 0.11851578\n",
            "Iteration 3, loss = 0.10937789\n",
            "Iteration 4, loss = 0.10094470\n",
            "Iteration 5, loss = 0.09316214\n",
            "Iteration 6, loss = 0.08598052\n",
            "Iteration 7, loss = 0.07935410\n",
            "Iteration 8, loss = 0.07324025\n",
            "Iteration 9, loss = 0.06759907\n",
            "Iteration 10, loss = 0.06239366\n",
            "Iteration 11, loss = 0.05759023\n",
            "Iteration 12, loss = 0.05315775\n",
            "Iteration 13, loss = 0.04906763\n",
            "Iteration 14, loss = 0.04529351\n",
            "Iteration 15, loss = 0.04181106\n",
            "Iteration 16, loss = 0.03859783\n",
            "Iteration 17, loss = 0.03563311\n",
            "Iteration 18, loss = 0.03289778\n",
            "Iteration 19, loss = 0.03037422\n",
            "Iteration 20, loss = 0.02804617\n",
            "Iteration 21, loss = 0.02589860\n",
            "Iteration 22, loss = 0.02391766\n",
            "Iteration 23, loss = 0.02209057\n",
            "Iteration 24, loss = 0.02040550\n",
            "Iteration 25, loss = 0.01885158\n",
            "Iteration 26, loss = 0.01741881\n",
            "Iteration 27, loss = 0.01609793\n",
            "Iteration 28, loss = 0.01488050\n",
            "Iteration 29, loss = 0.01375860\n",
            "Iteration 30, loss = 0.01272486\n",
            "Iteration 31, loss = 0.01177253\n",
            "Iteration 32, loss = 0.01089540\n",
            "Iteration 33, loss = 0.01008771\n",
            "Iteration 34, loss = 0.00934417\n",
            "Iteration 35, loss = 0.00865988\n",
            "Iteration 36, loss = 0.00803031\n",
            "Iteration 37, loss = 0.00745126\n",
            "Iteration 38, loss = 0.00691884\n",
            "Iteration 39, loss = 0.00642944\n",
            "Iteration 40, loss = 0.00597976\n",
            "Iteration 41, loss = 0.00556668\n",
            "Iteration 42, loss = 0.00518736\n",
            "Iteration 43, loss = 0.00483909\n",
            "Iteration 44, loss = 0.00451939\n",
            "Iteration 45, loss = 0.00422598\n",
            "Iteration 46, loss = 0.00395670\n",
            "Iteration 47, loss = 0.00370959\n",
            "Iteration 48, loss = 0.00348282\n",
            "Iteration 49, loss = 0.00327472\n",
            "Iteration 50, loss = 0.00308379\n",
            "Iteration 51, loss = 0.00290854\n",
            "Iteration 52, loss = 0.00274767\n",
            "Iteration 53, loss = 0.00259993\n",
            "Iteration 54, loss = 0.00246419\n",
            "Iteration 55, loss = 0.00233944\n",
            "Iteration 56, loss = 0.00222475\n",
            "Iteration 57, loss = 0.00211929\n",
            "Iteration 58, loss = 0.00202229\n",
            "Iteration 59, loss = 0.00193306\n",
            "Iteration 60, loss = 0.00185095\n",
            "Iteration 61, loss = 0.00177538\n",
            "Iteration 62, loss = 0.00170582\n",
            "Iteration 63, loss = 0.00164177\n",
            "Iteration 64, loss = 0.00158280\n",
            "Iteration 65, loss = 0.00152849\n",
            "Iteration 66, loss = 0.00147847\n",
            "Iteration 67, loss = 0.00143238\n",
            "Iteration 68, loss = 0.00138992\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.00136436\n",
            "Iteration 70, loss = 0.00135661\n",
            "Iteration 71, loss = 0.00134908\n",
            "Iteration 72, loss = 0.00134167\n",
            "Iteration 73, loss = 0.00133438\n",
            "Iteration 74, loss = 0.00132721\n",
            "Iteration 75, loss = 0.00132015\n",
            "Iteration 76, loss = 0.00131321\n",
            "Iteration 77, loss = 0.00130638\n",
            "Iteration 78, loss = 0.00129966\n",
            "Iteration 79, loss = 0.00129305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.00128887\n",
            "Iteration 81, loss = 0.00128756\n",
            "Iteration 82, loss = 0.00128627\n",
            "Iteration 83, loss = 0.00128498\n",
            "Iteration 84, loss = 0.00128370\n",
            "Iteration 85, loss = 0.00128242\n",
            "Iteration 86, loss = 0.00128115\n",
            "Iteration 87, loss = 0.00127988\n",
            "Iteration 88, loss = 0.00127861\n",
            "Iteration 89, loss = 0.00127735\n",
            "Iteration 90, loss = 0.00127609\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.00127529\n",
            "Iteration 92, loss = 0.00127503\n",
            "Iteration 93, loss = 0.00127478\n",
            "Iteration 94, loss = 0.00127453\n",
            "Iteration 95, loss = 0.00127428\n",
            "Iteration 96, loss = 0.00127403\n",
            "Iteration 97, loss = 0.00127378\n",
            "Iteration 98, loss = 0.00127353\n",
            "Iteration 99, loss = 0.00127328\n",
            "Iteration 100, loss = 0.00127303\n",
            "Iteration 101, loss = 0.00127278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.00127263\n",
            "Iteration 103, loss = 0.00127257\n",
            "Iteration 104, loss = 0.00127253\n",
            "Iteration 105, loss = 0.00127248\n",
            "Iteration 106, loss = 0.00127243\n",
            "Iteration 107, loss = 0.00127238\n",
            "Iteration 108, loss = 0.00127233\n",
            "Iteration 109, loss = 0.00127228\n",
            "Iteration 110, loss = 0.00127223\n",
            "Iteration 111, loss = 0.00127218\n",
            "Iteration 112, loss = 0.00127213\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 113, loss = 0.00127210\n",
            "Iteration 114, loss = 0.00127209\n",
            "Iteration 115, loss = 0.00127208\n",
            "Iteration 116, loss = 0.00127207\n",
            "Iteration 117, loss = 0.00127206\n",
            "Iteration 118, loss = 0.00127205\n",
            "Iteration 119, loss = 0.00127204\n",
            "Iteration 120, loss = 0.00127203\n",
            "Iteration 121, loss = 0.00127202\n",
            "Iteration 122, loss = 0.00127201\n",
            "Iteration 123, loss = 0.00127200\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13448559\n",
            "Iteration 2, loss = 0.12414440\n",
            "Iteration 3, loss = 0.11457633\n",
            "Iteration 4, loss = 0.10574650\n",
            "Iteration 5, loss = 0.09759794\n",
            "Iteration 6, loss = 0.09007814\n",
            "Iteration 7, loss = 0.08313862\n",
            "Iteration 8, loss = 0.07673466\n",
            "Iteration 9, loss = 0.07082498\n",
            "Iteration 10, loss = 0.06537149\n",
            "Iteration 11, loss = 0.06033905\n",
            "Iteration 12, loss = 0.05569522\n",
            "Iteration 13, loss = 0.05141006\n",
            "Iteration 14, loss = 0.04745597\n",
            "Iteration 15, loss = 0.04380746\n",
            "Iteration 16, loss = 0.04044100\n",
            "Iteration 17, loss = 0.03733489\n",
            "Iteration 18, loss = 0.03446911\n",
            "Iteration 19, loss = 0.03182518\n",
            "Iteration 20, loss = 0.02938604\n",
            "Iteration 21, loss = 0.02713597\n",
            "Iteration 22, loss = 0.02506044\n",
            "Iteration 23, loss = 0.02314605\n",
            "Iteration 24, loss = 0.02138045\n",
            "Iteration 25, loss = 0.01975222\n",
            "Iteration 26, loss = 0.01825084\n",
            "Iteration 27, loss = 0.01686659\n",
            "Iteration 28, loss = 0.01559051\n",
            "Iteration 29, loss = 0.01441433\n",
            "Iteration 30, loss = 0.01333040\n",
            "Iteration 31, loss = 0.01233167\n",
            "Iteration 32, loss = 0.01141165\n",
            "Iteration 33, loss = 0.01056431\n",
            "Iteration 34, loss = 0.00978408\n",
            "Iteration 35, loss = 0.00906584\n",
            "Iteration 36, loss = 0.00840484\n",
            "Iteration 37, loss = 0.00779668\n",
            "Iteration 38, loss = 0.00723730\n",
            "Iteration 39, loss = 0.00672293\n",
            "Iteration 40, loss = 0.00625009\n",
            "Iteration 41, loss = 0.00581555\n",
            "Iteration 42, loss = 0.00541632\n",
            "Iteration 43, loss = 0.00504962\n",
            "Iteration 44, loss = 0.00471288\n",
            "Iteration 45, loss = 0.00440370\n",
            "Iteration 46, loss = 0.00411987\n",
            "Iteration 47, loss = 0.00385934\n",
            "Iteration 48, loss = 0.00362021\n",
            "Iteration 49, loss = 0.00340074\n",
            "Iteration 50, loss = 0.00319931\n",
            "Iteration 51, loss = 0.00301458\n",
            "Iteration 52, loss = 0.00284521\n",
            "Iteration 53, loss = 0.00268978\n",
            "Iteration 54, loss = 0.00254707\n",
            "Iteration 55, loss = 0.00241597\n",
            "Iteration 56, loss = 0.00229551\n",
            "Iteration 57, loss = 0.00218479\n",
            "Iteration 58, loss = 0.00208300\n",
            "Iteration 59, loss = 0.00198940\n",
            "Iteration 60, loss = 0.00190331\n",
            "Iteration 61, loss = 0.00182411\n",
            "Iteration 62, loss = 0.00175124\n",
            "Iteration 63, loss = 0.00168417\n",
            "Iteration 64, loss = 0.00162243\n",
            "Iteration 65, loss = 0.00156559\n",
            "Iteration 66, loss = 0.00151325\n",
            "Iteration 67, loss = 0.00146505\n",
            "Iteration 68, loss = 0.00142065\n",
            "Iteration 69, loss = 0.00137974\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00135512\n",
            "Iteration 71, loss = 0.00134765\n",
            "Iteration 72, loss = 0.00134040\n",
            "Iteration 73, loss = 0.00133326\n",
            "Iteration 74, loss = 0.00132624\n",
            "Iteration 75, loss = 0.00131933\n",
            "Iteration 76, loss = 0.00131254\n",
            "Iteration 77, loss = 0.00130585\n",
            "Iteration 78, loss = 0.00129928\n",
            "Iteration 79, loss = 0.00129281\n",
            "Iteration 80, loss = 0.00128644\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00128241\n",
            "Iteration 82, loss = 0.00128115\n",
            "Iteration 83, loss = 0.00127990\n",
            "Iteration 84, loss = 0.00127866\n",
            "Iteration 85, loss = 0.00127743\n",
            "Iteration 86, loss = 0.00127620\n",
            "Iteration 87, loss = 0.00127497\n",
            "Iteration 88, loss = 0.00127375\n",
            "Iteration 89, loss = 0.00127253\n",
            "Iteration 90, loss = 0.00127131\n",
            "Iteration 91, loss = 0.00127010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00126933\n",
            "Iteration 93, loss = 0.00126908\n",
            "Iteration 94, loss = 0.00126884\n",
            "Iteration 95, loss = 0.00126860\n",
            "Iteration 96, loss = 0.00126836\n",
            "Iteration 97, loss = 0.00126812\n",
            "Iteration 98, loss = 0.00126788\n",
            "Iteration 99, loss = 0.00126764\n",
            "Iteration 100, loss = 0.00126740\n",
            "Iteration 101, loss = 0.00126716\n",
            "Iteration 102, loss = 0.00126692\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00126677\n",
            "Iteration 104, loss = 0.00126672\n",
            "Iteration 105, loss = 0.00126667\n",
            "Iteration 106, loss = 0.00126662\n",
            "Iteration 107, loss = 0.00126657\n",
            "Iteration 108, loss = 0.00126653\n",
            "Iteration 109, loss = 0.00126648\n",
            "Iteration 110, loss = 0.00126643\n",
            "Iteration 111, loss = 0.00126638\n",
            "Iteration 112, loss = 0.00126633\n",
            "Iteration 113, loss = 0.00126629\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00126626\n",
            "Iteration 115, loss = 0.00126625\n",
            "Iteration 116, loss = 0.00126624\n",
            "Iteration 117, loss = 0.00126623\n",
            "Iteration 118, loss = 0.00126622\n",
            "Iteration 119, loss = 0.00126621\n",
            "Iteration 120, loss = 0.00126620\n",
            "Iteration 121, loss = 0.00126619\n",
            "Iteration 122, loss = 0.00126618\n",
            "Iteration 123, loss = 0.00126617\n",
            "Iteration 124, loss = 0.00126616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.27479368\n",
            "Iteration 3, loss = 0.25360617\n",
            "Iteration 4, loss = 0.23405228\n",
            "Iteration 5, loss = 0.21600606\n",
            "Iteration 6, loss = 0.19935127\n",
            "Iteration 7, loss = 0.18398062\n",
            "Iteration 8, loss = 0.16979509\n",
            "Iteration 9, loss = 0.15670332\n",
            "Iteration 10, loss = 0.14462097\n",
            "Iteration 11, loss = 0.13347021\n",
            "Iteration 12, loss = 0.12317920\n",
            "Iteration 13, loss = 0.11368167\n",
            "Iteration 14, loss = 0.10491643\n",
            "Iteration 15, loss = 0.09682703\n",
            "Iteration 16, loss = 0.08936134\n",
            "Iteration 17, loss = 0.08247128\n",
            "Iteration 18, loss = 0.07611246\n",
            "Iteration 19, loss = 0.07024394\n",
            "Iteration 20, loss = 0.06482789\n",
            "Iteration 21, loss = 0.05982945\n",
            "Iteration 22, loss = 0.05521639\n",
            "Iteration 23, loss = 0.05095902\n",
            "Iteration 24, loss = 0.04702991\n",
            "Iteration 25, loss = 0.04340375\n",
            "Iteration 26, loss = 0.04005717\n",
            "Iteration 27, loss = 0.03696863\n",
            "Iteration 28, loss = 0.03411823\n",
            "Iteration 29, loss = 0.03148760\n",
            "Iteration 30, loss = 0.02905980\n",
            "Iteration 31, loss = 0.02681919\n",
            "Iteration 32, loss = 0.02475134\n",
            "Iteration 33, loss = 0.02284293\n",
            "Iteration 34, loss = 0.02108166\n",
            "Iteration 35, loss = 0.01945619\n",
            "Iteration 36, loss = 0.01795606\n",
            "Iteration 37, loss = 0.01657158\n",
            "Iteration 38, loss = 0.01529386\n",
            "Iteration 39, loss = 0.01411465\n",
            "Iteration 40, loss = 0.01302636\n",
            "Iteration 41, loss = 0.01202199\n",
            "Iteration 42, loss = 0.01109505\n",
            "Iteration 43, loss = 0.01023959\n",
            "Iteration 44, loss = 0.00945008\n",
            "Iteration 45, loss = 0.00872145\n",
            "Iteration 46, loss = 0.00804899\n",
            "Iteration 47, loss = 0.00742839\n",
            "Iteration 48, loss = 0.00685564\n",
            "Iteration 49, loss = 0.00632704\n",
            "Iteration 50, loss = 0.00583921\n",
            "Iteration 51, loss = 0.00538898\n",
            "Iteration 52, loss = 0.00497348\n",
            "Iteration 53, loss = 0.00459001\n",
            "Iteration 54, loss = 0.00423612\n",
            "Iteration 55, loss = 0.00390957\n",
            "Iteration 56, loss = 0.00360837\n",
            "Iteration 57, loss = 0.00333091\n",
            "Iteration 58, loss = 0.00307626\n",
            "Iteration 59, loss = 0.00284464\n",
            "Iteration 60, loss = 0.00263790\n",
            "Iteration 61, loss = 0.00245879\n",
            "Iteration 62, loss = 0.00230787\n",
            "Iteration 63, loss = 0.00218076\n",
            "Iteration 64, loss = 0.00207059\n",
            "Iteration 65, loss = 0.00197220\n",
            "Iteration 66, loss = 0.00188297\n",
            "Iteration 67, loss = 0.00180157\n",
            "Iteration 68, loss = 0.00172718\n",
            "Iteration 69, loss = 0.00165914\n",
            "Iteration 70, loss = 0.00159689\n",
            "Iteration 71, loss = 0.00153990\n",
            "Iteration 72, loss = 0.00148771\n",
            "Iteration 73, loss = 0.00143991\n",
            "Iteration 74, loss = 0.00139609\n",
            "Iteration 75, loss = 0.00135592\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.00133183\n",
            "Iteration 77, loss = 0.00132453\n",
            "Iteration 78, loss = 0.00131746\n",
            "Iteration 79, loss = 0.00131050\n",
            "Iteration 80, loss = 0.00130366\n",
            "Iteration 81, loss = 0.00129694\n",
            "Iteration 82, loss = 0.00129034\n",
            "Iteration 83, loss = 0.00128384\n",
            "Iteration 84, loss = 0.00127746\n",
            "Iteration 85, loss = 0.00127118\n",
            "Iteration 86, loss = 0.00126501\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.00126111\n",
            "Iteration 88, loss = 0.00125988\n",
            "Iteration 89, loss = 0.00125868\n",
            "Iteration 90, loss = 0.00125748\n",
            "Iteration 91, loss = 0.00125628\n",
            "Iteration 92, loss = 0.00125509\n",
            "Iteration 93, loss = 0.00125391\n",
            "Iteration 94, loss = 0.00125272\n",
            "Iteration 95, loss = 0.00125154\n",
            "Iteration 96, loss = 0.00125037\n",
            "Iteration 97, loss = 0.00124920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.00124845\n",
            "Iteration 99, loss = 0.00124821\n",
            "Iteration 100, loss = 0.00124798\n",
            "Iteration 101, loss = 0.00124774\n",
            "Iteration 102, loss = 0.00124751\n",
            "Iteration 103, loss = 0.00124728\n",
            "Iteration 104, loss = 0.00124705\n",
            "Iteration 105, loss = 0.00124681\n",
            "Iteration 106, loss = 0.00124658\n",
            "Iteration 107, loss = 0.00124635\n",
            "Iteration 108, loss = 0.00124612\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.00124597\n",
            "Iteration 110, loss = 0.00124592\n",
            "Iteration 111, loss = 0.00124588\n",
            "Iteration 112, loss = 0.00124583\n",
            "Iteration 113, loss = 0.00124578\n",
            "Iteration 114, loss = 0.00124574\n",
            "Iteration 115, loss = 0.00124569\n",
            "Iteration 116, loss = 0.00124565\n",
            "Iteration 117, loss = 0.00124560\n",
            "Iteration 118, loss = 0.00124555\n",
            "Iteration 119, loss = 0.00124551\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.00124548\n",
            "Iteration 121, loss = 0.00124547\n",
            "Iteration 122, loss = 0.00124546\n",
            "Iteration 123, loss = 0.00124545\n",
            "Iteration 124, loss = 0.00124544\n",
            "Iteration 125, loss = 0.00124543\n",
            "Iteration 126, loss = 0.00124542\n",
            "Iteration 127, loss = 0.00124541\n",
            "Iteration 128, loss = 0.00124540\n",
            "Iteration 129, loss = 0.00124539\n",
            "Iteration 130, loss = 0.00124538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32013589\n",
            "Iteration 3, loss = 0.29545234\n",
            "Iteration 4, loss = 0.27267197\n",
            "Iteration 5, loss = 0.25164805\n",
            "Iteration 6, loss = 0.23224514\n",
            "Iteration 7, loss = 0.21433826\n",
            "Iteration 8, loss = 0.19781206\n",
            "Iteration 9, loss = 0.18256008\n",
            "Iteration 10, loss = 0.16848409\n",
            "Iteration 11, loss = 0.15549340\n",
            "Iteration 12, loss = 0.14350433\n",
            "Iteration 13, loss = 0.13243967\n",
            "Iteration 14, loss = 0.12222812\n",
            "Iteration 15, loss = 0.11280392\n",
            "Iteration 16, loss = 0.10410636\n",
            "Iteration 17, loss = 0.09607941\n",
            "Iteration 18, loss = 0.08867137\n",
            "Iteration 19, loss = 0.08183451\n",
            "Iteration 20, loss = 0.07552479\n",
            "Iteration 21, loss = 0.06970158\n",
            "Iteration 22, loss = 0.06432735\n",
            "Iteration 23, loss = 0.05936750\n",
            "Iteration 24, loss = 0.05479006\n",
            "Iteration 25, loss = 0.05056556\n",
            "Iteration 26, loss = 0.04666679\n",
            "Iteration 27, loss = 0.04306862\n",
            "Iteration 28, loss = 0.03974789\n",
            "Iteration 29, loss = 0.03668319\n",
            "Iteration 30, loss = 0.03385479\n",
            "Iteration 31, loss = 0.03124448\n",
            "Iteration 32, loss = 0.02883542\n",
            "Iteration 33, loss = 0.02661211\n",
            "Iteration 34, loss = 0.02456023\n",
            "Iteration 35, loss = 0.02266655\n",
            "Iteration 36, loss = 0.02091889\n",
            "Iteration 37, loss = 0.01930597\n",
            "Iteration 38, loss = 0.01781742\n",
            "Iteration 39, loss = 0.01644363\n",
            "Iteration 40, loss = 0.01517577\n",
            "Iteration 41, loss = 0.01400567\n",
            "Iteration 42, loss = 0.01292579\n",
            "Iteration 43, loss = 0.01192916\n",
            "Iteration 44, loss = 0.01100939\n",
            "Iteration 45, loss = 0.01016053\n",
            "Iteration 46, loss = 0.00937711\n",
            "Iteration 47, loss = 0.00865411\n",
            "Iteration 48, loss = 0.00798685\n",
            "Iteration 49, loss = 0.00737103\n",
            "Iteration 50, loss = 0.00680270\n",
            "Iteration 51, loss = 0.00627819\n",
            "Iteration 52, loss = 0.00579412\n",
            "Iteration 53, loss = 0.00534738\n",
            "Iteration 54, loss = 0.00493507\n",
            "Iteration 55, loss = 0.00455456\n",
            "Iteration 56, loss = 0.00420339\n",
            "Iteration 57, loss = 0.00387930\n",
            "Iteration 58, loss = 0.00358020\n",
            "Iteration 59, loss = 0.00330420\n",
            "Iteration 60, loss = 0.00304961\n",
            "Iteration 61, loss = 0.00281500\n",
            "Iteration 62, loss = 0.00259951\n",
            "Iteration 63, loss = 0.00240318\n",
            "Iteration 64, loss = 0.00222751\n",
            "Iteration 65, loss = 0.00207533\n",
            "Iteration 66, loss = 0.00194869\n",
            "Iteration 67, loss = 0.00184548\n",
            "Iteration 68, loss = 0.00175955\n",
            "Iteration 69, loss = 0.00168485\n",
            "Iteration 70, loss = 0.00161789\n",
            "Iteration 71, loss = 0.00155704\n",
            "Iteration 72, loss = 0.00150146\n",
            "Iteration 73, loss = 0.00145062\n",
            "Iteration 74, loss = 0.00140411\n",
            "Iteration 75, loss = 0.00136156\n",
            "Iteration 76, loss = 0.00132263\n",
            "Iteration 77, loss = 0.00128698\n",
            "Iteration 78, loss = 0.00125434\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 79, loss = 0.00123478\n",
            "Iteration 80, loss = 0.00122886\n",
            "Iteration 81, loss = 0.00122312\n",
            "Iteration 82, loss = 0.00121748\n",
            "Iteration 83, loss = 0.00121193\n",
            "Iteration 84, loss = 0.00120648\n",
            "Iteration 85, loss = 0.00120112\n",
            "Iteration 86, loss = 0.00119586\n",
            "Iteration 87, loss = 0.00119068\n",
            "Iteration 88, loss = 0.00118560\n",
            "Iteration 89, loss = 0.00118060\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.00117744\n",
            "Iteration 91, loss = 0.00117645\n",
            "Iteration 92, loss = 0.00117547\n",
            "Iteration 93, loss = 0.00117450\n",
            "Iteration 94, loss = 0.00117353\n",
            "Iteration 95, loss = 0.00117257\n",
            "Iteration 96, loss = 0.00117161\n",
            "Iteration 97, loss = 0.00117065\n",
            "Iteration 98, loss = 0.00116969\n",
            "Iteration 99, loss = 0.00116874\n",
            "Iteration 100, loss = 0.00116779\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.00116719\n",
            "Iteration 102, loss = 0.00116699\n",
            "Iteration 103, loss = 0.00116681\n",
            "Iteration 104, loss = 0.00116662\n",
            "Iteration 105, loss = 0.00116643\n",
            "Iteration 106, loss = 0.00116624\n",
            "Iteration 107, loss = 0.00116605\n",
            "Iteration 108, loss = 0.00116586\n",
            "Iteration 109, loss = 0.00116568\n",
            "Iteration 110, loss = 0.00116549\n",
            "Iteration 111, loss = 0.00116530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 112, loss = 0.00116518\n",
            "Iteration 113, loss = 0.00116514\n",
            "Iteration 114, loss = 0.00116510\n",
            "Iteration 115, loss = 0.00116507\n",
            "Iteration 116, loss = 0.00116503\n",
            "Iteration 117, loss = 0.00116499\n",
            "Iteration 118, loss = 0.00116495\n",
            "Iteration 119, loss = 0.00116492\n",
            "Iteration 120, loss = 0.00116488\n",
            "Iteration 121, loss = 0.00116484\n",
            "Iteration 122, loss = 0.00116480\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 123, loss = 0.00116478\n",
            "Iteration 124, loss = 0.00116477\n",
            "Iteration 125, loss = 0.00116477\n",
            "Iteration 126, loss = 0.00116476\n",
            "Iteration 127, loss = 0.00116475\n",
            "Iteration 128, loss = 0.00116474\n",
            "Iteration 129, loss = 0.00116474\n",
            "Iteration 130, loss = 0.00116473\n",
            "Iteration 131, loss = 0.00116472\n",
            "Iteration 132, loss = 0.00116471\n",
            "Iteration 133, loss = 0.00116471\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13945940\n",
            "Iteration 2, loss = 0.12874110\n",
            "Iteration 3, loss = 0.11881473\n",
            "Iteration 4, loss = 0.10965371\n",
            "Iteration 5, loss = 0.10119904\n",
            "Iteration 6, loss = 0.09339626\n",
            "Iteration 7, loss = 0.08619509\n",
            "Iteration 8, loss = 0.07954916\n",
            "Iteration 9, loss = 0.07341565\n",
            "Iteration 10, loss = 0.06775506\n",
            "Iteration 11, loss = 0.06253092\n",
            "Iteration 12, loss = 0.05770957\n",
            "Iteration 13, loss = 0.05325997\n",
            "Iteration 14, loss = 0.04915345\n",
            "Iteration 15, loss = 0.04536355\n",
            "Iteration 16, loss = 0.04186587\n",
            "Iteration 17, loss = 0.03863787\n",
            "Iteration 18, loss = 0.03565877\n",
            "Iteration 19, loss = 0.03290938\n",
            "Iteration 20, loss = 0.03037203\n",
            "Iteration 21, loss = 0.02803051\n",
            "Iteration 22, loss = 0.02587007\n",
            "Iteration 23, loss = 0.02387756\n",
            "Iteration 24, loss = 0.02204160\n",
            "Iteration 25, loss = 0.02035209\n",
            "Iteration 26, loss = 0.01879867\n",
            "Iteration 27, loss = 0.01736984\n",
            "Iteration 28, loss = 0.01605428\n",
            "Iteration 29, loss = 0.01484224\n",
            "Iteration 30, loss = 0.01372543\n",
            "Iteration 31, loss = 0.01269643\n",
            "Iteration 32, loss = 0.01174850\n",
            "Iteration 33, loss = 0.01087542\n",
            "Iteration 34, loss = 0.01007151\n",
            "Iteration 35, loss = 0.00933158\n",
            "Iteration 36, loss = 0.00865082\n",
            "Iteration 37, loss = 0.00802473\n",
            "Iteration 38, loss = 0.00744905\n",
            "Iteration 39, loss = 0.00692004\n",
            "Iteration 40, loss = 0.00643415\n",
            "Iteration 41, loss = 0.00598773\n",
            "Iteration 42, loss = 0.00557751\n",
            "Iteration 43, loss = 0.00520057\n",
            "Iteration 44, loss = 0.00485422\n",
            "Iteration 45, loss = 0.00453600\n",
            "Iteration 46, loss = 0.00424363\n",
            "Iteration 47, loss = 0.00397501\n",
            "Iteration 48, loss = 0.00372821\n",
            "Iteration 49, loss = 0.00350141\n",
            "Iteration 50, loss = 0.00329295\n",
            "Iteration 51, loss = 0.00310132\n",
            "Iteration 52, loss = 0.00292516\n",
            "Iteration 53, loss = 0.00276318\n",
            "Iteration 54, loss = 0.00261418\n",
            "Iteration 55, loss = 0.00247704\n",
            "Iteration 56, loss = 0.00235088\n",
            "Iteration 57, loss = 0.00223486\n",
            "Iteration 58, loss = 0.00212815\n",
            "Iteration 59, loss = 0.00202999\n",
            "Iteration 60, loss = 0.00193966\n",
            "Iteration 61, loss = 0.00185654\n",
            "Iteration 62, loss = 0.00178004\n",
            "Iteration 63, loss = 0.00170961\n",
            "Iteration 64, loss = 0.00164477\n",
            "Iteration 65, loss = 0.00158505\n",
            "Iteration 66, loss = 0.00153005\n",
            "Iteration 67, loss = 0.00147939\n",
            "Iteration 68, loss = 0.00143272\n",
            "Iteration 69, loss = 0.00138971\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00136382\n",
            "Iteration 71, loss = 0.00135597\n",
            "Iteration 72, loss = 0.00134834\n",
            "Iteration 73, loss = 0.00134083\n",
            "Iteration 74, loss = 0.00133345\n",
            "Iteration 75, loss = 0.00132619\n",
            "Iteration 76, loss = 0.00131904\n",
            "Iteration 77, loss = 0.00131201\n",
            "Iteration 78, loss = 0.00130509\n",
            "Iteration 79, loss = 0.00129828\n",
            "Iteration 80, loss = 0.00129159\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00128735\n",
            "Iteration 82, loss = 0.00128602\n",
            "Iteration 83, loss = 0.00128471\n",
            "Iteration 84, loss = 0.00128341\n",
            "Iteration 85, loss = 0.00128211\n",
            "Iteration 86, loss = 0.00128081\n",
            "Iteration 87, loss = 0.00127952\n",
            "Iteration 88, loss = 0.00127823\n",
            "Iteration 89, loss = 0.00127695\n",
            "Iteration 90, loss = 0.00127567\n",
            "Iteration 91, loss = 0.00127440\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00127358\n",
            "Iteration 93, loss = 0.00127333\n",
            "Iteration 94, loss = 0.00127307\n",
            "Iteration 95, loss = 0.00127282\n",
            "Iteration 96, loss = 0.00127257\n",
            "Iteration 97, loss = 0.00127231\n",
            "Iteration 98, loss = 0.00127206\n",
            "Iteration 99, loss = 0.00127181\n",
            "Iteration 100, loss = 0.00127156\n",
            "Iteration 101, loss = 0.00127130\n",
            "Iteration 102, loss = 0.00127105\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00127089\n",
            "Iteration 104, loss = 0.00127084\n",
            "Iteration 105, loss = 0.00127079\n",
            "Iteration 106, loss = 0.00127074\n",
            "Iteration 107, loss = 0.00127069\n",
            "Iteration 108, loss = 0.00127064\n",
            "Iteration 109, loss = 0.00127059\n",
            "Iteration 110, loss = 0.00127054\n",
            "Iteration 111, loss = 0.00127048\n",
            "Iteration 112, loss = 0.00127043\n",
            "Iteration 113, loss = 0.00127038\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00127035\n",
            "Iteration 115, loss = 0.00127034\n",
            "Iteration 116, loss = 0.00127033\n",
            "Iteration 117, loss = 0.00127032\n",
            "Iteration 118, loss = 0.00127031\n",
            "Iteration 119, loss = 0.00127030\n",
            "Iteration 120, loss = 0.00127029\n",
            "Iteration 121, loss = 0.00127028\n",
            "Iteration 122, loss = 0.00127027\n",
            "Iteration 123, loss = 0.00127026\n",
            "Iteration 124, loss = 0.00127025\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.35296238\n",
            "Iteration 2, loss = 0.26560955\n",
            "Iteration 3, loss = 0.24513016\n",
            "Iteration 4, loss = 0.22622980\n",
            "Iteration 5, loss = 0.20878672\n",
            "Iteration 6, loss = 0.19268856\n",
            "Iteration 7, loss = 0.17783163\n",
            "Iteration 8, loss = 0.16412021\n",
            "Iteration 9, loss = 0.15146599\n",
            "Iteration 10, loss = 0.13978746\n",
            "Iteration 11, loss = 0.12900937\n",
            "Iteration 12, loss = 0.11906232\n",
            "Iteration 13, loss = 0.10988221\n",
            "Iteration 14, loss = 0.10140992\n",
            "Iteration 15, loss = 0.09359088\n",
            "Iteration 16, loss = 0.08637471\n",
            "Iteration 17, loss = 0.07971493\n",
            "Iteration 18, loss = 0.07356864\n",
            "Iteration 19, loss = 0.06789625\n",
            "Iteration 20, loss = 0.06266122\n",
            "Iteration 21, loss = 0.05782983\n",
            "Iteration 22, loss = 0.05337096\n",
            "Iteration 23, loss = 0.04925588\n",
            "Iteration 24, loss = 0.04545808\n",
            "Iteration 25, loss = 0.04195311\n",
            "Iteration 26, loss = 0.03871839\n",
            "Iteration 27, loss = 0.03573307\n",
            "Iteration 28, loss = 0.03297793\n",
            "Iteration 29, loss = 0.03043522\n",
            "Iteration 30, loss = 0.02808856\n",
            "Iteration 31, loss = 0.02592284\n",
            "Iteration 32, loss = 0.02392410\n",
            "Iteration 33, loss = 0.02207947\n",
            "Iteration 34, loss = 0.02037707\n",
            "Iteration 35, loss = 0.01880593\n",
            "Iteration 36, loss = 0.01735593\n",
            "Iteration 37, loss = 0.01601773\n",
            "Iteration 38, loss = 0.01478271\n",
            "Iteration 39, loss = 0.01364291\n",
            "Iteration 40, loss = 0.01259100\n",
            "Iteration 41, loss = 0.01162019\n",
            "Iteration 42, loss = 0.01072423\n",
            "Iteration 43, loss = 0.00989736\n",
            "Iteration 44, loss = 0.00913424\n",
            "Iteration 45, loss = 0.00842996\n",
            "Iteration 46, loss = 0.00777998\n",
            "Iteration 47, loss = 0.00718012\n",
            "Iteration 48, loss = 0.00662651\n",
            "Iteration 49, loss = 0.00611558\n",
            "Iteration 50, loss = 0.00564405\n",
            "Iteration 51, loss = 0.00520887\n",
            "Iteration 52, loss = 0.00480725\n",
            "Iteration 53, loss = 0.00443660\n",
            "Iteration 54, loss = 0.00409453\n",
            "Iteration 55, loss = 0.00377885\n",
            "Iteration 56, loss = 0.00348758\n",
            "Iteration 57, loss = 0.00321899\n",
            "Iteration 58, loss = 0.00297174\n",
            "Iteration 59, loss = 0.00274525\n",
            "Iteration 60, loss = 0.00254017\n",
            "Iteration 61, loss = 0.00235875\n",
            "Iteration 62, loss = 0.00220375\n",
            "Iteration 63, loss = 0.00207515\n",
            "Iteration 64, loss = 0.00196807\n",
            "Iteration 65, loss = 0.00187578\n",
            "Iteration 66, loss = 0.00179352\n",
            "Iteration 67, loss = 0.00171891\n",
            "Iteration 68, loss = 0.00165082\n",
            "Iteration 69, loss = 0.00158857\n",
            "Iteration 70, loss = 0.00153163\n",
            "Iteration 71, loss = 0.00147952\n",
            "Iteration 72, loss = 0.00143180\n",
            "Iteration 73, loss = 0.00138810\n",
            "Iteration 74, loss = 0.00134804\n",
            "Iteration 75, loss = 0.00131132\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.00128930\n",
            "Iteration 77, loss = 0.00128264\n",
            "Iteration 78, loss = 0.00127617\n",
            "Iteration 79, loss = 0.00126981\n",
            "Iteration 80, loss = 0.00126356\n",
            "Iteration 81, loss = 0.00125742\n",
            "Iteration 82, loss = 0.00125138\n",
            "Iteration 83, loss = 0.00124545\n",
            "Iteration 84, loss = 0.00123961\n",
            "Iteration 85, loss = 0.00123388\n",
            "Iteration 86, loss = 0.00122824\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.00122467\n",
            "Iteration 88, loss = 0.00122355\n",
            "Iteration 89, loss = 0.00122245\n",
            "Iteration 90, loss = 0.00122136\n",
            "Iteration 91, loss = 0.00122026\n",
            "Iteration 92, loss = 0.00121918\n",
            "Iteration 93, loss = 0.00121809\n",
            "Iteration 94, loss = 0.00121701\n",
            "Iteration 95, loss = 0.00121593\n",
            "Iteration 96, loss = 0.00121486\n",
            "Iteration 97, loss = 0.00121379\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.00121310\n",
            "Iteration 99, loss = 0.00121289\n",
            "Iteration 100, loss = 0.00121267\n",
            "Iteration 101, loss = 0.00121246\n",
            "Iteration 102, loss = 0.00121225\n",
            "Iteration 103, loss = 0.00121204\n",
            "Iteration 104, loss = 0.00121182\n",
            "Iteration 105, loss = 0.00121161\n",
            "Iteration 106, loss = 0.00121140\n",
            "Iteration 107, loss = 0.00121119\n",
            "Iteration 108, loss = 0.00121098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.00121084\n",
            "Iteration 110, loss = 0.00121080\n",
            "Iteration 111, loss = 0.00121075\n",
            "Iteration 112, loss = 0.00121071\n",
            "Iteration 113, loss = 0.00121067\n",
            "Iteration 114, loss = 0.00121063\n",
            "Iteration 115, loss = 0.00121059\n",
            "Iteration 116, loss = 0.00121054\n",
            "Iteration 117, loss = 0.00121050\n",
            "Iteration 118, loss = 0.00121046\n",
            "Iteration 119, loss = 0.00121042\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.00121039\n",
            "Iteration 121, loss = 0.00121038\n",
            "Iteration 122, loss = 0.00121037\n",
            "Iteration 123, loss = 0.00121036\n",
            "Iteration 124, loss = 0.00121036\n",
            "Iteration 125, loss = 0.00121035\n",
            "Iteration 126, loss = 0.00121034\n",
            "Iteration 127, loss = 0.00121033\n",
            "Iteration 128, loss = 0.00121032\n",
            "Iteration 129, loss = 0.00121031\n",
            "Iteration 130, loss = 0.00121030\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.27585830\n",
            "Iteration 3, loss = 0.25458870\n",
            "Iteration 4, loss = 0.23495906\n",
            "Iteration 5, loss = 0.21684292\n",
            "Iteration 6, loss = 0.20012361\n",
            "Iteration 7, loss = 0.18469340\n",
            "Iteration 8, loss = 0.17045292\n",
            "Iteration 9, loss = 0.15731043\n",
            "Iteration 10, loss = 0.14518127\n",
            "Iteration 11, loss = 0.13398730\n",
            "Iteration 12, loss = 0.12365643\n",
            "Iteration 13, loss = 0.11412210\n",
            "Iteration 14, loss = 0.10532291\n",
            "Iteration 15, loss = 0.09720216\n",
            "Iteration 16, loss = 0.08970754\n",
            "Iteration 17, loss = 0.08279079\n",
            "Iteration 18, loss = 0.07640734\n",
            "Iteration 19, loss = 0.07051608\n",
            "Iteration 20, loss = 0.06507905\n",
            "Iteration 21, loss = 0.06006124\n",
            "Iteration 22, loss = 0.05543032\n",
            "Iteration 23, loss = 0.05115645\n",
            "Iteration 24, loss = 0.04721212\n",
            "Iteration 25, loss = 0.04357191\n",
            "Iteration 26, loss = 0.04021236\n",
            "Iteration 27, loss = 0.03711186\n",
            "Iteration 28, loss = 0.03425041\n",
            "Iteration 29, loss = 0.03160959\n",
            "Iteration 30, loss = 0.02917238\n",
            "Iteration 31, loss = 0.02692309\n",
            "Iteration 32, loss = 0.02484723\n",
            "Iteration 33, loss = 0.02293143\n",
            "Iteration 34, loss = 0.02116334\n",
            "Iteration 35, loss = 0.01953157\n",
            "Iteration 36, loss = 0.01802562\n",
            "Iteration 37, loss = 0.01663579\n",
            "Iteration 38, loss = 0.01535311\n",
            "Iteration 39, loss = 0.01416933\n",
            "Iteration 40, loss = 0.01307683\n",
            "Iteration 41, loss = 0.01206856\n",
            "Iteration 42, loss = 0.01113804\n",
            "Iteration 43, loss = 0.01027926\n",
            "Iteration 44, loss = 0.00948669\n",
            "Iteration 45, loss = 0.00875524\n",
            "Iteration 46, loss = 0.00808018\n",
            "Iteration 47, loss = 0.00745717\n",
            "Iteration 48, loss = 0.00688220\n",
            "Iteration 49, loss = 0.00635156\n",
            "Iteration 50, loss = 0.00586183\n",
            "Iteration 51, loss = 0.00540986\n",
            "Iteration 52, loss = 0.00499274\n",
            "Iteration 53, loss = 0.00460779\n",
            "Iteration 54, loss = 0.00425252\n",
            "Iteration 55, loss = 0.00392465\n",
            "Iteration 56, loss = 0.00362212\n",
            "Iteration 57, loss = 0.00334309\n",
            "Iteration 58, loss = 0.00308610\n",
            "Iteration 59, loss = 0.00285037\n",
            "Iteration 60, loss = 0.00263629\n",
            "Iteration 61, loss = 0.00244594\n",
            "Iteration 62, loss = 0.00228237\n",
            "Iteration 63, loss = 0.00214651\n",
            "Iteration 64, loss = 0.00203415\n",
            "Iteration 65, loss = 0.00193808\n",
            "Iteration 66, loss = 0.00185274\n",
            "Iteration 67, loss = 0.00177531\n",
            "Iteration 68, loss = 0.00170448\n",
            "Iteration 69, loss = 0.00163948\n",
            "Iteration 70, loss = 0.00157967\n",
            "Iteration 71, loss = 0.00152480\n",
            "Iteration 72, loss = 0.00147451\n",
            "Iteration 73, loss = 0.00142842\n",
            "Iteration 74, loss = 0.00138615\n",
            "Iteration 75, loss = 0.00134738\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.00132412\n",
            "Iteration 77, loss = 0.00131708\n",
            "Iteration 78, loss = 0.00131025\n",
            "Iteration 79, loss = 0.00130353\n",
            "Iteration 80, loss = 0.00129693\n",
            "Iteration 81, loss = 0.00129043\n",
            "Iteration 82, loss = 0.00128405\n",
            "Iteration 83, loss = 0.00127778\n",
            "Iteration 84, loss = 0.00127161\n",
            "Iteration 85, loss = 0.00126555\n",
            "Iteration 86, loss = 0.00125958\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.00125581\n",
            "Iteration 88, loss = 0.00125463\n",
            "Iteration 89, loss = 0.00125347\n",
            "Iteration 90, loss = 0.00125231\n",
            "Iteration 91, loss = 0.00125115\n",
            "Iteration 92, loss = 0.00125000\n",
            "Iteration 93, loss = 0.00124885\n",
            "Iteration 94, loss = 0.00124771\n",
            "Iteration 95, loss = 0.00124657\n",
            "Iteration 96, loss = 0.00124543\n",
            "Iteration 97, loss = 0.00124430\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.00124358\n",
            "Iteration 99, loss = 0.00124335\n",
            "Iteration 100, loss = 0.00124312\n",
            "Iteration 101, loss = 0.00124290\n",
            "Iteration 102, loss = 0.00124267\n",
            "Iteration 103, loss = 0.00124245\n",
            "Iteration 104, loss = 0.00124222\n",
            "Iteration 105, loss = 0.00124200\n",
            "Iteration 106, loss = 0.00124178\n",
            "Iteration 107, loss = 0.00124155\n",
            "Iteration 108, loss = 0.00124133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.00124118\n",
            "Iteration 110, loss = 0.00124114\n",
            "Iteration 111, loss = 0.00124109\n",
            "Iteration 112, loss = 0.00124105\n",
            "Iteration 113, loss = 0.00124100\n",
            "Iteration 114, loss = 0.00124096\n",
            "Iteration 115, loss = 0.00124091\n",
            "Iteration 116, loss = 0.00124087\n",
            "Iteration 117, loss = 0.00124083\n",
            "Iteration 118, loss = 0.00124078\n",
            "Iteration 119, loss = 0.00124074\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.00124071\n",
            "Iteration 121, loss = 0.00124070\n",
            "Iteration 122, loss = 0.00124069\n",
            "Iteration 123, loss = 0.00124068\n",
            "Iteration 124, loss = 0.00124067\n",
            "Iteration 125, loss = 0.00124066\n",
            "Iteration 126, loss = 0.00124065\n",
            "Iteration 127, loss = 0.00124064\n",
            "Iteration 128, loss = 0.00124064\n",
            "Iteration 129, loss = 0.00124063\n",
            "Iteration 130, loss = 0.00124062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13655501\n",
            "Iteration 2, loss = 0.12605993\n",
            "Iteration 3, loss = 0.11634028\n",
            "Iteration 4, loss = 0.10737006\n",
            "Iteration 5, loss = 0.09909146\n",
            "Iteration 6, loss = 0.09145118\n",
            "Iteration 7, loss = 0.08439999\n",
            "Iteration 8, loss = 0.07789246\n",
            "Iteration 9, loss = 0.07188669\n",
            "Iteration 10, loss = 0.06634399\n",
            "Iteration 11, loss = 0.06122864\n",
            "Iteration 12, loss = 0.05650771\n",
            "Iteration 13, loss = 0.05215078\n",
            "Iteration 14, loss = 0.04812978\n",
            "Iteration 15, loss = 0.04441884\n",
            "Iteration 16, loss = 0.04099408\n",
            "Iteration 17, loss = 0.03783357\n",
            "Iteration 18, loss = 0.03491730\n",
            "Iteration 19, loss = 0.03222728\n",
            "Iteration 20, loss = 0.02974759\n",
            "Iteration 21, loss = 0.02746378\n",
            "Iteration 22, loss = 0.02536128\n",
            "Iteration 23, loss = 0.02342496\n",
            "Iteration 24, loss = 0.02164055\n",
            "Iteration 25, loss = 0.01999557\n",
            "Iteration 26, loss = 0.01847906\n",
            "Iteration 27, loss = 0.01708109\n",
            "Iteration 28, loss = 0.01579254\n",
            "Iteration 29, loss = 0.01460501\n",
            "Iteration 30, loss = 0.01351078\n",
            "Iteration 31, loss = 0.01250272\n",
            "Iteration 32, loss = 0.01157424\n",
            "Iteration 33, loss = 0.01071923\n",
            "Iteration 34, loss = 0.00993204\n",
            "Iteration 35, loss = 0.00920746\n",
            "Iteration 36, loss = 0.00854068\n",
            "Iteration 37, loss = 0.00792722\n",
            "Iteration 38, loss = 0.00736293\n",
            "Iteration 39, loss = 0.00684396\n",
            "Iteration 40, loss = 0.00636679\n",
            "Iteration 41, loss = 0.00592812\n",
            "Iteration 42, loss = 0.00552493\n",
            "Iteration 43, loss = 0.00515444\n",
            "Iteration 44, loss = 0.00481403\n",
            "Iteration 45, loss = 0.00450126\n",
            "Iteration 46, loss = 0.00421388\n",
            "Iteration 47, loss = 0.00394981\n",
            "Iteration 48, loss = 0.00370715\n",
            "Iteration 49, loss = 0.00348414\n",
            "Iteration 50, loss = 0.00327917\n",
            "Iteration 51, loss = 0.00309074\n",
            "Iteration 52, loss = 0.00291750\n",
            "Iteration 53, loss = 0.00275820\n",
            "Iteration 54, loss = 0.00261171\n",
            "Iteration 55, loss = 0.00247698\n",
            "Iteration 56, loss = 0.00235304\n",
            "Iteration 57, loss = 0.00223901\n",
            "Iteration 58, loss = 0.00213407\n",
            "Iteration 59, loss = 0.00203747\n",
            "Iteration 60, loss = 0.00194853\n",
            "Iteration 61, loss = 0.00186662\n",
            "Iteration 62, loss = 0.00179118\n",
            "Iteration 63, loss = 0.00172167\n",
            "Iteration 64, loss = 0.00165763\n",
            "Iteration 65, loss = 0.00159862\n",
            "Iteration 66, loss = 0.00154424\n",
            "Iteration 67, loss = 0.00149410\n",
            "Iteration 68, loss = 0.00144789\n",
            "Iteration 69, loss = 0.00140528\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00137962\n",
            "Iteration 71, loss = 0.00137184\n",
            "Iteration 72, loss = 0.00136427\n",
            "Iteration 73, loss = 0.00135683\n",
            "Iteration 74, loss = 0.00134951\n",
            "Iteration 75, loss = 0.00134231\n",
            "Iteration 76, loss = 0.00133522\n",
            "Iteration 77, loss = 0.00132825\n",
            "Iteration 78, loss = 0.00132138\n",
            "Iteration 79, loss = 0.00131463\n",
            "Iteration 80, loss = 0.00130799\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00130378\n",
            "Iteration 82, loss = 0.00130246\n",
            "Iteration 83, loss = 0.00130117\n",
            "Iteration 84, loss = 0.00129987\n",
            "Iteration 85, loss = 0.00129858\n",
            "Iteration 86, loss = 0.00129730\n",
            "Iteration 87, loss = 0.00129602\n",
            "Iteration 88, loss = 0.00129474\n",
            "Iteration 89, loss = 0.00129347\n",
            "Iteration 90, loss = 0.00129220\n",
            "Iteration 91, loss = 0.00129094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00129013\n",
            "Iteration 93, loss = 0.00128987\n",
            "Iteration 94, loss = 0.00128962\n",
            "Iteration 95, loss = 0.00128937\n",
            "Iteration 96, loss = 0.00128912\n",
            "Iteration 97, loss = 0.00128887\n",
            "Iteration 98, loss = 0.00128862\n",
            "Iteration 99, loss = 0.00128836\n",
            "Iteration 100, loss = 0.00128811\n",
            "Iteration 101, loss = 0.00128786\n",
            "Iteration 102, loss = 0.00128761\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00128745\n",
            "Iteration 104, loss = 0.00128740\n",
            "Iteration 105, loss = 0.00128735\n",
            "Iteration 106, loss = 0.00128730\n",
            "Iteration 107, loss = 0.00128725\n",
            "Iteration 108, loss = 0.00128720\n",
            "Iteration 109, loss = 0.00128715\n",
            "Iteration 110, loss = 0.00128710\n",
            "Iteration 111, loss = 0.00128705\n",
            "Iteration 112, loss = 0.00128700\n",
            "Iteration 113, loss = 0.00128695\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00128692\n",
            "Iteration 115, loss = 0.00128691\n",
            "Iteration 116, loss = 0.00128690\n",
            "Iteration 117, loss = 0.00128689\n",
            "Iteration 118, loss = 0.00128688\n",
            "Iteration 119, loss = 0.00128687\n",
            "Iteration 120, loss = 0.00128686\n",
            "Iteration 121, loss = 0.00128685\n",
            "Iteration 122, loss = 0.00128684\n",
            "Iteration 123, loss = 0.00128683\n",
            "Iteration 124, loss = 0.00128682\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.42110567\n",
            "Iteration 2, loss = 0.30615818\n",
            "Iteration 3, loss = 0.28255236\n",
            "Iteration 4, loss = 0.26076662\n",
            "Iteration 5, loss = 0.24066064\n",
            "Iteration 6, loss = 0.22210489\n",
            "Iteration 7, loss = 0.20497986\n",
            "Iteration 8, loss = 0.18917522\n",
            "Iteration 9, loss = 0.17458918\n",
            "Iteration 10, loss = 0.16112777\n",
            "Iteration 11, loss = 0.14870427\n",
            "Iteration 12, loss = 0.13723868\n",
            "Iteration 13, loss = 0.12665711\n",
            "Iteration 14, loss = 0.11689142\n",
            "Iteration 15, loss = 0.10787870\n",
            "Iteration 16, loss = 0.09956089\n",
            "Iteration 17, loss = 0.09188441\n",
            "Iteration 18, loss = 0.08479982\n",
            "Iteration 19, loss = 0.07826146\n",
            "Iteration 20, loss = 0.07222724\n",
            "Iteration 21, loss = 0.06665828\n",
            "Iteration 22, loss = 0.06151870\n",
            "Iteration 23, loss = 0.05677540\n",
            "Iteration 24, loss = 0.05239783\n",
            "Iteration 25, loss = 0.04835778\n",
            "Iteration 26, loss = 0.04462923\n",
            "Iteration 27, loss = 0.04118817\n",
            "Iteration 28, loss = 0.03801242\n",
            "Iteration 29, loss = 0.03508154\n",
            "Iteration 30, loss = 0.03237663\n",
            "Iteration 31, loss = 0.02988029\n",
            "Iteration 32, loss = 0.02757642\n",
            "Iteration 33, loss = 0.02545018\n",
            "Iteration 34, loss = 0.02348789\n",
            "Iteration 35, loss = 0.02167689\n",
            "Iteration 36, loss = 0.02000553\n",
            "Iteration 37, loss = 0.01846304\n",
            "Iteration 38, loss = 0.01703947\n",
            "Iteration 39, loss = 0.01572567\n",
            "Iteration 40, loss = 0.01451317\n",
            "Iteration 41, loss = 0.01339416\n",
            "Iteration 42, loss = 0.01236142\n",
            "Iteration 43, loss = 0.01140832\n",
            "Iteration 44, loss = 0.01052870\n",
            "Iteration 45, loss = 0.00971690\n",
            "Iteration 46, loss = 0.00896769\n",
            "Iteration 47, loss = 0.00827625\n",
            "Iteration 48, loss = 0.00763813\n",
            "Iteration 49, loss = 0.00704920\n",
            "Iteration 50, loss = 0.00650568\n",
            "Iteration 51, loss = 0.00600407\n",
            "Iteration 52, loss = 0.00554114\n",
            "Iteration 53, loss = 0.00511390\n",
            "Iteration 54, loss = 0.00471960\n",
            "Iteration 55, loss = 0.00435571\n",
            "Iteration 56, loss = 0.00401988\n",
            "Iteration 57, loss = 0.00370997\n",
            "Iteration 58, loss = 0.00342406\n",
            "Iteration 59, loss = 0.00316052\n",
            "Iteration 60, loss = 0.00291825\n",
            "Iteration 61, loss = 0.00269703\n",
            "Iteration 62, loss = 0.00249820\n",
            "Iteration 63, loss = 0.00232453\n",
            "Iteration 64, loss = 0.00217819\n",
            "Iteration 65, loss = 0.00205715\n",
            "Iteration 66, loss = 0.00195503\n",
            "Iteration 67, loss = 0.00186554\n",
            "Iteration 68, loss = 0.00178505\n",
            "Iteration 69, loss = 0.00171185\n",
            "Iteration 70, loss = 0.00164502\n",
            "Iteration 71, loss = 0.00158393\n",
            "Iteration 72, loss = 0.00152805\n",
            "Iteration 73, loss = 0.00147690\n",
            "Iteration 74, loss = 0.00143008\n",
            "Iteration 75, loss = 0.00138719\n",
            "Iteration 76, loss = 0.00134789\n",
            "Iteration 77, loss = 0.00131186\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.00129025\n",
            "Iteration 79, loss = 0.00128371\n",
            "Iteration 80, loss = 0.00127737\n",
            "Iteration 81, loss = 0.00127113\n",
            "Iteration 82, loss = 0.00126500\n",
            "Iteration 83, loss = 0.00125897\n",
            "Iteration 84, loss = 0.00125305\n",
            "Iteration 85, loss = 0.00124723\n",
            "Iteration 86, loss = 0.00124151\n",
            "Iteration 87, loss = 0.00123588\n",
            "Iteration 88, loss = 0.00123035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.00122685\n",
            "Iteration 90, loss = 0.00122576\n",
            "Iteration 91, loss = 0.00122468\n",
            "Iteration 92, loss = 0.00122360\n",
            "Iteration 93, loss = 0.00122253\n",
            "Iteration 94, loss = 0.00122146\n",
            "Iteration 95, loss = 0.00122040\n",
            "Iteration 96, loss = 0.00121934\n",
            "Iteration 97, loss = 0.00121828\n",
            "Iteration 98, loss = 0.00121723\n",
            "Iteration 99, loss = 0.00121618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 100, loss = 0.00121551\n",
            "Iteration 101, loss = 0.00121530\n",
            "Iteration 102, loss = 0.00121509\n",
            "Iteration 103, loss = 0.00121488\n",
            "Iteration 104, loss = 0.00121467\n",
            "Iteration 105, loss = 0.00121446\n",
            "Iteration 106, loss = 0.00121425\n",
            "Iteration 107, loss = 0.00121405\n",
            "Iteration 108, loss = 0.00121384\n",
            "Iteration 109, loss = 0.00121363\n",
            "Iteration 110, loss = 0.00121342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.00121329\n",
            "Iteration 112, loss = 0.00121325\n",
            "Iteration 113, loss = 0.00121321\n",
            "Iteration 114, loss = 0.00121316\n",
            "Iteration 115, loss = 0.00121312\n",
            "Iteration 116, loss = 0.00121308\n",
            "Iteration 117, loss = 0.00121304\n",
            "Iteration 118, loss = 0.00121300\n",
            "Iteration 119, loss = 0.00121296\n",
            "Iteration 120, loss = 0.00121292\n",
            "Iteration 121, loss = 0.00121287\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.00121285\n",
            "Iteration 123, loss = 0.00121284\n",
            "Iteration 124, loss = 0.00121283\n",
            "Iteration 125, loss = 0.00121282\n",
            "Iteration 126, loss = 0.00121281\n",
            "Iteration 127, loss = 0.00121281\n",
            "Iteration 128, loss = 0.00121280\n",
            "Iteration 129, loss = 0.00121279\n",
            "Iteration 130, loss = 0.00121278\n",
            "Iteration 131, loss = 0.00121277\n",
            "Iteration 132, loss = 0.00121276\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33847     0]\n",
            " [ 3774     0]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  8.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95     33847\n",
            "           1       0.00      0.00      0.00      3774\n",
            "\n",
            "    accuracy                           0.90     37621\n",
            "   macro avg       0.45      0.50      0.47     37621\n",
            "weighted avg       0.81      0.90      0.85     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Iteration 1, loss = 0.88426854\n",
            "Iteration 2, loss = 0.41425557\n",
            "Iteration 3, loss = 0.38557127\n",
            "Iteration 4, loss = 0.36511289\n",
            "Iteration 5, loss = 0.35034152\n",
            "Iteration 6, loss = 0.33965489\n",
            "Iteration 7, loss = 0.33166747\n",
            "Iteration 8, loss = 0.32589005\n",
            "Iteration 9, loss = 0.32154356\n",
            "Iteration 10, loss = 0.32002536\n",
            "Iteration 11, loss = 0.31670827\n",
            "Iteration 12, loss = 0.31574657\n",
            "Iteration 13, loss = 0.31402766\n",
            "Iteration 14, loss = 0.31298845\n",
            "Iteration 15, loss = 0.31281513\n",
            "Iteration 16, loss = 0.31312118\n",
            "Iteration 17, loss = 0.31225098\n",
            "Iteration 18, loss = 0.31275993\n",
            "Iteration 19, loss = 0.31444297\n",
            "Iteration 20, loss = 0.31306973\n",
            "Iteration 21, loss = 0.31353935\n",
            "Iteration 22, loss = 0.31201881\n",
            "Iteration 23, loss = 0.31231632\n",
            "Iteration 24, loss = 0.31162062\n",
            "Iteration 25, loss = 0.31194454\n",
            "Iteration 26, loss = 0.31132374\n",
            "Iteration 27, loss = 0.31137312\n",
            "Iteration 28, loss = 0.31098691\n",
            "Iteration 29, loss = 0.31086435\n",
            "Iteration 30, loss = 0.31133553\n",
            "Iteration 31, loss = 0.31097121\n",
            "Iteration 32, loss = 0.31179114\n",
            "Iteration 33, loss = 0.31124155\n",
            "Iteration 34, loss = 0.31119857\n",
            "Iteration 35, loss = 0.31126631\n",
            "Iteration 36, loss = 0.31048954\n",
            "Iteration 37, loss = 0.31107342\n",
            "Iteration 38, loss = 0.31016724\n",
            "Iteration 39, loss = 0.31116151\n",
            "Iteration 40, loss = 0.31112753\n",
            "Iteration 41, loss = 0.31113189\n",
            "Iteration 42, loss = 0.31081891\n",
            "Iteration 43, loss = 0.31050193\n",
            "Iteration 44, loss = 0.31045830\n",
            "Iteration 45, loss = 0.31106329\n",
            "Iteration 46, loss = 0.31050701\n",
            "Iteration 47, loss = 0.31044812\n",
            "Iteration 48, loss = 0.31128363\n",
            "Iteration 49, loss = 0.31051445\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 50, loss = 0.30155624\n",
            "Iteration 51, loss = 0.29894963\n",
            "Iteration 52, loss = 0.29727810\n",
            "Iteration 53, loss = 0.29531930\n",
            "Iteration 54, loss = 0.29367038\n",
            "Iteration 55, loss = 0.29162906\n",
            "Iteration 56, loss = 0.29026243\n",
            "Iteration 57, loss = 0.28932735\n",
            "Iteration 58, loss = 0.28806847\n",
            "Iteration 59, loss = 0.28693505\n",
            "Iteration 60, loss = 0.28706965\n",
            "Iteration 61, loss = 0.28915347\n",
            "Iteration 62, loss = 0.28641731\n",
            "Iteration 63, loss = 0.28563486\n",
            "Iteration 64, loss = 0.28906628\n",
            "Iteration 65, loss = 0.29075227\n",
            "Iteration 66, loss = 0.28792173\n",
            "Iteration 67, loss = 0.29114522\n",
            "Iteration 68, loss = 0.28922191\n",
            "Iteration 69, loss = 0.29161839\n",
            "Iteration 70, loss = 0.28728357\n",
            "Iteration 71, loss = 0.29412748\n",
            "Iteration 72, loss = 0.29250343\n",
            "Iteration 73, loss = 0.29178922\n",
            "Iteration 74, loss = 0.29074484\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 75, loss = 0.26738573\n",
            "Iteration 76, loss = 0.26660416\n",
            "Iteration 77, loss = 0.26586526\n",
            "Iteration 78, loss = 0.26528177\n",
            "Iteration 79, loss = 0.26463137\n",
            "Iteration 80, loss = 0.26417446\n",
            "Iteration 81, loss = 0.26351342\n",
            "Iteration 82, loss = 0.26309374\n",
            "Iteration 83, loss = 0.26244282\n",
            "Iteration 84, loss = 0.26191146\n",
            "Iteration 85, loss = 0.26120806\n",
            "Iteration 86, loss = 0.26068028\n",
            "Iteration 87, loss = 0.26025581\n",
            "Iteration 88, loss = 0.25954983\n",
            "Iteration 89, loss = 0.25879414\n",
            "Iteration 90, loss = 0.25819181\n",
            "Iteration 91, loss = 0.25786130\n",
            "Iteration 92, loss = 0.25723227\n",
            "Iteration 93, loss = 0.25686708\n",
            "Iteration 94, loss = 0.25627533\n",
            "Iteration 95, loss = 0.25592677\n",
            "Iteration 96, loss = 0.25539882\n",
            "Iteration 97, loss = 0.25481681\n",
            "Iteration 98, loss = 0.25442375\n",
            "Iteration 99, loss = 0.25392286\n",
            "Iteration 100, loss = 0.25346081\n",
            "Iteration 101, loss = 0.25315272\n",
            "Iteration 102, loss = 0.25269718\n",
            "Iteration 103, loss = 0.25202343\n",
            "Iteration 104, loss = 0.25148687\n",
            "Iteration 105, loss = 0.25107414\n",
            "Iteration 106, loss = 0.25080829\n",
            "Iteration 107, loss = 0.25043214\n",
            "Iteration 108, loss = 0.25019261\n",
            "Iteration 109, loss = 0.24981240\n",
            "Iteration 110, loss = 0.24955601\n",
            "Iteration 111, loss = 0.24868465\n",
            "Iteration 112, loss = 0.24819361\n",
            "Iteration 113, loss = 0.24790916\n",
            "Iteration 114, loss = 0.24739515\n",
            "Iteration 115, loss = 0.24744589\n",
            "Iteration 116, loss = 0.24691003\n",
            "Iteration 117, loss = 0.24669572\n",
            "Iteration 118, loss = 0.24613920\n",
            "Iteration 119, loss = 0.24569478\n",
            "Iteration 120, loss = 0.24541537\n",
            "Iteration 121, loss = 0.24551458\n",
            "Iteration 122, loss = 0.24476185\n",
            "Iteration 123, loss = 0.24442110\n",
            "Iteration 124, loss = 0.24442638\n",
            "Iteration 125, loss = 0.24401968\n",
            "Iteration 126, loss = 0.24328251\n",
            "Iteration 127, loss = 0.24343069\n",
            "Iteration 128, loss = 0.24307431\n",
            "Iteration 129, loss = 0.24300201\n",
            "Iteration 130, loss = 0.24260004\n",
            "Iteration 131, loss = 0.24225033\n",
            "Iteration 132, loss = 0.24251705\n",
            "Iteration 133, loss = 0.24165565\n",
            "Iteration 134, loss = 0.24179023\n",
            "Iteration 135, loss = 0.24137938\n",
            "Iteration 136, loss = 0.24112361\n",
            "Iteration 137, loss = 0.24086647\n",
            "Iteration 138, loss = 0.24087885\n",
            "Iteration 139, loss = 0.24058723\n",
            "Iteration 140, loss = 0.24038791\n",
            "Iteration 141, loss = 0.23995948\n",
            "Iteration 142, loss = 0.23975381\n",
            "Iteration 143, loss = 0.23970567\n",
            "Iteration 144, loss = 0.23914294\n",
            "Iteration 145, loss = 0.23873907\n",
            "Iteration 146, loss = 0.23898924\n",
            "Iteration 147, loss = 0.23862887\n",
            "Iteration 148, loss = 0.23857010\n",
            "Iteration 149, loss = 0.23841857\n",
            "Iteration 150, loss = 0.23812663\n",
            "Iteration 151, loss = 0.23825410\n",
            "Iteration 152, loss = 0.23801728\n",
            "Iteration 153, loss = 0.23737186\n",
            "Iteration 154, loss = 0.23752120\n",
            "Iteration 155, loss = 0.23748229\n",
            "Iteration 156, loss = 0.23725375\n",
            "Iteration 157, loss = 0.23721307\n",
            "Iteration 158, loss = 0.23689449\n",
            "Iteration 159, loss = 0.23732495\n",
            "Iteration 160, loss = 0.23670285\n",
            "Iteration 161, loss = 0.23627320\n",
            "Iteration 162, loss = 0.23638756\n",
            "Iteration 163, loss = 0.23636363\n",
            "Iteration 164, loss = 0.23597677\n",
            "Iteration 165, loss = 0.23629853\n",
            "Iteration 166, loss = 0.23586653\n",
            "Iteration 167, loss = 0.23550689\n",
            "Iteration 168, loss = 0.23566476\n",
            "Iteration 169, loss = 0.23590182\n",
            "Iteration 170, loss = 0.23528583\n",
            "Iteration 171, loss = 0.23525144\n",
            "Iteration 172, loss = 0.23513699\n",
            "Iteration 173, loss = 0.23491375\n",
            "Iteration 174, loss = 0.23500809\n",
            "Iteration 175, loss = 0.23469420\n",
            "Iteration 176, loss = 0.23456641\n",
            "Iteration 177, loss = 0.23472467\n",
            "Iteration 178, loss = 0.23469681\n",
            "Iteration 179, loss = 0.23432736\n",
            "Iteration 180, loss = 0.23425543\n",
            "Iteration 181, loss = 0.23431944\n",
            "Iteration 182, loss = 0.23427759\n",
            "Iteration 183, loss = 0.23422511\n",
            "Iteration 184, loss = 0.23421683\n",
            "Iteration 185, loss = 0.23446923\n",
            "Iteration 186, loss = 0.23393821\n",
            "Iteration 187, loss = 0.23391133\n",
            "Iteration 188, loss = 0.23395505\n",
            "Iteration 189, loss = 0.23372200\n",
            "Iteration 190, loss = 0.23390676\n",
            "Iteration 191, loss = 0.23336081\n",
            "Iteration 192, loss = 0.23358052\n",
            "Iteration 193, loss = 0.23381083\n",
            "Iteration 194, loss = 0.23355889\n",
            "Iteration 195, loss = 0.23327893\n",
            "Iteration 196, loss = 0.23354178\n",
            "Iteration 197, loss = 0.23334859\n",
            "Iteration 198, loss = 0.23325132\n",
            "Iteration 199, loss = 0.23314030\n",
            "Iteration 200, loss = 0.23325692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.31164048\n",
            "Iteration 2, loss = 0.14232214\n",
            "Iteration 3, loss = 0.12107091\n",
            "Iteration 4, loss = 0.10510848\n",
            "Iteration 5, loss = 0.09307183\n",
            "Iteration 6, loss = 0.08415779\n",
            "Iteration 7, loss = 0.07757818\n",
            "Iteration 8, loss = 0.07242012\n",
            "Iteration 9, loss = 0.06860781\n",
            "Iteration 10, loss = 0.06543858\n",
            "Iteration 11, loss = 0.06309820\n",
            "Iteration 12, loss = 0.06156440\n",
            "Iteration 13, loss = 0.06028812\n",
            "Iteration 14, loss = 0.05948571\n",
            "Iteration 15, loss = 0.05863422\n",
            "Iteration 16, loss = 0.05803001\n",
            "Iteration 17, loss = 0.05764203\n",
            "Iteration 18, loss = 0.05742008\n",
            "Iteration 19, loss = 0.05705013\n",
            "Iteration 20, loss = 0.05691184\n",
            "Iteration 21, loss = 0.05666510\n",
            "Iteration 22, loss = 0.05663035\n",
            "Iteration 23, loss = 0.05649246\n",
            "Iteration 24, loss = 0.05656142\n",
            "Iteration 25, loss = 0.05630609\n",
            "Iteration 26, loss = 0.05643778\n",
            "Iteration 27, loss = 0.05612313\n",
            "Iteration 28, loss = 0.05634833\n",
            "Iteration 29, loss = 0.05625758\n",
            "Iteration 30, loss = 0.05607296\n",
            "Iteration 31, loss = 0.05598787\n",
            "Iteration 32, loss = 0.05625308\n",
            "Iteration 33, loss = 0.05633403\n",
            "Iteration 34, loss = 0.05605885\n",
            "Iteration 35, loss = 0.05605918\n",
            "Iteration 36, loss = 0.05623723\n",
            "Iteration 37, loss = 0.05599817\n",
            "Iteration 38, loss = 0.05596608\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.05559151\n",
            "Iteration 40, loss = 0.05557260\n",
            "Iteration 41, loss = 0.05556379\n",
            "Iteration 42, loss = 0.05562429\n",
            "Iteration 43, loss = 0.05555736\n",
            "Iteration 44, loss = 0.05552270\n",
            "Iteration 45, loss = 0.05557875\n",
            "Iteration 46, loss = 0.05552055\n",
            "Iteration 47, loss = 0.05561532\n",
            "Iteration 48, loss = 0.05554254\n",
            "Iteration 49, loss = 0.05550237\n",
            "Iteration 50, loss = 0.05554204\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 51, loss = 0.05546346\n",
            "Iteration 52, loss = 0.05541393\n",
            "Iteration 53, loss = 0.05539874\n",
            "Iteration 54, loss = 0.05541175\n",
            "Iteration 55, loss = 0.05540978\n",
            "Iteration 56, loss = 0.05540077\n",
            "Iteration 57, loss = 0.05541563\n",
            "Iteration 58, loss = 0.05540206\n",
            "Iteration 59, loss = 0.05538844\n",
            "Iteration 60, loss = 0.05540977\n",
            "Iteration 61, loss = 0.05539406\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 62, loss = 0.05538506\n",
            "Iteration 63, loss = 0.05536857\n",
            "Iteration 64, loss = 0.05536406\n",
            "Iteration 65, loss = 0.05536948\n",
            "Iteration 66, loss = 0.05536773\n",
            "Iteration 67, loss = 0.05536942\n",
            "Iteration 68, loss = 0.05536908\n",
            "Iteration 69, loss = 0.05536649\n",
            "Iteration 70, loss = 0.05536828\n",
            "Iteration 71, loss = 0.05536851\n",
            "Iteration 72, loss = 0.05537098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 73, loss = 0.05536071\n",
            "Iteration 74, loss = 0.05536041\n",
            "Iteration 75, loss = 0.05535977\n",
            "Iteration 76, loss = 0.05535987\n",
            "Iteration 77, loss = 0.05536036\n",
            "Iteration 78, loss = 0.05535914\n",
            "Iteration 79, loss = 0.05535992\n",
            "Iteration 80, loss = 0.05535985\n",
            "Iteration 81, loss = 0.05535955\n",
            "Iteration 82, loss = 0.05536020\n",
            "Iteration 83, loss = 0.05536045\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 84, loss = 0.05535804\n",
            "Iteration 85, loss = 0.05535787\n",
            "Iteration 86, loss = 0.05535785\n",
            "Iteration 87, loss = 0.05535795\n",
            "Iteration 88, loss = 0.05535789\n",
            "Iteration 89, loss = 0.05535793\n",
            "Iteration 90, loss = 0.05535795\n",
            "Iteration 91, loss = 0.05535785\n",
            "Iteration 92, loss = 0.05535804\n",
            "Iteration 93, loss = 0.05535780\n",
            "Iteration 94, loss = 0.05535785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.33930831\n",
            "Iteration 2, loss = 0.14022845\n",
            "Iteration 3, loss = 0.11939360\n",
            "Iteration 4, loss = 0.10437571\n",
            "Iteration 5, loss = 0.09267556\n",
            "Iteration 6, loss = 0.08388209\n",
            "Iteration 7, loss = 0.07740333\n",
            "Iteration 8, loss = 0.07250194\n",
            "Iteration 9, loss = 0.06880479\n",
            "Iteration 10, loss = 0.06590968\n",
            "Iteration 11, loss = 0.06393869\n",
            "Iteration 12, loss = 0.06212257\n",
            "Iteration 13, loss = 0.06087408\n",
            "Iteration 14, loss = 0.06002538\n",
            "Iteration 15, loss = 0.05933381\n",
            "Iteration 16, loss = 0.05851443\n",
            "Iteration 17, loss = 0.05816601\n",
            "Iteration 18, loss = 0.05796617\n",
            "Iteration 19, loss = 0.05771404\n",
            "Iteration 20, loss = 0.05771535\n",
            "Iteration 21, loss = 0.05756538\n",
            "Iteration 22, loss = 0.05722602\n",
            "Iteration 23, loss = 0.05714294\n",
            "Iteration 24, loss = 0.05688504\n",
            "Iteration 25, loss = 0.05693919\n",
            "Iteration 26, loss = 0.05687348\n",
            "Iteration 27, loss = 0.05682141\n",
            "Iteration 28, loss = 0.05671861\n",
            "Iteration 29, loss = 0.05686703\n",
            "Iteration 30, loss = 0.05666416\n",
            "Iteration 31, loss = 0.05663744\n",
            "Iteration 32, loss = 0.05676871\n",
            "Iteration 33, loss = 0.05668413\n",
            "Iteration 34, loss = 0.05662902\n",
            "Iteration 35, loss = 0.05665862\n",
            "Iteration 36, loss = 0.05656048\n",
            "Iteration 37, loss = 0.05652908\n",
            "Iteration 38, loss = 0.05652094\n",
            "Iteration 39, loss = 0.05639799\n",
            "Iteration 40, loss = 0.05639453\n",
            "Iteration 41, loss = 0.05671808\n",
            "Iteration 42, loss = 0.05665355\n",
            "Iteration 43, loss = 0.05640611\n",
            "Iteration 44, loss = 0.05655885\n",
            "Iteration 45, loss = 0.05644733\n",
            "Iteration 46, loss = 0.05650919\n",
            "Iteration 47, loss = 0.05646021\n",
            "Iteration 48, loss = 0.05649380\n",
            "Iteration 49, loss = 0.05649406\n",
            "Iteration 50, loss = 0.05639618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 51, loss = 0.05601266\n",
            "Iteration 52, loss = 0.05600876\n",
            "Iteration 53, loss = 0.05594783\n",
            "Iteration 54, loss = 0.05594552\n",
            "Iteration 55, loss = 0.05592597\n",
            "Iteration 56, loss = 0.05593390\n",
            "Iteration 57, loss = 0.05592139\n",
            "Iteration 58, loss = 0.05595376\n",
            "Iteration 59, loss = 0.05588901\n",
            "Iteration 60, loss = 0.05592176\n",
            "Iteration 61, loss = 0.05592447\n",
            "Iteration 62, loss = 0.05591711\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 63, loss = 0.05581235\n",
            "Iteration 64, loss = 0.05579611\n",
            "Iteration 65, loss = 0.05579079\n",
            "Iteration 66, loss = 0.05577917\n",
            "Iteration 67, loss = 0.05577440\n",
            "Iteration 68, loss = 0.05578416\n",
            "Iteration 69, loss = 0.05577710\n",
            "Iteration 70, loss = 0.05578851\n",
            "Iteration 71, loss = 0.05576320\n",
            "Iteration 72, loss = 0.05577725\n",
            "Iteration 73, loss = 0.05578318\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 74, loss = 0.05575572\n",
            "Iteration 75, loss = 0.05574913\n",
            "Iteration 76, loss = 0.05574668\n",
            "Iteration 77, loss = 0.05574913\n",
            "Iteration 78, loss = 0.05574694\n",
            "Iteration 79, loss = 0.05574797\n",
            "Iteration 80, loss = 0.05574392\n",
            "Iteration 81, loss = 0.05574889\n",
            "Iteration 82, loss = 0.05574465\n",
            "Iteration 83, loss = 0.05574858\n",
            "Iteration 84, loss = 0.05574835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 85, loss = 0.05573914\n",
            "Iteration 86, loss = 0.05573785\n",
            "Iteration 87, loss = 0.05573676\n",
            "Iteration 88, loss = 0.05573890\n",
            "Iteration 89, loss = 0.05573817\n",
            "Iteration 90, loss = 0.05573771\n",
            "Iteration 91, loss = 0.05573680\n",
            "Iteration 92, loss = 0.05573764\n",
            "Iteration 93, loss = 0.05573643\n",
            "Iteration 94, loss = 0.05573737\n",
            "Iteration 95, loss = 0.05573747\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 96, loss = 0.05573559\n",
            "Iteration 97, loss = 0.05573565\n",
            "Iteration 98, loss = 0.05573570\n",
            "Iteration 99, loss = 0.05573553\n",
            "Iteration 100, loss = 0.05573551\n",
            "Iteration 101, loss = 0.05573543\n",
            "Iteration 102, loss = 0.05573565\n",
            "Iteration 103, loss = 0.05573563\n",
            "Iteration 104, loss = 0.05573544\n",
            "Iteration 105, loss = 0.05573551\n",
            "Iteration 106, loss = 0.05573562\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.41126248\n",
            "Iteration 2, loss = 0.13857448\n",
            "Iteration 3, loss = 0.11866670\n",
            "Iteration 4, loss = 0.10355055\n",
            "Iteration 5, loss = 0.09223838\n",
            "Iteration 6, loss = 0.08361612\n",
            "Iteration 7, loss = 0.07720380\n",
            "Iteration 8, loss = 0.07222966\n",
            "Iteration 9, loss = 0.06852847\n",
            "Iteration 10, loss = 0.06558078\n",
            "Iteration 11, loss = 0.06330684\n",
            "Iteration 12, loss = 0.06184398\n",
            "Iteration 13, loss = 0.06055456\n",
            "Iteration 14, loss = 0.05942574\n",
            "Iteration 15, loss = 0.05865031\n",
            "Iteration 16, loss = 0.05829083\n",
            "Iteration 17, loss = 0.05775359\n",
            "Iteration 18, loss = 0.05738442\n",
            "Iteration 19, loss = 0.05728851\n",
            "Iteration 20, loss = 0.05681812\n",
            "Iteration 21, loss = 0.05700202\n",
            "Iteration 22, loss = 0.05685287\n",
            "Iteration 23, loss = 0.05657963\n",
            "Iteration 24, loss = 0.05657757\n",
            "Iteration 25, loss = 0.05683922\n",
            "Iteration 26, loss = 0.05677143\n",
            "Iteration 27, loss = 0.05658657\n",
            "Iteration 28, loss = 0.05655443\n",
            "Iteration 29, loss = 0.05658802\n",
            "Iteration 30, loss = 0.05623178\n",
            "Iteration 31, loss = 0.05636946\n",
            "Iteration 32, loss = 0.05637787\n",
            "Iteration 33, loss = 0.05630701\n",
            "Iteration 34, loss = 0.05630474\n",
            "Iteration 35, loss = 0.05622907\n",
            "Iteration 36, loss = 0.05627985\n",
            "Iteration 37, loss = 0.05630825\n",
            "Iteration 38, loss = 0.05649344\n",
            "Iteration 39, loss = 0.05641560\n",
            "Iteration 40, loss = 0.05635305\n",
            "Iteration 41, loss = 0.05630379\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 42, loss = 0.05578298\n",
            "Iteration 43, loss = 0.05574684\n",
            "Iteration 44, loss = 0.05578553\n",
            "Iteration 45, loss = 0.05571540\n",
            "Iteration 46, loss = 0.05569353\n",
            "Iteration 47, loss = 0.05573334\n",
            "Iteration 48, loss = 0.05568917\n",
            "Iteration 49, loss = 0.05575721\n",
            "Iteration 50, loss = 0.05573238\n",
            "Iteration 51, loss = 0.05575425\n",
            "Iteration 52, loss = 0.05574846\n",
            "Iteration 53, loss = 0.05567374\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 54, loss = 0.05558597\n",
            "Iteration 55, loss = 0.05555027\n",
            "Iteration 56, loss = 0.05553948\n",
            "Iteration 57, loss = 0.05556060\n",
            "Iteration 58, loss = 0.05556055\n",
            "Iteration 59, loss = 0.05554771\n",
            "Iteration 60, loss = 0.05554465\n",
            "Iteration 61, loss = 0.05553909\n",
            "Iteration 62, loss = 0.05553468\n",
            "Iteration 63, loss = 0.05555691\n",
            "Iteration 64, loss = 0.05553483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 65, loss = 0.05552654\n",
            "Iteration 66, loss = 0.05551353\n",
            "Iteration 67, loss = 0.05550699\n",
            "Iteration 68, loss = 0.05550742\n",
            "Iteration 69, loss = 0.05550486\n",
            "Iteration 70, loss = 0.05550882\n",
            "Iteration 71, loss = 0.05550787\n",
            "Iteration 72, loss = 0.05550679\n",
            "Iteration 73, loss = 0.05550740\n",
            "Iteration 74, loss = 0.05550669\n",
            "Iteration 75, loss = 0.05550484\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 76, loss = 0.05550762\n",
            "Iteration 77, loss = 0.05549763\n",
            "Iteration 78, loss = 0.05549779\n",
            "Iteration 79, loss = 0.05549768\n",
            "Iteration 80, loss = 0.05549794\n",
            "Iteration 81, loss = 0.05549755\n",
            "Iteration 82, loss = 0.05549798\n",
            "Iteration 83, loss = 0.05549759\n",
            "Iteration 84, loss = 0.05549810\n",
            "Iteration 85, loss = 0.05549755\n",
            "Iteration 86, loss = 0.05549749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 87, loss = 0.05549596\n",
            "Iteration 88, loss = 0.05549592\n",
            "Iteration 89, loss = 0.05549579\n",
            "Iteration 90, loss = 0.05549581\n",
            "Iteration 91, loss = 0.05549575\n",
            "Iteration 92, loss = 0.05549577\n",
            "Iteration 93, loss = 0.05549567\n",
            "Iteration 94, loss = 0.05549563\n",
            "Iteration 95, loss = 0.05549558\n",
            "Iteration 96, loss = 0.05549572\n",
            "Iteration 97, loss = 0.05549580\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.25856954\n",
            "Iteration 2, loss = 0.13638626\n",
            "Iteration 3, loss = 0.11633458\n",
            "Iteration 4, loss = 0.10132425\n",
            "Iteration 5, loss = 0.09027942\n",
            "Iteration 6, loss = 0.08198538\n",
            "Iteration 7, loss = 0.07579291\n",
            "Iteration 8, loss = 0.07122889\n",
            "Iteration 9, loss = 0.06782006\n",
            "Iteration 10, loss = 0.06503713\n",
            "Iteration 11, loss = 0.06324060\n",
            "Iteration 12, loss = 0.06161341\n",
            "Iteration 13, loss = 0.06044248\n",
            "Iteration 14, loss = 0.05951364\n",
            "Iteration 15, loss = 0.05895623\n",
            "Iteration 16, loss = 0.05849896\n",
            "Iteration 17, loss = 0.05800423\n",
            "Iteration 18, loss = 0.05760536\n",
            "Iteration 19, loss = 0.05732223\n",
            "Iteration 20, loss = 0.05724774\n",
            "Iteration 21, loss = 0.05710081\n",
            "Iteration 22, loss = 0.05708248\n",
            "Iteration 23, loss = 0.05695626\n",
            "Iteration 24, loss = 0.05664631\n",
            "Iteration 25, loss = 0.05667893\n",
            "Iteration 26, loss = 0.05661994\n",
            "Iteration 27, loss = 0.05654387\n",
            "Iteration 28, loss = 0.05646290\n",
            "Iteration 29, loss = 0.05664462\n",
            "Iteration 30, loss = 0.05641201\n",
            "Iteration 31, loss = 0.05644232\n",
            "Iteration 32, loss = 0.05631042\n",
            "Iteration 33, loss = 0.05632957\n",
            "Iteration 34, loss = 0.05637033\n",
            "Iteration 35, loss = 0.05626770\n",
            "Iteration 36, loss = 0.05630362\n",
            "Iteration 37, loss = 0.05632586\n",
            "Iteration 38, loss = 0.05639308\n",
            "Iteration 39, loss = 0.05626539\n",
            "Iteration 40, loss = 0.05612140\n",
            "Iteration 41, loss = 0.05609934\n",
            "Iteration 42, loss = 0.05619157\n",
            "Iteration 43, loss = 0.05624986\n",
            "Iteration 44, loss = 0.05641303\n",
            "Iteration 45, loss = 0.05623394\n",
            "Iteration 46, loss = 0.05625065\n",
            "Iteration 47, loss = 0.05633293\n",
            "Iteration 48, loss = 0.05628350\n",
            "Iteration 49, loss = 0.05634959\n",
            "Iteration 50, loss = 0.05629102\n",
            "Iteration 51, loss = 0.05614307\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.05570830\n",
            "Iteration 53, loss = 0.05568531\n",
            "Iteration 54, loss = 0.05568047\n",
            "Iteration 55, loss = 0.05566593\n",
            "Iteration 56, loss = 0.05568289\n",
            "Iteration 57, loss = 0.05569148\n",
            "Iteration 58, loss = 0.05566984\n",
            "Iteration 59, loss = 0.05565487\n",
            "Iteration 60, loss = 0.05565327\n",
            "Iteration 61, loss = 0.05562547\n",
            "Iteration 62, loss = 0.05561648\n",
            "Iteration 63, loss = 0.05559717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 64, loss = 0.05552515\n",
            "Iteration 65, loss = 0.05549783\n",
            "Iteration 66, loss = 0.05549967\n",
            "Iteration 67, loss = 0.05548586\n",
            "Iteration 68, loss = 0.05550477\n",
            "Iteration 69, loss = 0.05550584\n",
            "Iteration 70, loss = 0.05549227\n",
            "Iteration 71, loss = 0.05547727\n",
            "Iteration 72, loss = 0.05548437\n",
            "Iteration 73, loss = 0.05549718\n",
            "Iteration 74, loss = 0.05550497\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 75, loss = 0.05547856\n",
            "Iteration 76, loss = 0.05546996\n",
            "Iteration 77, loss = 0.05546934\n",
            "Iteration 78, loss = 0.05547244\n",
            "Iteration 79, loss = 0.05546763\n",
            "Iteration 80, loss = 0.05546964\n",
            "Iteration 81, loss = 0.05547011\n",
            "Iteration 82, loss = 0.05546965\n",
            "Iteration 83, loss = 0.05546498\n",
            "Iteration 84, loss = 0.05547500\n",
            "Iteration 85, loss = 0.05546435\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 86, loss = 0.05547394\n",
            "Iteration 87, loss = 0.05546604\n",
            "Iteration 88, loss = 0.05546146\n",
            "Iteration 89, loss = 0.05546034\n",
            "Iteration 90, loss = 0.05546025\n",
            "Iteration 91, loss = 0.05546148\n",
            "Iteration 92, loss = 0.05546025\n",
            "Iteration 93, loss = 0.05545971\n",
            "Iteration 94, loss = 0.05546027\n",
            "Iteration 95, loss = 0.05546031\n",
            "Iteration 96, loss = 0.05546056\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 97, loss = 0.05545874\n",
            "Iteration 98, loss = 0.05545846\n",
            "Iteration 99, loss = 0.05545854\n",
            "Iteration 100, loss = 0.05545831\n",
            "Iteration 101, loss = 0.05545843\n",
            "Iteration 102, loss = 0.05545838\n",
            "Iteration 103, loss = 0.05545843\n",
            "Iteration 104, loss = 0.05545823\n",
            "Iteration 105, loss = 0.05545866\n",
            "Iteration 106, loss = 0.05545837\n",
            "Iteration 107, loss = 0.05545844\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.21870978\n",
            "Iteration 3, loss = 0.17836491\n",
            "Iteration 4, loss = 0.14905802\n",
            "Iteration 5, loss = 0.12676219\n",
            "Iteration 6, loss = 0.10950358\n",
            "Iteration 7, loss = 0.09653655\n",
            "Iteration 8, loss = 0.08695374\n",
            "Iteration 9, loss = 0.07951171\n",
            "Iteration 10, loss = 0.07389650\n",
            "Iteration 11, loss = 0.06965849\n",
            "Iteration 12, loss = 0.06655126\n",
            "Iteration 13, loss = 0.06397472\n",
            "Iteration 14, loss = 0.06208045\n",
            "Iteration 15, loss = 0.06078856\n",
            "Iteration 16, loss = 0.05967688\n",
            "Iteration 17, loss = 0.05897242\n",
            "Iteration 18, loss = 0.05827343\n",
            "Iteration 19, loss = 0.05787580\n",
            "Iteration 20, loss = 0.05726082\n",
            "Iteration 21, loss = 0.05716583\n",
            "Iteration 22, loss = 0.05677952\n",
            "Iteration 23, loss = 0.05678592\n",
            "Iteration 24, loss = 0.05648731\n",
            "Iteration 25, loss = 0.05637556\n",
            "Iteration 26, loss = 0.05628209\n",
            "Iteration 27, loss = 0.05644243\n",
            "Iteration 28, loss = 0.05627955\n",
            "Iteration 29, loss = 0.05621167\n",
            "Iteration 30, loss = 0.05611307\n",
            "Iteration 31, loss = 0.05602115\n",
            "Iteration 32, loss = 0.05621728\n",
            "Iteration 33, loss = 0.05614794\n",
            "Iteration 34, loss = 0.05608517\n",
            "Iteration 35, loss = 0.05603420\n",
            "Iteration 36, loss = 0.05622525\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.05554232\n",
            "Iteration 38, loss = 0.05556764\n",
            "Iteration 39, loss = 0.05550674\n",
            "Iteration 40, loss = 0.05553865\n",
            "Iteration 41, loss = 0.05552736\n",
            "Iteration 42, loss = 0.05553622\n",
            "Iteration 43, loss = 0.05557562\n",
            "Iteration 44, loss = 0.05554925\n",
            "Iteration 45, loss = 0.05556213\n",
            "Iteration 46, loss = 0.05551280\n",
            "Iteration 47, loss = 0.05554834\n",
            "Iteration 48, loss = 0.05555942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.05540111\n",
            "Iteration 50, loss = 0.05537643\n",
            "Iteration 51, loss = 0.05536620\n",
            "Iteration 52, loss = 0.05538021\n",
            "Iteration 53, loss = 0.05537985\n",
            "Iteration 54, loss = 0.05537489\n",
            "Iteration 55, loss = 0.05538385\n",
            "Iteration 56, loss = 0.05538403\n",
            "Iteration 57, loss = 0.05537851\n",
            "Iteration 58, loss = 0.05537934\n",
            "Iteration 59, loss = 0.05537450\n",
            "Iteration 60, loss = 0.05537708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 61, loss = 0.05534986\n",
            "Iteration 62, loss = 0.05534267\n",
            "Iteration 63, loss = 0.05534519\n",
            "Iteration 64, loss = 0.05534464\n",
            "Iteration 65, loss = 0.05534201\n",
            "Iteration 66, loss = 0.05534247\n",
            "Iteration 67, loss = 0.05534338\n",
            "Iteration 68, loss = 0.05534907\n",
            "Iteration 69, loss = 0.05534243\n",
            "Iteration 70, loss = 0.05534025\n",
            "Iteration 71, loss = 0.05534178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 72, loss = 0.05534147\n",
            "Iteration 73, loss = 0.05533467\n",
            "Iteration 74, loss = 0.05533302\n",
            "Iteration 75, loss = 0.05533376\n",
            "Iteration 76, loss = 0.05533331\n",
            "Iteration 77, loss = 0.05533393\n",
            "Iteration 78, loss = 0.05533385\n",
            "Iteration 79, loss = 0.05533305\n",
            "Iteration 80, loss = 0.05533387\n",
            "Iteration 81, loss = 0.05533381\n",
            "Iteration 82, loss = 0.05533327\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 83, loss = 0.05533431\n",
            "Iteration 84, loss = 0.05533232\n",
            "Iteration 85, loss = 0.05533203\n",
            "Iteration 86, loss = 0.05533197\n",
            "Iteration 87, loss = 0.05533188\n",
            "Iteration 88, loss = 0.05533204\n",
            "Iteration 89, loss = 0.05533198\n",
            "Iteration 90, loss = 0.05533209\n",
            "Iteration 91, loss = 0.05533190\n",
            "Iteration 92, loss = 0.05533193\n",
            "Iteration 93, loss = 0.05533197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.17540865\n",
            "Iteration 3, loss = 0.14653825\n",
            "Iteration 4, loss = 0.12477638\n",
            "Iteration 5, loss = 0.10833451\n",
            "Iteration 6, loss = 0.09589771\n",
            "Iteration 7, loss = 0.08651417\n",
            "Iteration 8, loss = 0.07930528\n",
            "Iteration 9, loss = 0.07385362\n",
            "Iteration 10, loss = 0.06969948\n",
            "Iteration 11, loss = 0.06640894\n",
            "Iteration 12, loss = 0.06416906\n",
            "Iteration 13, loss = 0.06233948\n",
            "Iteration 14, loss = 0.06098269\n",
            "Iteration 15, loss = 0.05974960\n",
            "Iteration 16, loss = 0.05893893\n",
            "Iteration 17, loss = 0.05824263\n",
            "Iteration 18, loss = 0.05787157\n",
            "Iteration 19, loss = 0.05762477\n",
            "Iteration 20, loss = 0.05722387\n",
            "Iteration 21, loss = 0.05704457\n",
            "Iteration 22, loss = 0.05675447\n",
            "Iteration 23, loss = 0.05663391\n",
            "Iteration 24, loss = 0.05653038\n",
            "Iteration 25, loss = 0.05662974\n",
            "Iteration 26, loss = 0.05663343\n",
            "Iteration 27, loss = 0.05641656\n",
            "Iteration 28, loss = 0.05634014\n",
            "Iteration 29, loss = 0.05630627\n",
            "Iteration 30, loss = 0.05632939\n",
            "Iteration 31, loss = 0.05628689\n",
            "Iteration 32, loss = 0.05619504\n",
            "Iteration 33, loss = 0.05633258\n",
            "Iteration 34, loss = 0.05611581\n",
            "Iteration 35, loss = 0.05614379\n",
            "Iteration 36, loss = 0.05625836\n",
            "Iteration 37, loss = 0.05620147\n",
            "Iteration 38, loss = 0.05646966\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.05568506\n",
            "Iteration 40, loss = 0.05563031\n",
            "Iteration 41, loss = 0.05565951\n",
            "Iteration 42, loss = 0.05565962\n",
            "Iteration 43, loss = 0.05562399\n",
            "Iteration 44, loss = 0.05572605\n",
            "Iteration 45, loss = 0.05565840\n",
            "Iteration 46, loss = 0.05564194\n",
            "Iteration 47, loss = 0.05561276\n",
            "Iteration 48, loss = 0.05561887\n",
            "Iteration 49, loss = 0.05565393\n",
            "Iteration 50, loss = 0.05567060\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 51, loss = 0.05550872\n",
            "Iteration 52, loss = 0.05550089\n",
            "Iteration 53, loss = 0.05548137\n",
            "Iteration 54, loss = 0.05548123\n",
            "Iteration 55, loss = 0.05548503\n",
            "Iteration 56, loss = 0.05550042\n",
            "Iteration 57, loss = 0.05548901\n",
            "Iteration 58, loss = 0.05546542\n",
            "Iteration 59, loss = 0.05549194\n",
            "Iteration 60, loss = 0.05547833\n",
            "Iteration 61, loss = 0.05547983\n",
            "Iteration 62, loss = 0.05548267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 63, loss = 0.05545938\n",
            "Iteration 64, loss = 0.05544818\n",
            "Iteration 65, loss = 0.05545225\n",
            "Iteration 66, loss = 0.05545061\n",
            "Iteration 67, loss = 0.05544971\n",
            "Iteration 68, loss = 0.05545171\n",
            "Iteration 69, loss = 0.05545033\n",
            "Iteration 70, loss = 0.05545587\n",
            "Iteration 71, loss = 0.05544589\n",
            "Iteration 72, loss = 0.05545499\n",
            "Iteration 73, loss = 0.05544969\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 74, loss = 0.05544381\n",
            "Iteration 75, loss = 0.05544251\n",
            "Iteration 76, loss = 0.05544290\n",
            "Iteration 77, loss = 0.05544147\n",
            "Iteration 78, loss = 0.05544099\n",
            "Iteration 79, loss = 0.05544100\n",
            "Iteration 80, loss = 0.05544168\n",
            "Iteration 81, loss = 0.05544154\n",
            "Iteration 82, loss = 0.05544144\n",
            "Iteration 83, loss = 0.05544173\n",
            "Iteration 84, loss = 0.05544079\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 85, loss = 0.05543924\n",
            "Iteration 86, loss = 0.05543915\n",
            "Iteration 87, loss = 0.05543924\n",
            "Iteration 88, loss = 0.05543922\n",
            "Iteration 89, loss = 0.05543941\n",
            "Iteration 90, loss = 0.05543921\n",
            "Iteration 91, loss = 0.05543902\n",
            "Iteration 92, loss = 0.05543927\n",
            "Iteration 93, loss = 0.05543924\n",
            "Iteration 94, loss = 0.05543916\n",
            "Iteration 95, loss = 0.05543919\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.19402665\n",
            "Iteration 3, loss = 0.16103889\n",
            "Iteration 4, loss = 0.13591516\n",
            "Iteration 5, loss = 0.11674586\n",
            "Iteration 6, loss = 0.10217262\n",
            "Iteration 7, loss = 0.09108280\n",
            "Iteration 8, loss = 0.08286299\n",
            "Iteration 9, loss = 0.07646452\n",
            "Iteration 10, loss = 0.07148522\n",
            "Iteration 11, loss = 0.06805158\n",
            "Iteration 12, loss = 0.06525643\n",
            "Iteration 13, loss = 0.06313368\n",
            "Iteration 14, loss = 0.06154042\n",
            "Iteration 15, loss = 0.06034099\n",
            "Iteration 16, loss = 0.05918429\n",
            "Iteration 17, loss = 0.05853917\n",
            "Iteration 18, loss = 0.05796155\n",
            "Iteration 19, loss = 0.05752676\n",
            "Iteration 20, loss = 0.05725467\n",
            "Iteration 21, loss = 0.05689882\n",
            "Iteration 22, loss = 0.05681500\n",
            "Iteration 23, loss = 0.05662788\n",
            "Iteration 24, loss = 0.05634939\n",
            "Iteration 25, loss = 0.05646289\n",
            "Iteration 26, loss = 0.05621251\n",
            "Iteration 27, loss = 0.05621182\n",
            "Iteration 28, loss = 0.05615863\n",
            "Iteration 29, loss = 0.05614065\n",
            "Iteration 30, loss = 0.05621332\n",
            "Iteration 31, loss = 0.05622920\n",
            "Iteration 32, loss = 0.05595538\n",
            "Iteration 33, loss = 0.05597064\n",
            "Iteration 34, loss = 0.05614387\n",
            "Iteration 35, loss = 0.05604464\n",
            "Iteration 36, loss = 0.05604317\n",
            "Iteration 37, loss = 0.05592810\n",
            "Iteration 38, loss = 0.05594188\n",
            "Iteration 39, loss = 0.05592750\n",
            "Iteration 40, loss = 0.05595837\n",
            "Iteration 41, loss = 0.05595628\n",
            "Iteration 42, loss = 0.05600693\n",
            "Iteration 43, loss = 0.05588667\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.05544051\n",
            "Iteration 45, loss = 0.05542036\n",
            "Iteration 46, loss = 0.05544266\n",
            "Iteration 47, loss = 0.05543411\n",
            "Iteration 48, loss = 0.05539375\n",
            "Iteration 49, loss = 0.05546975\n",
            "Iteration 50, loss = 0.05539091\n",
            "Iteration 51, loss = 0.05538297\n",
            "Iteration 52, loss = 0.05538277\n",
            "Iteration 53, loss = 0.05538965\n",
            "Iteration 54, loss = 0.05539214\n",
            "Iteration 55, loss = 0.05540768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 56, loss = 0.05525415\n",
            "Iteration 57, loss = 0.05524790\n",
            "Iteration 58, loss = 0.05524929\n",
            "Iteration 59, loss = 0.05525231\n",
            "Iteration 60, loss = 0.05524352\n",
            "Iteration 61, loss = 0.05523252\n",
            "Iteration 62, loss = 0.05522684\n",
            "Iteration 63, loss = 0.05524136\n",
            "Iteration 64, loss = 0.05523835\n",
            "Iteration 65, loss = 0.05522667\n",
            "Iteration 66, loss = 0.05522697\n",
            "Iteration 67, loss = 0.05522329\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 68, loss = 0.05521167\n",
            "Iteration 69, loss = 0.05520187\n",
            "Iteration 70, loss = 0.05520215\n",
            "Iteration 71, loss = 0.05519663\n",
            "Iteration 72, loss = 0.05520753\n",
            "Iteration 73, loss = 0.05520046\n",
            "Iteration 74, loss = 0.05519966\n",
            "Iteration 75, loss = 0.05520312\n",
            "Iteration 76, loss = 0.05520019\n",
            "Iteration 77, loss = 0.05520205\n",
            "Iteration 78, loss = 0.05519674\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 79, loss = 0.05519710\n",
            "Iteration 80, loss = 0.05519155\n",
            "Iteration 81, loss = 0.05519071\n",
            "Iteration 82, loss = 0.05519147\n",
            "Iteration 83, loss = 0.05519058\n",
            "Iteration 84, loss = 0.05519071\n",
            "Iteration 85, loss = 0.05519080\n",
            "Iteration 86, loss = 0.05519034\n",
            "Iteration 87, loss = 0.05519114\n",
            "Iteration 88, loss = 0.05519065\n",
            "Iteration 89, loss = 0.05519063\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 90, loss = 0.05519003\n",
            "Iteration 91, loss = 0.05518892\n",
            "Iteration 92, loss = 0.05518888\n",
            "Iteration 93, loss = 0.05518908\n",
            "Iteration 94, loss = 0.05518885\n",
            "Iteration 95, loss = 0.05518900\n",
            "Iteration 96, loss = 0.05518886\n",
            "Iteration 97, loss = 0.05518888\n",
            "Iteration 98, loss = 0.05518885\n",
            "Iteration 99, loss = 0.05518879\n",
            "Iteration 100, loss = 0.05518888\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.34852448\n",
            "Iteration 2, loss = 0.14233168\n",
            "Iteration 3, loss = 0.12098747\n",
            "Iteration 4, loss = 0.10525468\n",
            "Iteration 5, loss = 0.09317365\n",
            "Iteration 6, loss = 0.08436314\n",
            "Iteration 7, loss = 0.07752757\n",
            "Iteration 8, loss = 0.07271108\n",
            "Iteration 9, loss = 0.06843337\n",
            "Iteration 10, loss = 0.06547774\n",
            "Iteration 11, loss = 0.06330112\n",
            "Iteration 12, loss = 0.06164095\n",
            "Iteration 13, loss = 0.06033791\n",
            "Iteration 14, loss = 0.05925179\n",
            "Iteration 15, loss = 0.05861089\n",
            "Iteration 16, loss = 0.05789393\n",
            "Iteration 17, loss = 0.05765381\n",
            "Iteration 18, loss = 0.05733618\n",
            "Iteration 19, loss = 0.05693955\n",
            "Iteration 20, loss = 0.05673544\n",
            "Iteration 21, loss = 0.05667586\n",
            "Iteration 22, loss = 0.05655826\n",
            "Iteration 23, loss = 0.05630597\n",
            "Iteration 24, loss = 0.05622924\n",
            "Iteration 25, loss = 0.05630430\n",
            "Iteration 26, loss = 0.05619312\n",
            "Iteration 27, loss = 0.05624591\n",
            "Iteration 28, loss = 0.05601017\n",
            "Iteration 29, loss = 0.05615435\n",
            "Iteration 30, loss = 0.05604175\n",
            "Iteration 31, loss = 0.05610831\n",
            "Iteration 32, loss = 0.05610583\n",
            "Iteration 33, loss = 0.05601976\n",
            "Iteration 34, loss = 0.05623951\n",
            "Iteration 35, loss = 0.05622049\n",
            "Iteration 36, loss = 0.05604711\n",
            "Iteration 37, loss = 0.05604751\n",
            "Iteration 38, loss = 0.05603613\n",
            "Iteration 39, loss = 0.05605654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 40, loss = 0.05552982\n",
            "Iteration 41, loss = 0.05550707\n",
            "Iteration 42, loss = 0.05543829\n",
            "Iteration 43, loss = 0.05549942\n",
            "Iteration 44, loss = 0.05547152\n",
            "Iteration 45, loss = 0.05550435\n",
            "Iteration 46, loss = 0.05550412\n",
            "Iteration 47, loss = 0.05548969\n",
            "Iteration 48, loss = 0.05546201\n",
            "Iteration 49, loss = 0.05552625\n",
            "Iteration 50, loss = 0.05544155\n",
            "Iteration 51, loss = 0.05545696\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.05533351\n",
            "Iteration 53, loss = 0.05531434\n",
            "Iteration 54, loss = 0.05531539\n",
            "Iteration 55, loss = 0.05532046\n",
            "Iteration 56, loss = 0.05530479\n",
            "Iteration 57, loss = 0.05532661\n",
            "Iteration 58, loss = 0.05530985\n",
            "Iteration 59, loss = 0.05531237\n",
            "Iteration 60, loss = 0.05530889\n",
            "Iteration 61, loss = 0.05531978\n",
            "Iteration 62, loss = 0.05529180\n",
            "Iteration 63, loss = 0.05531505\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 64, loss = 0.05529156\n",
            "Iteration 65, loss = 0.05528321\n",
            "Iteration 66, loss = 0.05528204\n",
            "Iteration 67, loss = 0.05528289\n",
            "Iteration 68, loss = 0.05527952\n",
            "Iteration 69, loss = 0.05528084\n",
            "Iteration 70, loss = 0.05527924\n",
            "Iteration 71, loss = 0.05528118\n",
            "Iteration 72, loss = 0.05528104\n",
            "Iteration 73, loss = 0.05527871\n",
            "Iteration 74, loss = 0.05528062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 75, loss = 0.05527159\n",
            "Iteration 76, loss = 0.05527167\n",
            "Iteration 77, loss = 0.05527143\n",
            "Iteration 78, loss = 0.05527087\n",
            "Iteration 79, loss = 0.05527198\n",
            "Iteration 80, loss = 0.05527133\n",
            "Iteration 81, loss = 0.05527121\n",
            "Iteration 82, loss = 0.05527126\n",
            "Iteration 83, loss = 0.05526921\n",
            "Iteration 84, loss = 0.05527245\n",
            "Iteration 85, loss = 0.05527061\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 86, loss = 0.05526930\n",
            "Iteration 87, loss = 0.05526960\n",
            "Iteration 88, loss = 0.05526955\n",
            "Iteration 89, loss = 0.05526951\n",
            "Iteration 90, loss = 0.05526942\n",
            "Iteration 91, loss = 0.05526958\n",
            "Iteration 92, loss = 0.05526943\n",
            "Iteration 93, loss = 0.05526933\n",
            "Iteration 94, loss = 0.05526943\n",
            "Iteration 95, loss = 0.05526955\n",
            "Iteration 96, loss = 0.05526941\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.36239549\n",
            "Iteration 2, loss = 0.13512070\n",
            "Iteration 3, loss = 0.11573757\n",
            "Iteration 4, loss = 0.10139725\n",
            "Iteration 5, loss = 0.09085211\n",
            "Iteration 6, loss = 0.08246150\n",
            "Iteration 7, loss = 0.07618865\n",
            "Iteration 8, loss = 0.07159429\n",
            "Iteration 9, loss = 0.06799273\n",
            "Iteration 10, loss = 0.06509999\n",
            "Iteration 11, loss = 0.06323667\n",
            "Iteration 12, loss = 0.06149356\n",
            "Iteration 13, loss = 0.06023499\n",
            "Iteration 14, loss = 0.05936083\n",
            "Iteration 15, loss = 0.05884980\n",
            "Iteration 16, loss = 0.05817962\n",
            "Iteration 17, loss = 0.05773467\n",
            "Iteration 18, loss = 0.05756402\n",
            "Iteration 19, loss = 0.05728176\n",
            "Iteration 20, loss = 0.05708278\n",
            "Iteration 21, loss = 0.05691664\n",
            "Iteration 22, loss = 0.05672589\n",
            "Iteration 23, loss = 0.05654454\n",
            "Iteration 24, loss = 0.05652221\n",
            "Iteration 25, loss = 0.05653058\n",
            "Iteration 26, loss = 0.05642143\n",
            "Iteration 27, loss = 0.05632355\n",
            "Iteration 28, loss = 0.05639180\n",
            "Iteration 29, loss = 0.05624963\n",
            "Iteration 30, loss = 0.05641755\n",
            "Iteration 31, loss = 0.05617499\n",
            "Iteration 32, loss = 0.05631560\n",
            "Iteration 33, loss = 0.05614845\n",
            "Iteration 34, loss = 0.05637335\n",
            "Iteration 35, loss = 0.05625583\n",
            "Iteration 36, loss = 0.05634282\n",
            "Iteration 37, loss = 0.05628719\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.05570675\n",
            "Iteration 39, loss = 0.05575567\n",
            "Iteration 40, loss = 0.05574606\n",
            "Iteration 41, loss = 0.05568937\n",
            "Iteration 42, loss = 0.05571878\n",
            "Iteration 43, loss = 0.05572612\n",
            "Iteration 44, loss = 0.05567314\n",
            "Iteration 45, loss = 0.05567857\n",
            "Iteration 46, loss = 0.05568851\n",
            "Iteration 47, loss = 0.05570142\n",
            "Iteration 48, loss = 0.05573634\n",
            "Iteration 49, loss = 0.05568260\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 50, loss = 0.05552689\n",
            "Iteration 51, loss = 0.05553407\n",
            "Iteration 52, loss = 0.05552460\n",
            "Iteration 53, loss = 0.05552519\n",
            "Iteration 54, loss = 0.05550877\n",
            "Iteration 55, loss = 0.05552119\n",
            "Iteration 56, loss = 0.05552233\n",
            "Iteration 57, loss = 0.05551935\n",
            "Iteration 58, loss = 0.05551728\n",
            "Iteration 59, loss = 0.05552548\n",
            "Iteration 60, loss = 0.05552052\n",
            "Iteration 61, loss = 0.05552094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 62, loss = 0.05548992\n",
            "Iteration 63, loss = 0.05548749\n",
            "Iteration 64, loss = 0.05548811\n",
            "Iteration 65, loss = 0.05548982\n",
            "Iteration 66, loss = 0.05549017\n",
            "Iteration 67, loss = 0.05548486\n",
            "Iteration 68, loss = 0.05548345\n",
            "Iteration 69, loss = 0.05548794\n",
            "Iteration 70, loss = 0.05548806\n",
            "Iteration 71, loss = 0.05549172\n",
            "Iteration 72, loss = 0.05548651\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 73, loss = 0.05547369\n",
            "Iteration 74, loss = 0.05547871\n",
            "Iteration 75, loss = 0.05547633\n",
            "Iteration 76, loss = 0.05547749\n",
            "Iteration 77, loss = 0.05547704\n",
            "Iteration 78, loss = 0.05547587\n",
            "Iteration 79, loss = 0.05547574\n",
            "Iteration 80, loss = 0.05547633\n",
            "Iteration 81, loss = 0.05547610\n",
            "Iteration 82, loss = 0.05547564\n",
            "Iteration 83, loss = 0.05547601\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 84, loss = 0.05547445\n",
            "Iteration 85, loss = 0.05547437\n",
            "Iteration 86, loss = 0.05547439\n",
            "Iteration 87, loss = 0.05547429\n",
            "Iteration 88, loss = 0.05547425\n",
            "Iteration 89, loss = 0.05547410\n",
            "Iteration 90, loss = 0.05547412\n",
            "Iteration 91, loss = 0.05547413\n",
            "Iteration 92, loss = 0.05547391\n",
            "Iteration 93, loss = 0.05547425\n",
            "Iteration 94, loss = 0.05547413\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.14954238\n",
            "Iteration 3, loss = 0.12670269\n",
            "Iteration 4, loss = 0.10930077\n",
            "Iteration 5, loss = 0.09678332\n",
            "Iteration 6, loss = 0.08674808\n",
            "Iteration 7, loss = 0.07949527\n",
            "Iteration 8, loss = 0.07394350\n",
            "Iteration 9, loss = 0.06962972\n",
            "Iteration 10, loss = 0.06628953\n",
            "Iteration 11, loss = 0.06386004\n",
            "Iteration 12, loss = 0.06209616\n",
            "Iteration 13, loss = 0.06074661\n",
            "Iteration 14, loss = 0.05969063\n",
            "Iteration 15, loss = 0.05890137\n",
            "Iteration 16, loss = 0.05810509\n",
            "Iteration 17, loss = 0.05748393\n",
            "Iteration 18, loss = 0.05717607\n",
            "Iteration 19, loss = 0.05710005\n",
            "Iteration 20, loss = 0.05687320\n",
            "Iteration 21, loss = 0.05672774\n",
            "Iteration 22, loss = 0.05651087\n",
            "Iteration 23, loss = 0.05647037\n",
            "Iteration 24, loss = 0.05621979\n",
            "Iteration 25, loss = 0.05611307\n",
            "Iteration 26, loss = 0.05647320\n",
            "Iteration 27, loss = 0.05643413\n",
            "Iteration 28, loss = 0.05609305\n",
            "Iteration 29, loss = 0.05609537\n",
            "Iteration 30, loss = 0.05604110\n",
            "Iteration 31, loss = 0.05613253\n",
            "Iteration 32, loss = 0.05605096\n",
            "Iteration 33, loss = 0.05609204\n",
            "Iteration 34, loss = 0.05604277\n",
            "Iteration 35, loss = 0.05590895\n",
            "Iteration 36, loss = 0.05598432\n",
            "Iteration 37, loss = 0.05592744\n",
            "Iteration 38, loss = 0.05588091\n",
            "Iteration 39, loss = 0.05588538\n",
            "Iteration 40, loss = 0.05596836\n",
            "Iteration 41, loss = 0.05596432\n",
            "Iteration 42, loss = 0.05596732\n",
            "Iteration 43, loss = 0.05585930\n",
            "Iteration 44, loss = 0.05586940\n",
            "Iteration 45, loss = 0.05589877\n",
            "Iteration 46, loss = 0.05583438\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.05536958\n",
            "Iteration 48, loss = 0.05541103\n",
            "Iteration 49, loss = 0.05534303\n",
            "Iteration 50, loss = 0.05536813\n",
            "Iteration 51, loss = 0.05533153\n",
            "Iteration 52, loss = 0.05540966\n",
            "Iteration 53, loss = 0.05539865\n",
            "Iteration 54, loss = 0.05538846\n",
            "Iteration 55, loss = 0.05535066\n",
            "Iteration 56, loss = 0.05536914\n",
            "Iteration 57, loss = 0.05548730\n",
            "Iteration 58, loss = 0.05541263\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.05522506\n",
            "Iteration 60, loss = 0.05520180\n",
            "Iteration 61, loss = 0.05520415\n",
            "Iteration 62, loss = 0.05520469\n",
            "Iteration 63, loss = 0.05522562\n",
            "Iteration 64, loss = 0.05520485\n",
            "Iteration 65, loss = 0.05524078\n",
            "Iteration 66, loss = 0.05521553\n",
            "Iteration 67, loss = 0.05521342\n",
            "Iteration 68, loss = 0.05521008\n",
            "Iteration 69, loss = 0.05521317\n",
            "Iteration 70, loss = 0.05520470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.05518644\n",
            "Iteration 72, loss = 0.05518232\n",
            "Iteration 73, loss = 0.05517887\n",
            "Iteration 74, loss = 0.05517692\n",
            "Iteration 75, loss = 0.05517626\n",
            "Iteration 76, loss = 0.05517939\n",
            "Iteration 77, loss = 0.05518226\n",
            "Iteration 78, loss = 0.05517777\n",
            "Iteration 79, loss = 0.05518038\n",
            "Iteration 80, loss = 0.05517815\n",
            "Iteration 81, loss = 0.05517862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 82, loss = 0.05516971\n",
            "Iteration 83, loss = 0.05516905\n",
            "Iteration 84, loss = 0.05516935\n",
            "Iteration 85, loss = 0.05516926\n",
            "Iteration 86, loss = 0.05516879\n",
            "Iteration 87, loss = 0.05516998\n",
            "Iteration 88, loss = 0.05516903\n",
            "Iteration 89, loss = 0.05516911\n",
            "Iteration 90, loss = 0.05516934\n",
            "Iteration 91, loss = 0.05516841\n",
            "Iteration 92, loss = 0.05516853\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 93, loss = 0.05516809\n",
            "Iteration 94, loss = 0.05516733\n",
            "Iteration 95, loss = 0.05516726\n",
            "Iteration 96, loss = 0.05516719\n",
            "Iteration 97, loss = 0.05516748\n",
            "Iteration 98, loss = 0.05516730\n",
            "Iteration 99, loss = 0.05516741\n",
            "Iteration 100, loss = 0.05516729\n",
            "Iteration 101, loss = 0.05516732\n",
            "Iteration 102, loss = 0.05516732\n",
            "Iteration 103, loss = 0.05516730\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 23.0min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.03036843\n",
            "Iteration 2, loss = 0.17630195\n",
            "Iteration 3, loss = 0.16485098\n",
            "Iteration 4, loss = 0.15522861\n",
            "Iteration 5, loss = 0.14787368\n",
            "Iteration 6, loss = 0.14070830\n",
            "Iteration 7, loss = 0.13410011\n",
            "Iteration 8, loss = 0.12752084\n",
            "Iteration 9, loss = 0.12233399\n",
            "Iteration 10, loss = 0.11686681\n",
            "Iteration 11, loss = 0.11226550\n",
            "Iteration 12, loss = 0.10821973\n",
            "Iteration 13, loss = 0.10395740\n",
            "Iteration 14, loss = 0.10004492\n",
            "Iteration 15, loss = 0.09674070\n",
            "Iteration 16, loss = 0.09312979\n",
            "Iteration 17, loss = 0.09103807\n",
            "Iteration 18, loss = 0.08855662\n",
            "Iteration 19, loss = 0.08574281\n",
            "Iteration 20, loss = 0.08380435\n",
            "Iteration 21, loss = 0.08207706\n",
            "Iteration 22, loss = 0.07977847\n",
            "Iteration 23, loss = 0.07804645\n",
            "Iteration 24, loss = 0.07648727\n",
            "Iteration 25, loss = 0.07483238\n",
            "Iteration 26, loss = 0.07378160\n",
            "Iteration 27, loss = 0.07264174\n",
            "Iteration 28, loss = 0.07086966\n",
            "Iteration 29, loss = 0.06994415\n",
            "Iteration 30, loss = 0.06933702\n",
            "Iteration 31, loss = 0.06815720\n",
            "Iteration 32, loss = 0.06739406\n",
            "Iteration 33, loss = 0.06632753\n",
            "Iteration 34, loss = 0.06621991\n",
            "Iteration 35, loss = 0.06523154\n",
            "Iteration 36, loss = 0.06474563\n",
            "Iteration 37, loss = 0.06402519\n",
            "Iteration 38, loss = 0.06392339\n",
            "Iteration 39, loss = 0.06345440\n",
            "Iteration 40, loss = 0.06297108\n",
            "Iteration 41, loss = 0.06245676\n",
            "Iteration 42, loss = 0.06179218\n",
            "Iteration 43, loss = 0.06153760\n",
            "Iteration 44, loss = 0.06100352\n",
            "Iteration 45, loss = 0.06140599\n",
            "Iteration 46, loss = 0.06068393\n",
            "Iteration 47, loss = 0.06062508\n",
            "Iteration 48, loss = 0.06102347\n",
            "Iteration 49, loss = 0.06010927\n",
            "Iteration 50, loss = 0.06031219\n",
            "Iteration 51, loss = 0.05954002\n",
            "Iteration 52, loss = 0.05938091\n",
            "Iteration 53, loss = 0.05948430\n",
            "Iteration 54, loss = 0.05935380\n",
            "Iteration 55, loss = 0.05904455\n",
            "Iteration 56, loss = 0.05896561\n",
            "Iteration 57, loss = 0.05892124\n",
            "Iteration 58, loss = 0.05875612\n",
            "Iteration 59, loss = 0.05859901\n",
            "Iteration 60, loss = 0.05856104\n",
            "Iteration 61, loss = 0.05855676\n",
            "Iteration 62, loss = 0.05843402\n",
            "Iteration 63, loss = 0.05819273\n",
            "Iteration 64, loss = 0.05816176\n",
            "Iteration 65, loss = 0.05801224\n",
            "Iteration 66, loss = 0.05837992\n",
            "Iteration 67, loss = 0.05806060\n",
            "Iteration 68, loss = 0.05822015\n",
            "Iteration 69, loss = 0.05797927\n",
            "Iteration 70, loss = 0.05803041\n",
            "Iteration 71, loss = 0.05782699\n",
            "Iteration 72, loss = 0.05774874\n",
            "Iteration 73, loss = 0.05773535\n",
            "Iteration 74, loss = 0.05798414\n",
            "Iteration 75, loss = 0.05776989\n",
            "Iteration 76, loss = 0.05796913\n",
            "Iteration 77, loss = 0.05775124\n",
            "Iteration 78, loss = 0.05796982\n",
            "Iteration 79, loss = 0.05783966\n",
            "Iteration 80, loss = 0.05763662\n",
            "Iteration 81, loss = 0.05760102\n",
            "Iteration 82, loss = 0.05763080\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 83, loss = 0.05733265\n",
            "Iteration 84, loss = 0.05704316\n",
            "Iteration 85, loss = 0.05703145\n",
            "Iteration 86, loss = 0.05700936\n",
            "Iteration 87, loss = 0.05698428\n",
            "Iteration 88, loss = 0.05694901\n",
            "Iteration 89, loss = 0.05701388\n",
            "Iteration 90, loss = 0.05716085\n",
            "Iteration 91, loss = 0.05698820\n",
            "Iteration 92, loss = 0.05697286\n",
            "Iteration 93, loss = 0.05691550\n",
            "Iteration 94, loss = 0.05698719\n",
            "Iteration 95, loss = 0.05704779\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.05689036\n",
            "Iteration 97, loss = 0.05685840\n",
            "Iteration 98, loss = 0.05679130\n",
            "Iteration 99, loss = 0.05683469\n",
            "Iteration 100, loss = 0.05677730\n",
            "Iteration 101, loss = 0.05679945\n",
            "Iteration 102, loss = 0.05679916\n",
            "Iteration 103, loss = 0.05681328\n",
            "Iteration 104, loss = 0.05675628\n",
            "Iteration 105, loss = 0.05681359\n",
            "Iteration 106, loss = 0.05675248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 107, loss = 0.05680980\n",
            "Iteration 108, loss = 0.05674204\n",
            "Iteration 109, loss = 0.05674128\n",
            "Iteration 110, loss = 0.05673842\n",
            "Iteration 111, loss = 0.05673737\n",
            "Iteration 112, loss = 0.05674372\n",
            "Iteration 113, loss = 0.05674229\n",
            "Iteration 114, loss = 0.05674350\n",
            "Iteration 115, loss = 0.05674238\n",
            "Iteration 116, loss = 0.05674244\n",
            "Iteration 117, loss = 0.05673575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 118, loss = 0.05673161\n",
            "Iteration 119, loss = 0.05673071\n",
            "Iteration 120, loss = 0.05672924\n",
            "Iteration 121, loss = 0.05673006\n",
            "Iteration 122, loss = 0.05672933\n",
            "Iteration 123, loss = 0.05673056\n",
            "Iteration 124, loss = 0.05672922\n",
            "Iteration 125, loss = 0.05672876\n",
            "Iteration 126, loss = 0.05672965\n",
            "Iteration 127, loss = 0.05672902\n",
            "Iteration 128, loss = 0.05672850\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 129, loss = 0.05672629\n",
            "Iteration 130, loss = 0.05672663\n",
            "Iteration 131, loss = 0.05672662\n",
            "Iteration 132, loss = 0.05672686\n",
            "Iteration 133, loss = 0.05672663\n",
            "Iteration 134, loss = 0.05672648\n",
            "Iteration 135, loss = 0.05672634\n",
            "Iteration 136, loss = 0.05672645\n",
            "Iteration 137, loss = 0.05672653\n",
            "Iteration 138, loss = 0.05672675\n",
            "Iteration 139, loss = 0.05672672\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 2.25115393\n",
            "Iteration 2, loss = 0.23129263\n",
            "Iteration 3, loss = 0.21743219\n",
            "Iteration 4, loss = 0.20466430\n",
            "Iteration 5, loss = 0.19317165\n",
            "Iteration 6, loss = 0.18134501\n",
            "Iteration 7, loss = 0.17144069\n",
            "Iteration 8, loss = 0.16266390\n",
            "Iteration 9, loss = 0.15431976\n",
            "Iteration 10, loss = 0.14692306\n",
            "Iteration 11, loss = 0.13955302\n",
            "Iteration 12, loss = 0.13379786\n",
            "Iteration 13, loss = 0.12739800\n",
            "Iteration 14, loss = 0.12236340\n",
            "Iteration 15, loss = 0.11702141\n",
            "Iteration 16, loss = 0.11246346\n",
            "Iteration 17, loss = 0.10848035\n",
            "Iteration 18, loss = 0.10423118\n",
            "Iteration 19, loss = 0.10078286\n",
            "Iteration 20, loss = 0.09744761\n",
            "Iteration 21, loss = 0.09439016\n",
            "Iteration 22, loss = 0.09130575\n",
            "Iteration 23, loss = 0.08873512\n",
            "Iteration 24, loss = 0.08624744\n",
            "Iteration 25, loss = 0.08399067\n",
            "Iteration 26, loss = 0.08213776\n",
            "Iteration 27, loss = 0.08013159\n",
            "Iteration 28, loss = 0.07828961\n",
            "Iteration 29, loss = 0.07697936\n",
            "Iteration 30, loss = 0.07550542\n",
            "Iteration 31, loss = 0.07423926\n",
            "Iteration 32, loss = 0.07322086\n",
            "Iteration 33, loss = 0.07172277\n",
            "Iteration 34, loss = 0.07025519\n",
            "Iteration 35, loss = 0.06957572\n",
            "Iteration 36, loss = 0.06897317\n",
            "Iteration 37, loss = 0.06802760\n",
            "Iteration 38, loss = 0.06700558\n",
            "Iteration 39, loss = 0.06632672\n",
            "Iteration 40, loss = 0.06538846\n",
            "Iteration 41, loss = 0.06522352\n",
            "Iteration 42, loss = 0.06434506\n",
            "Iteration 43, loss = 0.06377252\n",
            "Iteration 44, loss = 0.06334385\n",
            "Iteration 45, loss = 0.06305635\n",
            "Iteration 46, loss = 0.06254626\n",
            "Iteration 47, loss = 0.06211280\n",
            "Iteration 48, loss = 0.06156590\n",
            "Iteration 49, loss = 0.06165178\n",
            "Iteration 50, loss = 0.06105020\n",
            "Iteration 51, loss = 0.06064024\n",
            "Iteration 52, loss = 0.06040491\n",
            "Iteration 53, loss = 0.06023283\n",
            "Iteration 54, loss = 0.06023006\n",
            "Iteration 55, loss = 0.05969281\n",
            "Iteration 56, loss = 0.05970869\n",
            "Iteration 57, loss = 0.05967882\n",
            "Iteration 58, loss = 0.05925588\n",
            "Iteration 59, loss = 0.05903033\n",
            "Iteration 60, loss = 0.05939296\n",
            "Iteration 61, loss = 0.05884301\n",
            "Iteration 62, loss = 0.05863349\n",
            "Iteration 63, loss = 0.05873467\n",
            "Iteration 64, loss = 0.05866330\n",
            "Iteration 65, loss = 0.05849746\n",
            "Iteration 66, loss = 0.05847711\n",
            "Iteration 67, loss = 0.05861084\n",
            "Iteration 68, loss = 0.05796436\n",
            "Iteration 69, loss = 0.05833345\n",
            "Iteration 70, loss = 0.05837061\n",
            "Iteration 71, loss = 0.05812433\n",
            "Iteration 72, loss = 0.05775623\n",
            "Iteration 73, loss = 0.05786100\n",
            "Iteration 74, loss = 0.05787594\n",
            "Iteration 75, loss = 0.05767449\n",
            "Iteration 76, loss = 0.05786975\n",
            "Iteration 77, loss = 0.05768112\n",
            "Iteration 78, loss = 0.05763627\n",
            "Iteration 79, loss = 0.05786516\n",
            "Iteration 80, loss = 0.05770268\n",
            "Iteration 81, loss = 0.05792884\n",
            "Iteration 82, loss = 0.05800807\n",
            "Iteration 83, loss = 0.05788471\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 84, loss = 0.05691324\n",
            "Iteration 85, loss = 0.05706810\n",
            "Iteration 86, loss = 0.05691685\n",
            "Iteration 87, loss = 0.05700684\n",
            "Iteration 88, loss = 0.05692061\n",
            "Iteration 89, loss = 0.05702460\n",
            "Iteration 90, loss = 0.05695830\n",
            "Iteration 91, loss = 0.05698336\n",
            "Iteration 92, loss = 0.05696167\n",
            "Iteration 93, loss = 0.05690281\n",
            "Iteration 94, loss = 0.05688629\n",
            "Iteration 95, loss = 0.05697746\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.05674947\n",
            "Iteration 97, loss = 0.05672475\n",
            "Iteration 98, loss = 0.05676009\n",
            "Iteration 99, loss = 0.05675158\n",
            "Iteration 100, loss = 0.05672968\n",
            "Iteration 101, loss = 0.05672439\n",
            "Iteration 102, loss = 0.05672126\n",
            "Iteration 103, loss = 0.05672976\n",
            "Iteration 104, loss = 0.05671561\n",
            "Iteration 105, loss = 0.05672490\n",
            "Iteration 106, loss = 0.05674201\n",
            "Iteration 107, loss = 0.05674252\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 108, loss = 0.05667635\n",
            "Iteration 109, loss = 0.05668456\n",
            "Iteration 110, loss = 0.05666928\n",
            "Iteration 111, loss = 0.05668235\n",
            "Iteration 112, loss = 0.05666935\n",
            "Iteration 113, loss = 0.05667216\n",
            "Iteration 114, loss = 0.05666908\n",
            "Iteration 115, loss = 0.05667749\n",
            "Iteration 116, loss = 0.05667131\n",
            "Iteration 117, loss = 0.05666044\n",
            "Iteration 118, loss = 0.05667151\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 119, loss = 0.05666167\n",
            "Iteration 120, loss = 0.05665929\n",
            "Iteration 121, loss = 0.05665849\n",
            "Iteration 122, loss = 0.05665814\n",
            "Iteration 123, loss = 0.05665817\n",
            "Iteration 124, loss = 0.05665675\n",
            "Iteration 125, loss = 0.05665745\n",
            "Iteration 126, loss = 0.05665758\n",
            "Iteration 127, loss = 0.05665688\n",
            "Iteration 128, loss = 0.05665642\n",
            "Iteration 129, loss = 0.05665801\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 130, loss = 0.05665485\n",
            "Iteration 131, loss = 0.05665449\n",
            "Iteration 132, loss = 0.05665458\n",
            "Iteration 133, loss = 0.05665449\n",
            "Iteration 134, loss = 0.05665465\n",
            "Iteration 135, loss = 0.05665463\n",
            "Iteration 136, loss = 0.05665491\n",
            "Iteration 137, loss = 0.05665455\n",
            "Iteration 138, loss = 0.05665450\n",
            "Iteration 139, loss = 0.05665531\n",
            "Iteration 140, loss = 0.05665439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23053737\n",
            "Iteration 3, loss = 0.21685410\n",
            "Iteration 4, loss = 0.20389647\n",
            "Iteration 5, loss = 0.19228544\n",
            "Iteration 6, loss = 0.18172453\n",
            "Iteration 7, loss = 0.17193536\n",
            "Iteration 8, loss = 0.16278262\n",
            "Iteration 9, loss = 0.15474141\n",
            "Iteration 10, loss = 0.14685718\n",
            "Iteration 11, loss = 0.13952838\n",
            "Iteration 12, loss = 0.13301452\n",
            "Iteration 13, loss = 0.12746949\n",
            "Iteration 14, loss = 0.12171743\n",
            "Iteration 15, loss = 0.11687765\n",
            "Iteration 16, loss = 0.11198492\n",
            "Iteration 17, loss = 0.10783438\n",
            "Iteration 18, loss = 0.10397652\n",
            "Iteration 19, loss = 0.10025781\n",
            "Iteration 20, loss = 0.09687397\n",
            "Iteration 21, loss = 0.09403034\n",
            "Iteration 22, loss = 0.09105316\n",
            "Iteration 23, loss = 0.08840542\n",
            "Iteration 24, loss = 0.08601856\n",
            "Iteration 25, loss = 0.08404108\n",
            "Iteration 26, loss = 0.08157034\n",
            "Iteration 27, loss = 0.07974782\n",
            "Iteration 28, loss = 0.07800081\n",
            "Iteration 29, loss = 0.07648570\n",
            "Iteration 30, loss = 0.07511529\n",
            "Iteration 31, loss = 0.07355044\n",
            "Iteration 32, loss = 0.07224553\n",
            "Iteration 33, loss = 0.07131847\n",
            "Iteration 34, loss = 0.06982806\n",
            "Iteration 35, loss = 0.06903247\n",
            "Iteration 36, loss = 0.06807882\n",
            "Iteration 37, loss = 0.06704148\n",
            "Iteration 38, loss = 0.06647662\n",
            "Iteration 39, loss = 0.06568110\n",
            "Iteration 40, loss = 0.06488632\n",
            "Iteration 41, loss = 0.06441555\n",
            "Iteration 42, loss = 0.06392731\n",
            "Iteration 43, loss = 0.06335177\n",
            "Iteration 44, loss = 0.06275808\n",
            "Iteration 45, loss = 0.06232478\n",
            "Iteration 46, loss = 0.06174997\n",
            "Iteration 47, loss = 0.06151532\n",
            "Iteration 48, loss = 0.06131978\n",
            "Iteration 49, loss = 0.06098799\n",
            "Iteration 50, loss = 0.06064124\n",
            "Iteration 51, loss = 0.06045316\n",
            "Iteration 52, loss = 0.05973493\n",
            "Iteration 53, loss = 0.05969441\n",
            "Iteration 54, loss = 0.05972527\n",
            "Iteration 55, loss = 0.05967078\n",
            "Iteration 56, loss = 0.05953202\n",
            "Iteration 57, loss = 0.05918313\n",
            "Iteration 58, loss = 0.05885117\n",
            "Iteration 59, loss = 0.05878501\n",
            "Iteration 60, loss = 0.05847630\n",
            "Iteration 61, loss = 0.05818544\n",
            "Iteration 62, loss = 0.05858810\n",
            "Iteration 63, loss = 0.05867214\n",
            "Iteration 64, loss = 0.05793593\n",
            "Iteration 65, loss = 0.05822569\n",
            "Iteration 66, loss = 0.05822273\n",
            "Iteration 67, loss = 0.05792396\n",
            "Iteration 68, loss = 0.05782423\n",
            "Iteration 69, loss = 0.05785102\n",
            "Iteration 70, loss = 0.05807293\n",
            "Iteration 71, loss = 0.05753097\n",
            "Iteration 72, loss = 0.05778794\n",
            "Iteration 73, loss = 0.05767321\n",
            "Iteration 74, loss = 0.05773048\n",
            "Iteration 75, loss = 0.05762593\n",
            "Iteration 76, loss = 0.05733006\n",
            "Iteration 77, loss = 0.05736125\n",
            "Iteration 78, loss = 0.05705538\n",
            "Iteration 79, loss = 0.05734380\n",
            "Iteration 80, loss = 0.05737823\n",
            "Iteration 81, loss = 0.05720523\n",
            "Iteration 82, loss = 0.05733407\n",
            "Iteration 83, loss = 0.05698881\n",
            "Iteration 84, loss = 0.05725750\n",
            "Iteration 85, loss = 0.05711579\n",
            "Iteration 86, loss = 0.05711622\n",
            "Iteration 87, loss = 0.05722377\n",
            "Iteration 88, loss = 0.05709360\n",
            "Iteration 89, loss = 0.05683728\n",
            "Iteration 90, loss = 0.05711345\n",
            "Iteration 91, loss = 0.05696338\n",
            "Iteration 92, loss = 0.05704693\n",
            "Iteration 93, loss = 0.05733948\n",
            "Iteration 94, loss = 0.05724971\n",
            "Iteration 95, loss = 0.05695125\n",
            "Iteration 96, loss = 0.05714985\n",
            "Iteration 97, loss = 0.05692841\n",
            "Iteration 98, loss = 0.05710645\n",
            "Iteration 99, loss = 0.05693769\n",
            "Iteration 100, loss = 0.05722395\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 101, loss = 0.05654719\n",
            "Iteration 102, loss = 0.05647213\n",
            "Iteration 103, loss = 0.05633107\n",
            "Iteration 104, loss = 0.05629268\n",
            "Iteration 105, loss = 0.05641645\n",
            "Iteration 106, loss = 0.05636934\n",
            "Iteration 107, loss = 0.05644413\n",
            "Iteration 108, loss = 0.05628281\n",
            "Iteration 109, loss = 0.05636994\n",
            "Iteration 110, loss = 0.05639027\n",
            "Iteration 111, loss = 0.05626293\n",
            "Iteration 112, loss = 0.05638911\n",
            "Iteration 113, loss = 0.05631521\n",
            "Iteration 114, loss = 0.05630277\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 115, loss = 0.05618820\n",
            "Iteration 116, loss = 0.05616606\n",
            "Iteration 117, loss = 0.05620496\n",
            "Iteration 118, loss = 0.05615861\n",
            "Iteration 119, loss = 0.05618153\n",
            "Iteration 120, loss = 0.05621757\n",
            "Iteration 121, loss = 0.05618680\n",
            "Iteration 122, loss = 0.05615817\n",
            "Iteration 123, loss = 0.05615391\n",
            "Iteration 124, loss = 0.05619683\n",
            "Iteration 125, loss = 0.05615600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 126, loss = 0.05612485\n",
            "Iteration 127, loss = 0.05612539\n",
            "Iteration 128, loss = 0.05612074\n",
            "Iteration 129, loss = 0.05612437\n",
            "Iteration 130, loss = 0.05611956\n",
            "Iteration 131, loss = 0.05612116\n",
            "Iteration 132, loss = 0.05612320\n",
            "Iteration 133, loss = 0.05612877\n",
            "Iteration 134, loss = 0.05612640\n",
            "Iteration 135, loss = 0.05612467\n",
            "Iteration 136, loss = 0.05611891\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 137, loss = 0.05611494\n",
            "Iteration 138, loss = 0.05611320\n",
            "Iteration 139, loss = 0.05611243\n",
            "Iteration 140, loss = 0.05611284\n",
            "Iteration 141, loss = 0.05611290\n",
            "Iteration 142, loss = 0.05611256\n",
            "Iteration 143, loss = 0.05611241\n",
            "Iteration 144, loss = 0.05611373\n",
            "Iteration 145, loss = 0.05611331\n",
            "Iteration 146, loss = 0.05611329\n",
            "Iteration 147, loss = 0.05611136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 148, loss = 0.05611021\n",
            "Iteration 149, loss = 0.05611067\n",
            "Iteration 150, loss = 0.05611016\n",
            "Iteration 151, loss = 0.05611055\n",
            "Iteration 152, loss = 0.05611093\n",
            "Iteration 153, loss = 0.05611057\n",
            "Iteration 154, loss = 0.05611029\n",
            "Iteration 155, loss = 0.05611052\n",
            "Iteration 156, loss = 0.05611046\n",
            "Iteration 157, loss = 0.05611040\n",
            "Iteration 158, loss = 0.05611066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.27783520\n",
            "Iteration 2, loss = 0.18299553\n",
            "Iteration 3, loss = 0.17221641\n",
            "Iteration 4, loss = 0.16344654\n",
            "Iteration 5, loss = 0.15463763\n",
            "Iteration 6, loss = 0.14658474\n",
            "Iteration 7, loss = 0.13988218\n",
            "Iteration 8, loss = 0.13337033\n",
            "Iteration 9, loss = 0.12750628\n",
            "Iteration 10, loss = 0.12153858\n",
            "Iteration 11, loss = 0.11673791\n",
            "Iteration 12, loss = 0.11215341\n",
            "Iteration 13, loss = 0.10791953\n",
            "Iteration 14, loss = 0.10408419\n",
            "Iteration 15, loss = 0.10073628\n",
            "Iteration 16, loss = 0.09722904\n",
            "Iteration 17, loss = 0.09409101\n",
            "Iteration 18, loss = 0.09126391\n",
            "Iteration 19, loss = 0.08853724\n",
            "Iteration 20, loss = 0.08626778\n",
            "Iteration 21, loss = 0.08386685\n",
            "Iteration 22, loss = 0.08222454\n",
            "Iteration 23, loss = 0.08046078\n",
            "Iteration 24, loss = 0.07837490\n",
            "Iteration 25, loss = 0.07709634\n",
            "Iteration 26, loss = 0.07544863\n",
            "Iteration 27, loss = 0.07442296\n",
            "Iteration 28, loss = 0.07347114\n",
            "Iteration 29, loss = 0.07194504\n",
            "Iteration 30, loss = 0.07028836\n",
            "Iteration 31, loss = 0.06977768\n",
            "Iteration 32, loss = 0.06886325\n",
            "Iteration 33, loss = 0.06804877\n",
            "Iteration 34, loss = 0.06743163\n",
            "Iteration 35, loss = 0.06660951\n",
            "Iteration 36, loss = 0.06617137\n",
            "Iteration 37, loss = 0.06565604\n",
            "Iteration 38, loss = 0.06536120\n",
            "Iteration 39, loss = 0.06461891\n",
            "Iteration 40, loss = 0.06411949\n",
            "Iteration 41, loss = 0.06373340\n",
            "Iteration 42, loss = 0.06344776\n",
            "Iteration 43, loss = 0.06269452\n",
            "Iteration 44, loss = 0.06241126\n",
            "Iteration 45, loss = 0.06191994\n",
            "Iteration 46, loss = 0.06189619\n",
            "Iteration 47, loss = 0.06204505\n",
            "Iteration 48, loss = 0.06124212\n",
            "Iteration 49, loss = 0.06068581\n",
            "Iteration 50, loss = 0.06101210\n",
            "Iteration 51, loss = 0.06033987\n",
            "Iteration 52, loss = 0.06067858\n",
            "Iteration 53, loss = 0.06035426\n",
            "Iteration 54, loss = 0.06013590\n",
            "Iteration 55, loss = 0.05948734\n",
            "Iteration 56, loss = 0.05948412\n",
            "Iteration 57, loss = 0.05958142\n",
            "Iteration 58, loss = 0.05972389\n",
            "Iteration 59, loss = 0.05940612\n",
            "Iteration 60, loss = 0.05936063\n",
            "Iteration 61, loss = 0.05933603\n",
            "Iteration 62, loss = 0.05932169\n",
            "Iteration 63, loss = 0.05872477\n",
            "Iteration 64, loss = 0.05921639\n",
            "Iteration 65, loss = 0.05890322\n",
            "Iteration 66, loss = 0.05857196\n",
            "Iteration 67, loss = 0.05841326\n",
            "Iteration 68, loss = 0.05856709\n",
            "Iteration 69, loss = 0.05844682\n",
            "Iteration 70, loss = 0.05844600\n",
            "Iteration 71, loss = 0.05819675\n",
            "Iteration 72, loss = 0.05859529\n",
            "Iteration 73, loss = 0.05826658\n",
            "Iteration 74, loss = 0.05827860\n",
            "Iteration 75, loss = 0.05849879\n",
            "Iteration 76, loss = 0.05810325\n",
            "Iteration 77, loss = 0.05817081\n",
            "Iteration 78, loss = 0.05850401\n",
            "Iteration 79, loss = 0.05801827\n",
            "Iteration 80, loss = 0.05850854\n",
            "Iteration 81, loss = 0.05807696\n",
            "Iteration 82, loss = 0.05817698\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 83, loss = 0.05749521\n",
            "Iteration 84, loss = 0.05731377\n",
            "Iteration 85, loss = 0.05746027\n",
            "Iteration 86, loss = 0.05747685\n",
            "Iteration 87, loss = 0.05733543\n",
            "Iteration 88, loss = 0.05726946\n",
            "Iteration 89, loss = 0.05747269\n",
            "Iteration 90, loss = 0.05727899\n",
            "Iteration 91, loss = 0.05724765\n",
            "Iteration 92, loss = 0.05735051\n",
            "Iteration 93, loss = 0.05737771\n",
            "Iteration 94, loss = 0.05739901\n",
            "Iteration 95, loss = 0.05738425\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.05736362\n",
            "Iteration 97, loss = 0.05713499\n",
            "Iteration 98, loss = 0.05715125\n",
            "Iteration 99, loss = 0.05710650\n",
            "Iteration 100, loss = 0.05713801\n",
            "Iteration 101, loss = 0.05714089\n",
            "Iteration 102, loss = 0.05711859\n",
            "Iteration 103, loss = 0.05713188\n",
            "Iteration 104, loss = 0.05712110\n",
            "Iteration 105, loss = 0.05713038\n",
            "Iteration 106, loss = 0.05709632\n",
            "Iteration 107, loss = 0.05711878\n",
            "Iteration 108, loss = 0.05713445\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 109, loss = 0.05707171\n",
            "Iteration 110, loss = 0.05707077\n",
            "Iteration 111, loss = 0.05706558\n",
            "Iteration 112, loss = 0.05707226\n",
            "Iteration 113, loss = 0.05706979\n",
            "Iteration 114, loss = 0.05706535\n",
            "Iteration 115, loss = 0.05705893\n",
            "Iteration 116, loss = 0.05705768\n",
            "Iteration 117, loss = 0.05705741\n",
            "Iteration 118, loss = 0.05706183\n",
            "Iteration 119, loss = 0.05706170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.05705022\n",
            "Iteration 121, loss = 0.05705017\n",
            "Iteration 122, loss = 0.05704939\n",
            "Iteration 123, loss = 0.05704984\n",
            "Iteration 124, loss = 0.05704959\n",
            "Iteration 125, loss = 0.05705040\n",
            "Iteration 126, loss = 0.05704851\n",
            "Iteration 127, loss = 0.05704937\n",
            "Iteration 128, loss = 0.05704868\n",
            "Iteration 129, loss = 0.05704839\n",
            "Iteration 130, loss = 0.05704902\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.05704622\n",
            "Iteration 132, loss = 0.05704646\n",
            "Iteration 133, loss = 0.05704615\n",
            "Iteration 134, loss = 0.05704630\n",
            "Iteration 135, loss = 0.05704624\n",
            "Iteration 136, loss = 0.05704601\n",
            "Iteration 137, loss = 0.05704615\n",
            "Iteration 138, loss = 0.05704609\n",
            "Iteration 139, loss = 0.05704610\n",
            "Iteration 140, loss = 0.05704606\n",
            "Iteration 141, loss = 0.05704581\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.03462287\n",
            "Iteration 2, loss = 0.16097500\n",
            "Iteration 3, loss = 0.15234071\n",
            "Iteration 4, loss = 0.14493599\n",
            "Iteration 5, loss = 0.13735241\n",
            "Iteration 6, loss = 0.13088214\n",
            "Iteration 7, loss = 0.12503934\n",
            "Iteration 8, loss = 0.11980360\n",
            "Iteration 9, loss = 0.11514801\n",
            "Iteration 10, loss = 0.11021158\n",
            "Iteration 11, loss = 0.10621809\n",
            "Iteration 12, loss = 0.10219229\n",
            "Iteration 13, loss = 0.09879827\n",
            "Iteration 14, loss = 0.09583979\n",
            "Iteration 15, loss = 0.09310784\n",
            "Iteration 16, loss = 0.09017769\n",
            "Iteration 17, loss = 0.08769017\n",
            "Iteration 18, loss = 0.08513570\n",
            "Iteration 19, loss = 0.08315049\n",
            "Iteration 20, loss = 0.08138313\n",
            "Iteration 21, loss = 0.07927895\n",
            "Iteration 22, loss = 0.07773097\n",
            "Iteration 23, loss = 0.07590743\n",
            "Iteration 24, loss = 0.07464844\n",
            "Iteration 25, loss = 0.07374420\n",
            "Iteration 26, loss = 0.07217451\n",
            "Iteration 27, loss = 0.07128757\n",
            "Iteration 28, loss = 0.06972467\n",
            "Iteration 29, loss = 0.06923457\n",
            "Iteration 30, loss = 0.06828981\n",
            "Iteration 31, loss = 0.06753685\n",
            "Iteration 32, loss = 0.06657904\n",
            "Iteration 33, loss = 0.06617362\n",
            "Iteration 34, loss = 0.06531002\n",
            "Iteration 35, loss = 0.06470767\n",
            "Iteration 36, loss = 0.06416788\n",
            "Iteration 37, loss = 0.06383940\n",
            "Iteration 38, loss = 0.06368272\n",
            "Iteration 39, loss = 0.06301123\n",
            "Iteration 40, loss = 0.06267797\n",
            "Iteration 41, loss = 0.06231392\n",
            "Iteration 42, loss = 0.06175348\n",
            "Iteration 43, loss = 0.06156085\n",
            "Iteration 44, loss = 0.06104531\n",
            "Iteration 45, loss = 0.06131281\n",
            "Iteration 46, loss = 0.06087142\n",
            "Iteration 47, loss = 0.06039115\n",
            "Iteration 48, loss = 0.06014463\n",
            "Iteration 49, loss = 0.06000600\n",
            "Iteration 50, loss = 0.06010439\n",
            "Iteration 51, loss = 0.06086304\n",
            "Iteration 52, loss = 0.05967361\n",
            "Iteration 53, loss = 0.05965685\n",
            "Iteration 54, loss = 0.05908024\n",
            "Iteration 55, loss = 0.05929657\n",
            "Iteration 56, loss = 0.05903516\n",
            "Iteration 57, loss = 0.05916228\n",
            "Iteration 58, loss = 0.05855800\n",
            "Iteration 59, loss = 0.05882861\n",
            "Iteration 60, loss = 0.05869073\n",
            "Iteration 61, loss = 0.05890750\n",
            "Iteration 62, loss = 0.05847188\n",
            "Iteration 63, loss = 0.05842236\n",
            "Iteration 64, loss = 0.05864599\n",
            "Iteration 65, loss = 0.05816371\n",
            "Iteration 66, loss = 0.05832340\n",
            "Iteration 67, loss = 0.05822947\n",
            "Iteration 68, loss = 0.05823388\n",
            "Iteration 69, loss = 0.05803523\n",
            "Iteration 70, loss = 0.05822655\n",
            "Iteration 71, loss = 0.05825344\n",
            "Iteration 72, loss = 0.05777181\n",
            "Iteration 73, loss = 0.05819127\n",
            "Iteration 74, loss = 0.05791589\n",
            "Iteration 75, loss = 0.05798347\n",
            "Iteration 76, loss = 0.05790795\n",
            "Iteration 77, loss = 0.05793197\n",
            "Iteration 78, loss = 0.05778154\n",
            "Iteration 79, loss = 0.05805052\n",
            "Iteration 80, loss = 0.05784207\n",
            "Iteration 81, loss = 0.05770017\n",
            "Iteration 82, loss = 0.05794276\n",
            "Iteration 83, loss = 0.05768427\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 84, loss = 0.05728449\n",
            "Iteration 85, loss = 0.05725873\n",
            "Iteration 86, loss = 0.05719821\n",
            "Iteration 87, loss = 0.05730499\n",
            "Iteration 88, loss = 0.05709876\n",
            "Iteration 89, loss = 0.05716198\n",
            "Iteration 90, loss = 0.05714800\n",
            "Iteration 91, loss = 0.05719653\n",
            "Iteration 92, loss = 0.05720699\n",
            "Iteration 93, loss = 0.05713895\n",
            "Iteration 94, loss = 0.05717941\n",
            "Iteration 95, loss = 0.05718537\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.05698807\n",
            "Iteration 97, loss = 0.05696844\n",
            "Iteration 98, loss = 0.05694075\n",
            "Iteration 99, loss = 0.05696407\n",
            "Iteration 100, loss = 0.05694061\n",
            "Iteration 101, loss = 0.05694217\n",
            "Iteration 102, loss = 0.05691946\n",
            "Iteration 103, loss = 0.05694008\n",
            "Iteration 104, loss = 0.05695307\n",
            "Iteration 105, loss = 0.05684390\n",
            "Iteration 106, loss = 0.05700513\n",
            "Iteration 107, loss = 0.05700224\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 108, loss = 0.05690104\n",
            "Iteration 109, loss = 0.05689205\n",
            "Iteration 110, loss = 0.05689372\n",
            "Iteration 111, loss = 0.05689638\n",
            "Iteration 112, loss = 0.05689369\n",
            "Iteration 113, loss = 0.05688936\n",
            "Iteration 114, loss = 0.05688814\n",
            "Iteration 115, loss = 0.05688718\n",
            "Iteration 116, loss = 0.05689076\n",
            "Iteration 117, loss = 0.05688790\n",
            "Iteration 118, loss = 0.05688767\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 119, loss = 0.05688539\n",
            "Iteration 120, loss = 0.05687879\n",
            "Iteration 121, loss = 0.05687833\n",
            "Iteration 122, loss = 0.05687679\n",
            "Iteration 123, loss = 0.05687640\n",
            "Iteration 124, loss = 0.05687727\n",
            "Iteration 125, loss = 0.05687797\n",
            "Iteration 126, loss = 0.05687663\n",
            "Iteration 127, loss = 0.05687676\n",
            "Iteration 128, loss = 0.05688010\n",
            "Iteration 129, loss = 0.05687723\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 130, loss = 0.05687501\n",
            "Iteration 131, loss = 0.05687487\n",
            "Iteration 132, loss = 0.05687478\n",
            "Iteration 133, loss = 0.05687459\n",
            "Iteration 134, loss = 0.05687468\n",
            "Iteration 135, loss = 0.05687481\n",
            "Iteration 136, loss = 0.05687459\n",
            "Iteration 137, loss = 0.05687500\n",
            "Iteration 138, loss = 0.05687488\n",
            "Iteration 139, loss = 0.05687460\n",
            "Iteration 140, loss = 0.05687460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.22922458\n",
            "Iteration 3, loss = 0.21535518\n",
            "Iteration 4, loss = 0.20140699\n",
            "Iteration 5, loss = 0.18956053\n",
            "Iteration 6, loss = 0.17964775\n",
            "Iteration 7, loss = 0.16957275\n",
            "Iteration 8, loss = 0.16087467\n",
            "Iteration 9, loss = 0.15260445\n",
            "Iteration 10, loss = 0.14504892\n",
            "Iteration 11, loss = 0.13839680\n",
            "Iteration 12, loss = 0.13189037\n",
            "Iteration 13, loss = 0.12591693\n",
            "Iteration 14, loss = 0.12072510\n",
            "Iteration 15, loss = 0.11567140\n",
            "Iteration 16, loss = 0.11081116\n",
            "Iteration 17, loss = 0.10647565\n",
            "Iteration 18, loss = 0.10280442\n",
            "Iteration 19, loss = 0.09944696\n",
            "Iteration 20, loss = 0.09633001\n",
            "Iteration 21, loss = 0.09287473\n",
            "Iteration 22, loss = 0.09054924\n",
            "Iteration 23, loss = 0.08798760\n",
            "Iteration 24, loss = 0.08547510\n",
            "Iteration 25, loss = 0.08328673\n",
            "Iteration 26, loss = 0.08119009\n",
            "Iteration 27, loss = 0.07941217\n",
            "Iteration 28, loss = 0.07782688\n",
            "Iteration 29, loss = 0.07612204\n",
            "Iteration 30, loss = 0.07438899\n",
            "Iteration 31, loss = 0.07329140\n",
            "Iteration 32, loss = 0.07209474\n",
            "Iteration 33, loss = 0.07091177\n",
            "Iteration 34, loss = 0.06981345\n",
            "Iteration 35, loss = 0.06906357\n",
            "Iteration 36, loss = 0.06804794\n",
            "Iteration 37, loss = 0.06719605\n",
            "Iteration 38, loss = 0.06632671\n",
            "Iteration 39, loss = 0.06555346\n",
            "Iteration 40, loss = 0.06519886\n",
            "Iteration 41, loss = 0.06444557\n",
            "Iteration 42, loss = 0.06387076\n",
            "Iteration 43, loss = 0.06355099\n",
            "Iteration 44, loss = 0.06322476\n",
            "Iteration 45, loss = 0.06249390\n",
            "Iteration 46, loss = 0.06185526\n",
            "Iteration 47, loss = 0.06167212\n",
            "Iteration 48, loss = 0.06140588\n",
            "Iteration 49, loss = 0.06112715\n",
            "Iteration 50, loss = 0.06092044\n",
            "Iteration 51, loss = 0.06052694\n",
            "Iteration 52, loss = 0.06045255\n",
            "Iteration 53, loss = 0.05993697\n",
            "Iteration 54, loss = 0.05960905\n",
            "Iteration 55, loss = 0.05953017\n",
            "Iteration 56, loss = 0.05923438\n",
            "Iteration 57, loss = 0.05938554\n",
            "Iteration 58, loss = 0.05903851\n",
            "Iteration 59, loss = 0.05945414\n",
            "Iteration 60, loss = 0.05916713\n",
            "Iteration 61, loss = 0.05866351\n",
            "Iteration 62, loss = 0.05860525\n",
            "Iteration 63, loss = 0.05836242\n",
            "Iteration 64, loss = 0.05859212\n",
            "Iteration 65, loss = 0.05807447\n",
            "Iteration 66, loss = 0.05826997\n",
            "Iteration 67, loss = 0.05820955\n",
            "Iteration 68, loss = 0.05818033\n",
            "Iteration 69, loss = 0.05814408\n",
            "Iteration 70, loss = 0.05783523\n",
            "Iteration 71, loss = 0.05807486\n",
            "Iteration 72, loss = 0.05789555\n",
            "Iteration 73, loss = 0.05791627\n",
            "Iteration 74, loss = 0.05773998\n",
            "Iteration 75, loss = 0.05767645\n",
            "Iteration 76, loss = 0.05744240\n",
            "Iteration 77, loss = 0.05765811\n",
            "Iteration 78, loss = 0.05790805\n",
            "Iteration 79, loss = 0.05765894\n",
            "Iteration 80, loss = 0.05747513\n",
            "Iteration 81, loss = 0.05734420\n",
            "Iteration 82, loss = 0.05742747\n",
            "Iteration 83, loss = 0.05749914\n",
            "Iteration 84, loss = 0.05736713\n",
            "Iteration 85, loss = 0.05727506\n",
            "Iteration 86, loss = 0.05759767\n",
            "Iteration 87, loss = 0.05720262\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 88, loss = 0.05692805\n",
            "Iteration 89, loss = 0.05673492\n",
            "Iteration 90, loss = 0.05668977\n",
            "Iteration 91, loss = 0.05672097\n",
            "Iteration 92, loss = 0.05673695\n",
            "Iteration 93, loss = 0.05673677\n",
            "Iteration 94, loss = 0.05688756\n",
            "Iteration 95, loss = 0.05661434\n",
            "Iteration 96, loss = 0.05681667\n",
            "Iteration 97, loss = 0.05665160\n",
            "Iteration 98, loss = 0.05659850\n",
            "Iteration 99, loss = 0.05660971\n",
            "Iteration 100, loss = 0.05667866\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 101, loss = 0.05650750\n",
            "Iteration 102, loss = 0.05654084\n",
            "Iteration 103, loss = 0.05646094\n",
            "Iteration 104, loss = 0.05651970\n",
            "Iteration 105, loss = 0.05650073\n",
            "Iteration 106, loss = 0.05649288\n",
            "Iteration 107, loss = 0.05652783\n",
            "Iteration 108, loss = 0.05647759\n",
            "Iteration 109, loss = 0.05647560\n",
            "Iteration 110, loss = 0.05648440\n",
            "Iteration 111, loss = 0.05648081\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 112, loss = 0.05645866\n",
            "Iteration 113, loss = 0.05642504\n",
            "Iteration 114, loss = 0.05642855\n",
            "Iteration 115, loss = 0.05642968\n",
            "Iteration 116, loss = 0.05643050\n",
            "Iteration 117, loss = 0.05642474\n",
            "Iteration 118, loss = 0.05642493\n",
            "Iteration 119, loss = 0.05643149\n",
            "Iteration 120, loss = 0.05642516\n",
            "Iteration 121, loss = 0.05641934\n",
            "Iteration 122, loss = 0.05642585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 123, loss = 0.05641758\n",
            "Iteration 124, loss = 0.05641589\n",
            "Iteration 125, loss = 0.05641445\n",
            "Iteration 126, loss = 0.05641514\n",
            "Iteration 127, loss = 0.05641437\n",
            "Iteration 128, loss = 0.05641584\n",
            "Iteration 129, loss = 0.05641568\n",
            "Iteration 130, loss = 0.05641403\n",
            "Iteration 131, loss = 0.05641381\n",
            "Iteration 132, loss = 0.05641533\n",
            "Iteration 133, loss = 0.05641503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 134, loss = 0.05641214\n",
            "Iteration 135, loss = 0.05641241\n",
            "Iteration 136, loss = 0.05641228\n",
            "Iteration 137, loss = 0.05641218\n",
            "Iteration 138, loss = 0.05641201\n",
            "Iteration 139, loss = 0.05641208\n",
            "Iteration 140, loss = 0.05641214\n",
            "Iteration 141, loss = 0.05641221\n",
            "Iteration 142, loss = 0.05641202\n",
            "Iteration 143, loss = 0.05641261\n",
            "Iteration 144, loss = 0.05641216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.42550028\n",
            "Iteration 2, loss = 0.20935604\n",
            "Iteration 3, loss = 0.17661968\n",
            "Iteration 4, loss = 0.16586051\n",
            "Iteration 5, loss = 0.15693095\n",
            "Iteration 6, loss = 0.14927452\n",
            "Iteration 7, loss = 0.14125381\n",
            "Iteration 8, loss = 0.13524777\n",
            "Iteration 9, loss = 0.12842540\n",
            "Iteration 10, loss = 0.12257668\n",
            "Iteration 11, loss = 0.11775919\n",
            "Iteration 12, loss = 0.11284022\n",
            "Iteration 13, loss = 0.10847145\n",
            "Iteration 14, loss = 0.10474064\n",
            "Iteration 15, loss = 0.10134014\n",
            "Iteration 16, loss = 0.09764359\n",
            "Iteration 17, loss = 0.09437412\n",
            "Iteration 18, loss = 0.09128983\n",
            "Iteration 19, loss = 0.08876422\n",
            "Iteration 20, loss = 0.08671445\n",
            "Iteration 21, loss = 0.08408019\n",
            "Iteration 22, loss = 0.08207743\n",
            "Iteration 23, loss = 0.08023750\n",
            "Iteration 24, loss = 0.07838508\n",
            "Iteration 25, loss = 0.07649816\n",
            "Iteration 26, loss = 0.07526996\n",
            "Iteration 27, loss = 0.07357924\n",
            "Iteration 28, loss = 0.07254080\n",
            "Iteration 29, loss = 0.07141205\n",
            "Iteration 30, loss = 0.07074440\n",
            "Iteration 31, loss = 0.06967288\n",
            "Iteration 32, loss = 0.06863416\n",
            "Iteration 33, loss = 0.06828932\n",
            "Iteration 34, loss = 0.06709722\n",
            "Iteration 35, loss = 0.06706135\n",
            "Iteration 36, loss = 0.06604878\n",
            "Iteration 37, loss = 0.06547453\n",
            "Iteration 38, loss = 0.06459881\n",
            "Iteration 39, loss = 0.06409831\n",
            "Iteration 40, loss = 0.06352714\n",
            "Iteration 41, loss = 0.06338029\n",
            "Iteration 42, loss = 0.06276847\n",
            "Iteration 43, loss = 0.06235733\n",
            "Iteration 44, loss = 0.06211466\n",
            "Iteration 45, loss = 0.06151539\n",
            "Iteration 46, loss = 0.06151503\n",
            "Iteration 47, loss = 0.06114056\n",
            "Iteration 48, loss = 0.06117676\n",
            "Iteration 49, loss = 0.06059742\n",
            "Iteration 50, loss = 0.06041224\n",
            "Iteration 51, loss = 0.06061890\n",
            "Iteration 52, loss = 0.06019943\n",
            "Iteration 53, loss = 0.05973646\n",
            "Iteration 54, loss = 0.05957496\n",
            "Iteration 55, loss = 0.05971467\n",
            "Iteration 56, loss = 0.05962325\n",
            "Iteration 57, loss = 0.05936264\n",
            "Iteration 58, loss = 0.05921969\n",
            "Iteration 59, loss = 0.05956454\n",
            "Iteration 60, loss = 0.05913647\n",
            "Iteration 61, loss = 0.05878155\n",
            "Iteration 62, loss = 0.05870970\n",
            "Iteration 63, loss = 0.05892854\n",
            "Iteration 64, loss = 0.05795728\n",
            "Iteration 65, loss = 0.05865296\n",
            "Iteration 66, loss = 0.05840195\n",
            "Iteration 67, loss = 0.05838828\n",
            "Iteration 68, loss = 0.05829888\n",
            "Iteration 69, loss = 0.05847812\n",
            "Iteration 70, loss = 0.05840536\n",
            "Iteration 71, loss = 0.05843458\n",
            "Iteration 72, loss = 0.05827754\n",
            "Iteration 73, loss = 0.05819936\n",
            "Iteration 74, loss = 0.05817802\n",
            "Iteration 75, loss = 0.05837276\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.05759251\n",
            "Iteration 77, loss = 0.05734126\n",
            "Iteration 78, loss = 0.05732765\n",
            "Iteration 79, loss = 0.05737936\n",
            "Iteration 80, loss = 0.05745764\n",
            "Iteration 81, loss = 0.05741646\n",
            "Iteration 82, loss = 0.05723553\n",
            "Iteration 83, loss = 0.05731614\n",
            "Iteration 84, loss = 0.05733841\n",
            "Iteration 85, loss = 0.05744323\n",
            "Iteration 86, loss = 0.05732183\n",
            "Iteration 87, loss = 0.05722785\n",
            "Iteration 88, loss = 0.05730857\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.05711624\n",
            "Iteration 90, loss = 0.05708068\n",
            "Iteration 91, loss = 0.05706748\n",
            "Iteration 92, loss = 0.05705996\n",
            "Iteration 93, loss = 0.05705706\n",
            "Iteration 94, loss = 0.05705273\n",
            "Iteration 95, loss = 0.05707304\n",
            "Iteration 96, loss = 0.05706941\n",
            "Iteration 97, loss = 0.05703623\n",
            "Iteration 98, loss = 0.05704604\n",
            "Iteration 99, loss = 0.05705605\n",
            "Iteration 100, loss = 0.05704496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.05701530\n",
            "Iteration 102, loss = 0.05700947\n",
            "Iteration 103, loss = 0.05701093\n",
            "Iteration 104, loss = 0.05700805\n",
            "Iteration 105, loss = 0.05700598\n",
            "Iteration 106, loss = 0.05700267\n",
            "Iteration 107, loss = 0.05700253\n",
            "Iteration 108, loss = 0.05699883\n",
            "Iteration 109, loss = 0.05699941\n",
            "Iteration 110, loss = 0.05700831\n",
            "Iteration 111, loss = 0.05699545\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 112, loss = 0.05699147\n",
            "Iteration 113, loss = 0.05698820\n",
            "Iteration 114, loss = 0.05698778\n",
            "Iteration 115, loss = 0.05698573\n",
            "Iteration 116, loss = 0.05698567\n",
            "Iteration 117, loss = 0.05698614\n",
            "Iteration 118, loss = 0.05698627\n",
            "Iteration 119, loss = 0.05698683\n",
            "Iteration 120, loss = 0.05698612\n",
            "Iteration 121, loss = 0.05698601\n",
            "Iteration 122, loss = 0.05698573\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 123, loss = 0.05698354\n",
            "Iteration 124, loss = 0.05698348\n",
            "Iteration 125, loss = 0.05698314\n",
            "Iteration 126, loss = 0.05698295\n",
            "Iteration 127, loss = 0.05698310\n",
            "Iteration 128, loss = 0.05698308\n",
            "Iteration 129, loss = 0.05698310\n",
            "Iteration 130, loss = 0.05698280\n",
            "Iteration 131, loss = 0.05698301\n",
            "Iteration 132, loss = 0.05698282\n",
            "Iteration 133, loss = 0.05698282\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.29545622\n",
            "Iteration 3, loss = 0.27647684\n",
            "Iteration 4, loss = 0.25951662\n",
            "Iteration 5, loss = 0.24406306\n",
            "Iteration 6, loss = 0.22904029\n",
            "Iteration 7, loss = 0.21564629\n",
            "Iteration 8, loss = 0.20315989\n",
            "Iteration 9, loss = 0.19195860\n",
            "Iteration 10, loss = 0.18166116\n",
            "Iteration 11, loss = 0.17174773\n",
            "Iteration 12, loss = 0.16327038\n",
            "Iteration 13, loss = 0.15524608\n",
            "Iteration 14, loss = 0.14752264\n",
            "Iteration 15, loss = 0.14047507\n",
            "Iteration 16, loss = 0.13441666\n",
            "Iteration 17, loss = 0.12845655\n",
            "Iteration 18, loss = 0.12292225\n",
            "Iteration 19, loss = 0.11824889\n",
            "Iteration 20, loss = 0.11383998\n",
            "Iteration 21, loss = 0.10924655\n",
            "Iteration 22, loss = 0.10538922\n",
            "Iteration 23, loss = 0.10201817\n",
            "Iteration 24, loss = 0.09849049\n",
            "Iteration 25, loss = 0.09527162\n",
            "Iteration 26, loss = 0.09245769\n",
            "Iteration 27, loss = 0.08987095\n",
            "Iteration 28, loss = 0.08755553\n",
            "Iteration 29, loss = 0.08530604\n",
            "Iteration 30, loss = 0.08320000\n",
            "Iteration 31, loss = 0.08132934\n",
            "Iteration 32, loss = 0.07978273\n",
            "Iteration 33, loss = 0.07810149\n",
            "Iteration 34, loss = 0.07677108\n",
            "Iteration 35, loss = 0.07522765\n",
            "Iteration 36, loss = 0.07394738\n",
            "Iteration 37, loss = 0.07299995\n",
            "Iteration 38, loss = 0.07190519\n",
            "Iteration 39, loss = 0.07067579\n",
            "Iteration 40, loss = 0.06968150\n",
            "Iteration 41, loss = 0.06926070\n",
            "Iteration 42, loss = 0.06825816\n",
            "Iteration 43, loss = 0.06737400\n",
            "Iteration 44, loss = 0.06670663\n",
            "Iteration 45, loss = 0.06615467\n",
            "Iteration 46, loss = 0.06559987\n",
            "Iteration 47, loss = 0.06526839\n",
            "Iteration 48, loss = 0.06489557\n",
            "Iteration 49, loss = 0.06446636\n",
            "Iteration 50, loss = 0.06385742\n",
            "Iteration 51, loss = 0.06348163\n",
            "Iteration 52, loss = 0.06287412\n",
            "Iteration 53, loss = 0.06274468\n",
            "Iteration 54, loss = 0.06232818\n",
            "Iteration 55, loss = 0.06242717\n",
            "Iteration 56, loss = 0.06177072\n",
            "Iteration 57, loss = 0.06169791\n",
            "Iteration 58, loss = 0.06150325\n",
            "Iteration 59, loss = 0.06130870\n",
            "Iteration 60, loss = 0.06116680\n",
            "Iteration 61, loss = 0.06087919\n",
            "Iteration 62, loss = 0.06066955\n",
            "Iteration 63, loss = 0.06045574\n",
            "Iteration 64, loss = 0.06068508\n",
            "Iteration 65, loss = 0.06027694\n",
            "Iteration 66, loss = 0.06024436\n",
            "Iteration 67, loss = 0.06014745\n",
            "Iteration 68, loss = 0.05993807\n",
            "Iteration 69, loss = 0.06003332\n",
            "Iteration 70, loss = 0.05982590\n",
            "Iteration 71, loss = 0.05993223\n",
            "Iteration 72, loss = 0.05952587\n",
            "Iteration 73, loss = 0.05965877\n",
            "Iteration 74, loss = 0.05935372\n",
            "Iteration 75, loss = 0.05994682\n",
            "Iteration 76, loss = 0.05963076\n",
            "Iteration 77, loss = 0.05939635\n",
            "Iteration 78, loss = 0.05922906\n",
            "Iteration 79, loss = 0.05932853\n",
            "Iteration 80, loss = 0.05946151\n",
            "Iteration 81, loss = 0.05948460\n",
            "Iteration 82, loss = 0.05936184\n",
            "Iteration 83, loss = 0.05927148\n",
            "Iteration 84, loss = 0.05908580\n",
            "Iteration 85, loss = 0.05940198\n",
            "Iteration 86, loss = 0.05937745\n",
            "Iteration 87, loss = 0.05919914\n",
            "Iteration 88, loss = 0.05932654\n",
            "Iteration 89, loss = 0.05928251\n",
            "Iteration 90, loss = 0.05910348\n",
            "Iteration 91, loss = 0.05891271\n",
            "Iteration 92, loss = 0.05898399\n",
            "Iteration 93, loss = 0.05920234\n",
            "Iteration 94, loss = 0.05927613\n",
            "Iteration 95, loss = 0.05899440\n",
            "Iteration 96, loss = 0.05909511\n",
            "Iteration 97, loss = 0.05903225\n",
            "Iteration 98, loss = 0.05913362\n",
            "Iteration 99, loss = 0.05925348\n",
            "Iteration 100, loss = 0.05911190\n",
            "Iteration 101, loss = 0.05926803\n",
            "Iteration 102, loss = 0.05913844\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 103, loss = 0.05850432\n",
            "Iteration 104, loss = 0.05843432\n",
            "Iteration 105, loss = 0.05854289\n",
            "Iteration 106, loss = 0.05844706\n",
            "Iteration 107, loss = 0.05836236\n",
            "Iteration 108, loss = 0.05835181\n",
            "Iteration 109, loss = 0.05844802\n",
            "Iteration 110, loss = 0.05841083\n",
            "Iteration 111, loss = 0.05836336\n",
            "Iteration 112, loss = 0.05841446\n",
            "Iteration 113, loss = 0.05842119\n",
            "Iteration 114, loss = 0.05833795\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 115, loss = 0.05822596\n",
            "Iteration 116, loss = 0.05820870\n",
            "Iteration 117, loss = 0.05825218\n",
            "Iteration 118, loss = 0.05824757\n",
            "Iteration 119, loss = 0.05820946\n",
            "Iteration 120, loss = 0.05820781\n",
            "Iteration 121, loss = 0.05823045\n",
            "Iteration 122, loss = 0.05821205\n",
            "Iteration 123, loss = 0.05823227\n",
            "Iteration 124, loss = 0.05821408\n",
            "Iteration 125, loss = 0.05821095\n",
            "Iteration 126, loss = 0.05821326\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 127, loss = 0.05818476\n",
            "Iteration 128, loss = 0.05817571\n",
            "Iteration 129, loss = 0.05816817\n",
            "Iteration 130, loss = 0.05816903\n",
            "Iteration 131, loss = 0.05817077\n",
            "Iteration 132, loss = 0.05817745\n",
            "Iteration 133, loss = 0.05817014\n",
            "Iteration 134, loss = 0.05817077\n",
            "Iteration 135, loss = 0.05817062\n",
            "Iteration 136, loss = 0.05816626\n",
            "Iteration 137, loss = 0.05817136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 138, loss = 0.05815928\n",
            "Iteration 139, loss = 0.05815982\n",
            "Iteration 140, loss = 0.05816062\n",
            "Iteration 141, loss = 0.05815936\n",
            "Iteration 142, loss = 0.05816014\n",
            "Iteration 143, loss = 0.05816037\n",
            "Iteration 144, loss = 0.05815919\n",
            "Iteration 145, loss = 0.05815885\n",
            "Iteration 146, loss = 0.05815959\n",
            "Iteration 147, loss = 0.05815918\n",
            "Iteration 148, loss = 0.05816022\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 149, loss = 0.05815705\n",
            "Iteration 150, loss = 0.05815711\n",
            "Iteration 151, loss = 0.05815729\n",
            "Iteration 152, loss = 0.05815705\n",
            "Iteration 153, loss = 0.05815706\n",
            "Iteration 154, loss = 0.05815704\n",
            "Iteration 155, loss = 0.05815705\n",
            "Iteration 156, loss = 0.05815709\n",
            "Iteration 157, loss = 0.05815701\n",
            "Iteration 158, loss = 0.05815738\n",
            "Iteration 159, loss = 0.05815721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.53731101\n",
            "Iteration 2, loss = 0.17742660\n",
            "Iteration 3, loss = 0.16725357\n",
            "Iteration 4, loss = 0.15874429\n",
            "Iteration 5, loss = 0.15012256\n",
            "Iteration 6, loss = 0.14270636\n",
            "Iteration 7, loss = 0.13612198\n",
            "Iteration 8, loss = 0.12956935\n",
            "Iteration 9, loss = 0.12391929\n",
            "Iteration 10, loss = 0.11874286\n",
            "Iteration 11, loss = 0.11396003\n",
            "Iteration 12, loss = 0.10951718\n",
            "Iteration 13, loss = 0.10495899\n",
            "Iteration 14, loss = 0.10159487\n",
            "Iteration 15, loss = 0.09787119\n",
            "Iteration 16, loss = 0.09496051\n",
            "Iteration 17, loss = 0.09191023\n",
            "Iteration 18, loss = 0.08913055\n",
            "Iteration 19, loss = 0.08712298\n",
            "Iteration 20, loss = 0.08468700\n",
            "Iteration 21, loss = 0.08261340\n",
            "Iteration 22, loss = 0.08061384\n",
            "Iteration 23, loss = 0.07903932\n",
            "Iteration 24, loss = 0.07718700\n",
            "Iteration 25, loss = 0.07544190\n",
            "Iteration 26, loss = 0.07441466\n",
            "Iteration 27, loss = 0.07346757\n",
            "Iteration 28, loss = 0.07187783\n",
            "Iteration 29, loss = 0.07028712\n",
            "Iteration 30, loss = 0.06963725\n",
            "Iteration 31, loss = 0.06910813\n",
            "Iteration 32, loss = 0.06774152\n",
            "Iteration 33, loss = 0.06673442\n",
            "Iteration 34, loss = 0.06628484\n",
            "Iteration 35, loss = 0.06562545\n",
            "Iteration 36, loss = 0.06519050\n",
            "Iteration 37, loss = 0.06417556\n",
            "Iteration 38, loss = 0.06397789\n",
            "Iteration 39, loss = 0.06345991\n",
            "Iteration 40, loss = 0.06278529\n",
            "Iteration 41, loss = 0.06253207\n",
            "Iteration 42, loss = 0.06209428\n",
            "Iteration 43, loss = 0.06184687\n",
            "Iteration 44, loss = 0.06144256\n",
            "Iteration 45, loss = 0.06129912\n",
            "Iteration 46, loss = 0.06082398\n",
            "Iteration 47, loss = 0.06034107\n",
            "Iteration 48, loss = 0.06028594\n",
            "Iteration 49, loss = 0.06032826\n",
            "Iteration 50, loss = 0.06015429\n",
            "Iteration 51, loss = 0.05971344\n",
            "Iteration 52, loss = 0.05967759\n",
            "Iteration 53, loss = 0.05951389\n",
            "Iteration 54, loss = 0.05929478\n",
            "Iteration 55, loss = 0.05893130\n",
            "Iteration 56, loss = 0.05905288\n",
            "Iteration 57, loss = 0.05882131\n",
            "Iteration 58, loss = 0.05886714\n",
            "Iteration 59, loss = 0.05868279\n",
            "Iteration 60, loss = 0.05885933\n",
            "Iteration 61, loss = 0.05845491\n",
            "Iteration 62, loss = 0.05850300\n",
            "Iteration 63, loss = 0.05854489\n",
            "Iteration 64, loss = 0.05832310\n",
            "Iteration 65, loss = 0.05836971\n",
            "Iteration 66, loss = 0.05806217\n",
            "Iteration 67, loss = 0.05806106\n",
            "Iteration 68, loss = 0.05844428\n",
            "Iteration 69, loss = 0.05805305\n",
            "Iteration 70, loss = 0.05825927\n",
            "Iteration 71, loss = 0.05807035\n",
            "Iteration 72, loss = 0.05773475\n",
            "Iteration 73, loss = 0.05783824\n",
            "Iteration 74, loss = 0.05793260\n",
            "Iteration 75, loss = 0.05755659\n",
            "Iteration 76, loss = 0.05761676\n",
            "Iteration 77, loss = 0.05766270\n",
            "Iteration 78, loss = 0.05800956\n",
            "Iteration 79, loss = 0.05778484\n",
            "Iteration 80, loss = 0.05765755\n",
            "Iteration 81, loss = 0.05793176\n",
            "Iteration 82, loss = 0.05774109\n",
            "Iteration 83, loss = 0.05769002\n",
            "Iteration 84, loss = 0.05762520\n",
            "Iteration 85, loss = 0.05754344\n",
            "Iteration 86, loss = 0.05730275\n",
            "Iteration 87, loss = 0.05730673\n",
            "Iteration 88, loss = 0.05757082\n",
            "Iteration 89, loss = 0.05755442\n",
            "Iteration 90, loss = 0.05698810\n",
            "Iteration 91, loss = 0.05730839\n",
            "Iteration 92, loss = 0.05753039\n",
            "Iteration 93, loss = 0.05731961\n",
            "Iteration 94, loss = 0.05739144\n",
            "Iteration 95, loss = 0.05717015\n",
            "Iteration 96, loss = 0.05727859\n",
            "Iteration 97, loss = 0.05737624\n",
            "Iteration 98, loss = 0.05736688\n",
            "Iteration 99, loss = 0.05734528\n",
            "Iteration 100, loss = 0.05724741\n",
            "Iteration 101, loss = 0.05750297\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 102, loss = 0.05692703\n",
            "Iteration 103, loss = 0.05667411\n",
            "Iteration 104, loss = 0.05679133\n",
            "Iteration 105, loss = 0.05685264\n",
            "Iteration 106, loss = 0.05677115\n",
            "Iteration 107, loss = 0.05667006\n",
            "Iteration 108, loss = 0.05674447\n",
            "Iteration 109, loss = 0.05673706\n",
            "Iteration 110, loss = 0.05676243\n",
            "Iteration 111, loss = 0.05670353\n",
            "Iteration 112, loss = 0.05664473\n",
            "Iteration 113, loss = 0.05679082\n",
            "Iteration 114, loss = 0.05675122\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 115, loss = 0.05656815\n",
            "Iteration 116, loss = 0.05656666\n",
            "Iteration 117, loss = 0.05654988\n",
            "Iteration 118, loss = 0.05654794\n",
            "Iteration 119, loss = 0.05654088\n",
            "Iteration 120, loss = 0.05656370\n",
            "Iteration 121, loss = 0.05652933\n",
            "Iteration 122, loss = 0.05654748\n",
            "Iteration 123, loss = 0.05654102\n",
            "Iteration 124, loss = 0.05653622\n",
            "Iteration 125, loss = 0.05659904\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 126, loss = 0.05650272\n",
            "Iteration 127, loss = 0.05649568\n",
            "Iteration 128, loss = 0.05650153\n",
            "Iteration 129, loss = 0.05649755\n",
            "Iteration 130, loss = 0.05650223\n",
            "Iteration 131, loss = 0.05648967\n",
            "Iteration 132, loss = 0.05650239\n",
            "Iteration 133, loss = 0.05648772\n",
            "Iteration 134, loss = 0.05649624\n",
            "Iteration 135, loss = 0.05648837\n",
            "Iteration 136, loss = 0.05649012\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 137, loss = 0.05648004\n",
            "Iteration 138, loss = 0.05648035\n",
            "Iteration 139, loss = 0.05647884\n",
            "Iteration 140, loss = 0.05647966\n",
            "Iteration 141, loss = 0.05647868\n",
            "Iteration 142, loss = 0.05648009\n",
            "Iteration 143, loss = 0.05648207\n",
            "Iteration 144, loss = 0.05647917\n",
            "Iteration 145, loss = 0.05647966\n",
            "Iteration 146, loss = 0.05647813\n",
            "Iteration 147, loss = 0.05647929\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 148, loss = 0.05647675\n",
            "Iteration 149, loss = 0.05647644\n",
            "Iteration 150, loss = 0.05647632\n",
            "Iteration 151, loss = 0.05647665\n",
            "Iteration 152, loss = 0.05647667\n",
            "Iteration 153, loss = 0.05647716\n",
            "Iteration 154, loss = 0.05647639\n",
            "Iteration 155, loss = 0.05647635\n",
            "Iteration 156, loss = 0.05647636\n",
            "Iteration 157, loss = 0.05647610\n",
            "Iteration 158, loss = 0.05647623\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.20222456\n",
            "Iteration 3, loss = 0.18944250\n",
            "Iteration 4, loss = 0.17851851\n",
            "Iteration 5, loss = 0.16902120\n",
            "Iteration 6, loss = 0.16002199\n",
            "Iteration 7, loss = 0.15158720\n",
            "Iteration 8, loss = 0.14444637\n",
            "Iteration 9, loss = 0.13704161\n",
            "Iteration 10, loss = 0.13081339\n",
            "Iteration 11, loss = 0.12515618\n",
            "Iteration 12, loss = 0.11989593\n",
            "Iteration 13, loss = 0.11527710\n",
            "Iteration 14, loss = 0.11053331\n",
            "Iteration 15, loss = 0.10623408\n",
            "Iteration 16, loss = 0.10273308\n",
            "Iteration 17, loss = 0.09889000\n",
            "Iteration 18, loss = 0.09572799\n",
            "Iteration 19, loss = 0.09295488\n",
            "Iteration 20, loss = 0.09011027\n",
            "Iteration 21, loss = 0.08764092\n",
            "Iteration 22, loss = 0.08542971\n",
            "Iteration 23, loss = 0.08294595\n",
            "Iteration 24, loss = 0.08094713\n",
            "Iteration 25, loss = 0.07921975\n",
            "Iteration 26, loss = 0.07758808\n",
            "Iteration 27, loss = 0.07598500\n",
            "Iteration 28, loss = 0.07433472\n",
            "Iteration 29, loss = 0.07335699\n",
            "Iteration 30, loss = 0.07185754\n",
            "Iteration 31, loss = 0.07092306\n",
            "Iteration 32, loss = 0.06984630\n",
            "Iteration 33, loss = 0.06884603\n",
            "Iteration 34, loss = 0.06804181\n",
            "Iteration 35, loss = 0.06700513\n",
            "Iteration 36, loss = 0.06662468\n",
            "Iteration 37, loss = 0.06576189\n",
            "Iteration 38, loss = 0.06527757\n",
            "Iteration 39, loss = 0.06428181\n",
            "Iteration 40, loss = 0.06409341\n",
            "Iteration 41, loss = 0.06343036\n",
            "Iteration 42, loss = 0.06276095\n",
            "Iteration 43, loss = 0.06248699\n",
            "Iteration 44, loss = 0.06229323\n",
            "Iteration 45, loss = 0.06174820\n",
            "Iteration 46, loss = 0.06125180\n",
            "Iteration 47, loss = 0.06114221\n",
            "Iteration 48, loss = 0.06050849\n",
            "Iteration 49, loss = 0.06025533\n",
            "Iteration 50, loss = 0.06016699\n",
            "Iteration 51, loss = 0.06019001\n",
            "Iteration 52, loss = 0.05973733\n",
            "Iteration 53, loss = 0.05976750\n",
            "Iteration 54, loss = 0.05943639\n",
            "Iteration 55, loss = 0.05954895\n",
            "Iteration 56, loss = 0.05912762\n",
            "Iteration 57, loss = 0.05930068\n",
            "Iteration 58, loss = 0.05876752\n",
            "Iteration 59, loss = 0.05874044\n",
            "Iteration 60, loss = 0.05860006\n",
            "Iteration 61, loss = 0.05839621\n",
            "Iteration 62, loss = 0.05830269\n",
            "Iteration 63, loss = 0.05803116\n",
            "Iteration 64, loss = 0.05824845\n",
            "Iteration 65, loss = 0.05843480\n",
            "Iteration 66, loss = 0.05844885\n",
            "Iteration 67, loss = 0.05792090\n",
            "Iteration 68, loss = 0.05790204\n",
            "Iteration 69, loss = 0.05793667\n",
            "Iteration 70, loss = 0.05813249\n",
            "Iteration 71, loss = 0.05772423\n",
            "Iteration 72, loss = 0.05781586\n",
            "Iteration 73, loss = 0.05766171\n",
            "Iteration 74, loss = 0.05778622\n",
            "Iteration 75, loss = 0.05766540\n",
            "Iteration 76, loss = 0.05755158\n",
            "Iteration 77, loss = 0.05743417\n",
            "Iteration 78, loss = 0.05755945\n",
            "Iteration 79, loss = 0.05756839\n",
            "Iteration 80, loss = 0.05759747\n",
            "Iteration 81, loss = 0.05748464\n",
            "Iteration 82, loss = 0.05757556\n",
            "Iteration 83, loss = 0.05739783\n",
            "Iteration 84, loss = 0.05770997\n",
            "Iteration 85, loss = 0.05737734\n",
            "Iteration 86, loss = 0.05732653\n",
            "Iteration 87, loss = 0.05724198\n",
            "Iteration 88, loss = 0.05762302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 89, loss = 0.05733051\n",
            "Iteration 90, loss = 0.05683403\n",
            "Iteration 91, loss = 0.05684061\n",
            "Iteration 92, loss = 0.05682852\n",
            "Iteration 93, loss = 0.05684801\n",
            "Iteration 94, loss = 0.05683182\n",
            "Iteration 95, loss = 0.05673383\n",
            "Iteration 96, loss = 0.05686318\n",
            "Iteration 97, loss = 0.05667607\n",
            "Iteration 98, loss = 0.05681470\n",
            "Iteration 99, loss = 0.05674619\n",
            "Iteration 100, loss = 0.05674850\n",
            "Iteration 101, loss = 0.05665208\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 102, loss = 0.05654405\n",
            "Iteration 103, loss = 0.05661712\n",
            "Iteration 104, loss = 0.05655473\n",
            "Iteration 105, loss = 0.05654871\n",
            "Iteration 106, loss = 0.05653970\n",
            "Iteration 107, loss = 0.05655527\n",
            "Iteration 108, loss = 0.05654703\n",
            "Iteration 109, loss = 0.05656980\n",
            "Iteration 110, loss = 0.05653929\n",
            "Iteration 111, loss = 0.05655129\n",
            "Iteration 112, loss = 0.05653065\n",
            "Iteration 113, loss = 0.05656098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 114, loss = 0.05651801\n",
            "Iteration 115, loss = 0.05650662\n",
            "Iteration 116, loss = 0.05650298\n",
            "Iteration 117, loss = 0.05650522\n",
            "Iteration 118, loss = 0.05650310\n",
            "Iteration 119, loss = 0.05649603\n",
            "Iteration 120, loss = 0.05650276\n",
            "Iteration 121, loss = 0.05649848\n",
            "Iteration 122, loss = 0.05650744\n",
            "Iteration 123, loss = 0.05650264\n",
            "Iteration 124, loss = 0.05650410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 125, loss = 0.05649269\n",
            "Iteration 126, loss = 0.05649281\n",
            "Iteration 127, loss = 0.05649138\n",
            "Iteration 128, loss = 0.05649150\n",
            "Iteration 129, loss = 0.05649141\n",
            "Iteration 130, loss = 0.05649206\n",
            "Iteration 131, loss = 0.05649172\n",
            "Iteration 132, loss = 0.05649186\n",
            "Iteration 133, loss = 0.05649229\n",
            "Iteration 134, loss = 0.05649222\n",
            "Iteration 135, loss = 0.05649096\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 136, loss = 0.05648900\n",
            "Iteration 137, loss = 0.05648920\n",
            "Iteration 138, loss = 0.05648915\n",
            "Iteration 139, loss = 0.05648937\n",
            "Iteration 140, loss = 0.05648928\n",
            "Iteration 141, loss = 0.05648946\n",
            "Iteration 142, loss = 0.05648951\n",
            "Iteration 143, loss = 0.05648906\n",
            "Iteration 144, loss = 0.05648937\n",
            "Iteration 145, loss = 0.05648876\n",
            "Iteration 146, loss = 0.05648900\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33835    12]\n",
            " [ 3295   479]]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  9.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95     33847\n",
            "           1       0.98      0.13      0.22      3774\n",
            "\n",
            "    accuracy                           0.91     37621\n",
            "   macro avg       0.94      0.56      0.59     37621\n",
            "weighted avg       0.92      0.91      0.88     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 98.770% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.689% (0.000%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.40731704\n",
            "Iteration 3, loss = 0.38151959\n",
            "Iteration 4, loss = 0.36205004\n",
            "Iteration 5, loss = 0.34827758\n",
            "Iteration 6, loss = 0.33808059\n",
            "Iteration 7, loss = 0.33060867\n",
            "Iteration 8, loss = 0.32609815\n",
            "Iteration 9, loss = 0.32213391\n",
            "Iteration 10, loss = 0.31914527\n",
            "Iteration 11, loss = 0.31665519\n",
            "Iteration 12, loss = 0.31556265\n",
            "Iteration 13, loss = 0.31422340\n",
            "Iteration 14, loss = 0.31389366\n",
            "Iteration 15, loss = 0.31295545\n",
            "Iteration 16, loss = 0.31298801\n",
            "Iteration 17, loss = 0.31186196\n",
            "Iteration 18, loss = 0.31173510\n",
            "Iteration 19, loss = 0.31172906\n",
            "Iteration 20, loss = 0.31253409\n",
            "Iteration 21, loss = 0.31198856\n",
            "Iteration 22, loss = 0.31168323\n",
            "Iteration 23, loss = 0.31327400\n",
            "Iteration 24, loss = 0.31293218\n",
            "Iteration 25, loss = 0.31262731\n",
            "Iteration 26, loss = 0.31161359\n",
            "Iteration 27, loss = 0.31225626\n",
            "Iteration 28, loss = 0.31310723\n",
            "Iteration 29, loss = 0.31305426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.30607265\n",
            "Iteration 31, loss = 0.30407373\n",
            "Iteration 32, loss = 0.30300352\n",
            "Iteration 33, loss = 0.30140437\n",
            "Iteration 34, loss = 0.30049328\n",
            "Iteration 35, loss = 0.29913592\n",
            "Iteration 36, loss = 0.29818527\n",
            "Iteration 37, loss = 0.29670368\n",
            "Iteration 38, loss = 0.29600444\n",
            "Iteration 39, loss = 0.29458660\n",
            "Iteration 40, loss = 0.29418706\n",
            "Iteration 41, loss = 0.29388209\n",
            "Iteration 42, loss = 0.29288265\n",
            "Iteration 43, loss = 0.29232605\n",
            "Iteration 44, loss = 0.29292957\n",
            "Iteration 45, loss = 0.29238615\n",
            "Iteration 46, loss = 0.29122467\n",
            "Iteration 47, loss = 0.29138713\n",
            "Iteration 48, loss = 0.29370781\n",
            "Iteration 49, loss = 0.29375321\n",
            "Iteration 50, loss = 0.29345378\n",
            "Iteration 51, loss = 0.29308765\n",
            "Iteration 52, loss = 0.29243880\n",
            "Iteration 53, loss = 0.29401777\n",
            "Iteration 54, loss = 0.29418738\n",
            "Iteration 55, loss = 0.29502298\n",
            "Iteration 56, loss = 0.29507133\n",
            "Iteration 57, loss = 0.29498142\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 58, loss = 0.27830676\n",
            "Iteration 59, loss = 0.27781118\n",
            "Iteration 60, loss = 0.27711288\n",
            "Iteration 61, loss = 0.27710233\n",
            "Iteration 62, loss = 0.27649523\n",
            "Iteration 63, loss = 0.27585048\n",
            "Iteration 64, loss = 0.27525055\n",
            "Iteration 65, loss = 0.27477788\n",
            "Iteration 66, loss = 0.27453518\n",
            "Iteration 67, loss = 0.27369594\n",
            "Iteration 68, loss = 0.27341790\n",
            "Iteration 69, loss = 0.27291256\n",
            "Iteration 70, loss = 0.27263362\n",
            "Iteration 71, loss = 0.27191536\n",
            "Iteration 72, loss = 0.27169557\n",
            "Iteration 73, loss = 0.27099700\n",
            "Iteration 74, loss = 0.27097439\n",
            "Iteration 75, loss = 0.27018866\n",
            "Iteration 76, loss = 0.26996793\n",
            "Iteration 77, loss = 0.26952073\n",
            "Iteration 78, loss = 0.26903491\n",
            "Iteration 79, loss = 0.26861656\n",
            "Iteration 80, loss = 0.26810819\n",
            "Iteration 81, loss = 0.26765789\n",
            "Iteration 82, loss = 0.26726116\n",
            "Iteration 83, loss = 0.26712578\n",
            "Iteration 84, loss = 0.26687096\n",
            "Iteration 85, loss = 0.26641820\n",
            "Iteration 86, loss = 0.26582712\n",
            "Iteration 87, loss = 0.26553137\n",
            "Iteration 88, loss = 0.26532727\n",
            "Iteration 89, loss = 0.26479613\n",
            "Iteration 90, loss = 0.26455135\n",
            "Iteration 91, loss = 0.26407263\n",
            "Iteration 92, loss = 0.26389128\n",
            "Iteration 93, loss = 0.26358168\n",
            "Iteration 94, loss = 0.26331833\n",
            "Iteration 95, loss = 0.26287198\n",
            "Iteration 96, loss = 0.26241855\n",
            "Iteration 97, loss = 0.26208217\n",
            "Iteration 98, loss = 0.26199348\n",
            "Iteration 99, loss = 0.26133160\n",
            "Iteration 100, loss = 0.26139381\n",
            "Iteration 101, loss = 0.26101043\n",
            "Iteration 102, loss = 0.26082859\n",
            "Iteration 103, loss = 0.26072567\n",
            "Iteration 104, loss = 0.26033646\n",
            "Iteration 105, loss = 0.25992929\n",
            "Iteration 106, loss = 0.25976682\n",
            "Iteration 107, loss = 0.25936825\n",
            "Iteration 108, loss = 0.25928095\n",
            "Iteration 109, loss = 0.25898831\n",
            "Iteration 110, loss = 0.25912857\n",
            "Iteration 111, loss = 0.25854227\n",
            "Iteration 112, loss = 0.25831888\n",
            "Iteration 113, loss = 0.25818487\n",
            "Iteration 114, loss = 0.25783566\n",
            "Iteration 115, loss = 0.25780293\n",
            "Iteration 116, loss = 0.25747046\n",
            "Iteration 117, loss = 0.25759532\n",
            "Iteration 118, loss = 0.25755551\n",
            "Iteration 119, loss = 0.25692155\n",
            "Iteration 120, loss = 0.25694034\n",
            "Iteration 121, loss = 0.25654157\n",
            "Iteration 122, loss = 0.25653795\n",
            "Iteration 123, loss = 0.25661959\n",
            "Iteration 124, loss = 0.25620530\n",
            "Iteration 125, loss = 0.25608515\n",
            "Iteration 126, loss = 0.25600052\n",
            "Iteration 127, loss = 0.25549863\n",
            "Iteration 128, loss = 0.25559920\n",
            "Iteration 129, loss = 0.25508306\n",
            "Iteration 130, loss = 0.25506839\n",
            "Iteration 131, loss = 0.25494286\n",
            "Iteration 132, loss = 0.25480723\n",
            "Iteration 133, loss = 0.25483679\n",
            "Iteration 134, loss = 0.25464413\n",
            "Iteration 135, loss = 0.25415838\n",
            "Iteration 136, loss = 0.25435269\n",
            "Iteration 137, loss = 0.25375501\n",
            "Iteration 138, loss = 0.25409157\n",
            "Iteration 139, loss = 0.25378941\n",
            "Iteration 140, loss = 0.25375529\n",
            "Iteration 141, loss = 0.25377404\n",
            "Iteration 142, loss = 0.25382272\n",
            "Iteration 143, loss = 0.25344335\n",
            "Iteration 144, loss = 0.25311790\n",
            "Iteration 145, loss = 0.25284946\n",
            "Iteration 146, loss = 0.25278735\n",
            "Iteration 147, loss = 0.25279558\n",
            "Iteration 148, loss = 0.25282662\n",
            "Iteration 149, loss = 0.25277106\n",
            "Iteration 150, loss = 0.25247621\n",
            "Iteration 151, loss = 0.25237963\n",
            "Iteration 152, loss = 0.25260725\n",
            "Iteration 153, loss = 0.25207171\n",
            "Iteration 154, loss = 0.25199184\n",
            "Iteration 155, loss = 0.25203623\n",
            "Iteration 156, loss = 0.25166036\n",
            "Iteration 157, loss = 0.25224551\n",
            "Iteration 158, loss = 0.25190067\n",
            "Iteration 159, loss = 0.25189550\n",
            "Iteration 160, loss = 0.25175313\n",
            "Iteration 161, loss = 0.25146356\n",
            "Iteration 162, loss = 0.25147015\n",
            "Iteration 163, loss = 0.25119557\n",
            "Iteration 164, loss = 0.25106749\n",
            "Iteration 165, loss = 0.25124718\n",
            "Iteration 166, loss = 0.25100676\n",
            "Iteration 167, loss = 0.25098646\n",
            "Iteration 168, loss = 0.25075990\n",
            "Iteration 169, loss = 0.25047222\n",
            "Iteration 170, loss = 0.25062736\n",
            "Iteration 171, loss = 0.25017485\n",
            "Iteration 172, loss = 0.25046019\n",
            "Iteration 173, loss = 0.25047927\n",
            "Iteration 174, loss = 0.25064724\n",
            "Iteration 175, loss = 0.25056105\n",
            "Iteration 176, loss = 0.25004737\n",
            "Iteration 177, loss = 0.25016711\n",
            "Iteration 178, loss = 0.24983438\n",
            "Iteration 179, loss = 0.25028458\n",
            "Iteration 180, loss = 0.25030491\n",
            "Iteration 181, loss = 0.24957305\n",
            "Iteration 182, loss = 0.25002455\n",
            "Iteration 183, loss = 0.24974474\n",
            "Iteration 184, loss = 0.24943379\n",
            "Iteration 185, loss = 0.24961914\n",
            "Iteration 186, loss = 0.24972728\n",
            "Iteration 187, loss = 0.24928873\n",
            "Iteration 188, loss = 0.24950315\n",
            "Iteration 189, loss = 0.24969946\n",
            "Iteration 190, loss = 0.24883022\n",
            "Iteration 191, loss = 0.24929994\n",
            "Iteration 192, loss = 0.24916929\n",
            "Iteration 193, loss = 0.24916369\n",
            "Iteration 194, loss = 0.24898891\n",
            "Iteration 195, loss = 0.24925774\n",
            "Iteration 196, loss = 0.24858930\n",
            "Iteration 197, loss = 0.24856976\n",
            "Iteration 198, loss = 0.24873673\n",
            "Iteration 199, loss = 0.24895161\n",
            "Iteration 200, loss = 0.24857765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23005640\n",
            "Iteration 3, loss = 0.17510909\n",
            "Iteration 4, loss = 0.13328555\n",
            "Iteration 5, loss = 0.10145126\n",
            "Iteration 6, loss = 0.07722036\n",
            "Iteration 7, loss = 0.05877684\n",
            "Iteration 8, loss = 0.04473841\n",
            "Iteration 9, loss = 0.03405297\n",
            "Iteration 10, loss = 0.02591966\n",
            "Iteration 11, loss = 0.01972894\n",
            "Iteration 12, loss = 0.01501682\n",
            "Iteration 13, loss = 0.01143016\n",
            "Iteration 14, loss = 0.00870015\n",
            "Iteration 15, loss = 0.00662218\n",
            "Iteration 16, loss = 0.00504052\n",
            "Iteration 17, loss = 0.00383664\n",
            "Iteration 18, loss = 0.00292094\n",
            "Iteration 19, loss = 0.00224006\n",
            "Iteration 20, loss = 0.00181318\n",
            "Iteration 21, loss = 0.00157105\n",
            "Iteration 22, loss = 0.00139936\n",
            "Iteration 23, loss = 0.00127118\n",
            "Iteration 24, loss = 0.00117504\n",
            "Iteration 25, loss = 0.00110270\n",
            "Iteration 26, loss = 0.00104813\n",
            "Iteration 27, loss = 0.00100687\n",
            "Iteration 28, loss = 0.00097561\n",
            "Iteration 29, loss = 0.00095191\n",
            "Iteration 30, loss = 0.00093391\n",
            "Iteration 31, loss = 0.00092023\n",
            "Iteration 32, loss = 0.00090983\n",
            "Iteration 33, loss = 0.00090191\n",
            "Iteration 34, loss = 0.00089587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00089286\n",
            "Iteration 36, loss = 0.00089199\n",
            "Iteration 37, loss = 0.00089118\n",
            "Iteration 38, loss = 0.00089040\n",
            "Iteration 39, loss = 0.00088967\n",
            "Iteration 40, loss = 0.00088897\n",
            "Iteration 41, loss = 0.00088831\n",
            "Iteration 42, loss = 0.00088769\n",
            "Iteration 43, loss = 0.00088710\n",
            "Iteration 44, loss = 0.00088654\n",
            "Iteration 45, loss = 0.00088601\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00088569\n",
            "Iteration 47, loss = 0.00088559\n",
            "Iteration 48, loss = 0.00088550\n",
            "Iteration 49, loss = 0.00088540\n",
            "Iteration 50, loss = 0.00088530\n",
            "Iteration 51, loss = 0.00088521\n",
            "Iteration 52, loss = 0.00088511\n",
            "Iteration 53, loss = 0.00088502\n",
            "Iteration 54, loss = 0.00088493\n",
            "Iteration 55, loss = 0.00088483\n",
            "Iteration 56, loss = 0.00088474\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00088469\n",
            "Iteration 58, loss = 0.00088467\n",
            "Iteration 59, loss = 0.00088465\n",
            "Iteration 60, loss = 0.00088464\n",
            "Iteration 61, loss = 0.00088462\n",
            "Iteration 62, loss = 0.00088460\n",
            "Iteration 63, loss = 0.00088458\n",
            "Iteration 64, loss = 0.00088456\n",
            "Iteration 65, loss = 0.00088455\n",
            "Iteration 66, loss = 0.00088453\n",
            "Iteration 67, loss = 0.00088451\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00088450\n",
            "Iteration 69, loss = 0.00088450\n",
            "Iteration 70, loss = 0.00088449\n",
            "Iteration 71, loss = 0.00088449\n",
            "Iteration 72, loss = 0.00088449\n",
            "Iteration 73, loss = 0.00088448\n",
            "Iteration 74, loss = 0.00088448\n",
            "Iteration 75, loss = 0.00088448\n",
            "Iteration 76, loss = 0.00088447\n",
            "Iteration 77, loss = 0.00088447\n",
            "Iteration 78, loss = 0.00088447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00088446\n",
            "Iteration 80, loss = 0.00088446\n",
            "Iteration 81, loss = 0.00088446\n",
            "Iteration 82, loss = 0.00088446\n",
            "Iteration 83, loss = 0.00088446\n",
            "Iteration 84, loss = 0.00088446\n",
            "Iteration 85, loss = 0.00088446\n",
            "Iteration 86, loss = 0.00088446\n",
            "Iteration 87, loss = 0.00088446\n",
            "Iteration 88, loss = 0.00088446\n",
            "Iteration 89, loss = 0.00088446\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12225565\n",
            "Iteration 2, loss = 0.09306469\n",
            "Iteration 3, loss = 0.07084112\n",
            "Iteration 4, loss = 0.05393313\n",
            "Iteration 5, loss = 0.04107049\n",
            "Iteration 6, loss = 0.03128846\n",
            "Iteration 7, loss = 0.02385340\n",
            "Iteration 8, loss = 0.01820667\n",
            "Iteration 9, loss = 0.01392318\n",
            "Iteration 10, loss = 0.01067942\n",
            "Iteration 11, loss = 0.00822949\n",
            "Iteration 12, loss = 0.00638559\n",
            "Iteration 13, loss = 0.00500278\n",
            "Iteration 14, loss = 0.00396901\n",
            "Iteration 15, loss = 0.00319761\n",
            "Iteration 16, loss = 0.00262219\n",
            "Iteration 17, loss = 0.00219267\n",
            "Iteration 18, loss = 0.00187125\n",
            "Iteration 19, loss = 0.00162997\n",
            "Iteration 20, loss = 0.00144826\n",
            "Iteration 21, loss = 0.00131103\n",
            "Iteration 22, loss = 0.00120720\n",
            "Iteration 23, loss = 0.00112853\n",
            "Iteration 24, loss = 0.00106885\n",
            "Iteration 25, loss = 0.00102353\n",
            "Iteration 26, loss = 0.00098909\n",
            "Iteration 27, loss = 0.00096290\n",
            "Iteration 28, loss = 0.00094298\n",
            "Iteration 29, loss = 0.00092782\n",
            "Iteration 30, loss = 0.00091627\n",
            "Iteration 31, loss = 0.00090747\n",
            "Iteration 32, loss = 0.00090076\n",
            "Iteration 33, loss = 0.00089565\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089309\n",
            "Iteration 35, loss = 0.00089235\n",
            "Iteration 36, loss = 0.00089166\n",
            "Iteration 37, loss = 0.00089100\n",
            "Iteration 38, loss = 0.00089038\n",
            "Iteration 39, loss = 0.00088979\n",
            "Iteration 40, loss = 0.00088923\n",
            "Iteration 41, loss = 0.00088870\n",
            "Iteration 42, loss = 0.00088819\n",
            "Iteration 43, loss = 0.00088771\n",
            "Iteration 44, loss = 0.00088726\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00088699\n",
            "Iteration 46, loss = 0.00088691\n",
            "Iteration 47, loss = 0.00088682\n",
            "Iteration 48, loss = 0.00088674\n",
            "Iteration 49, loss = 0.00088666\n",
            "Iteration 50, loss = 0.00088658\n",
            "Iteration 51, loss = 0.00088650\n",
            "Iteration 52, loss = 0.00088642\n",
            "Iteration 53, loss = 0.00088634\n",
            "Iteration 54, loss = 0.00088626\n",
            "Iteration 55, loss = 0.00088618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00088614\n",
            "Iteration 57, loss = 0.00088612\n",
            "Iteration 58, loss = 0.00088611\n",
            "Iteration 59, loss = 0.00088609\n",
            "Iteration 60, loss = 0.00088608\n",
            "Iteration 61, loss = 0.00088606\n",
            "Iteration 62, loss = 0.00088605\n",
            "Iteration 63, loss = 0.00088603\n",
            "Iteration 64, loss = 0.00088602\n",
            "Iteration 65, loss = 0.00088600\n",
            "Iteration 66, loss = 0.00088599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00088598\n",
            "Iteration 68, loss = 0.00088597\n",
            "Iteration 69, loss = 0.00088597\n",
            "Iteration 70, loss = 0.00088597\n",
            "Iteration 71, loss = 0.00088596\n",
            "Iteration 72, loss = 0.00088596\n",
            "Iteration 73, loss = 0.00088596\n",
            "Iteration 74, loss = 0.00088596\n",
            "Iteration 75, loss = 0.00088595\n",
            "Iteration 76, loss = 0.00088595\n",
            "Iteration 77, loss = 0.00088595\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00088595\n",
            "Iteration 79, loss = 0.00088594\n",
            "Iteration 80, loss = 0.00088594\n",
            "Iteration 81, loss = 0.00088594\n",
            "Iteration 82, loss = 0.00088594\n",
            "Iteration 83, loss = 0.00088594\n",
            "Iteration 84, loss = 0.00088594\n",
            "Iteration 85, loss = 0.00088594\n",
            "Iteration 86, loss = 0.00088594\n",
            "Iteration 87, loss = 0.00088594\n",
            "Iteration 88, loss = 0.00088594\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13358886\n",
            "Iteration 2, loss = 0.10169138\n",
            "Iteration 3, loss = 0.07740313\n",
            "Iteration 4, loss = 0.05891612\n",
            "Iteration 5, loss = 0.04484937\n",
            "Iteration 6, loss = 0.03416437\n",
            "Iteration 7, loss = 0.02604932\n",
            "Iteration 8, loss = 0.01988440\n",
            "Iteration 9, loss = 0.01520556\n",
            "Iteration 10, loss = 0.01166022\n",
            "Iteration 11, loss = 0.00897994\n",
            "Iteration 12, loss = 0.00695946\n",
            "Iteration 13, loss = 0.00544144\n",
            "Iteration 14, loss = 0.00430456\n",
            "Iteration 15, loss = 0.00345554\n",
            "Iteration 16, loss = 0.00282182\n",
            "Iteration 17, loss = 0.00234827\n",
            "Iteration 18, loss = 0.00199319\n",
            "Iteration 19, loss = 0.00172605\n",
            "Iteration 20, loss = 0.00152450\n",
            "Iteration 21, loss = 0.00137210\n",
            "Iteration 22, loss = 0.00125668\n",
            "Iteration 23, loss = 0.00116914\n",
            "Iteration 24, loss = 0.00110269\n",
            "Iteration 25, loss = 0.00105221\n",
            "Iteration 26, loss = 0.00101385\n",
            "Iteration 27, loss = 0.00098468\n",
            "Iteration 28, loss = 0.00096250\n",
            "Iteration 29, loss = 0.00094561\n",
            "Iteration 30, loss = 0.00093275\n",
            "Iteration 31, loss = 0.00092295\n",
            "Iteration 32, loss = 0.00091547\n",
            "Iteration 33, loss = 0.00090976\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00090690\n",
            "Iteration 35, loss = 0.00090608\n",
            "Iteration 36, loss = 0.00090531\n",
            "Iteration 37, loss = 0.00090458\n",
            "Iteration 38, loss = 0.00090388\n",
            "Iteration 39, loss = 0.00090322\n",
            "Iteration 40, loss = 0.00090260\n",
            "Iteration 41, loss = 0.00090201\n",
            "Iteration 42, loss = 0.00090144\n",
            "Iteration 43, loss = 0.00090091\n",
            "Iteration 44, loss = 0.00090041\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090011\n",
            "Iteration 46, loss = 0.00090002\n",
            "Iteration 47, loss = 0.00089992\n",
            "Iteration 48, loss = 0.00089983\n",
            "Iteration 49, loss = 0.00089974\n",
            "Iteration 50, loss = 0.00089965\n",
            "Iteration 51, loss = 0.00089956\n",
            "Iteration 52, loss = 0.00089947\n",
            "Iteration 53, loss = 0.00089938\n",
            "Iteration 54, loss = 0.00089930\n",
            "Iteration 55, loss = 0.00089921\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089916\n",
            "Iteration 57, loss = 0.00089914\n",
            "Iteration 58, loss = 0.00089912\n",
            "Iteration 59, loss = 0.00089911\n",
            "Iteration 60, loss = 0.00089909\n",
            "Iteration 61, loss = 0.00089907\n",
            "Iteration 62, loss = 0.00089906\n",
            "Iteration 63, loss = 0.00089904\n",
            "Iteration 64, loss = 0.00089902\n",
            "Iteration 65, loss = 0.00089901\n",
            "Iteration 66, loss = 0.00089899\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089898\n",
            "Iteration 68, loss = 0.00089898\n",
            "Iteration 69, loss = 0.00089897\n",
            "Iteration 70, loss = 0.00089897\n",
            "Iteration 71, loss = 0.00089897\n",
            "Iteration 72, loss = 0.00089896\n",
            "Iteration 73, loss = 0.00089896\n",
            "Iteration 74, loss = 0.00089896\n",
            "Iteration 75, loss = 0.00089895\n",
            "Iteration 76, loss = 0.00089895\n",
            "Iteration 77, loss = 0.00089895\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089895\n",
            "Iteration 79, loss = 0.00089895\n",
            "Iteration 80, loss = 0.00089894\n",
            "Iteration 81, loss = 0.00089894\n",
            "Iteration 82, loss = 0.00089894\n",
            "Iteration 83, loss = 0.00089894\n",
            "Iteration 84, loss = 0.00089894\n",
            "Iteration 85, loss = 0.00089894\n",
            "Iteration 86, loss = 0.00089894\n",
            "Iteration 87, loss = 0.00089894\n",
            "Iteration 88, loss = 0.00089894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.21213630\n",
            "Iteration 3, loss = 0.16146908\n",
            "Iteration 4, loss = 0.12290336\n",
            "Iteration 5, loss = 0.09354878\n",
            "Iteration 6, loss = 0.07120533\n",
            "Iteration 7, loss = 0.05419845\n",
            "Iteration 8, loss = 0.04125354\n",
            "Iteration 9, loss = 0.03140043\n",
            "Iteration 10, loss = 0.02390067\n",
            "Iteration 11, loss = 0.01819216\n",
            "Iteration 12, loss = 0.01384710\n",
            "Iteration 13, loss = 0.01053982\n",
            "Iteration 14, loss = 0.00802246\n",
            "Iteration 15, loss = 0.00610635\n",
            "Iteration 16, loss = 0.00464790\n",
            "Iteration 17, loss = 0.00353855\n",
            "Iteration 18, loss = 0.00271155\n",
            "Iteration 19, loss = 0.00217103\n",
            "Iteration 20, loss = 0.00183262\n",
            "Iteration 21, loss = 0.00158873\n",
            "Iteration 22, loss = 0.00140852\n",
            "Iteration 23, loss = 0.00127465\n",
            "Iteration 24, loss = 0.00117460\n",
            "Iteration 25, loss = 0.00109950\n",
            "Iteration 26, loss = 0.00104310\n",
            "Iteration 27, loss = 0.00100068\n",
            "Iteration 28, loss = 0.00096873\n",
            "Iteration 29, loss = 0.00094462\n",
            "Iteration 30, loss = 0.00092638\n",
            "Iteration 31, loss = 0.00091255\n",
            "Iteration 32, loss = 0.00090207\n",
            "Iteration 33, loss = 0.00089408\n",
            "Iteration 34, loss = 0.00088799\n",
            "Iteration 35, loss = 0.00088335\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.00088103\n",
            "Iteration 37, loss = 0.00088036\n",
            "Iteration 38, loss = 0.00087972\n",
            "Iteration 39, loss = 0.00087912\n",
            "Iteration 40, loss = 0.00087855\n",
            "Iteration 41, loss = 0.00087801\n",
            "Iteration 42, loss = 0.00087750\n",
            "Iteration 43, loss = 0.00087702\n",
            "Iteration 44, loss = 0.00087657\n",
            "Iteration 45, loss = 0.00087614\n",
            "Iteration 46, loss = 0.00087573\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.00087549\n",
            "Iteration 48, loss = 0.00087541\n",
            "Iteration 49, loss = 0.00087533\n",
            "Iteration 50, loss = 0.00087526\n",
            "Iteration 51, loss = 0.00087519\n",
            "Iteration 52, loss = 0.00087511\n",
            "Iteration 53, loss = 0.00087504\n",
            "Iteration 54, loss = 0.00087497\n",
            "Iteration 55, loss = 0.00087490\n",
            "Iteration 56, loss = 0.00087483\n",
            "Iteration 57, loss = 0.00087476\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 58, loss = 0.00087472\n",
            "Iteration 59, loss = 0.00087470\n",
            "Iteration 60, loss = 0.00087469\n",
            "Iteration 61, loss = 0.00087467\n",
            "Iteration 62, loss = 0.00087466\n",
            "Iteration 63, loss = 0.00087465\n",
            "Iteration 64, loss = 0.00087463\n",
            "Iteration 65, loss = 0.00087462\n",
            "Iteration 66, loss = 0.00087461\n",
            "Iteration 67, loss = 0.00087459\n",
            "Iteration 68, loss = 0.00087458\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 69, loss = 0.00087457\n",
            "Iteration 70, loss = 0.00087457\n",
            "Iteration 71, loss = 0.00087457\n",
            "Iteration 72, loss = 0.00087456\n",
            "Iteration 73, loss = 0.00087456\n",
            "Iteration 74, loss = 0.00087456\n",
            "Iteration 75, loss = 0.00087456\n",
            "Iteration 76, loss = 0.00087455\n",
            "Iteration 77, loss = 0.00087455\n",
            "Iteration 78, loss = 0.00087455\n",
            "Iteration 79, loss = 0.00087455\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 80, loss = 0.00087454\n",
            "Iteration 81, loss = 0.00087454\n",
            "Iteration 82, loss = 0.00087454\n",
            "Iteration 83, loss = 0.00087454\n",
            "Iteration 84, loss = 0.00087454\n",
            "Iteration 85, loss = 0.00087454\n",
            "Iteration 86, loss = 0.00087454\n",
            "Iteration 87, loss = 0.00087454\n",
            "Iteration 88, loss = 0.00087454\n",
            "Iteration 89, loss = 0.00087454\n",
            "Iteration 90, loss = 0.00087454\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13114136\n",
            "Iteration 2, loss = 0.09982828\n",
            "Iteration 3, loss = 0.07598501\n",
            "Iteration 4, loss = 0.05783664\n",
            "Iteration 5, loss = 0.04402654\n",
            "Iteration 6, loss = 0.03353686\n",
            "Iteration 7, loss = 0.02557344\n",
            "Iteration 8, loss = 0.01952487\n",
            "Iteration 9, loss = 0.01493609\n",
            "Iteration 10, loss = 0.01146109\n",
            "Iteration 11, loss = 0.00883584\n",
            "Iteration 12, loss = 0.00685947\n",
            "Iteration 13, loss = 0.00537684\n",
            "Iteration 14, loss = 0.00426672\n",
            "Iteration 15, loss = 0.00343542\n",
            "Iteration 16, loss = 0.00281221\n",
            "Iteration 17, loss = 0.00234444\n",
            "Iteration 18, loss = 0.00199263\n",
            "Iteration 19, loss = 0.00172697\n",
            "Iteration 20, loss = 0.00152601\n",
            "Iteration 21, loss = 0.00137376\n",
            "Iteration 22, loss = 0.00125830\n",
            "Iteration 23, loss = 0.00117064\n",
            "Iteration 24, loss = 0.00110406\n",
            "Iteration 25, loss = 0.00105344\n",
            "Iteration 26, loss = 0.00101495\n",
            "Iteration 27, loss = 0.00098566\n",
            "Iteration 28, loss = 0.00096337\n",
            "Iteration 29, loss = 0.00094639\n",
            "Iteration 30, loss = 0.00093345\n",
            "Iteration 31, loss = 0.00092359\n",
            "Iteration 32, loss = 0.00091608\n",
            "Iteration 33, loss = 0.00091036\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00090750\n",
            "Iteration 35, loss = 0.00090668\n",
            "Iteration 36, loss = 0.00090590\n",
            "Iteration 37, loss = 0.00090516\n",
            "Iteration 38, loss = 0.00090447\n",
            "Iteration 39, loss = 0.00090381\n",
            "Iteration 40, loss = 0.00090318\n",
            "Iteration 41, loss = 0.00090259\n",
            "Iteration 42, loss = 0.00090202\n",
            "Iteration 43, loss = 0.00090149\n",
            "Iteration 44, loss = 0.00090098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090069\n",
            "Iteration 46, loss = 0.00090059\n",
            "Iteration 47, loss = 0.00090050\n",
            "Iteration 48, loss = 0.00090040\n",
            "Iteration 49, loss = 0.00090031\n",
            "Iteration 50, loss = 0.00090022\n",
            "Iteration 51, loss = 0.00090013\n",
            "Iteration 52, loss = 0.00090004\n",
            "Iteration 53, loss = 0.00089995\n",
            "Iteration 54, loss = 0.00089987\n",
            "Iteration 55, loss = 0.00089978\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089973\n",
            "Iteration 57, loss = 0.00089971\n",
            "Iteration 58, loss = 0.00089970\n",
            "Iteration 59, loss = 0.00089968\n",
            "Iteration 60, loss = 0.00089966\n",
            "Iteration 61, loss = 0.00089964\n",
            "Iteration 62, loss = 0.00089963\n",
            "Iteration 63, loss = 0.00089961\n",
            "Iteration 64, loss = 0.00089959\n",
            "Iteration 65, loss = 0.00089958\n",
            "Iteration 66, loss = 0.00089956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089955\n",
            "Iteration 68, loss = 0.00089955\n",
            "Iteration 69, loss = 0.00089954\n",
            "Iteration 70, loss = 0.00089954\n",
            "Iteration 71, loss = 0.00089954\n",
            "Iteration 72, loss = 0.00089953\n",
            "Iteration 73, loss = 0.00089953\n",
            "Iteration 74, loss = 0.00089953\n",
            "Iteration 75, loss = 0.00089952\n",
            "Iteration 76, loss = 0.00089952\n",
            "Iteration 77, loss = 0.00089952\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089952\n",
            "Iteration 79, loss = 0.00089951\n",
            "Iteration 80, loss = 0.00089951\n",
            "Iteration 81, loss = 0.00089951\n",
            "Iteration 82, loss = 0.00089951\n",
            "Iteration 83, loss = 0.00089951\n",
            "Iteration 84, loss = 0.00089951\n",
            "Iteration 85, loss = 0.00089951\n",
            "Iteration 86, loss = 0.00089951\n",
            "Iteration 87, loss = 0.00089951\n",
            "Iteration 88, loss = 0.00089951\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13373040\n",
            "Iteration 2, loss = 0.10120729\n",
            "Iteration 3, loss = 0.07703466\n",
            "Iteration 4, loss = 0.05863549\n",
            "Iteration 5, loss = 0.04463082\n",
            "Iteration 6, loss = 0.03397107\n",
            "Iteration 7, loss = 0.02585733\n",
            "Iteration 8, loss = 0.01968151\n",
            "Iteration 9, loss = 0.01498170\n",
            "Iteration 10, loss = 0.01142418\n",
            "Iteration 11, loss = 0.00878876\n",
            "Iteration 12, loss = 0.00683079\n",
            "Iteration 13, loss = 0.00536208\n",
            "Iteration 14, loss = 0.00426185\n",
            "Iteration 15, loss = 0.00343755\n",
            "Iteration 16, loss = 0.00281877\n",
            "Iteration 17, loss = 0.00235346\n",
            "Iteration 18, loss = 0.00200287\n",
            "Iteration 19, loss = 0.00173784\n",
            "Iteration 20, loss = 0.00153710\n",
            "Iteration 21, loss = 0.00138484\n",
            "Iteration 22, loss = 0.00126923\n",
            "Iteration 23, loss = 0.00118139\n",
            "Iteration 24, loss = 0.00111461\n",
            "Iteration 25, loss = 0.00106383\n",
            "Iteration 26, loss = 0.00102520\n",
            "Iteration 27, loss = 0.00099580\n",
            "Iteration 28, loss = 0.00097343\n",
            "Iteration 29, loss = 0.00095639\n",
            "Iteration 30, loss = 0.00094341\n",
            "Iteration 31, loss = 0.00093352\n",
            "Iteration 32, loss = 0.00092598\n",
            "Iteration 33, loss = 0.00092023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091735\n",
            "Iteration 35, loss = 0.00091653\n",
            "Iteration 36, loss = 0.00091575\n",
            "Iteration 37, loss = 0.00091501\n",
            "Iteration 38, loss = 0.00091431\n",
            "Iteration 39, loss = 0.00091364\n",
            "Iteration 40, loss = 0.00091301\n",
            "Iteration 41, loss = 0.00091242\n",
            "Iteration 42, loss = 0.00091185\n",
            "Iteration 43, loss = 0.00091132\n",
            "Iteration 44, loss = 0.00091081\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00091051\n",
            "Iteration 46, loss = 0.00091041\n",
            "Iteration 47, loss = 0.00091032\n",
            "Iteration 48, loss = 0.00091023\n",
            "Iteration 49, loss = 0.00091013\n",
            "Iteration 50, loss = 0.00091004\n",
            "Iteration 51, loss = 0.00090995\n",
            "Iteration 52, loss = 0.00090986\n",
            "Iteration 53, loss = 0.00090977\n",
            "Iteration 54, loss = 0.00090969\n",
            "Iteration 55, loss = 0.00090960\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090955\n",
            "Iteration 57, loss = 0.00090953\n",
            "Iteration 58, loss = 0.00090951\n",
            "Iteration 59, loss = 0.00090950\n",
            "Iteration 60, loss = 0.00090948\n",
            "Iteration 61, loss = 0.00090946\n",
            "Iteration 62, loss = 0.00090945\n",
            "Iteration 63, loss = 0.00090943\n",
            "Iteration 64, loss = 0.00090941\n",
            "Iteration 65, loss = 0.00090939\n",
            "Iteration 66, loss = 0.00090938\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090937\n",
            "Iteration 68, loss = 0.00090936\n",
            "Iteration 69, loss = 0.00090936\n",
            "Iteration 70, loss = 0.00090936\n",
            "Iteration 71, loss = 0.00090935\n",
            "Iteration 72, loss = 0.00090935\n",
            "Iteration 73, loss = 0.00090935\n",
            "Iteration 74, loss = 0.00090934\n",
            "Iteration 75, loss = 0.00090934\n",
            "Iteration 76, loss = 0.00090934\n",
            "Iteration 77, loss = 0.00090933\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090933\n",
            "Iteration 79, loss = 0.00090933\n",
            "Iteration 80, loss = 0.00090933\n",
            "Iteration 81, loss = 0.00090933\n",
            "Iteration 82, loss = 0.00090933\n",
            "Iteration 83, loss = 0.00090933\n",
            "Iteration 84, loss = 0.00090933\n",
            "Iteration 85, loss = 0.00090933\n",
            "Iteration 86, loss = 0.00090933\n",
            "Iteration 87, loss = 0.00090933\n",
            "Iteration 88, loss = 0.00090933\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.21652921\n",
            "Iteration 3, loss = 0.16481278\n",
            "Iteration 4, loss = 0.12544843\n",
            "Iteration 5, loss = 0.09548598\n",
            "Iteration 6, loss = 0.07267985\n",
            "Iteration 7, loss = 0.05532079\n",
            "Iteration 8, loss = 0.04210782\n",
            "Iteration 9, loss = 0.03205067\n",
            "Iteration 10, loss = 0.02439560\n",
            "Iteration 11, loss = 0.01856889\n",
            "Iteration 12, loss = 0.01413384\n",
            "Iteration 13, loss = 0.01075808\n",
            "Iteration 14, loss = 0.00818859\n",
            "Iteration 15, loss = 0.00623280\n",
            "Iteration 16, loss = 0.00474414\n",
            "Iteration 17, loss = 0.00361126\n",
            "Iteration 18, loss = 0.00275670\n",
            "Iteration 19, loss = 0.00217625\n",
            "Iteration 20, loss = 0.00183658\n",
            "Iteration 21, loss = 0.00160129\n",
            "Iteration 22, loss = 0.00142422\n",
            "Iteration 23, loss = 0.00129202\n",
            "Iteration 24, loss = 0.00119366\n",
            "Iteration 25, loss = 0.00112014\n",
            "Iteration 26, loss = 0.00106496\n",
            "Iteration 27, loss = 0.00102342\n",
            "Iteration 28, loss = 0.00099206\n",
            "Iteration 29, loss = 0.00096834\n",
            "Iteration 30, loss = 0.00095037\n",
            "Iteration 31, loss = 0.00093674\n",
            "Iteration 32, loss = 0.00092639\n",
            "Iteration 33, loss = 0.00091851\n",
            "Iteration 34, loss = 0.00091251\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00090951\n",
            "Iteration 36, loss = 0.00090865\n",
            "Iteration 37, loss = 0.00090784\n",
            "Iteration 38, loss = 0.00090707\n",
            "Iteration 39, loss = 0.00090634\n",
            "Iteration 40, loss = 0.00090564\n",
            "Iteration 41, loss = 0.00090499\n",
            "Iteration 42, loss = 0.00090437\n",
            "Iteration 43, loss = 0.00090378\n",
            "Iteration 44, loss = 0.00090322\n",
            "Iteration 45, loss = 0.00090269\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00090238\n",
            "Iteration 47, loss = 0.00090228\n",
            "Iteration 48, loss = 0.00090218\n",
            "Iteration 49, loss = 0.00090209\n",
            "Iteration 50, loss = 0.00090199\n",
            "Iteration 51, loss = 0.00090190\n",
            "Iteration 52, loss = 0.00090180\n",
            "Iteration 53, loss = 0.00090171\n",
            "Iteration 54, loss = 0.00090162\n",
            "Iteration 55, loss = 0.00090153\n",
            "Iteration 56, loss = 0.00090144\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00090138\n",
            "Iteration 58, loss = 0.00090136\n",
            "Iteration 59, loss = 0.00090135\n",
            "Iteration 60, loss = 0.00090133\n",
            "Iteration 61, loss = 0.00090131\n",
            "Iteration 62, loss = 0.00090129\n",
            "Iteration 63, loss = 0.00090128\n",
            "Iteration 64, loss = 0.00090126\n",
            "Iteration 65, loss = 0.00090124\n",
            "Iteration 66, loss = 0.00090122\n",
            "Iteration 67, loss = 0.00090121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00090120\n",
            "Iteration 69, loss = 0.00090119\n",
            "Iteration 70, loss = 0.00090119\n",
            "Iteration 71, loss = 0.00090119\n",
            "Iteration 72, loss = 0.00090118\n",
            "Iteration 73, loss = 0.00090118\n",
            "Iteration 74, loss = 0.00090117\n",
            "Iteration 75, loss = 0.00090117\n",
            "Iteration 76, loss = 0.00090117\n",
            "Iteration 77, loss = 0.00090116\n",
            "Iteration 78, loss = 0.00090116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00090116\n",
            "Iteration 80, loss = 0.00090116\n",
            "Iteration 81, loss = 0.00090116\n",
            "Iteration 82, loss = 0.00090116\n",
            "Iteration 83, loss = 0.00090116\n",
            "Iteration 84, loss = 0.00090116\n",
            "Iteration 85, loss = 0.00090115\n",
            "Iteration 86, loss = 0.00090115\n",
            "Iteration 87, loss = 0.00090115\n",
            "Iteration 88, loss = 0.00090115\n",
            "Iteration 89, loss = 0.00090115\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12872463\n",
            "Iteration 2, loss = 0.09799017\n",
            "Iteration 3, loss = 0.07459727\n",
            "Iteration 4, loss = 0.05680285\n",
            "Iteration 5, loss = 0.04326608\n",
            "Iteration 6, loss = 0.03297106\n",
            "Iteration 7, loss = 0.02514528\n",
            "Iteration 8, loss = 0.01920119\n",
            "Iteration 9, loss = 0.01469184\n",
            "Iteration 10, loss = 0.01127779\n",
            "Iteration 11, loss = 0.00869970\n",
            "Iteration 12, loss = 0.00675851\n",
            "Iteration 13, loss = 0.00530147\n",
            "Iteration 14, loss = 0.00421120\n",
            "Iteration 15, loss = 0.00339533\n",
            "Iteration 16, loss = 0.00278390\n",
            "Iteration 17, loss = 0.00232458\n",
            "Iteration 18, loss = 0.00197879\n",
            "Iteration 19, loss = 0.00171789\n",
            "Iteration 20, loss = 0.00152047\n",
            "Iteration 21, loss = 0.00137080\n",
            "Iteration 22, loss = 0.00125718\n",
            "Iteration 23, loss = 0.00117071\n",
            "Iteration 24, loss = 0.00110504\n",
            "Iteration 25, loss = 0.00105515\n",
            "Iteration 26, loss = 0.00101724\n",
            "Iteration 27, loss = 0.00098842\n",
            "Iteration 28, loss = 0.00096649\n",
            "Iteration 29, loss = 0.00094979\n",
            "Iteration 30, loss = 0.00093708\n",
            "Iteration 31, loss = 0.00092740\n",
            "Iteration 32, loss = 0.00092002\n",
            "Iteration 33, loss = 0.00091439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00091158\n",
            "Iteration 35, loss = 0.00091077\n",
            "Iteration 36, loss = 0.00091000\n",
            "Iteration 37, loss = 0.00090927\n",
            "Iteration 38, loss = 0.00090859\n",
            "Iteration 39, loss = 0.00090793\n",
            "Iteration 40, loss = 0.00090732\n",
            "Iteration 41, loss = 0.00090673\n",
            "Iteration 42, loss = 0.00090618\n",
            "Iteration 43, loss = 0.00090565\n",
            "Iteration 44, loss = 0.00090515\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090486\n",
            "Iteration 46, loss = 0.00090477\n",
            "Iteration 47, loss = 0.00090467\n",
            "Iteration 48, loss = 0.00090458\n",
            "Iteration 49, loss = 0.00090449\n",
            "Iteration 50, loss = 0.00090440\n",
            "Iteration 51, loss = 0.00090431\n",
            "Iteration 52, loss = 0.00090423\n",
            "Iteration 53, loss = 0.00090414\n",
            "Iteration 54, loss = 0.00090405\n",
            "Iteration 55, loss = 0.00090397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090392\n",
            "Iteration 57, loss = 0.00090390\n",
            "Iteration 58, loss = 0.00090388\n",
            "Iteration 59, loss = 0.00090387\n",
            "Iteration 60, loss = 0.00090385\n",
            "Iteration 61, loss = 0.00090383\n",
            "Iteration 62, loss = 0.00090382\n",
            "Iteration 63, loss = 0.00090380\n",
            "Iteration 64, loss = 0.00090378\n",
            "Iteration 65, loss = 0.00090377\n",
            "Iteration 66, loss = 0.00090375\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090374\n",
            "Iteration 68, loss = 0.00090374\n",
            "Iteration 69, loss = 0.00090373\n",
            "Iteration 70, loss = 0.00090373\n",
            "Iteration 71, loss = 0.00090373\n",
            "Iteration 72, loss = 0.00090373\n",
            "Iteration 73, loss = 0.00090372\n",
            "Iteration 74, loss = 0.00090372\n",
            "Iteration 75, loss = 0.00090372\n",
            "Iteration 76, loss = 0.00090371\n",
            "Iteration 77, loss = 0.00090371\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090371\n",
            "Iteration 79, loss = 0.00090371\n",
            "Iteration 80, loss = 0.00090371\n",
            "Iteration 81, loss = 0.00090370\n",
            "Iteration 82, loss = 0.00090370\n",
            "Iteration 83, loss = 0.00090370\n",
            "Iteration 84, loss = 0.00090370\n",
            "Iteration 85, loss = 0.00090370\n",
            "Iteration 86, loss = 0.00090370\n",
            "Iteration 87, loss = 0.00090370\n",
            "Iteration 88, loss = 0.00090370\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.29361885\n",
            "Iteration 2, loss = 0.19384343\n",
            "Iteration 3, loss = 0.14754532\n",
            "Iteration 4, loss = 0.11230519\n",
            "Iteration 5, loss = 0.08548191\n",
            "Iteration 6, loss = 0.06506517\n",
            "Iteration 7, loss = 0.04952483\n",
            "Iteration 8, loss = 0.03769618\n",
            "Iteration 9, loss = 0.02869272\n",
            "Iteration 10, loss = 0.02183967\n",
            "Iteration 11, loss = 0.01662342\n",
            "Iteration 12, loss = 0.01265304\n",
            "Iteration 13, loss = 0.00963095\n",
            "Iteration 14, loss = 0.00733067\n",
            "Iteration 15, loss = 0.00557979\n",
            "Iteration 16, loss = 0.00424737\n",
            "Iteration 17, loss = 0.00324237\n",
            "Iteration 18, loss = 0.00255358\n",
            "Iteration 19, loss = 0.00212799\n",
            "Iteration 20, loss = 0.00182453\n",
            "Iteration 21, loss = 0.00159787\n",
            "Iteration 22, loss = 0.00142790\n",
            "Iteration 23, loss = 0.00130005\n",
            "Iteration 24, loss = 0.00120365\n",
            "Iteration 25, loss = 0.00113079\n",
            "Iteration 26, loss = 0.00107564\n",
            "Iteration 27, loss = 0.00103381\n",
            "Iteration 28, loss = 0.00100198\n",
            "Iteration 29, loss = 0.00097766\n",
            "Iteration 30, loss = 0.00095926\n",
            "Iteration 31, loss = 0.00094531\n",
            "Iteration 32, loss = 0.00093474\n",
            "Iteration 33, loss = 0.00092671\n",
            "Iteration 34, loss = 0.00092062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00091758\n",
            "Iteration 36, loss = 0.00091672\n",
            "Iteration 37, loss = 0.00091589\n",
            "Iteration 38, loss = 0.00091511\n",
            "Iteration 39, loss = 0.00091438\n",
            "Iteration 40, loss = 0.00091368\n",
            "Iteration 41, loss = 0.00091302\n",
            "Iteration 42, loss = 0.00091239\n",
            "Iteration 43, loss = 0.00091180\n",
            "Iteration 44, loss = 0.00091123\n",
            "Iteration 45, loss = 0.00091070\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00091039\n",
            "Iteration 47, loss = 0.00091029\n",
            "Iteration 48, loss = 0.00091019\n",
            "Iteration 49, loss = 0.00091009\n",
            "Iteration 50, loss = 0.00090999\n",
            "Iteration 51, loss = 0.00090990\n",
            "Iteration 52, loss = 0.00090980\n",
            "Iteration 53, loss = 0.00090971\n",
            "Iteration 54, loss = 0.00090962\n",
            "Iteration 55, loss = 0.00090952\n",
            "Iteration 56, loss = 0.00090943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00090938\n",
            "Iteration 58, loss = 0.00090936\n",
            "Iteration 59, loss = 0.00090934\n",
            "Iteration 60, loss = 0.00090932\n",
            "Iteration 61, loss = 0.00090931\n",
            "Iteration 62, loss = 0.00090929\n",
            "Iteration 63, loss = 0.00090927\n",
            "Iteration 64, loss = 0.00090925\n",
            "Iteration 65, loss = 0.00090924\n",
            "Iteration 66, loss = 0.00090922\n",
            "Iteration 67, loss = 0.00090920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00090919\n",
            "Iteration 69, loss = 0.00090919\n",
            "Iteration 70, loss = 0.00090918\n",
            "Iteration 71, loss = 0.00090918\n",
            "Iteration 72, loss = 0.00090918\n",
            "Iteration 73, loss = 0.00090917\n",
            "Iteration 74, loss = 0.00090917\n",
            "Iteration 75, loss = 0.00090917\n",
            "Iteration 76, loss = 0.00090916\n",
            "Iteration 77, loss = 0.00090916\n",
            "Iteration 78, loss = 0.00090916\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00090915\n",
            "Iteration 80, loss = 0.00090915\n",
            "Iteration 81, loss = 0.00090915\n",
            "Iteration 82, loss = 0.00090915\n",
            "Iteration 83, loss = 0.00090915\n",
            "Iteration 84, loss = 0.00090915\n",
            "Iteration 85, loss = 0.00090915\n",
            "Iteration 86, loss = 0.00090915\n",
            "Iteration 87, loss = 0.00090915\n",
            "Iteration 88, loss = 0.00090915\n",
            "Iteration 89, loss = 0.00090915\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.18453243\n",
            "Iteration 2, loss = 0.13805544\n",
            "Iteration 3, loss = 0.10508190\n",
            "Iteration 4, loss = 0.07998385\n",
            "Iteration 5, loss = 0.06088028\n",
            "Iteration 6, loss = 0.04633947\n",
            "Iteration 7, loss = 0.03527162\n",
            "Iteration 8, loss = 0.02684725\n",
            "Iteration 9, loss = 0.02043498\n",
            "Iteration 10, loss = 0.01555423\n",
            "Iteration 11, loss = 0.01183922\n",
            "Iteration 12, loss = 0.00901150\n",
            "Iteration 13, loss = 0.00685917\n",
            "Iteration 14, loss = 0.00522091\n",
            "Iteration 15, loss = 0.00397416\n",
            "Iteration 16, loss = 0.00303292\n",
            "Iteration 17, loss = 0.00238525\n",
            "Iteration 18, loss = 0.00199498\n",
            "Iteration 19, loss = 0.00172149\n",
            "Iteration 20, loss = 0.00151614\n",
            "Iteration 21, loss = 0.00136119\n",
            "Iteration 22, loss = 0.00124401\n",
            "Iteration 23, loss = 0.00115529\n",
            "Iteration 24, loss = 0.00108808\n",
            "Iteration 25, loss = 0.00103713\n",
            "Iteration 26, loss = 0.00099845\n",
            "Iteration 27, loss = 0.00096907\n",
            "Iteration 28, loss = 0.00094674\n",
            "Iteration 29, loss = 0.00092975\n",
            "Iteration 30, loss = 0.00091682\n",
            "Iteration 31, loss = 0.00090695\n",
            "Iteration 32, loss = 0.00089944\n",
            "Iteration 33, loss = 0.00089372\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089087\n",
            "Iteration 35, loss = 0.00089005\n",
            "Iteration 36, loss = 0.00088928\n",
            "Iteration 37, loss = 0.00088854\n",
            "Iteration 38, loss = 0.00088785\n",
            "Iteration 39, loss = 0.00088719\n",
            "Iteration 40, loss = 0.00088657\n",
            "Iteration 41, loss = 0.00088597\n",
            "Iteration 42, loss = 0.00088541\n",
            "Iteration 43, loss = 0.00088488\n",
            "Iteration 44, loss = 0.00088438\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00088408\n",
            "Iteration 46, loss = 0.00088399\n",
            "Iteration 47, loss = 0.00088389\n",
            "Iteration 48, loss = 0.00088380\n",
            "Iteration 49, loss = 0.00088371\n",
            "Iteration 50, loss = 0.00088362\n",
            "Iteration 51, loss = 0.00088353\n",
            "Iteration 52, loss = 0.00088344\n",
            "Iteration 53, loss = 0.00088335\n",
            "Iteration 54, loss = 0.00088327\n",
            "Iteration 55, loss = 0.00088318\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00088313\n",
            "Iteration 57, loss = 0.00088311\n",
            "Iteration 58, loss = 0.00088310\n",
            "Iteration 59, loss = 0.00088308\n",
            "Iteration 60, loss = 0.00088306\n",
            "Iteration 61, loss = 0.00088305\n",
            "Iteration 62, loss = 0.00088303\n",
            "Iteration 63, loss = 0.00088301\n",
            "Iteration 64, loss = 0.00088300\n",
            "Iteration 65, loss = 0.00088298\n",
            "Iteration 66, loss = 0.00088296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00088295\n",
            "Iteration 68, loss = 0.00088295\n",
            "Iteration 69, loss = 0.00088295\n",
            "Iteration 70, loss = 0.00088294\n",
            "Iteration 71, loss = 0.00088294\n",
            "Iteration 72, loss = 0.00088294\n",
            "Iteration 73, loss = 0.00088293\n",
            "Iteration 74, loss = 0.00088293\n",
            "Iteration 75, loss = 0.00088293\n",
            "Iteration 76, loss = 0.00088292\n",
            "Iteration 77, loss = 0.00088292\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00088292\n",
            "Iteration 79, loss = 0.00088292\n",
            "Iteration 80, loss = 0.00088292\n",
            "Iteration 81, loss = 0.00088292\n",
            "Iteration 82, loss = 0.00088291\n",
            "Iteration 83, loss = 0.00088291\n",
            "Iteration 84, loss = 0.00088291\n",
            "Iteration 85, loss = 0.00088291\n",
            "Iteration 86, loss = 0.00088291\n",
            "Iteration 87, loss = 0.00088291\n",
            "Iteration 88, loss = 0.00088291\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 20.5min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.13952028\n",
            "Iteration 2, loss = 0.12847922\n",
            "Iteration 3, loss = 0.11857304\n",
            "Iteration 4, loss = 0.10943066\n",
            "Iteration 5, loss = 0.10099319\n",
            "Iteration 6, loss = 0.09320627\n",
            "Iteration 7, loss = 0.08601976\n",
            "Iteration 8, loss = 0.07938735\n",
            "Iteration 9, loss = 0.07326631\n",
            "Iteration 10, loss = 0.06761724\n",
            "Iteration 11, loss = 0.06240372\n",
            "Iteration 12, loss = 0.05759218\n",
            "Iteration 13, loss = 0.05315163\n",
            "Iteration 14, loss = 0.04905346\n",
            "Iteration 15, loss = 0.04527128\n",
            "Iteration 16, loss = 0.04178074\n",
            "Iteration 17, loss = 0.03855939\n",
            "Iteration 18, loss = 0.03558662\n",
            "Iteration 19, loss = 0.03284366\n",
            "Iteration 20, loss = 0.03031372\n",
            "Iteration 21, loss = 0.02798196\n",
            "Iteration 22, loss = 0.02583467\n",
            "Iteration 23, loss = 0.02385766\n",
            "Iteration 24, loss = 0.02203641\n",
            "Iteration 25, loss = 0.02035769\n",
            "Iteration 26, loss = 0.01881003\n",
            "Iteration 27, loss = 0.01738322\n",
            "Iteration 28, loss = 0.01606798\n",
            "Iteration 29, loss = 0.01485575\n",
            "Iteration 30, loss = 0.01373866\n",
            "Iteration 31, loss = 0.01270945\n",
            "Iteration 32, loss = 0.01176138\n",
            "Iteration 33, loss = 0.01088824\n",
            "Iteration 34, loss = 0.01008427\n",
            "Iteration 35, loss = 0.00934420\n",
            "Iteration 36, loss = 0.00866314\n",
            "Iteration 37, loss = 0.00803655\n",
            "Iteration 38, loss = 0.00746022\n",
            "Iteration 39, loss = 0.00693026\n",
            "Iteration 40, loss = 0.00644306\n",
            "Iteration 41, loss = 0.00599530\n",
            "Iteration 42, loss = 0.00558388\n",
            "Iteration 43, loss = 0.00520605\n",
            "Iteration 44, loss = 0.00485931\n",
            "Iteration 45, loss = 0.00454111\n",
            "Iteration 46, loss = 0.00424907\n",
            "Iteration 47, loss = 0.00398099\n",
            "Iteration 48, loss = 0.00373484\n",
            "Iteration 49, loss = 0.00350880\n",
            "Iteration 50, loss = 0.00330121\n",
            "Iteration 51, loss = 0.00311053\n",
            "Iteration 52, loss = 0.00293535\n",
            "Iteration 53, loss = 0.00277440\n",
            "Iteration 54, loss = 0.00262651\n",
            "Iteration 55, loss = 0.00249062\n",
            "Iteration 56, loss = 0.00236569\n",
            "Iteration 57, loss = 0.00225081\n",
            "Iteration 58, loss = 0.00214512\n",
            "Iteration 59, loss = 0.00204787\n",
            "Iteration 60, loss = 0.00195837\n",
            "Iteration 61, loss = 0.00187597\n",
            "Iteration 62, loss = 0.00180010\n",
            "Iteration 63, loss = 0.00173023\n",
            "Iteration 64, loss = 0.00166588\n",
            "Iteration 65, loss = 0.00160659\n",
            "Iteration 66, loss = 0.00155196\n",
            "Iteration 67, loss = 0.00150163\n",
            "Iteration 68, loss = 0.00145524\n",
            "Iteration 69, loss = 0.00141249\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00138675\n",
            "Iteration 71, loss = 0.00137894\n",
            "Iteration 72, loss = 0.00137135\n",
            "Iteration 73, loss = 0.00136389\n",
            "Iteration 74, loss = 0.00135654\n",
            "Iteration 75, loss = 0.00134932\n",
            "Iteration 76, loss = 0.00134221\n",
            "Iteration 77, loss = 0.00133522\n",
            "Iteration 78, loss = 0.00132833\n",
            "Iteration 79, loss = 0.00132156\n",
            "Iteration 80, loss = 0.00131490\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00131069\n",
            "Iteration 82, loss = 0.00130936\n",
            "Iteration 83, loss = 0.00130806\n",
            "Iteration 84, loss = 0.00130677\n",
            "Iteration 85, loss = 0.00130547\n",
            "Iteration 86, loss = 0.00130418\n",
            "Iteration 87, loss = 0.00130290\n",
            "Iteration 88, loss = 0.00130162\n",
            "Iteration 89, loss = 0.00130034\n",
            "Iteration 90, loss = 0.00129907\n",
            "Iteration 91, loss = 0.00129780\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00129699\n",
            "Iteration 93, loss = 0.00129674\n",
            "Iteration 94, loss = 0.00129649\n",
            "Iteration 95, loss = 0.00129623\n",
            "Iteration 96, loss = 0.00129598\n",
            "Iteration 97, loss = 0.00129573\n",
            "Iteration 98, loss = 0.00129548\n",
            "Iteration 99, loss = 0.00129523\n",
            "Iteration 100, loss = 0.00129498\n",
            "Iteration 101, loss = 0.00129472\n",
            "Iteration 102, loss = 0.00129447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00129431\n",
            "Iteration 104, loss = 0.00129426\n",
            "Iteration 105, loss = 0.00129421\n",
            "Iteration 106, loss = 0.00129416\n",
            "Iteration 107, loss = 0.00129411\n",
            "Iteration 108, loss = 0.00129406\n",
            "Iteration 109, loss = 0.00129401\n",
            "Iteration 110, loss = 0.00129396\n",
            "Iteration 111, loss = 0.00129391\n",
            "Iteration 112, loss = 0.00129386\n",
            "Iteration 113, loss = 0.00129381\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00129378\n",
            "Iteration 115, loss = 0.00129377\n",
            "Iteration 116, loss = 0.00129376\n",
            "Iteration 117, loss = 0.00129375\n",
            "Iteration 118, loss = 0.00129374\n",
            "Iteration 119, loss = 0.00129373\n",
            "Iteration 120, loss = 0.00129372\n",
            "Iteration 121, loss = 0.00129371\n",
            "Iteration 122, loss = 0.00129370\n",
            "Iteration 123, loss = 0.00129369\n",
            "Iteration 124, loss = 0.00129368\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.15057799\n",
            "Iteration 2, loss = 0.13873430\n",
            "Iteration 3, loss = 0.12803742\n",
            "Iteration 4, loss = 0.11816531\n",
            "Iteration 5, loss = 0.10905436\n",
            "Iteration 6, loss = 0.10064591\n",
            "Iteration 7, loss = 0.09288577\n",
            "Iteration 8, loss = 0.08572396\n",
            "Iteration 9, loss = 0.07911436\n",
            "Iteration 10, loss = 0.07301437\n",
            "Iteration 11, loss = 0.06738472\n",
            "Iteration 12, loss = 0.06218913\n",
            "Iteration 13, loss = 0.05739414\n",
            "Iteration 14, loss = 0.05296886\n",
            "Iteration 15, loss = 0.04888478\n",
            "Iteration 16, loss = 0.04511561\n",
            "Iteration 17, loss = 0.04163707\n",
            "Iteration 18, loss = 0.03842680\n",
            "Iteration 19, loss = 0.03546425\n",
            "Iteration 20, loss = 0.03273072\n",
            "Iteration 21, loss = 0.03020941\n",
            "Iteration 22, loss = 0.02788540\n",
            "Iteration 23, loss = 0.02574486\n",
            "Iteration 24, loss = 0.02377364\n",
            "Iteration 25, loss = 0.02195742\n",
            "Iteration 26, loss = 0.02028313\n",
            "Iteration 27, loss = 0.01873940\n",
            "Iteration 28, loss = 0.01731614\n",
            "Iteration 29, loss = 0.01600412\n",
            "Iteration 30, loss = 0.01479482\n",
            "Iteration 31, loss = 0.01368039\n",
            "Iteration 32, loss = 0.01265357\n",
            "Iteration 33, loss = 0.01170766\n",
            "Iteration 34, loss = 0.01083649\n",
            "Iteration 35, loss = 0.01003434\n",
            "Iteration 36, loss = 0.00929595\n",
            "Iteration 37, loss = 0.00861651\n",
            "Iteration 38, loss = 0.00799162\n",
            "Iteration 39, loss = 0.00741708\n",
            "Iteration 40, loss = 0.00688891\n",
            "Iteration 41, loss = 0.00640342\n",
            "Iteration 42, loss = 0.00595725\n",
            "Iteration 43, loss = 0.00554730\n",
            "Iteration 44, loss = 0.00517071\n",
            "Iteration 45, loss = 0.00482484\n",
            "Iteration 46, loss = 0.00450722\n",
            "Iteration 47, loss = 0.00421558\n",
            "Iteration 48, loss = 0.00394781\n",
            "Iteration 49, loss = 0.00370199\n",
            "Iteration 50, loss = 0.00347632\n",
            "Iteration 51, loss = 0.00326915\n",
            "Iteration 52, loss = 0.00307898\n",
            "Iteration 53, loss = 0.00290440\n",
            "Iteration 54, loss = 0.00274414\n",
            "Iteration 55, loss = 0.00259696\n",
            "Iteration 56, loss = 0.00246173\n",
            "Iteration 57, loss = 0.00233742\n",
            "Iteration 58, loss = 0.00222314\n",
            "Iteration 59, loss = 0.00211803\n",
            "Iteration 60, loss = 0.00202134\n",
            "Iteration 61, loss = 0.00193237\n",
            "Iteration 62, loss = 0.00185050\n",
            "Iteration 63, loss = 0.00177513\n",
            "Iteration 64, loss = 0.00170574\n",
            "Iteration 65, loss = 0.00164187\n",
            "Iteration 66, loss = 0.00158308\n",
            "Iteration 67, loss = 0.00152892\n",
            "Iteration 68, loss = 0.00147902\n",
            "Iteration 69, loss = 0.00143302\n",
            "Iteration 70, loss = 0.00139062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 71, loss = 0.00136509\n",
            "Iteration 72, loss = 0.00135735\n",
            "Iteration 73, loss = 0.00134982\n",
            "Iteration 74, loss = 0.00134242\n",
            "Iteration 75, loss = 0.00133514\n",
            "Iteration 76, loss = 0.00132797\n",
            "Iteration 77, loss = 0.00132092\n",
            "Iteration 78, loss = 0.00131399\n",
            "Iteration 79, loss = 0.00130716\n",
            "Iteration 80, loss = 0.00130045\n",
            "Iteration 81, loss = 0.00129384\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.00128966\n",
            "Iteration 83, loss = 0.00128835\n",
            "Iteration 84, loss = 0.00128706\n",
            "Iteration 85, loss = 0.00128577\n",
            "Iteration 86, loss = 0.00128449\n",
            "Iteration 87, loss = 0.00128321\n",
            "Iteration 88, loss = 0.00128194\n",
            "Iteration 89, loss = 0.00128067\n",
            "Iteration 90, loss = 0.00127940\n",
            "Iteration 91, loss = 0.00127814\n",
            "Iteration 92, loss = 0.00127688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 93, loss = 0.00127608\n",
            "Iteration 94, loss = 0.00127582\n",
            "Iteration 95, loss = 0.00127557\n",
            "Iteration 96, loss = 0.00127532\n",
            "Iteration 97, loss = 0.00127507\n",
            "Iteration 98, loss = 0.00127482\n",
            "Iteration 99, loss = 0.00127457\n",
            "Iteration 100, loss = 0.00127432\n",
            "Iteration 101, loss = 0.00127408\n",
            "Iteration 102, loss = 0.00127383\n",
            "Iteration 103, loss = 0.00127358\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 104, loss = 0.00127342\n",
            "Iteration 105, loss = 0.00127337\n",
            "Iteration 106, loss = 0.00127332\n",
            "Iteration 107, loss = 0.00127327\n",
            "Iteration 108, loss = 0.00127322\n",
            "Iteration 109, loss = 0.00127317\n",
            "Iteration 110, loss = 0.00127312\n",
            "Iteration 111, loss = 0.00127307\n",
            "Iteration 112, loss = 0.00127302\n",
            "Iteration 113, loss = 0.00127297\n",
            "Iteration 114, loss = 0.00127292\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 115, loss = 0.00127289\n",
            "Iteration 116, loss = 0.00127288\n",
            "Iteration 117, loss = 0.00127287\n",
            "Iteration 118, loss = 0.00127286\n",
            "Iteration 119, loss = 0.00127285\n",
            "Iteration 120, loss = 0.00127284\n",
            "Iteration 121, loss = 0.00127283\n",
            "Iteration 122, loss = 0.00127282\n",
            "Iteration 123, loss = 0.00127281\n",
            "Iteration 124, loss = 0.00127280\n",
            "Iteration 125, loss = 0.00127279\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13160110\n",
            "Iteration 2, loss = 0.12148676\n",
            "Iteration 3, loss = 0.11211972\n",
            "Iteration 4, loss = 0.10347491\n",
            "Iteration 5, loss = 0.09549665\n",
            "Iteration 6, loss = 0.08813354\n",
            "Iteration 7, loss = 0.08133815\n",
            "Iteration 8, loss = 0.07506670\n",
            "Iteration 9, loss = 0.06927881\n",
            "Iteration 10, loss = 0.06393718\n",
            "Iteration 11, loss = 0.05900741\n",
            "Iteration 12, loss = 0.05445774\n",
            "Iteration 13, loss = 0.05025886\n",
            "Iteration 14, loss = 0.04638374\n",
            "Iteration 15, loss = 0.04280740\n",
            "Iteration 16, loss = 0.03950684\n",
            "Iteration 17, loss = 0.03646084\n",
            "Iteration 18, loss = 0.03364996\n",
            "Iteration 19, loss = 0.03105655\n",
            "Iteration 20, loss = 0.02866486\n",
            "Iteration 21, loss = 0.02646099\n",
            "Iteration 22, loss = 0.02443179\n",
            "Iteration 23, loss = 0.02256347\n",
            "Iteration 24, loss = 0.02084220\n",
            "Iteration 25, loss = 0.01925559\n",
            "Iteration 26, loss = 0.01779289\n",
            "Iteration 27, loss = 0.01644449\n",
            "Iteration 28, loss = 0.01520162\n",
            "Iteration 29, loss = 0.01405621\n",
            "Iteration 30, loss = 0.01300082\n",
            "Iteration 31, loss = 0.01202859\n",
            "Iteration 32, loss = 0.01113317\n",
            "Iteration 33, loss = 0.01030869\n",
            "Iteration 34, loss = 0.00954970\n",
            "Iteration 35, loss = 0.00885119\n",
            "Iteration 36, loss = 0.00820850\n",
            "Iteration 37, loss = 0.00761741\n",
            "Iteration 38, loss = 0.00707395\n",
            "Iteration 39, loss = 0.00657434\n",
            "Iteration 40, loss = 0.00611516\n",
            "Iteration 41, loss = 0.00569325\n",
            "Iteration 42, loss = 0.00530568\n",
            "Iteration 43, loss = 0.00494970\n",
            "Iteration 44, loss = 0.00462280\n",
            "Iteration 45, loss = 0.00432263\n",
            "Iteration 46, loss = 0.00404701\n",
            "Iteration 47, loss = 0.00379396\n",
            "Iteration 48, loss = 0.00356165\n",
            "Iteration 49, loss = 0.00334842\n",
            "Iteration 50, loss = 0.00315268\n",
            "Iteration 51, loss = 0.00297309\n",
            "Iteration 52, loss = 0.00280834\n",
            "Iteration 53, loss = 0.00265698\n",
            "Iteration 54, loss = 0.00251784\n",
            "Iteration 55, loss = 0.00238988\n",
            "Iteration 56, loss = 0.00227216\n",
            "Iteration 57, loss = 0.00216386\n",
            "Iteration 58, loss = 0.00206420\n",
            "Iteration 59, loss = 0.00197248\n",
            "Iteration 60, loss = 0.00188806\n",
            "Iteration 61, loss = 0.00181033\n",
            "Iteration 62, loss = 0.00173877\n",
            "Iteration 63, loss = 0.00167288\n",
            "Iteration 64, loss = 0.00161221\n",
            "Iteration 65, loss = 0.00155634\n",
            "Iteration 66, loss = 0.00150487\n",
            "Iteration 67, loss = 0.00145746\n",
            "Iteration 68, loss = 0.00141378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.00138749\n",
            "Iteration 70, loss = 0.00137951\n",
            "Iteration 71, loss = 0.00137177\n",
            "Iteration 72, loss = 0.00136415\n",
            "Iteration 73, loss = 0.00135665\n",
            "Iteration 74, loss = 0.00134927\n",
            "Iteration 75, loss = 0.00134201\n",
            "Iteration 76, loss = 0.00133487\n",
            "Iteration 77, loss = 0.00132785\n",
            "Iteration 78, loss = 0.00132093\n",
            "Iteration 79, loss = 0.00131413\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.00130983\n",
            "Iteration 81, loss = 0.00130848\n",
            "Iteration 82, loss = 0.00130715\n",
            "Iteration 83, loss = 0.00130583\n",
            "Iteration 84, loss = 0.00130451\n",
            "Iteration 85, loss = 0.00130319\n",
            "Iteration 86, loss = 0.00130188\n",
            "Iteration 87, loss = 0.00130058\n",
            "Iteration 88, loss = 0.00129927\n",
            "Iteration 89, loss = 0.00129798\n",
            "Iteration 90, loss = 0.00129668\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.00129585\n",
            "Iteration 92, loss = 0.00129559\n",
            "Iteration 93, loss = 0.00129534\n",
            "Iteration 94, loss = 0.00129508\n",
            "Iteration 95, loss = 0.00129482\n",
            "Iteration 96, loss = 0.00129456\n",
            "Iteration 97, loss = 0.00129431\n",
            "Iteration 98, loss = 0.00129405\n",
            "Iteration 99, loss = 0.00129379\n",
            "Iteration 100, loss = 0.00129354\n",
            "Iteration 101, loss = 0.00129328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.00129312\n",
            "Iteration 103, loss = 0.00129306\n",
            "Iteration 104, loss = 0.00129301\n",
            "Iteration 105, loss = 0.00129296\n",
            "Iteration 106, loss = 0.00129291\n",
            "Iteration 107, loss = 0.00129286\n",
            "Iteration 108, loss = 0.00129281\n",
            "Iteration 109, loss = 0.00129276\n",
            "Iteration 110, loss = 0.00129271\n",
            "Iteration 111, loss = 0.00129265\n",
            "Iteration 112, loss = 0.00129260\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 113, loss = 0.00129257\n",
            "Iteration 114, loss = 0.00129256\n",
            "Iteration 115, loss = 0.00129255\n",
            "Iteration 116, loss = 0.00129254\n",
            "Iteration 117, loss = 0.00129253\n",
            "Iteration 118, loss = 0.00129252\n",
            "Iteration 119, loss = 0.00129251\n",
            "Iteration 120, loss = 0.00129250\n",
            "Iteration 121, loss = 0.00129249\n",
            "Iteration 122, loss = 0.00129248\n",
            "Iteration 123, loss = 0.00129247\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.42710546\n",
            "Iteration 2, loss = 0.27978805\n",
            "Iteration 3, loss = 0.25821546\n",
            "Iteration 4, loss = 0.23830618\n",
            "Iteration 5, loss = 0.21993197\n",
            "Iteration 6, loss = 0.20297448\n",
            "Iteration 7, loss = 0.18732446\n",
            "Iteration 8, loss = 0.17288112\n",
            "Iteration 9, loss = 0.15955140\n",
            "Iteration 10, loss = 0.14724945\n",
            "Iteration 11, loss = 0.13589602\n",
            "Iteration 12, loss = 0.12541798\n",
            "Iteration 13, loss = 0.11574784\n",
            "Iteration 14, loss = 0.10682329\n",
            "Iteration 15, loss = 0.09858685\n",
            "Iteration 16, loss = 0.09098548\n",
            "Iteration 17, loss = 0.08397019\n",
            "Iteration 18, loss = 0.07749581\n",
            "Iteration 19, loss = 0.07152062\n",
            "Iteration 20, loss = 0.06600614\n",
            "Iteration 21, loss = 0.06091684\n",
            "Iteration 22, loss = 0.05621995\n",
            "Iteration 23, loss = 0.05188520\n",
            "Iteration 24, loss = 0.04788468\n",
            "Iteration 25, loss = 0.04419261\n",
            "Iteration 26, loss = 0.04078521\n",
            "Iteration 27, loss = 0.03764054\n",
            "Iteration 28, loss = 0.03473832\n",
            "Iteration 29, loss = 0.03205988\n",
            "Iteration 30, loss = 0.02958796\n",
            "Iteration 31, loss = 0.02730663\n",
            "Iteration 32, loss = 0.02520119\n",
            "Iteration 33, loss = 0.02325810\n",
            "Iteration 34, loss = 0.02146482\n",
            "Iteration 35, loss = 0.01980981\n",
            "Iteration 36, loss = 0.01828241\n",
            "Iteration 37, loss = 0.01687277\n",
            "Iteration 38, loss = 0.01557182\n",
            "Iteration 39, loss = 0.01437118\n",
            "Iteration 40, loss = 0.01326312\n",
            "Iteration 41, loss = 0.01224049\n",
            "Iteration 42, loss = 0.01129670\n",
            "Iteration 43, loss = 0.01042569\n",
            "Iteration 44, loss = 0.00962183\n",
            "Iteration 45, loss = 0.00887996\n",
            "Iteration 46, loss = 0.00819528\n",
            "Iteration 47, loss = 0.00756340\n",
            "Iteration 48, loss = 0.00698024\n",
            "Iteration 49, loss = 0.00644204\n",
            "Iteration 50, loss = 0.00594533\n",
            "Iteration 51, loss = 0.00548693\n",
            "Iteration 52, loss = 0.00506387\n",
            "Iteration 53, loss = 0.00467343\n",
            "Iteration 54, loss = 0.00431309\n",
            "Iteration 55, loss = 0.00398055\n",
            "Iteration 56, loss = 0.00367366\n",
            "Iteration 57, loss = 0.00339053\n",
            "Iteration 58, loss = 0.00312951\n",
            "Iteration 59, loss = 0.00288944\n",
            "Iteration 60, loss = 0.00267000\n",
            "Iteration 61, loss = 0.00247237\n",
            "Iteration 62, loss = 0.00229936\n",
            "Iteration 63, loss = 0.00215373\n",
            "Iteration 64, loss = 0.00203439\n",
            "Iteration 65, loss = 0.00193519\n",
            "Iteration 66, loss = 0.00184917\n",
            "Iteration 67, loss = 0.00177202\n",
            "Iteration 68, loss = 0.00170178\n",
            "Iteration 69, loss = 0.00163751\n",
            "Iteration 70, loss = 0.00157861\n",
            "Iteration 71, loss = 0.00152460\n",
            "Iteration 72, loss = 0.00147504\n",
            "Iteration 73, loss = 0.00142956\n",
            "Iteration 74, loss = 0.00138780\n",
            "Iteration 75, loss = 0.00134945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.00132641\n",
            "Iteration 77, loss = 0.00131944\n",
            "Iteration 78, loss = 0.00131267\n",
            "Iteration 79, loss = 0.00130601\n",
            "Iteration 80, loss = 0.00129946\n",
            "Iteration 81, loss = 0.00129302\n",
            "Iteration 82, loss = 0.00128669\n",
            "Iteration 83, loss = 0.00128047\n",
            "Iteration 84, loss = 0.00127435\n",
            "Iteration 85, loss = 0.00126833\n",
            "Iteration 86, loss = 0.00126241\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.00125866\n",
            "Iteration 88, loss = 0.00125749\n",
            "Iteration 89, loss = 0.00125633\n",
            "Iteration 90, loss = 0.00125518\n",
            "Iteration 91, loss = 0.00125404\n",
            "Iteration 92, loss = 0.00125289\n",
            "Iteration 93, loss = 0.00125175\n",
            "Iteration 94, loss = 0.00125062\n",
            "Iteration 95, loss = 0.00124948\n",
            "Iteration 96, loss = 0.00124836\n",
            "Iteration 97, loss = 0.00124723\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.00124651\n",
            "Iteration 99, loss = 0.00124629\n",
            "Iteration 100, loss = 0.00124606\n",
            "Iteration 101, loss = 0.00124584\n",
            "Iteration 102, loss = 0.00124562\n",
            "Iteration 103, loss = 0.00124539\n",
            "Iteration 104, loss = 0.00124517\n",
            "Iteration 105, loss = 0.00124495\n",
            "Iteration 106, loss = 0.00124472\n",
            "Iteration 107, loss = 0.00124450\n",
            "Iteration 108, loss = 0.00124428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.00124414\n",
            "Iteration 110, loss = 0.00124409\n",
            "Iteration 111, loss = 0.00124405\n",
            "Iteration 112, loss = 0.00124400\n",
            "Iteration 113, loss = 0.00124396\n",
            "Iteration 114, loss = 0.00124391\n",
            "Iteration 115, loss = 0.00124387\n",
            "Iteration 116, loss = 0.00124382\n",
            "Iteration 117, loss = 0.00124378\n",
            "Iteration 118, loss = 0.00124373\n",
            "Iteration 119, loss = 0.00124369\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.00124366\n",
            "Iteration 121, loss = 0.00124365\n",
            "Iteration 122, loss = 0.00124364\n",
            "Iteration 123, loss = 0.00124363\n",
            "Iteration 124, loss = 0.00124363\n",
            "Iteration 125, loss = 0.00124362\n",
            "Iteration 126, loss = 0.00124361\n",
            "Iteration 127, loss = 0.00124360\n",
            "Iteration 128, loss = 0.00124359\n",
            "Iteration 129, loss = 0.00124358\n",
            "Iteration 130, loss = 0.00124357\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.38450969\n",
            "Iteration 2, loss = 0.33353825\n",
            "Iteration 3, loss = 0.30782133\n",
            "Iteration 4, loss = 0.28408727\n",
            "Iteration 5, loss = 0.26218319\n",
            "Iteration 6, loss = 0.24196798\n",
            "Iteration 7, loss = 0.22331144\n",
            "Iteration 8, loss = 0.20609338\n",
            "Iteration 9, loss = 0.19020288\n",
            "Iteration 10, loss = 0.17553760\n",
            "Iteration 11, loss = 0.16200306\n",
            "Iteration 12, loss = 0.14951208\n",
            "Iteration 13, loss = 0.13798420\n",
            "Iteration 14, loss = 0.12734515\n",
            "Iteration 15, loss = 0.11752641\n",
            "Iteration 16, loss = 0.10846473\n",
            "Iteration 17, loss = 0.10010174\n",
            "Iteration 18, loss = 0.09238356\n",
            "Iteration 19, loss = 0.08526047\n",
            "Iteration 20, loss = 0.07868661\n",
            "Iteration 21, loss = 0.07261960\n",
            "Iteration 22, loss = 0.06702039\n",
            "Iteration 23, loss = 0.06185289\n",
            "Iteration 24, loss = 0.05708383\n",
            "Iteration 25, loss = 0.05268247\n",
            "Iteration 26, loss = 0.04862048\n",
            "Iteration 27, loss = 0.04487167\n",
            "Iteration 28, loss = 0.04141192\n",
            "Iteration 29, loss = 0.03821892\n",
            "Iteration 30, loss = 0.03527211\n",
            "Iteration 31, loss = 0.03255251\n",
            "Iteration 32, loss = 0.03004261\n",
            "Iteration 33, loss = 0.02772622\n",
            "Iteration 34, loss = 0.02558843\n",
            "Iteration 35, loss = 0.02361548\n",
            "Iteration 36, loss = 0.02179465\n",
            "Iteration 37, loss = 0.02011421\n",
            "Iteration 38, loss = 0.01856333\n",
            "Iteration 39, loss = 0.01713204\n",
            "Iteration 40, loss = 0.01581110\n",
            "Iteration 41, loss = 0.01459201\n",
            "Iteration 42, loss = 0.01346692\n",
            "Iteration 43, loss = 0.01242857\n",
            "Iteration 44, loss = 0.01147029\n",
            "Iteration 45, loss = 0.01058589\n",
            "Iteration 46, loss = 0.00976968\n",
            "Iteration 47, loss = 0.00901641\n",
            "Iteration 48, loss = 0.00832121\n",
            "Iteration 49, loss = 0.00767962\n",
            "Iteration 50, loss = 0.00708749\n",
            "Iteration 51, loss = 0.00654103\n",
            "Iteration 52, loss = 0.00603669\n",
            "Iteration 53, loss = 0.00557124\n",
            "Iteration 54, loss = 0.00514168\n",
            "Iteration 55, loss = 0.00474524\n",
            "Iteration 56, loss = 0.00437937\n",
            "Iteration 57, loss = 0.00404170\n",
            "Iteration 58, loss = 0.00373008\n",
            "Iteration 59, loss = 0.00344249\n",
            "Iteration 60, loss = 0.00317710\n",
            "Iteration 61, loss = 0.00293229\n",
            "Iteration 62, loss = 0.00270670\n",
            "Iteration 63, loss = 0.00249946\n",
            "Iteration 64, loss = 0.00231054\n",
            "Iteration 65, loss = 0.00214128\n",
            "Iteration 66, loss = 0.00199431\n",
            "Iteration 67, loss = 0.00187162\n",
            "Iteration 68, loss = 0.00177151\n",
            "Iteration 69, loss = 0.00168834\n",
            "Iteration 70, loss = 0.00161641\n",
            "Iteration 71, loss = 0.00155231\n",
            "Iteration 72, loss = 0.00149438\n",
            "Iteration 73, loss = 0.00144177\n",
            "Iteration 74, loss = 0.00139392\n",
            "Iteration 75, loss = 0.00135035\n",
            "Iteration 76, loss = 0.00131066\n",
            "Iteration 77, loss = 0.00127449\n",
            "Iteration 78, loss = 0.00124149\n",
            "Iteration 79, loss = 0.00121138\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 80, loss = 0.00119338\n",
            "Iteration 81, loss = 0.00118795\n",
            "Iteration 82, loss = 0.00118268\n",
            "Iteration 83, loss = 0.00117750\n",
            "Iteration 84, loss = 0.00117242\n",
            "Iteration 85, loss = 0.00116743\n",
            "Iteration 86, loss = 0.00116252\n",
            "Iteration 87, loss = 0.00115771\n",
            "Iteration 88, loss = 0.00115298\n",
            "Iteration 89, loss = 0.00114833\n",
            "Iteration 90, loss = 0.00114377\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 91, loss = 0.00114088\n",
            "Iteration 92, loss = 0.00113998\n",
            "Iteration 93, loss = 0.00113909\n",
            "Iteration 94, loss = 0.00113820\n",
            "Iteration 95, loss = 0.00113732\n",
            "Iteration 96, loss = 0.00113644\n",
            "Iteration 97, loss = 0.00113556\n",
            "Iteration 98, loss = 0.00113469\n",
            "Iteration 99, loss = 0.00113382\n",
            "Iteration 100, loss = 0.00113295\n",
            "Iteration 101, loss = 0.00113209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 102, loss = 0.00113154\n",
            "Iteration 103, loss = 0.00113136\n",
            "Iteration 104, loss = 0.00113119\n",
            "Iteration 105, loss = 0.00113102\n",
            "Iteration 106, loss = 0.00113085\n",
            "Iteration 107, loss = 0.00113067\n",
            "Iteration 108, loss = 0.00113050\n",
            "Iteration 109, loss = 0.00113033\n",
            "Iteration 110, loss = 0.00113016\n",
            "Iteration 111, loss = 0.00112999\n",
            "Iteration 112, loss = 0.00112982\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.00112971\n",
            "Iteration 114, loss = 0.00112967\n",
            "Iteration 115, loss = 0.00112964\n",
            "Iteration 116, loss = 0.00112961\n",
            "Iteration 117, loss = 0.00112957\n",
            "Iteration 118, loss = 0.00112954\n",
            "Iteration 119, loss = 0.00112950\n",
            "Iteration 120, loss = 0.00112947\n",
            "Iteration 121, loss = 0.00112943\n",
            "Iteration 122, loss = 0.00112940\n",
            "Iteration 123, loss = 0.00112937\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 124, loss = 0.00112934\n",
            "Iteration 125, loss = 0.00112934\n",
            "Iteration 126, loss = 0.00112933\n",
            "Iteration 127, loss = 0.00112932\n",
            "Iteration 128, loss = 0.00112932\n",
            "Iteration 129, loss = 0.00112931\n",
            "Iteration 130, loss = 0.00112930\n",
            "Iteration 131, loss = 0.00112930\n",
            "Iteration 132, loss = 0.00112929\n",
            "Iteration 133, loss = 0.00112928\n",
            "Iteration 134, loss = 0.00112928\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.14498120\n",
            "Iteration 2, loss = 0.13383789\n",
            "Iteration 3, loss = 0.12352160\n",
            "Iteration 4, loss = 0.11400126\n",
            "Iteration 5, loss = 0.10521548\n",
            "Iteration 6, loss = 0.09710760\n",
            "Iteration 7, loss = 0.08962537\n",
            "Iteration 8, loss = 0.08272054\n",
            "Iteration 9, loss = 0.07634861\n",
            "Iteration 10, loss = 0.07046849\n",
            "Iteration 11, loss = 0.06504229\n",
            "Iteration 12, loss = 0.06003503\n",
            "Iteration 13, loss = 0.05541444\n",
            "Iteration 14, loss = 0.05115073\n",
            "Iteration 15, loss = 0.04721642\n",
            "Iteration 16, loss = 0.04358616\n",
            "Iteration 17, loss = 0.04023653\n",
            "Iteration 18, loss = 0.03714595\n",
            "Iteration 19, loss = 0.03429450\n",
            "Iteration 20, loss = 0.03166378\n",
            "Iteration 21, loss = 0.02923683\n",
            "Iteration 22, loss = 0.02699799\n",
            "Iteration 23, loss = 0.02493283\n",
            "Iteration 24, loss = 0.02302801\n",
            "Iteration 25, loss = 0.02127123\n",
            "Iteration 26, loss = 0.01965115\n",
            "Iteration 27, loss = 0.01815730\n",
            "Iteration 28, loss = 0.01678000\n",
            "Iteration 29, loss = 0.01551036\n",
            "Iteration 30, loss = 0.01434014\n",
            "Iteration 31, loss = 0.01326173\n",
            "Iteration 32, loss = 0.01226812\n",
            "Iteration 33, loss = 0.01135285\n",
            "Iteration 34, loss = 0.01050992\n",
            "Iteration 35, loss = 0.00973381\n",
            "Iteration 36, loss = 0.00901940\n",
            "Iteration 37, loss = 0.00836199\n",
            "Iteration 38, loss = 0.00775720\n",
            "Iteration 39, loss = 0.00720102\n",
            "Iteration 40, loss = 0.00668974\n",
            "Iteration 41, loss = 0.00621985\n",
            "Iteration 42, loss = 0.00578816\n",
            "Iteration 43, loss = 0.00539169\n",
            "Iteration 44, loss = 0.00502767\n",
            "Iteration 45, loss = 0.00469351\n",
            "Iteration 46, loss = 0.00438683\n",
            "Iteration 47, loss = 0.00410545\n",
            "Iteration 48, loss = 0.00384731\n",
            "Iteration 49, loss = 0.00361049\n",
            "Iteration 50, loss = 0.00339320\n",
            "Iteration 51, loss = 0.00319380\n",
            "Iteration 52, loss = 0.00301072\n",
            "Iteration 53, loss = 0.00284250\n",
            "Iteration 54, loss = 0.00268783\n",
            "Iteration 55, loss = 0.00254574\n",
            "Iteration 56, loss = 0.00241522\n",
            "Iteration 57, loss = 0.00229531\n",
            "Iteration 58, loss = 0.00218514\n",
            "Iteration 59, loss = 0.00208389\n",
            "Iteration 60, loss = 0.00199083\n",
            "Iteration 61, loss = 0.00190524\n",
            "Iteration 62, loss = 0.00182651\n",
            "Iteration 63, loss = 0.00175406\n",
            "Iteration 64, loss = 0.00168736\n",
            "Iteration 65, loss = 0.00162596\n",
            "Iteration 66, loss = 0.00156942\n",
            "Iteration 67, loss = 0.00151734\n",
            "Iteration 68, loss = 0.00146937\n",
            "Iteration 69, loss = 0.00142518\n",
            "Iteration 70, loss = 0.00138446\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 71, loss = 0.00135995\n",
            "Iteration 72, loss = 0.00135252\n",
            "Iteration 73, loss = 0.00134529\n",
            "Iteration 74, loss = 0.00133819\n",
            "Iteration 75, loss = 0.00133120\n",
            "Iteration 76, loss = 0.00132432\n",
            "Iteration 77, loss = 0.00131756\n",
            "Iteration 78, loss = 0.00131090\n",
            "Iteration 79, loss = 0.00130435\n",
            "Iteration 80, loss = 0.00129791\n",
            "Iteration 81, loss = 0.00129157\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.00128755\n",
            "Iteration 83, loss = 0.00128630\n",
            "Iteration 84, loss = 0.00128506\n",
            "Iteration 85, loss = 0.00128382\n",
            "Iteration 86, loss = 0.00128259\n",
            "Iteration 87, loss = 0.00128137\n",
            "Iteration 88, loss = 0.00128015\n",
            "Iteration 89, loss = 0.00127893\n",
            "Iteration 90, loss = 0.00127771\n",
            "Iteration 91, loss = 0.00127650\n",
            "Iteration 92, loss = 0.00127530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 93, loss = 0.00127453\n",
            "Iteration 94, loss = 0.00127428\n",
            "Iteration 95, loss = 0.00127404\n",
            "Iteration 96, loss = 0.00127380\n",
            "Iteration 97, loss = 0.00127356\n",
            "Iteration 98, loss = 0.00127332\n",
            "Iteration 99, loss = 0.00127308\n",
            "Iteration 100, loss = 0.00127284\n",
            "Iteration 101, loss = 0.00127260\n",
            "Iteration 102, loss = 0.00127237\n",
            "Iteration 103, loss = 0.00127213\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 104, loss = 0.00127197\n",
            "Iteration 105, loss = 0.00127193\n",
            "Iteration 106, loss = 0.00127188\n",
            "Iteration 107, loss = 0.00127183\n",
            "Iteration 108, loss = 0.00127178\n",
            "Iteration 109, loss = 0.00127173\n",
            "Iteration 110, loss = 0.00127169\n",
            "Iteration 111, loss = 0.00127164\n",
            "Iteration 112, loss = 0.00127159\n",
            "Iteration 113, loss = 0.00127154\n",
            "Iteration 114, loss = 0.00127150\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 115, loss = 0.00127147\n",
            "Iteration 116, loss = 0.00127146\n",
            "Iteration 117, loss = 0.00127145\n",
            "Iteration 118, loss = 0.00127144\n",
            "Iteration 119, loss = 0.00127143\n",
            "Iteration 120, loss = 0.00127142\n",
            "Iteration 121, loss = 0.00127141\n",
            "Iteration 122, loss = 0.00127140\n",
            "Iteration 123, loss = 0.00127139\n",
            "Iteration 124, loss = 0.00127138\n",
            "Iteration 125, loss = 0.00127137\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.28070707\n",
            "Iteration 3, loss = 0.25906362\n",
            "Iteration 4, loss = 0.23908894\n",
            "Iteration 5, loss = 0.22065438\n",
            "Iteration 6, loss = 0.20364119\n",
            "Iteration 7, loss = 0.18793977\n",
            "Iteration 8, loss = 0.17344898\n",
            "Iteration 9, loss = 0.16007548\n",
            "Iteration 10, loss = 0.14773312\n",
            "Iteration 11, loss = 0.13634240\n",
            "Iteration 12, loss = 0.12582994\n",
            "Iteration 13, loss = 0.11612803\n",
            "Iteration 14, loss = 0.10717417\n",
            "Iteration 15, loss = 0.09891068\n",
            "Iteration 16, loss = 0.09128434\n",
            "Iteration 17, loss = 0.08424601\n",
            "Iteration 18, loss = 0.07775036\n",
            "Iteration 19, loss = 0.07175554\n",
            "Iteration 20, loss = 0.06622295\n",
            "Iteration 21, loss = 0.06111694\n",
            "Iteration 22, loss = 0.05640462\n",
            "Iteration 23, loss = 0.05205563\n",
            "Iteration 24, loss = 0.04804197\n",
            "Iteration 25, loss = 0.04433777\n",
            "Iteration 26, loss = 0.04091918\n",
            "Iteration 27, loss = 0.03776417\n",
            "Iteration 28, loss = 0.03485243\n",
            "Iteration 29, loss = 0.03216519\n",
            "Iteration 30, loss = 0.02968514\n",
            "Iteration 31, loss = 0.02739632\n",
            "Iteration 32, loss = 0.02528397\n",
            "Iteration 33, loss = 0.02333449\n",
            "Iteration 34, loss = 0.02153533\n",
            "Iteration 35, loss = 0.01987488\n",
            "Iteration 36, loss = 0.01834246\n",
            "Iteration 37, loss = 0.01692819\n",
            "Iteration 38, loss = 0.01562297\n",
            "Iteration 39, loss = 0.01441839\n",
            "Iteration 40, loss = 0.01330668\n",
            "Iteration 41, loss = 0.01228069\n",
            "Iteration 42, loss = 0.01133381\n",
            "Iteration 43, loss = 0.01045994\n",
            "Iteration 44, loss = 0.00965344\n",
            "Iteration 45, loss = 0.00890913\n",
            "Iteration 46, loss = 0.00822220\n",
            "Iteration 47, loss = 0.00758824\n",
            "Iteration 48, loss = 0.00700316\n",
            "Iteration 49, loss = 0.00646320\n",
            "Iteration 50, loss = 0.00596486\n",
            "Iteration 51, loss = 0.00550495\n",
            "Iteration 52, loss = 0.00508051\n",
            "Iteration 53, loss = 0.00468880\n",
            "Iteration 54, loss = 0.00432733\n",
            "Iteration 55, loss = 0.00399387\n",
            "Iteration 56, loss = 0.00368654\n",
            "Iteration 57, loss = 0.00340404\n",
            "Iteration 58, loss = 0.00314610\n",
            "Iteration 59, loss = 0.00291393\n",
            "Iteration 60, loss = 0.00270986\n",
            "Iteration 61, loss = 0.00253495\n",
            "Iteration 62, loss = 0.00238604\n",
            "Iteration 63, loss = 0.00225671\n",
            "Iteration 64, loss = 0.00214139\n",
            "Iteration 65, loss = 0.00203692\n",
            "Iteration 66, loss = 0.00194167\n",
            "Iteration 67, loss = 0.00185463\n",
            "Iteration 68, loss = 0.00177502\n",
            "Iteration 69, loss = 0.00170218\n",
            "Iteration 70, loss = 0.00163551\n",
            "Iteration 71, loss = 0.00157446\n",
            "Iteration 72, loss = 0.00151855\n",
            "Iteration 73, loss = 0.00146732\n",
            "Iteration 74, loss = 0.00142036\n",
            "Iteration 75, loss = 0.00137731\n",
            "Iteration 76, loss = 0.00133781\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00131411\n",
            "Iteration 78, loss = 0.00130693\n",
            "Iteration 79, loss = 0.00129997\n",
            "Iteration 80, loss = 0.00129313\n",
            "Iteration 81, loss = 0.00128640\n",
            "Iteration 82, loss = 0.00127979\n",
            "Iteration 83, loss = 0.00127328\n",
            "Iteration 84, loss = 0.00126689\n",
            "Iteration 85, loss = 0.00126060\n",
            "Iteration 86, loss = 0.00125443\n",
            "Iteration 87, loss = 0.00124835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00124451\n",
            "Iteration 89, loss = 0.00124330\n",
            "Iteration 90, loss = 0.00124212\n",
            "Iteration 91, loss = 0.00124094\n",
            "Iteration 92, loss = 0.00123976\n",
            "Iteration 93, loss = 0.00123859\n",
            "Iteration 94, loss = 0.00123742\n",
            "Iteration 95, loss = 0.00123625\n",
            "Iteration 96, loss = 0.00123509\n",
            "Iteration 97, loss = 0.00123393\n",
            "Iteration 98, loss = 0.00123278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00123204\n",
            "Iteration 100, loss = 0.00123181\n",
            "Iteration 101, loss = 0.00123158\n",
            "Iteration 102, loss = 0.00123135\n",
            "Iteration 103, loss = 0.00123112\n",
            "Iteration 104, loss = 0.00123089\n",
            "Iteration 105, loss = 0.00123066\n",
            "Iteration 106, loss = 0.00123043\n",
            "Iteration 107, loss = 0.00123021\n",
            "Iteration 108, loss = 0.00122998\n",
            "Iteration 109, loss = 0.00122975\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00122960\n",
            "Iteration 111, loss = 0.00122956\n",
            "Iteration 112, loss = 0.00122951\n",
            "Iteration 113, loss = 0.00122946\n",
            "Iteration 114, loss = 0.00122942\n",
            "Iteration 115, loss = 0.00122937\n",
            "Iteration 116, loss = 0.00122933\n",
            "Iteration 117, loss = 0.00122928\n",
            "Iteration 118, loss = 0.00122924\n",
            "Iteration 119, loss = 0.00122919\n",
            "Iteration 120, loss = 0.00122915\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00122912\n",
            "Iteration 122, loss = 0.00122911\n",
            "Iteration 123, loss = 0.00122910\n",
            "Iteration 124, loss = 0.00122909\n",
            "Iteration 125, loss = 0.00122908\n",
            "Iteration 126, loss = 0.00122907\n",
            "Iteration 127, loss = 0.00122906\n",
            "Iteration 128, loss = 0.00122905\n",
            "Iteration 129, loss = 0.00122904\n",
            "Iteration 130, loss = 0.00122903\n",
            "Iteration 131, loss = 0.00122902\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12917804\n",
            "Iteration 2, loss = 0.11924992\n",
            "Iteration 3, loss = 0.11005536\n",
            "Iteration 4, loss = 0.10156972\n",
            "Iteration 5, loss = 0.09373838\n",
            "Iteration 6, loss = 0.08651091\n",
            "Iteration 7, loss = 0.07984085\n",
            "Iteration 8, loss = 0.07368546\n",
            "Iteration 9, loss = 0.06800553\n",
            "Iteration 10, loss = 0.06276518\n",
            "Iteration 11, loss = 0.05793126\n",
            "Iteration 12, loss = 0.05347255\n",
            "Iteration 13, loss = 0.04935955\n",
            "Iteration 14, loss = 0.04556504\n",
            "Iteration 15, loss = 0.04206418\n",
            "Iteration 16, loss = 0.03883429\n",
            "Iteration 17, loss = 0.03585449\n",
            "Iteration 18, loss = 0.03310553\n",
            "Iteration 19, loss = 0.03056966\n",
            "Iteration 20, loss = 0.02823053\n",
            "Iteration 21, loss = 0.02607303\n",
            "Iteration 22, loss = 0.02408323\n",
            "Iteration 23, loss = 0.02224824\n",
            "Iteration 24, loss = 0.02055619\n",
            "Iteration 25, loss = 0.01899610\n",
            "Iteration 26, loss = 0.01755785\n",
            "Iteration 27, loss = 0.01623209\n",
            "Iteration 28, loss = 0.01501017\n",
            "Iteration 29, loss = 0.01388410\n",
            "Iteration 30, loss = 0.01284652\n",
            "Iteration 31, loss = 0.01189061\n",
            "Iteration 32, loss = 0.01101014\n",
            "Iteration 33, loss = 0.01019929\n",
            "Iteration 34, loss = 0.00945276\n",
            "Iteration 35, loss = 0.00876562\n",
            "Iteration 36, loss = 0.00813329\n",
            "Iteration 37, loss = 0.00755154\n",
            "Iteration 38, loss = 0.00701642\n",
            "Iteration 39, loss = 0.00652435\n",
            "Iteration 40, loss = 0.00607195\n",
            "Iteration 41, loss = 0.00565608\n",
            "Iteration 42, loss = 0.00527384\n",
            "Iteration 43, loss = 0.00492254\n",
            "Iteration 44, loss = 0.00459971\n",
            "Iteration 45, loss = 0.00430304\n",
            "Iteration 46, loss = 0.00403042\n",
            "Iteration 47, loss = 0.00377987\n",
            "Iteration 48, loss = 0.00354958\n",
            "Iteration 49, loss = 0.00333789\n",
            "Iteration 50, loss = 0.00314326\n",
            "Iteration 51, loss = 0.00296430\n",
            "Iteration 52, loss = 0.00279971\n",
            "Iteration 53, loss = 0.00264835\n",
            "Iteration 54, loss = 0.00250918\n",
            "Iteration 55, loss = 0.00238117\n",
            "Iteration 56, loss = 0.00226336\n",
            "Iteration 57, loss = 0.00215489\n",
            "Iteration 58, loss = 0.00205500\n",
            "Iteration 59, loss = 0.00196299\n",
            "Iteration 60, loss = 0.00187824\n",
            "Iteration 61, loss = 0.00180015\n",
            "Iteration 62, loss = 0.00172819\n",
            "Iteration 63, loss = 0.00166188\n",
            "Iteration 64, loss = 0.00160077\n",
            "Iteration 65, loss = 0.00154443\n",
            "Iteration 66, loss = 0.00149250\n",
            "Iteration 67, loss = 0.00144463\n",
            "Iteration 68, loss = 0.00140049\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.00137390\n",
            "Iteration 70, loss = 0.00136584\n",
            "Iteration 71, loss = 0.00135800\n",
            "Iteration 72, loss = 0.00135029\n",
            "Iteration 73, loss = 0.00134270\n",
            "Iteration 74, loss = 0.00133524\n",
            "Iteration 75, loss = 0.00132790\n",
            "Iteration 76, loss = 0.00132067\n",
            "Iteration 77, loss = 0.00131356\n",
            "Iteration 78, loss = 0.00130656\n",
            "Iteration 79, loss = 0.00129968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.00129532\n",
            "Iteration 81, loss = 0.00129395\n",
            "Iteration 82, loss = 0.00129261\n",
            "Iteration 83, loss = 0.00129127\n",
            "Iteration 84, loss = 0.00128993\n",
            "Iteration 85, loss = 0.00128860\n",
            "Iteration 86, loss = 0.00128727\n",
            "Iteration 87, loss = 0.00128595\n",
            "Iteration 88, loss = 0.00128463\n",
            "Iteration 89, loss = 0.00128332\n",
            "Iteration 90, loss = 0.00128201\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.00128117\n",
            "Iteration 92, loss = 0.00128091\n",
            "Iteration 93, loss = 0.00128065\n",
            "Iteration 94, loss = 0.00128038\n",
            "Iteration 95, loss = 0.00128012\n",
            "Iteration 96, loss = 0.00127986\n",
            "Iteration 97, loss = 0.00127960\n",
            "Iteration 98, loss = 0.00127934\n",
            "Iteration 99, loss = 0.00127908\n",
            "Iteration 100, loss = 0.00127882\n",
            "Iteration 101, loss = 0.00127856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.00127840\n",
            "Iteration 103, loss = 0.00127835\n",
            "Iteration 104, loss = 0.00127829\n",
            "Iteration 105, loss = 0.00127824\n",
            "Iteration 106, loss = 0.00127819\n",
            "Iteration 107, loss = 0.00127814\n",
            "Iteration 108, loss = 0.00127809\n",
            "Iteration 109, loss = 0.00127803\n",
            "Iteration 110, loss = 0.00127798\n",
            "Iteration 111, loss = 0.00127793\n",
            "Iteration 112, loss = 0.00127788\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 113, loss = 0.00127785\n",
            "Iteration 114, loss = 0.00127784\n",
            "Iteration 115, loss = 0.00127783\n",
            "Iteration 116, loss = 0.00127782\n",
            "Iteration 117, loss = 0.00127780\n",
            "Iteration 118, loss = 0.00127779\n",
            "Iteration 119, loss = 0.00127778\n",
            "Iteration 120, loss = 0.00127777\n",
            "Iteration 121, loss = 0.00127776\n",
            "Iteration 122, loss = 0.00127775\n",
            "Iteration 123, loss = 0.00127774\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.15837872\n",
            "Iteration 2, loss = 0.14195275\n",
            "Iteration 3, loss = 0.13100771\n",
            "Iteration 4, loss = 0.12090658\n",
            "Iteration 5, loss = 0.11158427\n",
            "Iteration 6, loss = 0.10298075\n",
            "Iteration 7, loss = 0.09504059\n",
            "Iteration 8, loss = 0.08771264\n",
            "Iteration 9, loss = 0.08094970\n",
            "Iteration 10, loss = 0.07470821\n",
            "Iteration 11, loss = 0.06894795\n",
            "Iteration 12, loss = 0.06363183\n",
            "Iteration 13, loss = 0.05872561\n",
            "Iteration 14, loss = 0.05419766\n",
            "Iteration 15, loss = 0.05001884\n",
            "Iteration 16, loss = 0.04616222\n",
            "Iteration 17, loss = 0.04260296\n",
            "Iteration 18, loss = 0.03931813\n",
            "Iteration 19, loss = 0.03628657\n",
            "Iteration 20, loss = 0.03348875\n",
            "Iteration 21, loss = 0.03090666\n",
            "Iteration 22, loss = 0.02852365\n",
            "Iteration 23, loss = 0.02632438\n",
            "Iteration 24, loss = 0.02429468\n",
            "Iteration 25, loss = 0.02242148\n",
            "Iteration 26, loss = 0.02069271\n",
            "Iteration 27, loss = 0.01909723\n",
            "Iteration 28, loss = 0.01762477\n",
            "Iteration 29, loss = 0.01626584\n",
            "Iteration 30, loss = 0.01501169\n",
            "Iteration 31, loss = 0.01385424\n",
            "Iteration 32, loss = 0.01278603\n",
            "Iteration 33, loss = 0.01180018\n",
            "Iteration 34, loss = 0.01089035\n",
            "Iteration 35, loss = 0.01005067\n",
            "Iteration 36, loss = 0.00927573\n",
            "Iteration 37, loss = 0.00856054\n",
            "Iteration 38, loss = 0.00790051\n",
            "Iteration 39, loss = 0.00729139\n",
            "Iteration 40, loss = 0.00672936\n",
            "Iteration 41, loss = 0.00621102\n",
            "Iteration 42, loss = 0.00573366\n",
            "Iteration 43, loss = 0.00529567\n",
            "Iteration 44, loss = 0.00489715\n",
            "Iteration 45, loss = 0.00453987\n",
            "Iteration 46, loss = 0.00422508\n",
            "Iteration 47, loss = 0.00394964\n",
            "Iteration 48, loss = 0.00370569\n",
            "Iteration 49, loss = 0.00348535\n",
            "Iteration 50, loss = 0.00328376\n",
            "Iteration 51, loss = 0.00309834\n",
            "Iteration 52, loss = 0.00292750\n",
            "Iteration 53, loss = 0.00277002\n",
            "Iteration 54, loss = 0.00262484\n",
            "Iteration 55, loss = 0.00249099\n",
            "Iteration 56, loss = 0.00236757\n",
            "Iteration 57, loss = 0.00225376\n",
            "Iteration 58, loss = 0.00214881\n",
            "Iteration 59, loss = 0.00205203\n",
            "Iteration 60, loss = 0.00196276\n",
            "Iteration 61, loss = 0.00188043\n",
            "Iteration 62, loss = 0.00180449\n",
            "Iteration 63, loss = 0.00173444\n",
            "Iteration 64, loss = 0.00166982\n",
            "Iteration 65, loss = 0.00161022\n",
            "Iteration 66, loss = 0.00155523\n",
            "Iteration 67, loss = 0.00150449\n",
            "Iteration 68, loss = 0.00145769\n",
            "Iteration 69, loss = 0.00141451\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00138849\n",
            "Iteration 71, loss = 0.00138059\n",
            "Iteration 72, loss = 0.00137292\n",
            "Iteration 73, loss = 0.00136537\n",
            "Iteration 74, loss = 0.00135794\n",
            "Iteration 75, loss = 0.00135063\n",
            "Iteration 76, loss = 0.00134344\n",
            "Iteration 77, loss = 0.00133636\n",
            "Iteration 78, loss = 0.00132939\n",
            "Iteration 79, loss = 0.00132253\n",
            "Iteration 80, loss = 0.00131579\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00131152\n",
            "Iteration 82, loss = 0.00131018\n",
            "Iteration 83, loss = 0.00130886\n",
            "Iteration 84, loss = 0.00130754\n",
            "Iteration 85, loss = 0.00130624\n",
            "Iteration 86, loss = 0.00130493\n",
            "Iteration 87, loss = 0.00130363\n",
            "Iteration 88, loss = 0.00130233\n",
            "Iteration 89, loss = 0.00130104\n",
            "Iteration 90, loss = 0.00129975\n",
            "Iteration 91, loss = 0.00129847\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00129764\n",
            "Iteration 93, loss = 0.00129739\n",
            "Iteration 94, loss = 0.00129713\n",
            "Iteration 95, loss = 0.00129687\n",
            "Iteration 96, loss = 0.00129662\n",
            "Iteration 97, loss = 0.00129636\n",
            "Iteration 98, loss = 0.00129611\n",
            "Iteration 99, loss = 0.00129585\n",
            "Iteration 100, loss = 0.00129560\n",
            "Iteration 101, loss = 0.00129534\n",
            "Iteration 102, loss = 0.00129509\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00129493\n",
            "Iteration 104, loss = 0.00129488\n",
            "Iteration 105, loss = 0.00129483\n",
            "Iteration 106, loss = 0.00129477\n",
            "Iteration 107, loss = 0.00129472\n",
            "Iteration 108, loss = 0.00129467\n",
            "Iteration 109, loss = 0.00129462\n",
            "Iteration 110, loss = 0.00129457\n",
            "Iteration 111, loss = 0.00129452\n",
            "Iteration 112, loss = 0.00129447\n",
            "Iteration 113, loss = 0.00129442\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00129439\n",
            "Iteration 115, loss = 0.00129438\n",
            "Iteration 116, loss = 0.00129437\n",
            "Iteration 117, loss = 0.00129436\n",
            "Iteration 118, loss = 0.00129435\n",
            "Iteration 119, loss = 0.00129434\n",
            "Iteration 120, loss = 0.00129433\n",
            "Iteration 121, loss = 0.00129432\n",
            "Iteration 122, loss = 0.00129430\n",
            "Iteration 123, loss = 0.00129429\n",
            "Iteration 124, loss = 0.00129428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13520764\n",
            "Iteration 2, loss = 0.12481611\n",
            "Iteration 3, loss = 0.11519237\n",
            "Iteration 4, loss = 0.10631065\n",
            "Iteration 5, loss = 0.09811374\n",
            "Iteration 6, loss = 0.09054884\n",
            "Iteration 7, loss = 0.08356722\n",
            "Iteration 8, loss = 0.07712391\n",
            "Iteration 9, loss = 0.07117740\n",
            "Iteration 10, loss = 0.06568940\n",
            "Iteration 11, loss = 0.06062457\n",
            "Iteration 12, loss = 0.05595038\n",
            "Iteration 13, loss = 0.05163691\n",
            "Iteration 14, loss = 0.04765685\n",
            "Iteration 15, loss = 0.04398544\n",
            "Iteration 16, loss = 0.04060003\n",
            "Iteration 17, loss = 0.03747909\n",
            "Iteration 18, loss = 0.03460166\n",
            "Iteration 19, loss = 0.03194811\n",
            "Iteration 20, loss = 0.02950067\n",
            "Iteration 21, loss = 0.02724331\n",
            "Iteration 22, loss = 0.02516136\n",
            "Iteration 23, loss = 0.02324134\n",
            "Iteration 24, loss = 0.02147084\n",
            "Iteration 25, loss = 0.01983838\n",
            "Iteration 26, loss = 0.01833339\n",
            "Iteration 27, loss = 0.01694609\n",
            "Iteration 28, loss = 0.01566745\n",
            "Iteration 29, loss = 0.01448914\n",
            "Iteration 30, loss = 0.01340343\n",
            "Iteration 31, loss = 0.01240321\n",
            "Iteration 32, loss = 0.01148189\n",
            "Iteration 33, loss = 0.01063341\n",
            "Iteration 34, loss = 0.00985213\n",
            "Iteration 35, loss = 0.00913288\n",
            "Iteration 36, loss = 0.00847088\n",
            "Iteration 37, loss = 0.00786169\n",
            "Iteration 38, loss = 0.00730125\n",
            "Iteration 39, loss = 0.00678579\n",
            "Iteration 40, loss = 0.00631181\n",
            "Iteration 41, loss = 0.00587600\n",
            "Iteration 42, loss = 0.00547534\n",
            "Iteration 43, loss = 0.00510700\n",
            "Iteration 44, loss = 0.00476840\n",
            "Iteration 45, loss = 0.00445712\n",
            "Iteration 46, loss = 0.00417096\n",
            "Iteration 47, loss = 0.00390794\n",
            "Iteration 48, loss = 0.00366620\n",
            "Iteration 49, loss = 0.00344404\n",
            "Iteration 50, loss = 0.00323989\n",
            "Iteration 51, loss = 0.00305228\n",
            "Iteration 52, loss = 0.00287982\n",
            "Iteration 53, loss = 0.00272115\n",
            "Iteration 54, loss = 0.00257500\n",
            "Iteration 55, loss = 0.00244050\n",
            "Iteration 56, loss = 0.00231684\n",
            "Iteration 57, loss = 0.00220313\n",
            "Iteration 58, loss = 0.00209855\n",
            "Iteration 59, loss = 0.00200234\n",
            "Iteration 60, loss = 0.00191381\n",
            "Iteration 61, loss = 0.00183234\n",
            "Iteration 62, loss = 0.00175733\n",
            "Iteration 63, loss = 0.00168830\n",
            "Iteration 64, loss = 0.00162473\n",
            "Iteration 65, loss = 0.00156618\n",
            "Iteration 66, loss = 0.00151224\n",
            "Iteration 67, loss = 0.00146253\n",
            "Iteration 68, loss = 0.00141673\n",
            "Iteration 69, loss = 0.00137450\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00134909\n",
            "Iteration 71, loss = 0.00134138\n",
            "Iteration 72, loss = 0.00133389\n",
            "Iteration 73, loss = 0.00132652\n",
            "Iteration 74, loss = 0.00131927\n",
            "Iteration 75, loss = 0.00131214\n",
            "Iteration 76, loss = 0.00130512\n",
            "Iteration 77, loss = 0.00129822\n",
            "Iteration 78, loss = 0.00129143\n",
            "Iteration 79, loss = 0.00128475\n",
            "Iteration 80, loss = 0.00127818\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00127402\n",
            "Iteration 82, loss = 0.00127271\n",
            "Iteration 83, loss = 0.00127143\n",
            "Iteration 84, loss = 0.00127015\n",
            "Iteration 85, loss = 0.00126888\n",
            "Iteration 86, loss = 0.00126760\n",
            "Iteration 87, loss = 0.00126634\n",
            "Iteration 88, loss = 0.00126507\n",
            "Iteration 89, loss = 0.00126382\n",
            "Iteration 90, loss = 0.00126256\n",
            "Iteration 91, loss = 0.00126131\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00126051\n",
            "Iteration 93, loss = 0.00126026\n",
            "Iteration 94, loss = 0.00126001\n",
            "Iteration 95, loss = 0.00125976\n",
            "Iteration 96, loss = 0.00125951\n",
            "Iteration 97, loss = 0.00125926\n",
            "Iteration 98, loss = 0.00125902\n",
            "Iteration 99, loss = 0.00125877\n",
            "Iteration 100, loss = 0.00125852\n",
            "Iteration 101, loss = 0.00125827\n",
            "Iteration 102, loss = 0.00125802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00125787\n",
            "Iteration 104, loss = 0.00125782\n",
            "Iteration 105, loss = 0.00125777\n",
            "Iteration 106, loss = 0.00125772\n",
            "Iteration 107, loss = 0.00125767\n",
            "Iteration 108, loss = 0.00125762\n",
            "Iteration 109, loss = 0.00125757\n",
            "Iteration 110, loss = 0.00125752\n",
            "Iteration 111, loss = 0.00125747\n",
            "Iteration 112, loss = 0.00125742\n",
            "Iteration 113, loss = 0.00125737\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00125734\n",
            "Iteration 115, loss = 0.00125733\n",
            "Iteration 116, loss = 0.00125732\n",
            "Iteration 117, loss = 0.00125731\n",
            "Iteration 118, loss = 0.00125730\n",
            "Iteration 119, loss = 0.00125729\n",
            "Iteration 120, loss = 0.00125728\n",
            "Iteration 121, loss = 0.00125727\n",
            "Iteration 122, loss = 0.00125726\n",
            "Iteration 123, loss = 0.00125725\n",
            "Iteration 124, loss = 0.00125724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33847     0]\n",
            " [ 3774     0]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  8.8min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95     33847\n",
            "           1       0.00      0.00      0.00      3774\n",
            "\n",
            "    accuracy                           0.90     37621\n",
            "   macro avg       0.45      0.50      0.47     37621\n",
            "weighted avg       0.81      0.90      0.85     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Iteration 1, loss = 0.75167941\n",
            "Iteration 2, loss = 0.41010527\n",
            "Iteration 3, loss = 0.38297016\n",
            "Iteration 4, loss = 0.36296854\n",
            "Iteration 5, loss = 0.34884346\n",
            "Iteration 6, loss = 0.33892902\n",
            "Iteration 7, loss = 0.33201184\n",
            "Iteration 8, loss = 0.32611928\n",
            "Iteration 9, loss = 0.32221220\n",
            "Iteration 10, loss = 0.31923695\n",
            "Iteration 11, loss = 0.31771364\n",
            "Iteration 12, loss = 0.31603117\n",
            "Iteration 13, loss = 0.31598691\n",
            "Iteration 14, loss = 0.31502619\n",
            "Iteration 15, loss = 0.31441838\n",
            "Iteration 16, loss = 0.31288235\n",
            "Iteration 17, loss = 0.31428524\n",
            "Iteration 18, loss = 0.31248909\n",
            "Iteration 19, loss = 0.31246490\n",
            "Iteration 20, loss = 0.31317515\n",
            "Iteration 21, loss = 0.31263836\n",
            "Iteration 22, loss = 0.31294336\n",
            "Iteration 23, loss = 0.31251295\n",
            "Iteration 24, loss = 0.31219330\n",
            "Iteration 25, loss = 0.31239187\n",
            "Iteration 26, loss = 0.31244425\n",
            "Iteration 27, loss = 0.31279822\n",
            "Iteration 28, loss = 0.31291849\n",
            "Iteration 29, loss = 0.31292882\n",
            "Iteration 30, loss = 0.31279140\n",
            "Iteration 31, loss = 0.31213311\n",
            "Iteration 32, loss = 0.31213521\n",
            "Iteration 33, loss = 0.31099776\n",
            "Iteration 34, loss = 0.31304128\n",
            "Iteration 35, loss = 0.31245344\n",
            "Iteration 36, loss = 0.31269784\n",
            "Iteration 37, loss = 0.31254302\n",
            "Iteration 38, loss = 0.31239880\n",
            "Iteration 39, loss = 0.31139444\n",
            "Iteration 40, loss = 0.31253941\n",
            "Iteration 41, loss = 0.31211678\n",
            "Iteration 42, loss = 0.31213625\n",
            "Iteration 43, loss = 0.31251616\n",
            "Iteration 44, loss = 0.31111736\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.30225912\n",
            "Iteration 46, loss = 0.29947810\n",
            "Iteration 47, loss = 0.29765484\n",
            "Iteration 48, loss = 0.29598460\n",
            "Iteration 49, loss = 0.29395417\n",
            "Iteration 50, loss = 0.29268165\n",
            "Iteration 51, loss = 0.29098686\n",
            "Iteration 52, loss = 0.28983016\n",
            "Iteration 53, loss = 0.28834893\n",
            "Iteration 54, loss = 0.28787665\n",
            "Iteration 55, loss = 0.28779797\n",
            "Iteration 56, loss = 0.28706359\n",
            "Iteration 57, loss = 0.28763955\n",
            "Iteration 58, loss = 0.29128621\n",
            "Iteration 59, loss = 0.29138497\n",
            "Iteration 60, loss = 0.29376236\n",
            "Iteration 61, loss = 0.29051139\n",
            "Iteration 62, loss = 0.29176141\n",
            "Iteration 63, loss = 0.29507296\n",
            "Iteration 64, loss = 0.29002624\n",
            "Iteration 65, loss = 0.29494025\n",
            "Iteration 66, loss = 0.29619590\n",
            "Iteration 67, loss = 0.28922371\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.26978245\n",
            "Iteration 69, loss = 0.26887411\n",
            "Iteration 70, loss = 0.26820818\n",
            "Iteration 71, loss = 0.26760696\n",
            "Iteration 72, loss = 0.26727296\n",
            "Iteration 73, loss = 0.26635128\n",
            "Iteration 74, loss = 0.26562696\n",
            "Iteration 75, loss = 0.26504320\n",
            "Iteration 76, loss = 0.26478200\n",
            "Iteration 77, loss = 0.26392493\n",
            "Iteration 78, loss = 0.26359165\n",
            "Iteration 79, loss = 0.26297323\n",
            "Iteration 80, loss = 0.26239671\n",
            "Iteration 81, loss = 0.26189649\n",
            "Iteration 82, loss = 0.26111108\n",
            "Iteration 83, loss = 0.26072940\n",
            "Iteration 84, loss = 0.26039621\n",
            "Iteration 85, loss = 0.25988767\n",
            "Iteration 86, loss = 0.25907654\n",
            "Iteration 87, loss = 0.25849572\n",
            "Iteration 88, loss = 0.25795876\n",
            "Iteration 89, loss = 0.25766923\n",
            "Iteration 90, loss = 0.25696851\n",
            "Iteration 91, loss = 0.25681218\n",
            "Iteration 92, loss = 0.25587046\n",
            "Iteration 93, loss = 0.25559772\n",
            "Iteration 94, loss = 0.25514759\n",
            "Iteration 95, loss = 0.25479339\n",
            "Iteration 96, loss = 0.25426490\n",
            "Iteration 97, loss = 0.25397697\n",
            "Iteration 98, loss = 0.25340515\n",
            "Iteration 99, loss = 0.25281743\n",
            "Iteration 100, loss = 0.25222570\n",
            "Iteration 101, loss = 0.25209863\n",
            "Iteration 102, loss = 0.25160783\n",
            "Iteration 103, loss = 0.25102015\n",
            "Iteration 104, loss = 0.25088720\n",
            "Iteration 105, loss = 0.25074454\n",
            "Iteration 106, loss = 0.25021276\n",
            "Iteration 107, loss = 0.24986458\n",
            "Iteration 108, loss = 0.24931195\n",
            "Iteration 109, loss = 0.24902561\n",
            "Iteration 110, loss = 0.24863130\n",
            "Iteration 111, loss = 0.24807378\n",
            "Iteration 112, loss = 0.24783798\n",
            "Iteration 113, loss = 0.24768240\n",
            "Iteration 114, loss = 0.24711403\n",
            "Iteration 115, loss = 0.24687909\n",
            "Iteration 116, loss = 0.24686604\n",
            "Iteration 117, loss = 0.24647778\n",
            "Iteration 118, loss = 0.24565986\n",
            "Iteration 119, loss = 0.24546330\n",
            "Iteration 120, loss = 0.24563272\n",
            "Iteration 121, loss = 0.24496041\n",
            "Iteration 122, loss = 0.24504031\n",
            "Iteration 123, loss = 0.24419517\n",
            "Iteration 124, loss = 0.24389384\n",
            "Iteration 125, loss = 0.24366777\n",
            "Iteration 126, loss = 0.24364072\n",
            "Iteration 127, loss = 0.24312705\n",
            "Iteration 128, loss = 0.24334555\n",
            "Iteration 129, loss = 0.24299603\n",
            "Iteration 130, loss = 0.24236750\n",
            "Iteration 131, loss = 0.24236233\n",
            "Iteration 132, loss = 0.24209761\n",
            "Iteration 133, loss = 0.24166083\n",
            "Iteration 134, loss = 0.24182569\n",
            "Iteration 135, loss = 0.24130481\n",
            "Iteration 136, loss = 0.24134703\n",
            "Iteration 137, loss = 0.24095233\n",
            "Iteration 138, loss = 0.24092410\n",
            "Iteration 139, loss = 0.24036916\n",
            "Iteration 140, loss = 0.24052386\n",
            "Iteration 141, loss = 0.24002631\n",
            "Iteration 142, loss = 0.23981807\n",
            "Iteration 143, loss = 0.23989626\n",
            "Iteration 144, loss = 0.23988120\n",
            "Iteration 145, loss = 0.23939409\n",
            "Iteration 146, loss = 0.23905548\n",
            "Iteration 147, loss = 0.23924974\n",
            "Iteration 148, loss = 0.23875131\n",
            "Iteration 149, loss = 0.23891144\n",
            "Iteration 150, loss = 0.23819902\n",
            "Iteration 151, loss = 0.23808551\n",
            "Iteration 152, loss = 0.23803118\n",
            "Iteration 153, loss = 0.23816807\n",
            "Iteration 154, loss = 0.23778848\n",
            "Iteration 155, loss = 0.23764569\n",
            "Iteration 156, loss = 0.23744924\n",
            "Iteration 157, loss = 0.23742797\n",
            "Iteration 158, loss = 0.23745551\n",
            "Iteration 159, loss = 0.23740786\n",
            "Iteration 160, loss = 0.23716082\n",
            "Iteration 161, loss = 0.23699731\n",
            "Iteration 162, loss = 0.23707677\n",
            "Iteration 163, loss = 0.23691168\n",
            "Iteration 164, loss = 0.23636823\n",
            "Iteration 165, loss = 0.23727191\n",
            "Iteration 166, loss = 0.23615699\n",
            "Iteration 167, loss = 0.23628198\n",
            "Iteration 168, loss = 0.23655863\n",
            "Iteration 169, loss = 0.23607916\n",
            "Iteration 170, loss = 0.23589764\n",
            "Iteration 171, loss = 0.23570684\n",
            "Iteration 172, loss = 0.23586312\n",
            "Iteration 173, loss = 0.23581285\n",
            "Iteration 174, loss = 0.23572992\n",
            "Iteration 175, loss = 0.23566935\n",
            "Iteration 176, loss = 0.23580143\n",
            "Iteration 177, loss = 0.23539946\n",
            "Iteration 178, loss = 0.23544626\n",
            "Iteration 179, loss = 0.23502895\n",
            "Iteration 180, loss = 0.23548328\n",
            "Iteration 181, loss = 0.23517028\n",
            "Iteration 182, loss = 0.23528581\n",
            "Iteration 183, loss = 0.23451656\n",
            "Iteration 184, loss = 0.23473244\n",
            "Iteration 185, loss = 0.23503630\n",
            "Iteration 186, loss = 0.23532741\n",
            "Iteration 187, loss = 0.23483563\n",
            "Iteration 188, loss = 0.23490657\n",
            "Iteration 189, loss = 0.23446551\n",
            "Iteration 190, loss = 0.23485824\n",
            "Iteration 191, loss = 0.23468431\n",
            "Iteration 192, loss = 0.23430001\n",
            "Iteration 193, loss = 0.23440087\n",
            "Iteration 194, loss = 0.23422250\n",
            "Iteration 195, loss = 0.23430307\n",
            "Iteration 196, loss = 0.23418960\n",
            "Iteration 197, loss = 0.23406402\n",
            "Iteration 198, loss = 0.23412520\n",
            "Iteration 199, loss = 0.23418142\n",
            "Iteration 200, loss = 0.23419383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.91618744\n",
            "Iteration 2, loss = 0.32231176\n",
            "Iteration 3, loss = 0.29390364\n",
            "Iteration 4, loss = 0.27388200\n",
            "Iteration 5, loss = 0.25692418\n",
            "Iteration 6, loss = 0.24547584\n",
            "Iteration 7, loss = 0.23519613\n",
            "Iteration 8, loss = 0.22862787\n",
            "Iteration 9, loss = 0.22260270\n",
            "Iteration 10, loss = 0.21918958\n",
            "Iteration 11, loss = 0.21678113\n",
            "Iteration 12, loss = 0.21308241\n",
            "Iteration 13, loss = 0.21229197\n",
            "Iteration 14, loss = 0.21120427\n",
            "Iteration 15, loss = 0.21013156\n",
            "Iteration 16, loss = 0.20873109\n",
            "Iteration 17, loss = 0.20936063\n",
            "Iteration 18, loss = 0.20818695\n",
            "Iteration 19, loss = 0.20711215\n",
            "Iteration 20, loss = 0.20774183\n",
            "Iteration 21, loss = 0.20755765\n",
            "Iteration 22, loss = 0.20773457\n",
            "Iteration 23, loss = 0.20769137\n",
            "Iteration 24, loss = 0.20730771\n",
            "Iteration 25, loss = 0.20719382\n",
            "Iteration 26, loss = 0.20684475\n",
            "Iteration 27, loss = 0.20776665\n",
            "Iteration 28, loss = 0.20693107\n",
            "Iteration 29, loss = 0.20661093\n",
            "Iteration 30, loss = 0.20821876\n",
            "Iteration 31, loss = 0.20751660\n",
            "Iteration 32, loss = 0.20705647\n",
            "Iteration 33, loss = 0.20668795\n",
            "Iteration 34, loss = 0.20753020\n",
            "Iteration 35, loss = 0.20621381\n",
            "Iteration 36, loss = 0.20744680\n",
            "Iteration 37, loss = 0.20678899\n",
            "Iteration 38, loss = 0.20711234\n",
            "Iteration 39, loss = 0.20779396\n",
            "Iteration 40, loss = 0.20609543\n",
            "Iteration 41, loss = 0.20758786\n",
            "Iteration 42, loss = 0.20704735\n",
            "Iteration 43, loss = 0.20703191\n",
            "Iteration 44, loss = 0.20738644\n",
            "Iteration 45, loss = 0.20652526\n",
            "Iteration 46, loss = 0.20759474\n",
            "Iteration 47, loss = 0.20748501\n",
            "Iteration 48, loss = 0.20657652\n",
            "Iteration 49, loss = 0.20709757\n",
            "Iteration 50, loss = 0.20670035\n",
            "Iteration 51, loss = 0.20578945\n",
            "Iteration 52, loss = 0.20739530\n",
            "Iteration 53, loss = 0.20707684\n",
            "Iteration 54, loss = 0.20625994\n",
            "Iteration 55, loss = 0.20622480\n",
            "Iteration 56, loss = 0.20721754\n",
            "Iteration 57, loss = 0.20743819\n",
            "Iteration 58, loss = 0.20759796\n",
            "Iteration 59, loss = 0.20656894\n",
            "Iteration 60, loss = 0.20715066\n",
            "Iteration 61, loss = 0.20711461\n",
            "Iteration 62, loss = 0.20584126\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 63, loss = 0.19863424\n",
            "Iteration 64, loss = 0.19720183\n",
            "Iteration 65, loss = 0.19613259\n",
            "Iteration 66, loss = 0.19579930\n",
            "Iteration 67, loss = 0.19486326\n",
            "Iteration 68, loss = 0.19413936\n",
            "Iteration 69, loss = 0.19362671\n",
            "Iteration 70, loss = 0.19306005\n",
            "Iteration 71, loss = 0.19273614\n",
            "Iteration 72, loss = 0.19211770\n",
            "Iteration 73, loss = 0.19125706\n",
            "Iteration 74, loss = 0.19065960\n",
            "Iteration 75, loss = 0.19043442\n",
            "Iteration 76, loss = 0.18936157\n",
            "Iteration 77, loss = 0.18932050\n",
            "Iteration 78, loss = 0.18903231\n",
            "Iteration 79, loss = 0.18845493\n",
            "Iteration 80, loss = 0.18861607\n",
            "Iteration 81, loss = 0.18813542\n",
            "Iteration 82, loss = 0.18765440\n",
            "Iteration 83, loss = 0.18701302\n",
            "Iteration 84, loss = 0.18703606\n",
            "Iteration 85, loss = 0.18664966\n",
            "Iteration 86, loss = 0.18724758\n",
            "Iteration 87, loss = 0.18654545\n",
            "Iteration 88, loss = 0.18605892\n",
            "Iteration 89, loss = 0.18525238\n",
            "Iteration 90, loss = 0.18760247\n",
            "Iteration 91, loss = 0.18669249\n",
            "Iteration 92, loss = 0.18634587\n",
            "Iteration 93, loss = 0.18573455\n",
            "Iteration 94, loss = 0.18657693\n",
            "Iteration 95, loss = 0.18488448\n",
            "Iteration 96, loss = 0.18660134\n",
            "Iteration 97, loss = 0.18731654\n",
            "Iteration 98, loss = 0.18707547\n",
            "Iteration 99, loss = 0.18444011\n",
            "Iteration 100, loss = 0.18613108\n",
            "Iteration 101, loss = 0.19055197\n",
            "Iteration 102, loss = 0.18558002\n",
            "Iteration 103, loss = 0.18669884\n",
            "Iteration 104, loss = 0.18760224\n",
            "Iteration 105, loss = 0.18787510\n",
            "Iteration 106, loss = 0.18715583\n",
            "Iteration 107, loss = 0.18643718\n",
            "Iteration 108, loss = 0.19286765\n",
            "Iteration 109, loss = 0.18833051\n",
            "Iteration 110, loss = 0.18727528\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 111, loss = 0.17480723\n",
            "Iteration 112, loss = 0.17454859\n",
            "Iteration 113, loss = 0.17446509\n",
            "Iteration 114, loss = 0.17435741\n",
            "Iteration 115, loss = 0.17416830\n",
            "Iteration 116, loss = 0.17395687\n",
            "Iteration 117, loss = 0.17410505\n",
            "Iteration 118, loss = 0.17386476\n",
            "Iteration 119, loss = 0.17341138\n",
            "Iteration 120, loss = 0.17349322\n",
            "Iteration 121, loss = 0.17346493\n",
            "Iteration 122, loss = 0.17337779\n",
            "Iteration 123, loss = 0.17311208\n",
            "Iteration 124, loss = 0.17322995\n",
            "Iteration 125, loss = 0.17294455\n",
            "Iteration 126, loss = 0.17280147\n",
            "Iteration 127, loss = 0.17276870\n",
            "Iteration 128, loss = 0.17265694\n",
            "Iteration 129, loss = 0.17261061\n",
            "Iteration 130, loss = 0.17239213\n",
            "Iteration 131, loss = 0.17228040\n",
            "Iteration 132, loss = 0.17228234\n",
            "Iteration 133, loss = 0.17199480\n",
            "Iteration 134, loss = 0.17199875\n",
            "Iteration 135, loss = 0.17204011\n",
            "Iteration 136, loss = 0.17163579\n",
            "Iteration 137, loss = 0.17162065\n",
            "Iteration 138, loss = 0.17147900\n",
            "Iteration 139, loss = 0.17137356\n",
            "Iteration 140, loss = 0.17136999\n",
            "Iteration 141, loss = 0.17126722\n",
            "Iteration 142, loss = 0.17099498\n",
            "Iteration 143, loss = 0.17100294\n",
            "Iteration 144, loss = 0.17089173\n",
            "Iteration 145, loss = 0.17077129\n",
            "Iteration 146, loss = 0.17074618\n",
            "Iteration 147, loss = 0.17056199\n",
            "Iteration 148, loss = 0.17050524\n",
            "Iteration 149, loss = 0.17048889\n",
            "Iteration 150, loss = 0.17041993\n",
            "Iteration 151, loss = 0.17043134\n",
            "Iteration 152, loss = 0.17004812\n",
            "Iteration 153, loss = 0.17001517\n",
            "Iteration 154, loss = 0.17016187\n",
            "Iteration 155, loss = 0.17007594\n",
            "Iteration 156, loss = 0.16974539\n",
            "Iteration 157, loss = 0.16973951\n",
            "Iteration 158, loss = 0.16974342\n",
            "Iteration 159, loss = 0.16955329\n",
            "Iteration 160, loss = 0.16950696\n",
            "Iteration 161, loss = 0.16955297\n",
            "Iteration 162, loss = 0.16938350\n",
            "Iteration 163, loss = 0.16937050\n",
            "Iteration 164, loss = 0.16925554\n",
            "Iteration 165, loss = 0.16908768\n",
            "Iteration 166, loss = 0.16911936\n",
            "Iteration 167, loss = 0.16904553\n",
            "Iteration 168, loss = 0.16894993\n",
            "Iteration 169, loss = 0.16865710\n",
            "Iteration 170, loss = 0.16870282\n",
            "Iteration 171, loss = 0.16881443\n",
            "Iteration 172, loss = 0.16863362\n",
            "Iteration 173, loss = 0.16849908\n",
            "Iteration 174, loss = 0.16872528\n",
            "Iteration 175, loss = 0.16842234\n",
            "Iteration 176, loss = 0.16842319\n",
            "Iteration 177, loss = 0.16847023\n",
            "Iteration 178, loss = 0.16831287\n",
            "Iteration 179, loss = 0.16822629\n",
            "Iteration 180, loss = 0.16823107\n",
            "Iteration 181, loss = 0.16806946\n",
            "Iteration 182, loss = 0.16809723\n",
            "Iteration 183, loss = 0.16795982\n",
            "Iteration 184, loss = 0.16786338\n",
            "Iteration 185, loss = 0.16799334\n",
            "Iteration 186, loss = 0.16772662\n",
            "Iteration 187, loss = 0.16768700\n",
            "Iteration 188, loss = 0.16777508\n",
            "Iteration 189, loss = 0.16757279\n",
            "Iteration 190, loss = 0.16765080\n",
            "Iteration 191, loss = 0.16729989\n",
            "Iteration 192, loss = 0.16766060\n",
            "Iteration 193, loss = 0.16744597\n",
            "Iteration 194, loss = 0.16741597\n",
            "Iteration 195, loss = 0.16729175\n",
            "Iteration 196, loss = 0.16738628\n",
            "Iteration 197, loss = 0.16732076\n",
            "Iteration 198, loss = 0.16711420\n",
            "Iteration 199, loss = 0.16703712\n",
            "Iteration 200, loss = 0.16703897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.70624796\n",
            "Iteration 2, loss = 0.29385984\n",
            "Iteration 3, loss = 0.27177907\n",
            "Iteration 4, loss = 0.25600437\n",
            "Iteration 5, loss = 0.24401264\n",
            "Iteration 6, loss = 0.23541423\n",
            "Iteration 7, loss = 0.22803770\n",
            "Iteration 8, loss = 0.22286276\n",
            "Iteration 9, loss = 0.21911392\n",
            "Iteration 10, loss = 0.21628886\n",
            "Iteration 11, loss = 0.21352429\n",
            "Iteration 12, loss = 0.21199786\n",
            "Iteration 13, loss = 0.21073407\n",
            "Iteration 14, loss = 0.21042666\n",
            "Iteration 15, loss = 0.20902534\n",
            "Iteration 16, loss = 0.20804407\n",
            "Iteration 17, loss = 0.20834370\n",
            "Iteration 18, loss = 0.20836464\n",
            "Iteration 19, loss = 0.20953784\n",
            "Iteration 20, loss = 0.20851624\n",
            "Iteration 21, loss = 0.20731638\n",
            "Iteration 22, loss = 0.20742133\n",
            "Iteration 23, loss = 0.20764315\n",
            "Iteration 24, loss = 0.20713058\n",
            "Iteration 25, loss = 0.20724350\n",
            "Iteration 26, loss = 0.20626142\n",
            "Iteration 27, loss = 0.20605860\n",
            "Iteration 28, loss = 0.20773221\n",
            "Iteration 29, loss = 0.20692504\n",
            "Iteration 30, loss = 0.20652303\n",
            "Iteration 31, loss = 0.20581076\n",
            "Iteration 32, loss = 0.20721286\n",
            "Iteration 33, loss = 0.20706640\n",
            "Iteration 34, loss = 0.20690891\n",
            "Iteration 35, loss = 0.20610087\n",
            "Iteration 36, loss = 0.20721072\n",
            "Iteration 37, loss = 0.20972364\n",
            "Iteration 38, loss = 0.20850659\n",
            "Iteration 39, loss = 0.20817922\n",
            "Iteration 40, loss = 0.20796729\n",
            "Iteration 41, loss = 0.20751978\n",
            "Iteration 42, loss = 0.20713064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.20051819\n",
            "Iteration 44, loss = 0.19943714\n",
            "Iteration 45, loss = 0.19892667\n",
            "Iteration 46, loss = 0.19821180\n",
            "Iteration 47, loss = 0.19786670\n",
            "Iteration 48, loss = 0.19720266\n",
            "Iteration 49, loss = 0.19665708\n",
            "Iteration 50, loss = 0.19600044\n",
            "Iteration 51, loss = 0.19538751\n",
            "Iteration 52, loss = 0.19488411\n",
            "Iteration 53, loss = 0.19444775\n",
            "Iteration 54, loss = 0.19368170\n",
            "Iteration 55, loss = 0.19359067\n",
            "Iteration 56, loss = 0.19331051\n",
            "Iteration 57, loss = 0.19294722\n",
            "Iteration 58, loss = 0.19208812\n",
            "Iteration 59, loss = 0.19196344\n",
            "Iteration 60, loss = 0.19113891\n",
            "Iteration 61, loss = 0.19080011\n",
            "Iteration 62, loss = 0.19102820\n",
            "Iteration 63, loss = 0.19019202\n",
            "Iteration 64, loss = 0.18973904\n",
            "Iteration 65, loss = 0.18954672\n",
            "Iteration 66, loss = 0.18895773\n",
            "Iteration 67, loss = 0.18866413\n",
            "Iteration 68, loss = 0.18826131\n",
            "Iteration 69, loss = 0.18887676\n",
            "Iteration 70, loss = 0.18796468\n",
            "Iteration 71, loss = 0.18696547\n",
            "Iteration 72, loss = 0.18652661\n",
            "Iteration 73, loss = 0.18833105\n",
            "Iteration 74, loss = 0.18728383\n",
            "Iteration 75, loss = 0.18699392\n",
            "Iteration 76, loss = 0.18697647\n",
            "Iteration 77, loss = 0.18649917\n",
            "Iteration 78, loss = 0.18604800\n",
            "Iteration 79, loss = 0.18651902\n",
            "Iteration 80, loss = 0.18553236\n",
            "Iteration 81, loss = 0.18653396\n",
            "Iteration 82, loss = 0.18777309\n",
            "Iteration 83, loss = 0.18780777\n",
            "Iteration 84, loss = 0.18607153\n",
            "Iteration 85, loss = 0.18761449\n",
            "Iteration 86, loss = 0.18741110\n",
            "Iteration 87, loss = 0.18786149\n",
            "Iteration 88, loss = 0.18752173\n",
            "Iteration 89, loss = 0.18757133\n",
            "Iteration 90, loss = 0.18984651\n",
            "Iteration 91, loss = 0.18549184\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 92, loss = 0.17576418\n",
            "Iteration 93, loss = 0.17552289\n",
            "Iteration 94, loss = 0.17551595\n",
            "Iteration 95, loss = 0.17536004\n",
            "Iteration 96, loss = 0.17510430\n",
            "Iteration 97, loss = 0.17514539\n",
            "Iteration 98, loss = 0.17501697\n",
            "Iteration 99, loss = 0.17485164\n",
            "Iteration 100, loss = 0.17461899\n",
            "Iteration 101, loss = 0.17461326\n",
            "Iteration 102, loss = 0.17447778\n",
            "Iteration 103, loss = 0.17440068\n",
            "Iteration 104, loss = 0.17413378\n",
            "Iteration 105, loss = 0.17414376\n",
            "Iteration 106, loss = 0.17390390\n",
            "Iteration 107, loss = 0.17388226\n",
            "Iteration 108, loss = 0.17367218\n",
            "Iteration 109, loss = 0.17362329\n",
            "Iteration 110, loss = 0.17353639\n",
            "Iteration 111, loss = 0.17335436\n",
            "Iteration 112, loss = 0.17348456\n",
            "Iteration 113, loss = 0.17315001\n",
            "Iteration 114, loss = 0.17305079\n",
            "Iteration 115, loss = 0.17304738\n",
            "Iteration 116, loss = 0.17282918\n",
            "Iteration 117, loss = 0.17269131\n",
            "Iteration 118, loss = 0.17274589\n",
            "Iteration 119, loss = 0.17258211\n",
            "Iteration 120, loss = 0.17265143\n",
            "Iteration 121, loss = 0.17218716\n",
            "Iteration 122, loss = 0.17233673\n",
            "Iteration 123, loss = 0.17215585\n",
            "Iteration 124, loss = 0.17200720\n",
            "Iteration 125, loss = 0.17182394\n",
            "Iteration 126, loss = 0.17166026\n",
            "Iteration 127, loss = 0.17181874\n",
            "Iteration 128, loss = 0.17166947\n",
            "Iteration 129, loss = 0.17139461\n",
            "Iteration 130, loss = 0.17143139\n",
            "Iteration 131, loss = 0.17123372\n",
            "Iteration 132, loss = 0.17115685\n",
            "Iteration 133, loss = 0.17123988\n",
            "Iteration 134, loss = 0.17110187\n",
            "Iteration 135, loss = 0.17084849\n",
            "Iteration 136, loss = 0.17084596\n",
            "Iteration 137, loss = 0.17076495\n",
            "Iteration 138, loss = 0.17049401\n",
            "Iteration 139, loss = 0.17057337\n",
            "Iteration 140, loss = 0.17045555\n",
            "Iteration 141, loss = 0.17035789\n",
            "Iteration 142, loss = 0.17021669\n",
            "Iteration 143, loss = 0.17021331\n",
            "Iteration 144, loss = 0.17008581\n",
            "Iteration 145, loss = 0.17011923\n",
            "Iteration 146, loss = 0.17000901\n",
            "Iteration 147, loss = 0.16992574\n",
            "Iteration 148, loss = 0.16979232\n",
            "Iteration 149, loss = 0.16993633\n",
            "Iteration 150, loss = 0.16976149\n",
            "Iteration 151, loss = 0.16961486\n",
            "Iteration 152, loss = 0.16949547\n",
            "Iteration 153, loss = 0.16955319\n",
            "Iteration 154, loss = 0.16956538\n",
            "Iteration 155, loss = 0.16923879\n",
            "Iteration 156, loss = 0.16935193\n",
            "Iteration 157, loss = 0.16918932\n",
            "Iteration 158, loss = 0.16931111\n",
            "Iteration 159, loss = 0.16890706\n",
            "Iteration 160, loss = 0.16913803\n",
            "Iteration 161, loss = 0.16880985\n",
            "Iteration 162, loss = 0.16893083\n",
            "Iteration 163, loss = 0.16868957\n",
            "Iteration 164, loss = 0.16868393\n",
            "Iteration 165, loss = 0.16849493\n",
            "Iteration 166, loss = 0.16869212\n",
            "Iteration 167, loss = 0.16853710\n",
            "Iteration 168, loss = 0.16839079\n",
            "Iteration 169, loss = 0.16850918\n",
            "Iteration 170, loss = 0.16830356\n",
            "Iteration 171, loss = 0.16824853\n",
            "Iteration 172, loss = 0.16812278\n",
            "Iteration 173, loss = 0.16822239\n",
            "Iteration 174, loss = 0.16810145\n",
            "Iteration 175, loss = 0.16804524\n",
            "Iteration 176, loss = 0.16806216\n",
            "Iteration 177, loss = 0.16798334\n",
            "Iteration 178, loss = 0.16782542\n",
            "Iteration 179, loss = 0.16772967\n",
            "Iteration 180, loss = 0.16762851\n",
            "Iteration 181, loss = 0.16778247\n",
            "Iteration 182, loss = 0.16767889\n",
            "Iteration 183, loss = 0.16758883\n",
            "Iteration 184, loss = 0.16763302\n",
            "Iteration 185, loss = 0.16731620\n",
            "Iteration 186, loss = 0.16732167\n",
            "Iteration 187, loss = 0.16747377\n",
            "Iteration 188, loss = 0.16723002\n",
            "Iteration 189, loss = 0.16714138\n",
            "Iteration 190, loss = 0.16733274\n",
            "Iteration 191, loss = 0.16724914\n",
            "Iteration 192, loss = 0.16707221\n",
            "Iteration 193, loss = 0.16699812\n",
            "Iteration 194, loss = 0.16711223\n",
            "Iteration 195, loss = 0.16681720\n",
            "Iteration 196, loss = 0.16705875\n",
            "Iteration 197, loss = 0.16684646\n",
            "Iteration 198, loss = 0.16717060\n",
            "Iteration 199, loss = 0.16670696\n",
            "Iteration 200, loss = 0.16664480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32200710\n",
            "Iteration 3, loss = 0.29352068\n",
            "Iteration 4, loss = 0.27241042\n",
            "Iteration 5, loss = 0.25627892\n",
            "Iteration 6, loss = 0.24400622\n",
            "Iteration 7, loss = 0.23555869\n",
            "Iteration 8, loss = 0.22889870\n",
            "Iteration 9, loss = 0.22407544\n",
            "Iteration 10, loss = 0.21985002\n",
            "Iteration 11, loss = 0.21721655\n",
            "Iteration 12, loss = 0.21417567\n",
            "Iteration 13, loss = 0.21379897\n",
            "Iteration 14, loss = 0.21168997\n",
            "Iteration 15, loss = 0.21104153\n",
            "Iteration 16, loss = 0.21031970\n",
            "Iteration 17, loss = 0.20879645\n",
            "Iteration 18, loss = 0.20995455\n",
            "Iteration 19, loss = 0.20802953\n",
            "Iteration 20, loss = 0.20848414\n",
            "Iteration 21, loss = 0.20712531\n",
            "Iteration 22, loss = 0.20698917\n",
            "Iteration 23, loss = 0.20670072\n",
            "Iteration 24, loss = 0.20794614\n",
            "Iteration 25, loss = 0.20741127\n",
            "Iteration 26, loss = 0.20745615\n",
            "Iteration 27, loss = 0.20811906\n",
            "Iteration 28, loss = 0.20724929\n",
            "Iteration 29, loss = 0.20812213\n",
            "Iteration 30, loss = 0.20715581\n",
            "Iteration 31, loss = 0.20811683\n",
            "Iteration 32, loss = 0.20711482\n",
            "Iteration 33, loss = 0.20747866\n",
            "Iteration 34, loss = 0.20736293\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.20027438\n",
            "Iteration 36, loss = 0.19893580\n",
            "Iteration 37, loss = 0.19825346\n",
            "Iteration 38, loss = 0.19745323\n",
            "Iteration 39, loss = 0.19696138\n",
            "Iteration 40, loss = 0.19649716\n",
            "Iteration 41, loss = 0.19547281\n",
            "Iteration 42, loss = 0.19502995\n",
            "Iteration 43, loss = 0.19422732\n",
            "Iteration 44, loss = 0.19387508\n",
            "Iteration 45, loss = 0.19339312\n",
            "Iteration 46, loss = 0.19309280\n",
            "Iteration 47, loss = 0.19226391\n",
            "Iteration 48, loss = 0.19158101\n",
            "Iteration 49, loss = 0.19128563\n",
            "Iteration 50, loss = 0.19132182\n",
            "Iteration 51, loss = 0.19063429\n",
            "Iteration 52, loss = 0.18978050\n",
            "Iteration 53, loss = 0.19009802\n",
            "Iteration 54, loss = 0.18900275\n",
            "Iteration 55, loss = 0.18876625\n",
            "Iteration 56, loss = 0.18886610\n",
            "Iteration 57, loss = 0.18851446\n",
            "Iteration 58, loss = 0.18805241\n",
            "Iteration 59, loss = 0.18830521\n",
            "Iteration 60, loss = 0.18688236\n",
            "Iteration 61, loss = 0.18832124\n",
            "Iteration 62, loss = 0.18667762\n",
            "Iteration 63, loss = 0.18742279\n",
            "Iteration 64, loss = 0.18678447\n",
            "Iteration 65, loss = 0.18585159\n",
            "Iteration 66, loss = 0.18806299\n",
            "Iteration 67, loss = 0.18810207\n",
            "Iteration 68, loss = 0.18728520\n",
            "Iteration 69, loss = 0.18671893\n",
            "Iteration 70, loss = 0.18584187\n",
            "Iteration 71, loss = 0.18742219\n",
            "Iteration 72, loss = 0.18770353\n",
            "Iteration 73, loss = 0.18700443\n",
            "Iteration 74, loss = 0.18727069\n",
            "Iteration 75, loss = 0.18580605\n",
            "Iteration 76, loss = 0.18692591\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 77, loss = 0.17736384\n",
            "Iteration 78, loss = 0.17704223\n",
            "Iteration 79, loss = 0.17677739\n",
            "Iteration 80, loss = 0.17702975\n",
            "Iteration 81, loss = 0.17653974\n",
            "Iteration 82, loss = 0.17637237\n",
            "Iteration 83, loss = 0.17637570\n",
            "Iteration 84, loss = 0.17622544\n",
            "Iteration 85, loss = 0.17597757\n",
            "Iteration 86, loss = 0.17605468\n",
            "Iteration 87, loss = 0.17585125\n",
            "Iteration 88, loss = 0.17586029\n",
            "Iteration 89, loss = 0.17551083\n",
            "Iteration 90, loss = 0.17548819\n",
            "Iteration 91, loss = 0.17552267\n",
            "Iteration 92, loss = 0.17524482\n",
            "Iteration 93, loss = 0.17514857\n",
            "Iteration 94, loss = 0.17480147\n",
            "Iteration 95, loss = 0.17481166\n",
            "Iteration 96, loss = 0.17484799\n",
            "Iteration 97, loss = 0.17468920\n",
            "Iteration 98, loss = 0.17457875\n",
            "Iteration 99, loss = 0.17428323\n",
            "Iteration 100, loss = 0.17416708\n",
            "Iteration 101, loss = 0.17421608\n",
            "Iteration 102, loss = 0.17413548\n",
            "Iteration 103, loss = 0.17417832\n",
            "Iteration 104, loss = 0.17387905\n",
            "Iteration 105, loss = 0.17377929\n",
            "Iteration 106, loss = 0.17377753\n",
            "Iteration 107, loss = 0.17367661\n",
            "Iteration 108, loss = 0.17330478\n",
            "Iteration 109, loss = 0.17331847\n",
            "Iteration 110, loss = 0.17328726\n",
            "Iteration 111, loss = 0.17321480\n",
            "Iteration 112, loss = 0.17314324\n",
            "Iteration 113, loss = 0.17315562\n",
            "Iteration 114, loss = 0.17296051\n",
            "Iteration 115, loss = 0.17261630\n",
            "Iteration 116, loss = 0.17277979\n",
            "Iteration 117, loss = 0.17245401\n",
            "Iteration 118, loss = 0.17246935\n",
            "Iteration 119, loss = 0.17231122\n",
            "Iteration 120, loss = 0.17227969\n",
            "Iteration 121, loss = 0.17203401\n",
            "Iteration 122, loss = 0.17202500\n",
            "Iteration 123, loss = 0.17198326\n",
            "Iteration 124, loss = 0.17173756\n",
            "Iteration 125, loss = 0.17179406\n",
            "Iteration 126, loss = 0.17189287\n",
            "Iteration 127, loss = 0.17183979\n",
            "Iteration 128, loss = 0.17160866\n",
            "Iteration 129, loss = 0.17158156\n",
            "Iteration 130, loss = 0.17139075\n",
            "Iteration 131, loss = 0.17132302\n",
            "Iteration 132, loss = 0.17131221\n",
            "Iteration 133, loss = 0.17112652\n",
            "Iteration 134, loss = 0.17112176\n",
            "Iteration 135, loss = 0.17099676\n",
            "Iteration 136, loss = 0.17081247\n",
            "Iteration 137, loss = 0.17093201\n",
            "Iteration 138, loss = 0.17058792\n",
            "Iteration 139, loss = 0.17079686\n",
            "Iteration 140, loss = 0.17059640\n",
            "Iteration 141, loss = 0.17058504\n",
            "Iteration 142, loss = 0.17052448\n",
            "Iteration 143, loss = 0.17045925\n",
            "Iteration 144, loss = 0.17029723\n",
            "Iteration 145, loss = 0.17024285\n",
            "Iteration 146, loss = 0.17025812\n",
            "Iteration 147, loss = 0.17020835\n",
            "Iteration 148, loss = 0.17022042\n",
            "Iteration 149, loss = 0.16980273\n",
            "Iteration 150, loss = 0.16980136\n",
            "Iteration 151, loss = 0.16985158\n",
            "Iteration 152, loss = 0.16978215\n",
            "Iteration 153, loss = 0.16976288\n",
            "Iteration 154, loss = 0.16962863\n",
            "Iteration 155, loss = 0.16974023\n",
            "Iteration 156, loss = 0.16948865\n",
            "Iteration 157, loss = 0.16941805\n",
            "Iteration 158, loss = 0.16940536\n",
            "Iteration 159, loss = 0.16935189\n",
            "Iteration 160, loss = 0.16929823\n",
            "Iteration 161, loss = 0.16919047\n",
            "Iteration 162, loss = 0.16910701\n",
            "Iteration 163, loss = 0.16907753\n",
            "Iteration 164, loss = 0.16908307\n",
            "Iteration 165, loss = 0.16926894\n",
            "Iteration 166, loss = 0.16885815\n",
            "Iteration 167, loss = 0.16883258\n",
            "Iteration 168, loss = 0.16884804\n",
            "Iteration 169, loss = 0.16878474\n",
            "Iteration 170, loss = 0.16859040\n",
            "Iteration 171, loss = 0.16871253\n",
            "Iteration 172, loss = 0.16872613\n",
            "Iteration 173, loss = 0.16849991\n",
            "Iteration 174, loss = 0.16841284\n",
            "Iteration 175, loss = 0.16844788\n",
            "Iteration 176, loss = 0.16847027\n",
            "Iteration 177, loss = 0.16846230\n",
            "Iteration 178, loss = 0.16818927\n",
            "Iteration 179, loss = 0.16824585\n",
            "Iteration 180, loss = 0.16829607\n",
            "Iteration 181, loss = 0.16832998\n",
            "Iteration 182, loss = 0.16795509\n",
            "Iteration 183, loss = 0.16803028\n",
            "Iteration 184, loss = 0.16784873\n",
            "Iteration 185, loss = 0.16796503\n",
            "Iteration 186, loss = 0.16792717\n",
            "Iteration 187, loss = 0.16768213\n",
            "Iteration 188, loss = 0.16767861\n",
            "Iteration 189, loss = 0.16791947\n",
            "Iteration 190, loss = 0.16765913\n",
            "Iteration 191, loss = 0.16778382\n",
            "Iteration 192, loss = 0.16775520\n",
            "Iteration 193, loss = 0.16757363\n",
            "Iteration 194, loss = 0.16753098\n",
            "Iteration 195, loss = 0.16748735\n",
            "Iteration 196, loss = 0.16741797\n",
            "Iteration 197, loss = 0.16745159\n",
            "Iteration 198, loss = 0.16736086\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 199, loss = 0.16644561\n",
            "Iteration 200, loss = 0.16640894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32433596\n",
            "Iteration 3, loss = 0.29572430\n",
            "Iteration 4, loss = 0.27419031\n",
            "Iteration 5, loss = 0.25782158\n",
            "Iteration 6, loss = 0.24573250\n",
            "Iteration 7, loss = 0.23638321\n",
            "Iteration 8, loss = 0.22819305\n",
            "Iteration 9, loss = 0.22259664\n",
            "Iteration 10, loss = 0.21962033\n",
            "Iteration 11, loss = 0.21613142\n",
            "Iteration 12, loss = 0.21395381\n",
            "Iteration 13, loss = 0.21161325\n",
            "Iteration 14, loss = 0.21077076\n",
            "Iteration 15, loss = 0.21014264\n",
            "Iteration 16, loss = 0.20878372\n",
            "Iteration 17, loss = 0.20874013\n",
            "Iteration 18, loss = 0.20844601\n",
            "Iteration 19, loss = 0.20793954\n",
            "Iteration 20, loss = 0.20789416\n",
            "Iteration 21, loss = 0.20735884\n",
            "Iteration 22, loss = 0.20622758\n",
            "Iteration 23, loss = 0.20718183\n",
            "Iteration 24, loss = 0.20719919\n",
            "Iteration 25, loss = 0.20787331\n",
            "Iteration 26, loss = 0.20699544\n",
            "Iteration 27, loss = 0.20696983\n",
            "Iteration 28, loss = 0.20764121\n",
            "Iteration 29, loss = 0.20649433\n",
            "Iteration 30, loss = 0.20623310\n",
            "Iteration 31, loss = 0.20756324\n",
            "Iteration 32, loss = 0.20605088\n",
            "Iteration 33, loss = 0.20632123\n",
            "Iteration 34, loss = 0.20683953\n",
            "Iteration 35, loss = 0.20664987\n",
            "Iteration 36, loss = 0.20865094\n",
            "Iteration 37, loss = 0.20608473\n",
            "Iteration 38, loss = 0.20696767\n",
            "Iteration 39, loss = 0.20790807\n",
            "Iteration 40, loss = 0.20653114\n",
            "Iteration 41, loss = 0.20700447\n",
            "Iteration 42, loss = 0.20654562\n",
            "Iteration 43, loss = 0.20700506\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.19841568\n",
            "Iteration 45, loss = 0.19704622\n",
            "Iteration 46, loss = 0.19669193\n",
            "Iteration 47, loss = 0.19591547\n",
            "Iteration 48, loss = 0.19503451\n",
            "Iteration 49, loss = 0.19449190\n",
            "Iteration 50, loss = 0.19396018\n",
            "Iteration 51, loss = 0.19344002\n",
            "Iteration 52, loss = 0.19269778\n",
            "Iteration 53, loss = 0.19164102\n",
            "Iteration 54, loss = 0.19153387\n",
            "Iteration 55, loss = 0.19062427\n",
            "Iteration 56, loss = 0.19028236\n",
            "Iteration 57, loss = 0.19019602\n",
            "Iteration 58, loss = 0.18960485\n",
            "Iteration 59, loss = 0.18911930\n",
            "Iteration 60, loss = 0.18922015\n",
            "Iteration 61, loss = 0.18910152\n",
            "Iteration 62, loss = 0.18747703\n",
            "Iteration 63, loss = 0.18817184\n",
            "Iteration 64, loss = 0.18781334\n",
            "Iteration 65, loss = 0.18720794\n",
            "Iteration 66, loss = 0.18659725\n",
            "Iteration 67, loss = 0.18631054\n",
            "Iteration 68, loss = 0.18644115\n",
            "Iteration 69, loss = 0.18610271\n",
            "Iteration 70, loss = 0.18698202\n",
            "Iteration 71, loss = 0.18667173\n",
            "Iteration 72, loss = 0.18588260\n",
            "Iteration 73, loss = 0.18705736\n",
            "Iteration 74, loss = 0.18649364\n",
            "Iteration 75, loss = 0.18800982\n",
            "Iteration 76, loss = 0.18577377\n",
            "Iteration 77, loss = 0.18791977\n",
            "Iteration 78, loss = 0.18545135\n",
            "Iteration 79, loss = 0.18581116\n",
            "Iteration 80, loss = 0.18747267\n",
            "Iteration 81, loss = 0.18522215\n",
            "Iteration 82, loss = 0.18619187\n",
            "Iteration 83, loss = 0.18582863\n",
            "Iteration 84, loss = 0.18622884\n",
            "Iteration 85, loss = 0.18609593\n",
            "Iteration 86, loss = 0.18752445\n",
            "Iteration 87, loss = 0.18707184\n",
            "Iteration 88, loss = 0.18837525\n",
            "Iteration 89, loss = 0.18657447\n",
            "Iteration 90, loss = 0.18960273\n",
            "Iteration 91, loss = 0.18838934\n",
            "Iteration 92, loss = 0.18605555\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 93, loss = 0.17494089\n",
            "Iteration 94, loss = 0.17495842\n",
            "Iteration 95, loss = 0.17471971\n",
            "Iteration 96, loss = 0.17449566\n",
            "Iteration 97, loss = 0.17424856\n",
            "Iteration 98, loss = 0.17438947\n",
            "Iteration 99, loss = 0.17431475\n",
            "Iteration 100, loss = 0.17413661\n",
            "Iteration 101, loss = 0.17402026\n",
            "Iteration 102, loss = 0.17357295\n",
            "Iteration 103, loss = 0.17367177\n",
            "Iteration 104, loss = 0.17356094\n",
            "Iteration 105, loss = 0.17360641\n",
            "Iteration 106, loss = 0.17333597\n",
            "Iteration 107, loss = 0.17345448\n",
            "Iteration 108, loss = 0.17327345\n",
            "Iteration 109, loss = 0.17302847\n",
            "Iteration 110, loss = 0.17289740\n",
            "Iteration 111, loss = 0.17291747\n",
            "Iteration 112, loss = 0.17297967\n",
            "Iteration 113, loss = 0.17248766\n",
            "Iteration 114, loss = 0.17244746\n",
            "Iteration 115, loss = 0.17242839\n",
            "Iteration 116, loss = 0.17260379\n",
            "Iteration 117, loss = 0.17232775\n",
            "Iteration 118, loss = 0.17219877\n",
            "Iteration 119, loss = 0.17217263\n",
            "Iteration 120, loss = 0.17204977\n",
            "Iteration 121, loss = 0.17179713\n",
            "Iteration 122, loss = 0.17158014\n",
            "Iteration 123, loss = 0.17189048\n",
            "Iteration 124, loss = 0.17160432\n",
            "Iteration 125, loss = 0.17153173\n",
            "Iteration 126, loss = 0.17136875\n",
            "Iteration 127, loss = 0.17121386\n",
            "Iteration 128, loss = 0.17149517\n",
            "Iteration 129, loss = 0.17125784\n",
            "Iteration 130, loss = 0.17093999\n",
            "Iteration 131, loss = 0.17112429\n",
            "Iteration 132, loss = 0.17078765\n",
            "Iteration 133, loss = 0.17075834\n",
            "Iteration 134, loss = 0.17081075\n",
            "Iteration 135, loss = 0.17077603\n",
            "Iteration 136, loss = 0.17049227\n",
            "Iteration 137, loss = 0.17066273\n",
            "Iteration 138, loss = 0.17064075\n",
            "Iteration 139, loss = 0.17050657\n",
            "Iteration 140, loss = 0.17029478\n",
            "Iteration 141, loss = 0.17013340\n",
            "Iteration 142, loss = 0.17035949\n",
            "Iteration 143, loss = 0.17015549\n",
            "Iteration 144, loss = 0.17001563\n",
            "Iteration 145, loss = 0.16988566\n",
            "Iteration 146, loss = 0.16992319\n",
            "Iteration 147, loss = 0.16996017\n",
            "Iteration 148, loss = 0.16960756\n",
            "Iteration 149, loss = 0.16953928\n",
            "Iteration 150, loss = 0.16954759\n",
            "Iteration 151, loss = 0.16945969\n",
            "Iteration 152, loss = 0.16944293\n",
            "Iteration 153, loss = 0.16923935\n",
            "Iteration 154, loss = 0.16929408\n",
            "Iteration 155, loss = 0.16916705\n",
            "Iteration 156, loss = 0.16909608\n",
            "Iteration 157, loss = 0.16926758\n",
            "Iteration 158, loss = 0.16912540\n",
            "Iteration 159, loss = 0.16895100\n",
            "Iteration 160, loss = 0.16903921\n",
            "Iteration 161, loss = 0.16906006\n",
            "Iteration 162, loss = 0.16883941\n",
            "Iteration 163, loss = 0.16885791\n",
            "Iteration 164, loss = 0.16869520\n",
            "Iteration 165, loss = 0.16874396\n",
            "Iteration 166, loss = 0.16883622\n",
            "Iteration 167, loss = 0.16869181\n",
            "Iteration 168, loss = 0.16868224\n",
            "Iteration 169, loss = 0.16850008\n",
            "Iteration 170, loss = 0.16843497\n",
            "Iteration 171, loss = 0.16860177\n",
            "Iteration 172, loss = 0.16813363\n",
            "Iteration 173, loss = 0.16834716\n",
            "Iteration 174, loss = 0.16825257\n",
            "Iteration 175, loss = 0.16838871\n",
            "Iteration 176, loss = 0.16809852\n",
            "Iteration 177, loss = 0.16815306\n",
            "Iteration 178, loss = 0.16800221\n",
            "Iteration 179, loss = 0.16804169\n",
            "Iteration 180, loss = 0.16795422\n",
            "Iteration 181, loss = 0.16795906\n",
            "Iteration 182, loss = 0.16803641\n",
            "Iteration 183, loss = 0.16769835\n",
            "Iteration 184, loss = 0.16762583\n",
            "Iteration 185, loss = 0.16775970\n",
            "Iteration 186, loss = 0.16767847\n",
            "Iteration 187, loss = 0.16766360\n",
            "Iteration 188, loss = 0.16765006\n",
            "Iteration 189, loss = 0.16757988\n",
            "Iteration 190, loss = 0.16772979\n",
            "Iteration 191, loss = 0.16755510\n",
            "Iteration 192, loss = 0.16723653\n",
            "Iteration 193, loss = 0.16741067\n",
            "Iteration 194, loss = 0.16736381\n",
            "Iteration 195, loss = 0.16748422\n",
            "Iteration 196, loss = 0.16729106\n",
            "Iteration 197, loss = 0.16721429\n",
            "Iteration 198, loss = 0.16732131\n",
            "Iteration 199, loss = 0.16714543\n",
            "Iteration 200, loss = 0.16729103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.68455243\n",
            "Iteration 2, loss = 0.29857793\n",
            "Iteration 3, loss = 0.27566037\n",
            "Iteration 4, loss = 0.25893496\n",
            "Iteration 5, loss = 0.24625350\n",
            "Iteration 6, loss = 0.23704884\n",
            "Iteration 7, loss = 0.22944184\n",
            "Iteration 8, loss = 0.22335072\n",
            "Iteration 9, loss = 0.21927378\n",
            "Iteration 10, loss = 0.21618757\n",
            "Iteration 11, loss = 0.21416417\n",
            "Iteration 12, loss = 0.21220214\n",
            "Iteration 13, loss = 0.21118698\n",
            "Iteration 14, loss = 0.20902974\n",
            "Iteration 15, loss = 0.20898614\n",
            "Iteration 16, loss = 0.20936718\n",
            "Iteration 17, loss = 0.20750896\n",
            "Iteration 18, loss = 0.20751320\n",
            "Iteration 19, loss = 0.20772740\n",
            "Iteration 20, loss = 0.20717223\n",
            "Iteration 21, loss = 0.20669528\n",
            "Iteration 22, loss = 0.20679420\n",
            "Iteration 23, loss = 0.20693108\n",
            "Iteration 24, loss = 0.20497369\n",
            "Iteration 25, loss = 0.20607941\n",
            "Iteration 26, loss = 0.20753996\n",
            "Iteration 27, loss = 0.20634017\n",
            "Iteration 28, loss = 0.20641979\n",
            "Iteration 29, loss = 0.20683588\n",
            "Iteration 30, loss = 0.20709939\n",
            "Iteration 31, loss = 0.20653342\n",
            "Iteration 32, loss = 0.20892530\n",
            "Iteration 33, loss = 0.20699336\n",
            "Iteration 34, loss = 0.20674390\n",
            "Iteration 35, loss = 0.20696228\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.19922215\n",
            "Iteration 37, loss = 0.19813684\n",
            "Iteration 38, loss = 0.19724227\n",
            "Iteration 39, loss = 0.19652519\n",
            "Iteration 40, loss = 0.19570970\n",
            "Iteration 41, loss = 0.19539238\n",
            "Iteration 42, loss = 0.19441257\n",
            "Iteration 43, loss = 0.19381392\n",
            "Iteration 44, loss = 0.19361029\n",
            "Iteration 45, loss = 0.19276125\n",
            "Iteration 46, loss = 0.19209299\n",
            "Iteration 47, loss = 0.19170323\n",
            "Iteration 48, loss = 0.19144595\n",
            "Iteration 49, loss = 0.19109482\n",
            "Iteration 50, loss = 0.19047433\n",
            "Iteration 51, loss = 0.18960944\n",
            "Iteration 52, loss = 0.18918135\n",
            "Iteration 53, loss = 0.18944826\n",
            "Iteration 54, loss = 0.18859923\n",
            "Iteration 55, loss = 0.18825294\n",
            "Iteration 56, loss = 0.18764151\n",
            "Iteration 57, loss = 0.18763476\n",
            "Iteration 58, loss = 0.18747162\n",
            "Iteration 59, loss = 0.18732134\n",
            "Iteration 60, loss = 0.18581245\n",
            "Iteration 61, loss = 0.18682070\n",
            "Iteration 62, loss = 0.18602742\n",
            "Iteration 63, loss = 0.18548821\n",
            "Iteration 64, loss = 0.18693080\n",
            "Iteration 65, loss = 0.18831842\n",
            "Iteration 66, loss = 0.18646950\n",
            "Iteration 67, loss = 0.18553433\n",
            "Iteration 68, loss = 0.18611246\n",
            "Iteration 69, loss = 0.18609676\n",
            "Iteration 70, loss = 0.18677159\n",
            "Iteration 71, loss = 0.18485457\n",
            "Iteration 72, loss = 0.18670843\n",
            "Iteration 73, loss = 0.18620317\n",
            "Iteration 74, loss = 0.18779967\n",
            "Iteration 75, loss = 0.18590522\n",
            "Iteration 76, loss = 0.18684878\n",
            "Iteration 77, loss = 0.18713399\n",
            "Iteration 78, loss = 0.19033843\n",
            "Iteration 79, loss = 0.18579579\n",
            "Iteration 80, loss = 0.18440352\n",
            "Iteration 81, loss = 0.19293757\n",
            "Iteration 82, loss = 0.18679929\n",
            "Iteration 83, loss = 0.18822182\n",
            "Iteration 84, loss = 0.18691236\n",
            "Iteration 85, loss = 0.18653150\n",
            "Iteration 86, loss = 0.18566039\n",
            "Iteration 87, loss = 0.18501962\n",
            "Iteration 88, loss = 0.18568449\n",
            "Iteration 89, loss = 0.18932512\n",
            "Iteration 90, loss = 0.18663333\n",
            "Iteration 91, loss = 0.19018624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 92, loss = 0.17402211\n",
            "Iteration 93, loss = 0.17379063\n",
            "Iteration 94, loss = 0.17351565\n",
            "Iteration 95, loss = 0.17336021\n",
            "Iteration 96, loss = 0.17312199\n",
            "Iteration 97, loss = 0.17306427\n",
            "Iteration 98, loss = 0.17300622\n",
            "Iteration 99, loss = 0.17277716\n",
            "Iteration 100, loss = 0.17261990\n",
            "Iteration 101, loss = 0.17250669\n",
            "Iteration 102, loss = 0.17236439\n",
            "Iteration 103, loss = 0.17239093\n",
            "Iteration 104, loss = 0.17245478\n",
            "Iteration 105, loss = 0.17212887\n",
            "Iteration 106, loss = 0.17201031\n",
            "Iteration 107, loss = 0.17196082\n",
            "Iteration 108, loss = 0.17188395\n",
            "Iteration 109, loss = 0.17164379\n",
            "Iteration 110, loss = 0.17147521\n",
            "Iteration 111, loss = 0.17136099\n",
            "Iteration 112, loss = 0.17129716\n",
            "Iteration 113, loss = 0.17123790\n",
            "Iteration 114, loss = 0.17096294\n",
            "Iteration 115, loss = 0.17112056\n",
            "Iteration 116, loss = 0.17083279\n",
            "Iteration 117, loss = 0.17083329\n",
            "Iteration 118, loss = 0.17070792\n",
            "Iteration 119, loss = 0.17066668\n",
            "Iteration 120, loss = 0.17060978\n",
            "Iteration 121, loss = 0.17050709\n",
            "Iteration 122, loss = 0.17042497\n",
            "Iteration 123, loss = 0.17037544\n",
            "Iteration 124, loss = 0.17008913\n",
            "Iteration 125, loss = 0.17021947\n",
            "Iteration 126, loss = 0.17008094\n",
            "Iteration 127, loss = 0.16988097\n",
            "Iteration 128, loss = 0.17004428\n",
            "Iteration 129, loss = 0.16958873\n",
            "Iteration 130, loss = 0.16961597\n",
            "Iteration 131, loss = 0.16962548\n",
            "Iteration 132, loss = 0.16948130\n",
            "Iteration 133, loss = 0.16935618\n",
            "Iteration 134, loss = 0.16946163\n",
            "Iteration 135, loss = 0.16932846\n",
            "Iteration 136, loss = 0.16913605\n",
            "Iteration 137, loss = 0.16917423\n",
            "Iteration 138, loss = 0.16900643\n",
            "Iteration 139, loss = 0.16893078\n",
            "Iteration 140, loss = 0.16890822\n",
            "Iteration 141, loss = 0.16907085\n",
            "Iteration 142, loss = 0.16883524\n",
            "Iteration 143, loss = 0.16853995\n",
            "Iteration 144, loss = 0.16875360\n",
            "Iteration 145, loss = 0.16875172\n",
            "Iteration 146, loss = 0.16862170\n",
            "Iteration 147, loss = 0.16828795\n",
            "Iteration 148, loss = 0.16839687\n",
            "Iteration 149, loss = 0.16825850\n",
            "Iteration 150, loss = 0.16827788\n",
            "Iteration 151, loss = 0.16797892\n",
            "Iteration 152, loss = 0.16801329\n",
            "Iteration 153, loss = 0.16802989\n",
            "Iteration 154, loss = 0.16788761\n",
            "Iteration 155, loss = 0.16792598\n",
            "Iteration 156, loss = 0.16780162\n",
            "Iteration 157, loss = 0.16763790\n",
            "Iteration 158, loss = 0.16784331\n",
            "Iteration 159, loss = 0.16768214\n",
            "Iteration 160, loss = 0.16757115\n",
            "Iteration 161, loss = 0.16751998\n",
            "Iteration 162, loss = 0.16731011\n",
            "Iteration 163, loss = 0.16729104\n",
            "Iteration 164, loss = 0.16727926\n",
            "Iteration 165, loss = 0.16728657\n",
            "Iteration 166, loss = 0.16712512\n",
            "Iteration 167, loss = 0.16711299\n",
            "Iteration 168, loss = 0.16697595\n",
            "Iteration 169, loss = 0.16727478\n",
            "Iteration 170, loss = 0.16702428\n",
            "Iteration 171, loss = 0.16705093\n",
            "Iteration 172, loss = 0.16698232\n",
            "Iteration 173, loss = 0.16711090\n",
            "Iteration 174, loss = 0.16713487\n",
            "Iteration 175, loss = 0.16696752\n",
            "Iteration 176, loss = 0.16667094\n",
            "Iteration 177, loss = 0.16680037\n",
            "Iteration 178, loss = 0.16664120\n",
            "Iteration 179, loss = 0.16672733\n",
            "Iteration 180, loss = 0.16657904\n",
            "Iteration 181, loss = 0.16653273\n",
            "Iteration 182, loss = 0.16653982\n",
            "Iteration 183, loss = 0.16657692\n",
            "Iteration 184, loss = 0.16651857\n",
            "Iteration 185, loss = 0.16641019\n",
            "Iteration 186, loss = 0.16623788\n",
            "Iteration 187, loss = 0.16639517\n",
            "Iteration 188, loss = 0.16626798\n",
            "Iteration 189, loss = 0.16642301\n",
            "Iteration 190, loss = 0.16636717\n",
            "Iteration 191, loss = 0.16627600\n",
            "Iteration 192, loss = 0.16618411\n",
            "Iteration 193, loss = 0.16618487\n",
            "Iteration 194, loss = 0.16604350\n",
            "Iteration 195, loss = 0.16636127\n",
            "Iteration 196, loss = 0.16609067\n",
            "Iteration 197, loss = 0.16606718\n",
            "Iteration 198, loss = 0.16608612\n",
            "Iteration 199, loss = 0.16587596\n",
            "Iteration 200, loss = 0.16610861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.33015575\n",
            "Iteration 3, loss = 0.30026108\n",
            "Iteration 4, loss = 0.27754803\n",
            "Iteration 5, loss = 0.26054594\n",
            "Iteration 6, loss = 0.24721900\n",
            "Iteration 7, loss = 0.23736818\n",
            "Iteration 8, loss = 0.23155451\n",
            "Iteration 9, loss = 0.22416720\n",
            "Iteration 10, loss = 0.22054031\n",
            "Iteration 11, loss = 0.21708437\n",
            "Iteration 12, loss = 0.21547525\n",
            "Iteration 13, loss = 0.21176972\n",
            "Iteration 14, loss = 0.21150252\n",
            "Iteration 15, loss = 0.21131346\n",
            "Iteration 16, loss = 0.20892669\n",
            "Iteration 17, loss = 0.20919756\n",
            "Iteration 18, loss = 0.20784678\n",
            "Iteration 19, loss = 0.20846190\n",
            "Iteration 20, loss = 0.20853682\n",
            "Iteration 21, loss = 0.20763700\n",
            "Iteration 22, loss = 0.20752687\n",
            "Iteration 23, loss = 0.20566246\n",
            "Iteration 24, loss = 0.20822125\n",
            "Iteration 25, loss = 0.20783754\n",
            "Iteration 26, loss = 0.20723394\n",
            "Iteration 27, loss = 0.20808101\n",
            "Iteration 28, loss = 0.20726854\n",
            "Iteration 29, loss = 0.20680845\n",
            "Iteration 30, loss = 0.20801870\n",
            "Iteration 31, loss = 0.20784386\n",
            "Iteration 32, loss = 0.20727654\n",
            "Iteration 33, loss = 0.20734829\n",
            "Iteration 34, loss = 0.20703358\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.19929499\n",
            "Iteration 36, loss = 0.19806301\n",
            "Iteration 37, loss = 0.19759439\n",
            "Iteration 38, loss = 0.19666421\n",
            "Iteration 39, loss = 0.19600489\n",
            "Iteration 40, loss = 0.19537625\n",
            "Iteration 41, loss = 0.19500649\n",
            "Iteration 42, loss = 0.19416819\n",
            "Iteration 43, loss = 0.19347533\n",
            "Iteration 44, loss = 0.19319941\n",
            "Iteration 45, loss = 0.19226947\n",
            "Iteration 46, loss = 0.19183383\n",
            "Iteration 47, loss = 0.19144618\n",
            "Iteration 48, loss = 0.19110590\n",
            "Iteration 49, loss = 0.19054258\n",
            "Iteration 50, loss = 0.19048280\n",
            "Iteration 51, loss = 0.18968505\n",
            "Iteration 52, loss = 0.18900885\n",
            "Iteration 53, loss = 0.18863051\n",
            "Iteration 54, loss = 0.18842834\n",
            "Iteration 55, loss = 0.18844292\n",
            "Iteration 56, loss = 0.18753739\n",
            "Iteration 57, loss = 0.18735017\n",
            "Iteration 58, loss = 0.18806564\n",
            "Iteration 59, loss = 0.18785289\n",
            "Iteration 60, loss = 0.18677539\n",
            "Iteration 61, loss = 0.18600884\n",
            "Iteration 62, loss = 0.18694753\n",
            "Iteration 63, loss = 0.18717758\n",
            "Iteration 64, loss = 0.18524440\n",
            "Iteration 65, loss = 0.18635934\n",
            "Iteration 66, loss = 0.18660532\n",
            "Iteration 67, loss = 0.18743645\n",
            "Iteration 68, loss = 0.18586542\n",
            "Iteration 69, loss = 0.18699631\n",
            "Iteration 70, loss = 0.18900986\n",
            "Iteration 71, loss = 0.18713669\n",
            "Iteration 72, loss = 0.18738846\n",
            "Iteration 73, loss = 0.18742628\n",
            "Iteration 74, loss = 0.18870676\n",
            "Iteration 75, loss = 0.18851246\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.17687329\n",
            "Iteration 77, loss = 0.17659532\n",
            "Iteration 78, loss = 0.17625601\n",
            "Iteration 79, loss = 0.17648102\n",
            "Iteration 80, loss = 0.17621342\n",
            "Iteration 81, loss = 0.17576680\n",
            "Iteration 82, loss = 0.17574083\n",
            "Iteration 83, loss = 0.17577544\n",
            "Iteration 84, loss = 0.17558797\n",
            "Iteration 85, loss = 0.17557101\n",
            "Iteration 86, loss = 0.17552019\n",
            "Iteration 87, loss = 0.17519695\n",
            "Iteration 88, loss = 0.17505551\n",
            "Iteration 89, loss = 0.17482639\n",
            "Iteration 90, loss = 0.17475851\n",
            "Iteration 91, loss = 0.17451587\n",
            "Iteration 92, loss = 0.17457485\n",
            "Iteration 93, loss = 0.17449178\n",
            "Iteration 94, loss = 0.17443577\n",
            "Iteration 95, loss = 0.17428801\n",
            "Iteration 96, loss = 0.17390016\n",
            "Iteration 97, loss = 0.17408960\n",
            "Iteration 98, loss = 0.17366666\n",
            "Iteration 99, loss = 0.17364194\n",
            "Iteration 100, loss = 0.17370687\n",
            "Iteration 101, loss = 0.17348168\n",
            "Iteration 102, loss = 0.17349713\n",
            "Iteration 103, loss = 0.17326133\n",
            "Iteration 104, loss = 0.17313542\n",
            "Iteration 105, loss = 0.17301552\n",
            "Iteration 106, loss = 0.17311410\n",
            "Iteration 107, loss = 0.17282420\n",
            "Iteration 108, loss = 0.17269566\n",
            "Iteration 109, loss = 0.17253149\n",
            "Iteration 110, loss = 0.17256968\n",
            "Iteration 111, loss = 0.17253602\n",
            "Iteration 112, loss = 0.17229704\n",
            "Iteration 113, loss = 0.17230067\n",
            "Iteration 114, loss = 0.17225711\n",
            "Iteration 115, loss = 0.17203347\n",
            "Iteration 116, loss = 0.17203876\n",
            "Iteration 117, loss = 0.17214316\n",
            "Iteration 118, loss = 0.17194448\n",
            "Iteration 119, loss = 0.17182321\n",
            "Iteration 120, loss = 0.17154678\n",
            "Iteration 121, loss = 0.17143377\n",
            "Iteration 122, loss = 0.17132324\n",
            "Iteration 123, loss = 0.17138325\n",
            "Iteration 124, loss = 0.17099901\n",
            "Iteration 125, loss = 0.17125214\n",
            "Iteration 126, loss = 0.17118228\n",
            "Iteration 127, loss = 0.17100208\n",
            "Iteration 128, loss = 0.17088140\n",
            "Iteration 129, loss = 0.17084685\n",
            "Iteration 130, loss = 0.17052887\n",
            "Iteration 131, loss = 0.17062054\n",
            "Iteration 132, loss = 0.17055830\n",
            "Iteration 133, loss = 0.17069081\n",
            "Iteration 134, loss = 0.17064417\n",
            "Iteration 135, loss = 0.17039086\n",
            "Iteration 136, loss = 0.17047309\n",
            "Iteration 137, loss = 0.17010710\n",
            "Iteration 138, loss = 0.17014547\n",
            "Iteration 139, loss = 0.16997483\n",
            "Iteration 140, loss = 0.16989941\n",
            "Iteration 141, loss = 0.17002907\n",
            "Iteration 142, loss = 0.16976726\n",
            "Iteration 143, loss = 0.16994898\n",
            "Iteration 144, loss = 0.16961926\n",
            "Iteration 145, loss = 0.16968357\n",
            "Iteration 146, loss = 0.16935042\n",
            "Iteration 147, loss = 0.16942065\n",
            "Iteration 148, loss = 0.16934721\n",
            "Iteration 149, loss = 0.16925915\n",
            "Iteration 150, loss = 0.16912910\n",
            "Iteration 151, loss = 0.16922220\n",
            "Iteration 152, loss = 0.16906594\n",
            "Iteration 153, loss = 0.16898912\n",
            "Iteration 154, loss = 0.16909009\n",
            "Iteration 155, loss = 0.16884815\n",
            "Iteration 156, loss = 0.16893860\n",
            "Iteration 157, loss = 0.16861942\n",
            "Iteration 158, loss = 0.16861703\n",
            "Iteration 159, loss = 0.16874240\n",
            "Iteration 160, loss = 0.16868654\n",
            "Iteration 161, loss = 0.16852337\n",
            "Iteration 162, loss = 0.16836222\n",
            "Iteration 163, loss = 0.16853527\n",
            "Iteration 164, loss = 0.16856386\n",
            "Iteration 165, loss = 0.16855400\n",
            "Iteration 166, loss = 0.16841765\n",
            "Iteration 167, loss = 0.16823801\n",
            "Iteration 168, loss = 0.16808717\n",
            "Iteration 169, loss = 0.16809240\n",
            "Iteration 170, loss = 0.16800777\n",
            "Iteration 171, loss = 0.16826580\n",
            "Iteration 172, loss = 0.16793793\n",
            "Iteration 173, loss = 0.16787830\n",
            "Iteration 174, loss = 0.16760694\n",
            "Iteration 175, loss = 0.16785761\n",
            "Iteration 176, loss = 0.16766685\n",
            "Iteration 177, loss = 0.16756187\n",
            "Iteration 178, loss = 0.16765438\n",
            "Iteration 179, loss = 0.16749157\n",
            "Iteration 180, loss = 0.16754591\n",
            "Iteration 181, loss = 0.16769707\n",
            "Iteration 182, loss = 0.16761151\n",
            "Iteration 183, loss = 0.16713201\n",
            "Iteration 184, loss = 0.16735833\n",
            "Iteration 185, loss = 0.16721503\n",
            "Iteration 186, loss = 0.16732036\n",
            "Iteration 187, loss = 0.16701088\n",
            "Iteration 188, loss = 0.16710292\n",
            "Iteration 189, loss = 0.16710224\n",
            "Iteration 190, loss = 0.16700292\n",
            "Iteration 191, loss = 0.16692352\n",
            "Iteration 192, loss = 0.16677952\n",
            "Iteration 193, loss = 0.16690778\n",
            "Iteration 194, loss = 0.16668107\n",
            "Iteration 195, loss = 0.16668527\n",
            "Iteration 196, loss = 0.16673199\n",
            "Iteration 197, loss = 0.16670498\n",
            "Iteration 198, loss = 0.16686331\n",
            "Iteration 199, loss = 0.16671081\n",
            "Iteration 200, loss = 0.16666192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.65939938\n",
            "Iteration 2, loss = 0.30878015\n",
            "Iteration 3, loss = 0.28367372\n",
            "Iteration 4, loss = 0.26537543\n",
            "Iteration 5, loss = 0.25126882\n",
            "Iteration 6, loss = 0.24130161\n",
            "Iteration 7, loss = 0.23212565\n",
            "Iteration 8, loss = 0.22655698\n",
            "Iteration 9, loss = 0.22235849\n",
            "Iteration 10, loss = 0.21816912\n",
            "Iteration 11, loss = 0.21497858\n",
            "Iteration 12, loss = 0.21314972\n",
            "Iteration 13, loss = 0.21178523\n",
            "Iteration 14, loss = 0.21027094\n",
            "Iteration 15, loss = 0.20868405\n",
            "Iteration 16, loss = 0.20920550\n",
            "Iteration 17, loss = 0.20781056\n",
            "Iteration 18, loss = 0.20775916\n",
            "Iteration 19, loss = 0.20746402\n",
            "Iteration 20, loss = 0.20691588\n",
            "Iteration 21, loss = 0.20733132\n",
            "Iteration 22, loss = 0.20766853\n",
            "Iteration 23, loss = 0.20646181\n",
            "Iteration 24, loss = 0.20595104\n",
            "Iteration 25, loss = 0.20795647\n",
            "Iteration 26, loss = 0.20759731\n",
            "Iteration 27, loss = 0.20775723\n",
            "Iteration 28, loss = 0.20714833\n",
            "Iteration 29, loss = 0.20657756\n",
            "Iteration 30, loss = 0.20773553\n",
            "Iteration 31, loss = 0.20725774\n",
            "Iteration 32, loss = 0.20671245\n",
            "Iteration 33, loss = 0.20717543\n",
            "Iteration 34, loss = 0.20686792\n",
            "Iteration 35, loss = 0.20721137\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.19931402\n",
            "Iteration 37, loss = 0.19856824\n",
            "Iteration 38, loss = 0.19781383\n",
            "Iteration 39, loss = 0.19680399\n",
            "Iteration 40, loss = 0.19632767\n",
            "Iteration 41, loss = 0.19560911\n",
            "Iteration 42, loss = 0.19507345\n",
            "Iteration 43, loss = 0.19488302\n",
            "Iteration 44, loss = 0.19443700\n",
            "Iteration 45, loss = 0.19339912\n",
            "Iteration 46, loss = 0.19316710\n",
            "Iteration 47, loss = 0.19262565\n",
            "Iteration 48, loss = 0.19220740\n",
            "Iteration 49, loss = 0.19179030\n",
            "Iteration 50, loss = 0.19123943\n",
            "Iteration 51, loss = 0.19037021\n",
            "Iteration 52, loss = 0.19013344\n",
            "Iteration 53, loss = 0.19029281\n",
            "Iteration 54, loss = 0.18913507\n",
            "Iteration 55, loss = 0.18953509\n",
            "Iteration 56, loss = 0.18877905\n",
            "Iteration 57, loss = 0.18823344\n",
            "Iteration 58, loss = 0.18796426\n",
            "Iteration 59, loss = 0.18779031\n",
            "Iteration 60, loss = 0.18652690\n",
            "Iteration 61, loss = 0.18726064\n",
            "Iteration 62, loss = 0.18687913\n",
            "Iteration 63, loss = 0.18680662\n",
            "Iteration 64, loss = 0.18641415\n",
            "Iteration 65, loss = 0.18588683\n",
            "Iteration 66, loss = 0.18631188\n",
            "Iteration 67, loss = 0.18739152\n",
            "Iteration 68, loss = 0.18721685\n",
            "Iteration 69, loss = 0.18511384\n",
            "Iteration 70, loss = 0.18608570\n",
            "Iteration 71, loss = 0.18817283\n",
            "Iteration 72, loss = 0.18778972\n",
            "Iteration 73, loss = 0.18651410\n",
            "Iteration 74, loss = 0.18626072\n",
            "Iteration 75, loss = 0.18607165\n",
            "Iteration 76, loss = 0.18738889\n",
            "Iteration 77, loss = 0.18791414\n",
            "Iteration 78, loss = 0.18631146\n",
            "Iteration 79, loss = 0.18843871\n",
            "Iteration 80, loss = 0.18509676\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.17581819\n",
            "Iteration 82, loss = 0.17563127\n",
            "Iteration 83, loss = 0.17552492\n",
            "Iteration 84, loss = 0.17535094\n",
            "Iteration 85, loss = 0.17515821\n",
            "Iteration 86, loss = 0.17510642\n",
            "Iteration 87, loss = 0.17495008\n",
            "Iteration 88, loss = 0.17509429\n",
            "Iteration 89, loss = 0.17468790\n",
            "Iteration 90, loss = 0.17458920\n",
            "Iteration 91, loss = 0.17454775\n",
            "Iteration 92, loss = 0.17425988\n",
            "Iteration 93, loss = 0.17413557\n",
            "Iteration 94, loss = 0.17397742\n",
            "Iteration 95, loss = 0.17398977\n",
            "Iteration 96, loss = 0.17399844\n",
            "Iteration 97, loss = 0.17372114\n",
            "Iteration 98, loss = 0.17353582\n",
            "Iteration 99, loss = 0.17340730\n",
            "Iteration 100, loss = 0.17328834\n",
            "Iteration 101, loss = 0.17328255\n",
            "Iteration 102, loss = 0.17321199\n",
            "Iteration 103, loss = 0.17302547\n",
            "Iteration 104, loss = 0.17298565\n",
            "Iteration 105, loss = 0.17301801\n",
            "Iteration 106, loss = 0.17255166\n",
            "Iteration 107, loss = 0.17245389\n",
            "Iteration 108, loss = 0.17240808\n",
            "Iteration 109, loss = 0.17239410\n",
            "Iteration 110, loss = 0.17233719\n",
            "Iteration 111, loss = 0.17220394\n",
            "Iteration 112, loss = 0.17205124\n",
            "Iteration 113, loss = 0.17198995\n",
            "Iteration 114, loss = 0.17180268\n",
            "Iteration 115, loss = 0.17181119\n",
            "Iteration 116, loss = 0.17156950\n",
            "Iteration 117, loss = 0.17160386\n",
            "Iteration 118, loss = 0.17134410\n",
            "Iteration 119, loss = 0.17126604\n",
            "Iteration 120, loss = 0.17119212\n",
            "Iteration 121, loss = 0.17113550\n",
            "Iteration 122, loss = 0.17092623\n",
            "Iteration 123, loss = 0.17105279\n",
            "Iteration 124, loss = 0.17097000\n",
            "Iteration 125, loss = 0.17052913\n",
            "Iteration 126, loss = 0.17066431\n",
            "Iteration 127, loss = 0.17065829\n",
            "Iteration 128, loss = 0.17058845\n",
            "Iteration 129, loss = 0.17050778\n",
            "Iteration 130, loss = 0.17037578\n",
            "Iteration 131, loss = 0.17023912\n",
            "Iteration 132, loss = 0.17009212\n",
            "Iteration 133, loss = 0.17007181\n",
            "Iteration 134, loss = 0.16991964\n",
            "Iteration 135, loss = 0.17004532\n",
            "Iteration 136, loss = 0.16991995\n",
            "Iteration 137, loss = 0.16973077\n",
            "Iteration 138, loss = 0.16971397\n",
            "Iteration 139, loss = 0.16973408\n",
            "Iteration 140, loss = 0.16933434\n",
            "Iteration 141, loss = 0.16929221\n",
            "Iteration 142, loss = 0.16957205\n",
            "Iteration 143, loss = 0.16923149\n",
            "Iteration 144, loss = 0.16931436\n",
            "Iteration 145, loss = 0.16918985\n",
            "Iteration 146, loss = 0.16923172\n",
            "Iteration 147, loss = 0.16903153\n",
            "Iteration 148, loss = 0.16925642\n",
            "Iteration 149, loss = 0.16879116\n",
            "Iteration 150, loss = 0.16883318\n",
            "Iteration 151, loss = 0.16859651\n",
            "Iteration 152, loss = 0.16889050\n",
            "Iteration 153, loss = 0.16875694\n",
            "Iteration 154, loss = 0.16831726\n",
            "Iteration 155, loss = 0.16856034\n",
            "Iteration 156, loss = 0.16866041\n",
            "Iteration 157, loss = 0.16841015\n",
            "Iteration 158, loss = 0.16833362\n",
            "Iteration 159, loss = 0.16802900\n",
            "Iteration 160, loss = 0.16826974\n",
            "Iteration 161, loss = 0.16818055\n",
            "Iteration 162, loss = 0.16792591\n",
            "Iteration 163, loss = 0.16788358\n",
            "Iteration 164, loss = 0.16803302\n",
            "Iteration 165, loss = 0.16793672\n",
            "Iteration 166, loss = 0.16774718\n",
            "Iteration 167, loss = 0.16766616\n",
            "Iteration 168, loss = 0.16757524\n",
            "Iteration 169, loss = 0.16755907\n",
            "Iteration 170, loss = 0.16760960\n",
            "Iteration 171, loss = 0.16747737\n",
            "Iteration 172, loss = 0.16742836\n",
            "Iteration 173, loss = 0.16759639\n",
            "Iteration 174, loss = 0.16724226\n",
            "Iteration 175, loss = 0.16732519\n",
            "Iteration 176, loss = 0.16711819\n",
            "Iteration 177, loss = 0.16721528\n",
            "Iteration 178, loss = 0.16715531\n",
            "Iteration 179, loss = 0.16704963\n",
            "Iteration 180, loss = 0.16714023\n",
            "Iteration 181, loss = 0.16695901\n",
            "Iteration 182, loss = 0.16682870\n",
            "Iteration 183, loss = 0.16685837\n",
            "Iteration 184, loss = 0.16689130\n",
            "Iteration 185, loss = 0.16668160\n",
            "Iteration 186, loss = 0.16666602\n",
            "Iteration 187, loss = 0.16663106\n",
            "Iteration 188, loss = 0.16662101\n",
            "Iteration 189, loss = 0.16663514\n",
            "Iteration 190, loss = 0.16662208\n",
            "Iteration 191, loss = 0.16656759\n",
            "Iteration 192, loss = 0.16650099\n",
            "Iteration 193, loss = 0.16645974\n",
            "Iteration 194, loss = 0.16633998\n",
            "Iteration 195, loss = 0.16650871\n",
            "Iteration 196, loss = 0.16632437\n",
            "Iteration 197, loss = 0.16629330\n",
            "Iteration 198, loss = 0.16623769\n",
            "Iteration 199, loss = 0.16635402\n",
            "Iteration 200, loss = 0.16614534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32931391\n",
            "Iteration 3, loss = 0.29869246\n",
            "Iteration 4, loss = 0.27674482\n",
            "Iteration 5, loss = 0.25961434\n",
            "Iteration 6, loss = 0.24672253\n",
            "Iteration 7, loss = 0.23648507\n",
            "Iteration 8, loss = 0.23008783\n",
            "Iteration 9, loss = 0.22390716\n",
            "Iteration 10, loss = 0.22052680\n",
            "Iteration 11, loss = 0.21672930\n",
            "Iteration 12, loss = 0.21430139\n",
            "Iteration 13, loss = 0.21212330\n",
            "Iteration 14, loss = 0.21071285\n",
            "Iteration 15, loss = 0.21040101\n",
            "Iteration 16, loss = 0.20933131\n",
            "Iteration 17, loss = 0.20865010\n",
            "Iteration 18, loss = 0.20780416\n",
            "Iteration 19, loss = 0.20750051\n",
            "Iteration 20, loss = 0.20838367\n",
            "Iteration 21, loss = 0.20758055\n",
            "Iteration 22, loss = 0.20655013\n",
            "Iteration 23, loss = 0.20695427\n",
            "Iteration 24, loss = 0.20746892\n",
            "Iteration 25, loss = 0.20686139\n",
            "Iteration 26, loss = 0.20710178\n",
            "Iteration 27, loss = 0.20784782\n",
            "Iteration 28, loss = 0.20554492\n",
            "Iteration 29, loss = 0.20738083\n",
            "Iteration 30, loss = 0.20719886\n",
            "Iteration 31, loss = 0.20791600\n",
            "Iteration 32, loss = 0.20732561\n",
            "Iteration 33, loss = 0.20678131\n",
            "Iteration 34, loss = 0.20683293\n",
            "Iteration 35, loss = 0.20717035\n",
            "Iteration 36, loss = 0.20820903\n",
            "Iteration 37, loss = 0.20713142\n",
            "Iteration 38, loss = 0.20650255\n",
            "Iteration 39, loss = 0.20697058\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 40, loss = 0.19859722\n",
            "Iteration 41, loss = 0.19759734\n",
            "Iteration 42, loss = 0.19673833\n",
            "Iteration 43, loss = 0.19662198\n",
            "Iteration 44, loss = 0.19528005\n",
            "Iteration 45, loss = 0.19497460\n",
            "Iteration 46, loss = 0.19418989\n",
            "Iteration 47, loss = 0.19368549\n",
            "Iteration 48, loss = 0.19325931\n",
            "Iteration 49, loss = 0.19227237\n",
            "Iteration 50, loss = 0.19178842\n",
            "Iteration 51, loss = 0.19143364\n",
            "Iteration 52, loss = 0.19114075\n",
            "Iteration 53, loss = 0.19048706\n",
            "Iteration 54, loss = 0.18967157\n",
            "Iteration 55, loss = 0.18935811\n",
            "Iteration 56, loss = 0.18930409\n",
            "Iteration 57, loss = 0.18868886\n",
            "Iteration 58, loss = 0.18807328\n",
            "Iteration 59, loss = 0.18856388\n",
            "Iteration 60, loss = 0.18778155\n",
            "Iteration 61, loss = 0.18720806\n",
            "Iteration 62, loss = 0.18762548\n",
            "Iteration 63, loss = 0.18663919\n",
            "Iteration 64, loss = 0.18584178\n",
            "Iteration 65, loss = 0.18714191\n",
            "Iteration 66, loss = 0.18575920\n",
            "Iteration 67, loss = 0.18718967\n",
            "Iteration 68, loss = 0.18616197\n",
            "Iteration 69, loss = 0.18612486\n",
            "Iteration 70, loss = 0.18525482\n",
            "Iteration 71, loss = 0.18553952\n",
            "Iteration 72, loss = 0.18564385\n",
            "Iteration 73, loss = 0.18675563\n",
            "Iteration 74, loss = 0.18676014\n",
            "Iteration 75, loss = 0.18591023\n",
            "Iteration 76, loss = 0.18992138\n",
            "Iteration 77, loss = 0.18594074\n",
            "Iteration 78, loss = 0.18642030\n",
            "Iteration 79, loss = 0.18624573\n",
            "Iteration 80, loss = 0.18554491\n",
            "Iteration 81, loss = 0.19000970\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.17597516\n",
            "Iteration 83, loss = 0.17576864\n",
            "Iteration 84, loss = 0.17553433\n",
            "Iteration 85, loss = 0.17555125\n",
            "Iteration 86, loss = 0.17542799\n",
            "Iteration 87, loss = 0.17545766\n",
            "Iteration 88, loss = 0.17518582\n",
            "Iteration 89, loss = 0.17508284\n",
            "Iteration 90, loss = 0.17500474\n",
            "Iteration 91, loss = 0.17483571\n",
            "Iteration 92, loss = 0.17467140\n",
            "Iteration 93, loss = 0.17449591\n",
            "Iteration 94, loss = 0.17465413\n",
            "Iteration 95, loss = 0.17432880\n",
            "Iteration 96, loss = 0.17407971\n",
            "Iteration 97, loss = 0.17401373\n",
            "Iteration 98, loss = 0.17403969\n",
            "Iteration 99, loss = 0.17397605\n",
            "Iteration 100, loss = 0.17361286\n",
            "Iteration 101, loss = 0.17372601\n",
            "Iteration 102, loss = 0.17366764\n",
            "Iteration 103, loss = 0.17341063\n",
            "Iteration 104, loss = 0.17327563\n",
            "Iteration 105, loss = 0.17319354\n",
            "Iteration 106, loss = 0.17299881\n",
            "Iteration 107, loss = 0.17283477\n",
            "Iteration 108, loss = 0.17263771\n",
            "Iteration 109, loss = 0.17278344\n",
            "Iteration 110, loss = 0.17260356\n",
            "Iteration 111, loss = 0.17236231\n",
            "Iteration 112, loss = 0.17219913\n",
            "Iteration 113, loss = 0.17223370\n",
            "Iteration 114, loss = 0.17226327\n",
            "Iteration 115, loss = 0.17223537\n",
            "Iteration 116, loss = 0.17199062\n",
            "Iteration 117, loss = 0.17206714\n",
            "Iteration 118, loss = 0.17186356\n",
            "Iteration 119, loss = 0.17175619\n",
            "Iteration 120, loss = 0.17159592\n",
            "Iteration 121, loss = 0.17166612\n",
            "Iteration 122, loss = 0.17144123\n",
            "Iteration 123, loss = 0.17140970\n",
            "Iteration 124, loss = 0.17118805\n",
            "Iteration 125, loss = 0.17117312\n",
            "Iteration 126, loss = 0.17121840\n",
            "Iteration 127, loss = 0.17097631\n",
            "Iteration 128, loss = 0.17114445\n",
            "Iteration 129, loss = 0.17083407\n",
            "Iteration 130, loss = 0.17072044\n",
            "Iteration 131, loss = 0.17065852\n",
            "Iteration 132, loss = 0.17064216\n",
            "Iteration 133, loss = 0.17036439\n",
            "Iteration 134, loss = 0.17052210\n",
            "Iteration 135, loss = 0.17039477\n",
            "Iteration 136, loss = 0.17049207\n",
            "Iteration 137, loss = 0.17031882\n",
            "Iteration 138, loss = 0.17022604\n",
            "Iteration 139, loss = 0.17018515\n",
            "Iteration 140, loss = 0.17003924\n",
            "Iteration 141, loss = 0.16984612\n",
            "Iteration 142, loss = 0.16981055\n",
            "Iteration 143, loss = 0.16951124\n",
            "Iteration 144, loss = 0.16943451\n",
            "Iteration 145, loss = 0.16956286\n",
            "Iteration 146, loss = 0.16954702\n",
            "Iteration 147, loss = 0.16950815\n",
            "Iteration 148, loss = 0.16945360\n",
            "Iteration 149, loss = 0.16923174\n",
            "Iteration 150, loss = 0.16911776\n",
            "Iteration 151, loss = 0.16887923\n",
            "Iteration 152, loss = 0.16935225\n",
            "Iteration 153, loss = 0.16919971\n",
            "Iteration 154, loss = 0.16902052\n",
            "Iteration 155, loss = 0.16892348\n",
            "Iteration 156, loss = 0.16905685\n",
            "Iteration 157, loss = 0.16877720\n",
            "Iteration 158, loss = 0.16868478\n",
            "Iteration 159, loss = 0.16853816\n",
            "Iteration 160, loss = 0.16872595\n",
            "Iteration 161, loss = 0.16853312\n",
            "Iteration 162, loss = 0.16845367\n",
            "Iteration 163, loss = 0.16840375\n",
            "Iteration 164, loss = 0.16835278\n",
            "Iteration 165, loss = 0.16830916\n",
            "Iteration 166, loss = 0.16822801\n",
            "Iteration 167, loss = 0.16833053\n",
            "Iteration 168, loss = 0.16825285\n",
            "Iteration 169, loss = 0.16790283\n",
            "Iteration 170, loss = 0.16824825\n",
            "Iteration 171, loss = 0.16793054\n",
            "Iteration 172, loss = 0.16807318\n",
            "Iteration 173, loss = 0.16795397\n",
            "Iteration 174, loss = 0.16764816\n",
            "Iteration 175, loss = 0.16789579\n",
            "Iteration 176, loss = 0.16760062\n",
            "Iteration 177, loss = 0.16772625\n",
            "Iteration 178, loss = 0.16765812\n",
            "Iteration 179, loss = 0.16754609\n",
            "Iteration 180, loss = 0.16751700\n",
            "Iteration 181, loss = 0.16750499\n",
            "Iteration 182, loss = 0.16733031\n",
            "Iteration 183, loss = 0.16731203\n",
            "Iteration 184, loss = 0.16729689\n",
            "Iteration 185, loss = 0.16718171\n",
            "Iteration 186, loss = 0.16728129\n",
            "Iteration 187, loss = 0.16728512\n",
            "Iteration 188, loss = 0.16717410\n",
            "Iteration 189, loss = 0.16708774\n",
            "Iteration 190, loss = 0.16690010\n",
            "Iteration 191, loss = 0.16704705\n",
            "Iteration 192, loss = 0.16688777\n",
            "Iteration 193, loss = 0.16681620\n",
            "Iteration 194, loss = 0.16685343\n",
            "Iteration 195, loss = 0.16687933\n",
            "Iteration 196, loss = 0.16692685\n",
            "Iteration 197, loss = 0.16676551\n",
            "Iteration 198, loss = 0.16667520\n",
            "Iteration 199, loss = 0.16681119\n",
            "Iteration 200, loss = 0.16656314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.88926210\n",
            "Iteration 2, loss = 0.32723321\n",
            "Iteration 3, loss = 0.29702350\n",
            "Iteration 4, loss = 0.27519431\n",
            "Iteration 5, loss = 0.25897146\n",
            "Iteration 6, loss = 0.24634017\n",
            "Iteration 7, loss = 0.23645218\n",
            "Iteration 8, loss = 0.22922991\n",
            "Iteration 9, loss = 0.22369512\n",
            "Iteration 10, loss = 0.21972344\n",
            "Iteration 11, loss = 0.21587084\n",
            "Iteration 12, loss = 0.21373849\n",
            "Iteration 13, loss = 0.21157811\n",
            "Iteration 14, loss = 0.21030950\n",
            "Iteration 15, loss = 0.20994392\n",
            "Iteration 16, loss = 0.20977459\n",
            "Iteration 17, loss = 0.20823109\n",
            "Iteration 18, loss = 0.20805971\n",
            "Iteration 19, loss = 0.20787767\n",
            "Iteration 20, loss = 0.20701735\n",
            "Iteration 21, loss = 0.20737408\n",
            "Iteration 22, loss = 0.20637726\n",
            "Iteration 23, loss = 0.20784751\n",
            "Iteration 24, loss = 0.20624457\n",
            "Iteration 25, loss = 0.20637579\n",
            "Iteration 26, loss = 0.20633979\n",
            "Iteration 27, loss = 0.20735804\n",
            "Iteration 28, loss = 0.20665562\n",
            "Iteration 29, loss = 0.20713395\n",
            "Iteration 30, loss = 0.20641968\n",
            "Iteration 31, loss = 0.20772286\n",
            "Iteration 32, loss = 0.20685894\n",
            "Iteration 33, loss = 0.20709614\n",
            "Iteration 34, loss = 0.20639063\n",
            "Iteration 35, loss = 0.20536319\n",
            "Iteration 36, loss = 0.20763222\n",
            "Iteration 37, loss = 0.20679497\n",
            "Iteration 38, loss = 0.20580248\n",
            "Iteration 39, loss = 0.20659163\n",
            "Iteration 40, loss = 0.20665862\n",
            "Iteration 41, loss = 0.20682818\n",
            "Iteration 42, loss = 0.20638196\n",
            "Iteration 43, loss = 0.20726557\n",
            "Iteration 44, loss = 0.20755868\n",
            "Iteration 45, loss = 0.20721155\n",
            "Iteration 46, loss = 0.20735986\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.19950623\n",
            "Iteration 48, loss = 0.19803416\n",
            "Iteration 49, loss = 0.19712808\n",
            "Iteration 50, loss = 0.19623151\n",
            "Iteration 51, loss = 0.19561610\n",
            "Iteration 52, loss = 0.19506355\n",
            "Iteration 53, loss = 0.19426250\n",
            "Iteration 54, loss = 0.19380173\n",
            "Iteration 55, loss = 0.19332875\n",
            "Iteration 56, loss = 0.19261956\n",
            "Iteration 57, loss = 0.19209835\n",
            "Iteration 58, loss = 0.19147654\n",
            "Iteration 59, loss = 0.19106247\n",
            "Iteration 60, loss = 0.19063236\n",
            "Iteration 61, loss = 0.19048024\n",
            "Iteration 62, loss = 0.18994761\n",
            "Iteration 63, loss = 0.18930634\n",
            "Iteration 64, loss = 0.18958929\n",
            "Iteration 65, loss = 0.18887967\n",
            "Iteration 66, loss = 0.18790827\n",
            "Iteration 67, loss = 0.18784851\n",
            "Iteration 68, loss = 0.18772954\n",
            "Iteration 69, loss = 0.18777194\n",
            "Iteration 70, loss = 0.18761244\n",
            "Iteration 71, loss = 0.18747114\n",
            "Iteration 72, loss = 0.18681019\n",
            "Iteration 73, loss = 0.18644580\n",
            "Iteration 74, loss = 0.18514217\n",
            "Iteration 75, loss = 0.18616916\n",
            "Iteration 76, loss = 0.18633088\n",
            "Iteration 77, loss = 0.18734574\n",
            "Iteration 78, loss = 0.18520388\n",
            "Iteration 79, loss = 0.18608429\n",
            "Iteration 80, loss = 0.18587976\n",
            "Iteration 81, loss = 0.18717474\n",
            "Iteration 82, loss = 0.18856237\n",
            "Iteration 83, loss = 0.18543372\n",
            "Iteration 84, loss = 0.18590023\n",
            "Iteration 85, loss = 0.18711125\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 86, loss = 0.17706250\n",
            "Iteration 87, loss = 0.17680804\n",
            "Iteration 88, loss = 0.17677844\n",
            "Iteration 89, loss = 0.17663356\n",
            "Iteration 90, loss = 0.17655594\n",
            "Iteration 91, loss = 0.17657997\n",
            "Iteration 92, loss = 0.17636717\n",
            "Iteration 93, loss = 0.17622527\n",
            "Iteration 94, loss = 0.17613407\n",
            "Iteration 95, loss = 0.17589623\n",
            "Iteration 96, loss = 0.17596876\n",
            "Iteration 97, loss = 0.17579396\n",
            "Iteration 98, loss = 0.17570491\n",
            "Iteration 99, loss = 0.17548006\n",
            "Iteration 100, loss = 0.17518207\n",
            "Iteration 101, loss = 0.17535556\n",
            "Iteration 102, loss = 0.17527286\n",
            "Iteration 103, loss = 0.17486677\n",
            "Iteration 104, loss = 0.17489233\n",
            "Iteration 105, loss = 0.17480233\n",
            "Iteration 106, loss = 0.17451656\n",
            "Iteration 107, loss = 0.17467742\n",
            "Iteration 108, loss = 0.17436811\n",
            "Iteration 109, loss = 0.17413732\n",
            "Iteration 110, loss = 0.17410628\n",
            "Iteration 111, loss = 0.17415544\n",
            "Iteration 112, loss = 0.17398725\n",
            "Iteration 113, loss = 0.17383936\n",
            "Iteration 114, loss = 0.17382275\n",
            "Iteration 115, loss = 0.17357229\n",
            "Iteration 116, loss = 0.17344352\n",
            "Iteration 117, loss = 0.17351844\n",
            "Iteration 118, loss = 0.17329507\n",
            "Iteration 119, loss = 0.17319313\n",
            "Iteration 120, loss = 0.17313317\n",
            "Iteration 121, loss = 0.17290205\n",
            "Iteration 122, loss = 0.17300828\n",
            "Iteration 123, loss = 0.17282765\n",
            "Iteration 124, loss = 0.17305833\n",
            "Iteration 125, loss = 0.17285164\n",
            "Iteration 126, loss = 0.17255634\n",
            "Iteration 127, loss = 0.17234509\n",
            "Iteration 128, loss = 0.17243117\n",
            "Iteration 129, loss = 0.17225969\n",
            "Iteration 130, loss = 0.17247048\n",
            "Iteration 131, loss = 0.17207966\n",
            "Iteration 132, loss = 0.17198102\n",
            "Iteration 133, loss = 0.17188869\n",
            "Iteration 134, loss = 0.17193589\n",
            "Iteration 135, loss = 0.17185830\n",
            "Iteration 136, loss = 0.17153385\n",
            "Iteration 137, loss = 0.17170670\n",
            "Iteration 138, loss = 0.17161679\n",
            "Iteration 139, loss = 0.17151694\n",
            "Iteration 140, loss = 0.17130732\n",
            "Iteration 141, loss = 0.17122846\n",
            "Iteration 142, loss = 0.17134077\n",
            "Iteration 143, loss = 0.17122176\n",
            "Iteration 144, loss = 0.17116542\n",
            "Iteration 145, loss = 0.17088636\n",
            "Iteration 146, loss = 0.17078792\n",
            "Iteration 147, loss = 0.17081585\n",
            "Iteration 148, loss = 0.17097552\n",
            "Iteration 149, loss = 0.17054438\n",
            "Iteration 150, loss = 0.17060947\n",
            "Iteration 151, loss = 0.17057195\n",
            "Iteration 152, loss = 0.17040749\n",
            "Iteration 153, loss = 0.17050873\n",
            "Iteration 154, loss = 0.17037933\n",
            "Iteration 155, loss = 0.17015191\n",
            "Iteration 156, loss = 0.17000085\n",
            "Iteration 157, loss = 0.17028156\n",
            "Iteration 158, loss = 0.16994052\n",
            "Iteration 159, loss = 0.17019712\n",
            "Iteration 160, loss = 0.16986969\n",
            "Iteration 161, loss = 0.16985676\n",
            "Iteration 162, loss = 0.16975615\n",
            "Iteration 163, loss = 0.16977536\n",
            "Iteration 164, loss = 0.16961150\n",
            "Iteration 165, loss = 0.16960237\n",
            "Iteration 166, loss = 0.16968921\n",
            "Iteration 167, loss = 0.16944765\n",
            "Iteration 168, loss = 0.16948548\n",
            "Iteration 169, loss = 0.16933722\n",
            "Iteration 170, loss = 0.16919950\n",
            "Iteration 171, loss = 0.16919594\n",
            "Iteration 172, loss = 0.16908359\n",
            "Iteration 173, loss = 0.16922346\n",
            "Iteration 174, loss = 0.16901788\n",
            "Iteration 175, loss = 0.16900970\n",
            "Iteration 176, loss = 0.16875690\n",
            "Iteration 177, loss = 0.16889801\n",
            "Iteration 178, loss = 0.16885973\n",
            "Iteration 179, loss = 0.16875055\n",
            "Iteration 180, loss = 0.16897588\n",
            "Iteration 181, loss = 0.16851463\n",
            "Iteration 182, loss = 0.16864725\n",
            "Iteration 183, loss = 0.16852622\n",
            "Iteration 184, loss = 0.16863039\n",
            "Iteration 185, loss = 0.16830260\n",
            "Iteration 186, loss = 0.16839394\n",
            "Iteration 187, loss = 0.16826871\n",
            "Iteration 188, loss = 0.16820117\n",
            "Iteration 189, loss = 0.16831614\n",
            "Iteration 190, loss = 0.16806545\n",
            "Iteration 191, loss = 0.16824989\n",
            "Iteration 192, loss = 0.16803557\n",
            "Iteration 193, loss = 0.16809286\n",
            "Iteration 194, loss = 0.16792839\n",
            "Iteration 195, loss = 0.16794373\n",
            "Iteration 196, loss = 0.16776057\n",
            "Iteration 197, loss = 0.16777270\n",
            "Iteration 198, loss = 0.16779827\n",
            "Iteration 199, loss = 0.16778317\n",
            "Iteration 200, loss = 0.16774531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.66846377\n",
            "Iteration 2, loss = 0.29821996\n",
            "Iteration 3, loss = 0.27525605\n",
            "Iteration 4, loss = 0.25897965\n",
            "Iteration 5, loss = 0.24637453\n",
            "Iteration 6, loss = 0.23677394\n",
            "Iteration 7, loss = 0.22894815\n",
            "Iteration 8, loss = 0.22382244\n",
            "Iteration 9, loss = 0.21958452\n",
            "Iteration 10, loss = 0.21735352\n",
            "Iteration 11, loss = 0.21417276\n",
            "Iteration 12, loss = 0.21197586\n",
            "Iteration 13, loss = 0.21083080\n",
            "Iteration 14, loss = 0.21036649\n",
            "Iteration 15, loss = 0.20976122\n",
            "Iteration 16, loss = 0.20929278\n",
            "Iteration 17, loss = 0.20748127\n",
            "Iteration 18, loss = 0.20824370\n",
            "Iteration 19, loss = 0.20799388\n",
            "Iteration 20, loss = 0.20833667\n",
            "Iteration 21, loss = 0.20805647\n",
            "Iteration 22, loss = 0.20662467\n",
            "Iteration 23, loss = 0.20732543\n",
            "Iteration 24, loss = 0.20868653\n",
            "Iteration 25, loss = 0.20700593\n",
            "Iteration 26, loss = 0.20684496\n",
            "Iteration 27, loss = 0.20784866\n",
            "Iteration 28, loss = 0.20780974\n",
            "Iteration 29, loss = 0.20744995\n",
            "Iteration 30, loss = 0.20717612\n",
            "Iteration 31, loss = 0.20707453\n",
            "Iteration 32, loss = 0.20744500\n",
            "Iteration 33, loss = 0.20733982\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.19936095\n",
            "Iteration 35, loss = 0.19851189\n",
            "Iteration 36, loss = 0.19762642\n",
            "Iteration 37, loss = 0.19698669\n",
            "Iteration 38, loss = 0.19624052\n",
            "Iteration 39, loss = 0.19573670\n",
            "Iteration 40, loss = 0.19492191\n",
            "Iteration 41, loss = 0.19461331\n",
            "Iteration 42, loss = 0.19390194\n",
            "Iteration 43, loss = 0.19367466\n",
            "Iteration 44, loss = 0.19309177\n",
            "Iteration 45, loss = 0.19204940\n",
            "Iteration 46, loss = 0.19175610\n",
            "Iteration 47, loss = 0.19165892\n",
            "Iteration 48, loss = 0.19087385\n",
            "Iteration 49, loss = 0.19038898\n",
            "Iteration 50, loss = 0.19019514\n",
            "Iteration 51, loss = 0.18899356\n",
            "Iteration 52, loss = 0.18940861\n",
            "Iteration 53, loss = 0.18887344\n",
            "Iteration 54, loss = 0.18824832\n",
            "Iteration 55, loss = 0.18807883\n",
            "Iteration 56, loss = 0.18789121\n",
            "Iteration 57, loss = 0.18844283\n",
            "Iteration 58, loss = 0.18841751\n",
            "Iteration 59, loss = 0.18668583\n",
            "Iteration 60, loss = 0.18768129\n",
            "Iteration 61, loss = 0.18699705\n",
            "Iteration 62, loss = 0.18767683\n",
            "Iteration 63, loss = 0.18672942\n",
            "Iteration 64, loss = 0.18739751\n",
            "Iteration 65, loss = 0.18767157\n",
            "Iteration 66, loss = 0.18738679\n",
            "Iteration 67, loss = 0.18786029\n",
            "Iteration 68, loss = 0.18638545\n",
            "Iteration 69, loss = 0.18583676\n",
            "Iteration 70, loss = 0.18887914\n",
            "Iteration 71, loss = 0.18664785\n",
            "Iteration 72, loss = 0.18964129\n",
            "Iteration 73, loss = 0.18626633\n",
            "Iteration 74, loss = 0.18772692\n",
            "Iteration 75, loss = 0.18754760\n",
            "Iteration 76, loss = 0.19107171\n",
            "Iteration 77, loss = 0.18712876\n",
            "Iteration 78, loss = 0.18779927\n",
            "Iteration 79, loss = 0.18855642\n",
            "Iteration 80, loss = 0.18774024\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.17577293\n",
            "Iteration 82, loss = 0.17588574\n",
            "Iteration 83, loss = 0.17555804\n",
            "Iteration 84, loss = 0.17532415\n",
            "Iteration 85, loss = 0.17546561\n",
            "Iteration 86, loss = 0.17521866\n",
            "Iteration 87, loss = 0.17505312\n",
            "Iteration 88, loss = 0.17488629\n",
            "Iteration 89, loss = 0.17490601\n",
            "Iteration 90, loss = 0.17475588\n",
            "Iteration 91, loss = 0.17465903\n",
            "Iteration 92, loss = 0.17447314\n",
            "Iteration 93, loss = 0.17418399\n",
            "Iteration 94, loss = 0.17438453\n",
            "Iteration 95, loss = 0.17402853\n",
            "Iteration 96, loss = 0.17395125\n",
            "Iteration 97, loss = 0.17386826\n",
            "Iteration 98, loss = 0.17379152\n",
            "Iteration 99, loss = 0.17360821\n",
            "Iteration 100, loss = 0.17360232\n",
            "Iteration 101, loss = 0.17338197\n",
            "Iteration 102, loss = 0.17321191\n",
            "Iteration 103, loss = 0.17313999\n",
            "Iteration 104, loss = 0.17316571\n",
            "Iteration 105, loss = 0.17296187\n",
            "Iteration 106, loss = 0.17293546\n",
            "Iteration 107, loss = 0.17278147\n",
            "Iteration 108, loss = 0.17289940\n",
            "Iteration 109, loss = 0.17259874\n",
            "Iteration 110, loss = 0.17241188\n",
            "Iteration 111, loss = 0.17221964\n",
            "Iteration 112, loss = 0.17228450\n",
            "Iteration 113, loss = 0.17225853\n",
            "Iteration 114, loss = 0.17216739\n",
            "Iteration 115, loss = 0.17192672\n",
            "Iteration 116, loss = 0.17197241\n",
            "Iteration 117, loss = 0.17174183\n",
            "Iteration 118, loss = 0.17157336\n",
            "Iteration 119, loss = 0.17161559\n",
            "Iteration 120, loss = 0.17162891\n",
            "Iteration 121, loss = 0.17130513\n",
            "Iteration 122, loss = 0.17131714\n",
            "Iteration 123, loss = 0.17124992\n",
            "Iteration 124, loss = 0.17116720\n",
            "Iteration 125, loss = 0.17117497\n",
            "Iteration 126, loss = 0.17081043\n",
            "Iteration 127, loss = 0.17100929\n",
            "Iteration 128, loss = 0.17073566\n",
            "Iteration 129, loss = 0.17075383\n",
            "Iteration 130, loss = 0.17090787\n",
            "Iteration 131, loss = 0.17044115\n",
            "Iteration 132, loss = 0.17040549\n",
            "Iteration 133, loss = 0.17058008\n",
            "Iteration 134, loss = 0.17044512\n",
            "Iteration 135, loss = 0.17036423\n",
            "Iteration 136, loss = 0.17018435\n",
            "Iteration 137, loss = 0.17012505\n",
            "Iteration 138, loss = 0.17004444\n",
            "Iteration 139, loss = 0.16988779\n",
            "Iteration 140, loss = 0.16973284\n",
            "Iteration 141, loss = 0.16977272\n",
            "Iteration 142, loss = 0.16989426\n",
            "Iteration 143, loss = 0.16968485\n",
            "Iteration 144, loss = 0.16957487\n",
            "Iteration 145, loss = 0.16958030\n",
            "Iteration 146, loss = 0.16966475\n",
            "Iteration 147, loss = 0.16934622\n",
            "Iteration 148, loss = 0.16933849\n",
            "Iteration 149, loss = 0.16942722\n",
            "Iteration 150, loss = 0.16920687\n",
            "Iteration 151, loss = 0.16904951\n",
            "Iteration 152, loss = 0.16907615\n",
            "Iteration 153, loss = 0.16891502\n",
            "Iteration 154, loss = 0.16910464\n",
            "Iteration 155, loss = 0.16895558\n",
            "Iteration 156, loss = 0.16884466\n",
            "Iteration 157, loss = 0.16879236\n",
            "Iteration 158, loss = 0.16861214\n",
            "Iteration 159, loss = 0.16865048\n",
            "Iteration 160, loss = 0.16871307\n",
            "Iteration 161, loss = 0.16851877\n",
            "Iteration 162, loss = 0.16836554\n",
            "Iteration 163, loss = 0.16838077\n",
            "Iteration 164, loss = 0.16841691\n",
            "Iteration 165, loss = 0.16850622\n",
            "Iteration 166, loss = 0.16824645\n",
            "Iteration 167, loss = 0.16814690\n",
            "Iteration 168, loss = 0.16816004\n",
            "Iteration 169, loss = 0.16817675\n",
            "Iteration 170, loss = 0.16808318\n",
            "Iteration 171, loss = 0.16813319\n",
            "Iteration 172, loss = 0.16798239\n",
            "Iteration 173, loss = 0.16799663\n",
            "Iteration 174, loss = 0.16799890\n",
            "Iteration 175, loss = 0.16774691\n",
            "Iteration 176, loss = 0.16766863\n",
            "Iteration 177, loss = 0.16770511\n",
            "Iteration 178, loss = 0.16766798\n",
            "Iteration 179, loss = 0.16759834\n",
            "Iteration 180, loss = 0.16742123\n",
            "Iteration 181, loss = 0.16754801\n",
            "Iteration 182, loss = 0.16770724\n",
            "Iteration 183, loss = 0.16740656\n",
            "Iteration 184, loss = 0.16737589\n",
            "Iteration 185, loss = 0.16727041\n",
            "Iteration 186, loss = 0.16717732\n",
            "Iteration 187, loss = 0.16724888\n",
            "Iteration 188, loss = 0.16720669\n",
            "Iteration 189, loss = 0.16711544\n",
            "Iteration 190, loss = 0.16710378\n",
            "Iteration 191, loss = 0.16727893\n",
            "Iteration 192, loss = 0.16686183\n",
            "Iteration 193, loss = 0.16709365\n",
            "Iteration 194, loss = 0.16709323\n",
            "Iteration 195, loss = 0.16684668\n",
            "Iteration 196, loss = 0.16702259\n",
            "Iteration 197, loss = 0.16686197\n",
            "Iteration 198, loss = 0.16689283\n",
            "Iteration 199, loss = 0.16695392\n",
            "Iteration 200, loss = 0.16678505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 43.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.36656718\n",
            "Iteration 3, loss = 0.35059139\n",
            "Iteration 4, loss = 0.33681774\n",
            "Iteration 5, loss = 0.32558773\n",
            "Iteration 6, loss = 0.31601972\n",
            "Iteration 7, loss = 0.30687426\n",
            "Iteration 8, loss = 0.29952124\n",
            "Iteration 9, loss = 0.29141021\n",
            "Iteration 10, loss = 0.28385024\n",
            "Iteration 11, loss = 0.27812362\n",
            "Iteration 12, loss = 0.27426440\n",
            "Iteration 13, loss = 0.26809206\n",
            "Iteration 14, loss = 0.26280207\n",
            "Iteration 15, loss = 0.25856683\n",
            "Iteration 16, loss = 0.25462660\n",
            "Iteration 17, loss = 0.24915327\n",
            "Iteration 18, loss = 0.24746848\n",
            "Iteration 19, loss = 0.24327167\n",
            "Iteration 20, loss = 0.24116563\n",
            "Iteration 21, loss = 0.23588584\n",
            "Iteration 22, loss = 0.23522296\n",
            "Iteration 23, loss = 0.23358346\n",
            "Iteration 24, loss = 0.23028848\n",
            "Iteration 25, loss = 0.22866210\n",
            "Iteration 26, loss = 0.22642335\n",
            "Iteration 27, loss = 0.22367575\n",
            "Iteration 28, loss = 0.22428083\n",
            "Iteration 29, loss = 0.22156235\n",
            "Iteration 30, loss = 0.22262861\n",
            "Iteration 31, loss = 0.22079357\n",
            "Iteration 32, loss = 0.21800046\n",
            "Iteration 33, loss = 0.21777595\n",
            "Iteration 34, loss = 0.21745838\n",
            "Iteration 35, loss = 0.21549283\n",
            "Iteration 36, loss = 0.21682519\n",
            "Iteration 37, loss = 0.21568912\n",
            "Iteration 38, loss = 0.21338596\n",
            "Iteration 39, loss = 0.21199832\n",
            "Iteration 40, loss = 0.21034700\n",
            "Iteration 41, loss = 0.21715674\n",
            "Iteration 42, loss = 0.20949607\n",
            "Iteration 43, loss = 0.21392468\n",
            "Iteration 44, loss = 0.21023384\n",
            "Iteration 45, loss = 0.21147321\n",
            "Iteration 46, loss = 0.20734357\n",
            "Iteration 47, loss = 0.20751415\n",
            "Iteration 48, loss = 0.20635761\n",
            "Iteration 49, loss = 0.20780088\n",
            "Iteration 50, loss = 0.20970657\n",
            "Iteration 51, loss = 0.21144781\n",
            "Iteration 52, loss = 0.20803520\n",
            "Iteration 53, loss = 0.20979059\n",
            "Iteration 54, loss = 0.20910598\n",
            "Iteration 55, loss = 0.21036193\n",
            "Iteration 56, loss = 0.20540178\n",
            "Iteration 57, loss = 0.20605781\n",
            "Iteration 58, loss = 0.20845302\n",
            "Iteration 59, loss = 0.20675764\n",
            "Iteration 60, loss = 0.20786657\n",
            "Iteration 61, loss = 0.20826722\n",
            "Iteration 62, loss = 0.20545150\n",
            "Iteration 63, loss = 0.20861730\n",
            "Iteration 64, loss = 0.20342502\n",
            "Iteration 65, loss = 0.20901721\n",
            "Iteration 66, loss = 0.20595948\n",
            "Iteration 67, loss = 0.20572803\n",
            "Iteration 68, loss = 0.20513601\n",
            "Iteration 69, loss = 0.20743933\n",
            "Iteration 70, loss = 0.20528401\n",
            "Iteration 71, loss = 0.20591592\n",
            "Iteration 72, loss = 0.20691286\n",
            "Iteration 73, loss = 0.20619866\n",
            "Iteration 74, loss = 0.20522167\n",
            "Iteration 75, loss = 0.20850333\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.19854050\n",
            "Iteration 77, loss = 0.19668634\n",
            "Iteration 78, loss = 0.19673750\n",
            "Iteration 79, loss = 0.19651333\n",
            "Iteration 80, loss = 0.19564479\n",
            "Iteration 81, loss = 0.19618072\n",
            "Iteration 82, loss = 0.19540287\n",
            "Iteration 83, loss = 0.19549700\n",
            "Iteration 84, loss = 0.19488315\n",
            "Iteration 85, loss = 0.19450113\n",
            "Iteration 86, loss = 0.19448042\n",
            "Iteration 87, loss = 0.19438931\n",
            "Iteration 88, loss = 0.19370037\n",
            "Iteration 89, loss = 0.19415015\n",
            "Iteration 90, loss = 0.19352001\n",
            "Iteration 91, loss = 0.19339809\n",
            "Iteration 92, loss = 0.19323795\n",
            "Iteration 93, loss = 0.19267285\n",
            "Iteration 94, loss = 0.19212728\n",
            "Iteration 95, loss = 0.19255955\n",
            "Iteration 96, loss = 0.19242327\n",
            "Iteration 97, loss = 0.19198988\n",
            "Iteration 98, loss = 0.19204384\n",
            "Iteration 99, loss = 0.19264884\n",
            "Iteration 100, loss = 0.19165003\n",
            "Iteration 101, loss = 0.19183284\n",
            "Iteration 102, loss = 0.19108270\n",
            "Iteration 103, loss = 0.19149159\n",
            "Iteration 104, loss = 0.19089964\n",
            "Iteration 105, loss = 0.19090025\n",
            "Iteration 106, loss = 0.19062830\n",
            "Iteration 107, loss = 0.19024822\n",
            "Iteration 108, loss = 0.19124762\n",
            "Iteration 109, loss = 0.19026016\n",
            "Iteration 110, loss = 0.19044560\n",
            "Iteration 111, loss = 0.19002028\n",
            "Iteration 112, loss = 0.18969468\n",
            "Iteration 113, loss = 0.18935634\n",
            "Iteration 114, loss = 0.18981855\n",
            "Iteration 115, loss = 0.18946867\n",
            "Iteration 116, loss = 0.18915301\n",
            "Iteration 117, loss = 0.18852258\n",
            "Iteration 118, loss = 0.18779981\n",
            "Iteration 119, loss = 0.18880602\n",
            "Iteration 120, loss = 0.18814757\n",
            "Iteration 121, loss = 0.18848712\n",
            "Iteration 122, loss = 0.18855003\n",
            "Iteration 123, loss = 0.18864066\n",
            "Iteration 124, loss = 0.18811482\n",
            "Iteration 125, loss = 0.18761088\n",
            "Iteration 126, loss = 0.18803112\n",
            "Iteration 127, loss = 0.18781030\n",
            "Iteration 128, loss = 0.18657666\n",
            "Iteration 129, loss = 0.18692429\n",
            "Iteration 130, loss = 0.18682593\n",
            "Iteration 131, loss = 0.18721172\n",
            "Iteration 132, loss = 0.18761538\n",
            "Iteration 133, loss = 0.18711281\n",
            "Iteration 134, loss = 0.18708917\n",
            "Iteration 135, loss = 0.18635099\n",
            "Iteration 136, loss = 0.18627420\n",
            "Iteration 137, loss = 0.18631663\n",
            "Iteration 138, loss = 0.18657163\n",
            "Iteration 139, loss = 0.18638305\n",
            "Iteration 140, loss = 0.18603078\n",
            "Iteration 141, loss = 0.18656092\n",
            "Iteration 142, loss = 0.18654287\n",
            "Iteration 143, loss = 0.18664097\n",
            "Iteration 144, loss = 0.18635072\n",
            "Iteration 145, loss = 0.18497000\n",
            "Iteration 146, loss = 0.18580756\n",
            "Iteration 147, loss = 0.18520019\n",
            "Iteration 148, loss = 0.18513805\n",
            "Iteration 149, loss = 0.18660486\n",
            "Iteration 150, loss = 0.18416513\n",
            "Iteration 151, loss = 0.18390774\n",
            "Iteration 152, loss = 0.18592919\n",
            "Iteration 153, loss = 0.18587775\n",
            "Iteration 154, loss = 0.18503732\n",
            "Iteration 155, loss = 0.18437382\n",
            "Iteration 156, loss = 0.18382235\n",
            "Iteration 157, loss = 0.18475285\n",
            "Iteration 158, loss = 0.18407492\n",
            "Iteration 159, loss = 0.18453748\n",
            "Iteration 160, loss = 0.18461988\n",
            "Iteration 161, loss = 0.18359859\n",
            "Iteration 162, loss = 0.18377595\n",
            "Iteration 163, loss = 0.18435087\n",
            "Iteration 164, loss = 0.18424238\n",
            "Iteration 165, loss = 0.18250072\n",
            "Iteration 166, loss = 0.18386749\n",
            "Iteration 167, loss = 0.18538033\n",
            "Iteration 168, loss = 0.18485662\n",
            "Iteration 169, loss = 0.18360358\n",
            "Iteration 170, loss = 0.18727818\n",
            "Iteration 171, loss = 0.18347634\n",
            "Iteration 172, loss = 0.18321596\n",
            "Iteration 173, loss = 0.18418379\n",
            "Iteration 174, loss = 0.18531651\n",
            "Iteration 175, loss = 0.18388028\n",
            "Iteration 176, loss = 0.18681184\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 177, loss = 0.17756739\n",
            "Iteration 178, loss = 0.17727524\n",
            "Iteration 179, loss = 0.17762665\n",
            "Iteration 180, loss = 0.17707594\n",
            "Iteration 181, loss = 0.17781482\n",
            "Iteration 182, loss = 0.17733167\n",
            "Iteration 183, loss = 0.17708872\n",
            "Iteration 184, loss = 0.17731742\n",
            "Iteration 185, loss = 0.17688085\n",
            "Iteration 186, loss = 0.17713425\n",
            "Iteration 187, loss = 0.17704861\n",
            "Iteration 188, loss = 0.17696156\n",
            "Iteration 189, loss = 0.17728051\n",
            "Iteration 190, loss = 0.17683219\n",
            "Iteration 191, loss = 0.17713925\n",
            "Iteration 192, loss = 0.17708006\n",
            "Iteration 193, loss = 0.17683352\n",
            "Iteration 194, loss = 0.17679333\n",
            "Iteration 195, loss = 0.17687066\n",
            "Iteration 196, loss = 0.17677319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 197, loss = 0.17584511\n",
            "Iteration 198, loss = 0.17584476\n",
            "Iteration 199, loss = 0.17576543\n",
            "Iteration 200, loss = 0.17582625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.38414399\n",
            "Iteration 3, loss = 0.36849265\n",
            "Iteration 4, loss = 0.35492061\n",
            "Iteration 5, loss = 0.34375526\n",
            "Iteration 6, loss = 0.33262957\n",
            "Iteration 7, loss = 0.32089442\n",
            "Iteration 8, loss = 0.31221204\n",
            "Iteration 9, loss = 0.30471452\n",
            "Iteration 10, loss = 0.29666172\n",
            "Iteration 11, loss = 0.28937600\n",
            "Iteration 12, loss = 0.28387869\n",
            "Iteration 13, loss = 0.27686661\n",
            "Iteration 14, loss = 0.27145881\n",
            "Iteration 15, loss = 0.26661388\n",
            "Iteration 16, loss = 0.26162269\n",
            "Iteration 17, loss = 0.25665306\n",
            "Iteration 18, loss = 0.25356976\n",
            "Iteration 19, loss = 0.24987664\n",
            "Iteration 20, loss = 0.24641898\n",
            "Iteration 21, loss = 0.24219152\n",
            "Iteration 22, loss = 0.23977827\n",
            "Iteration 23, loss = 0.23746466\n",
            "Iteration 24, loss = 0.23534403\n",
            "Iteration 25, loss = 0.23344672\n",
            "Iteration 26, loss = 0.23058764\n",
            "Iteration 27, loss = 0.22844987\n",
            "Iteration 28, loss = 0.22776309\n",
            "Iteration 29, loss = 0.22551045\n",
            "Iteration 30, loss = 0.22329517\n",
            "Iteration 31, loss = 0.22260411\n",
            "Iteration 32, loss = 0.22014467\n",
            "Iteration 33, loss = 0.22016287\n",
            "Iteration 34, loss = 0.21966838\n",
            "Iteration 35, loss = 0.21783464\n",
            "Iteration 36, loss = 0.21698834\n",
            "Iteration 37, loss = 0.21494751\n",
            "Iteration 38, loss = 0.21534501\n",
            "Iteration 39, loss = 0.21345670\n",
            "Iteration 40, loss = 0.21397055\n",
            "Iteration 41, loss = 0.21326447\n",
            "Iteration 42, loss = 0.21244367\n",
            "Iteration 43, loss = 0.21304778\n",
            "Iteration 44, loss = 0.21151017\n",
            "Iteration 45, loss = 0.21283110\n",
            "Iteration 46, loss = 0.21107911\n",
            "Iteration 47, loss = 0.21029456\n",
            "Iteration 48, loss = 0.20846502\n",
            "Iteration 49, loss = 0.21084789\n",
            "Iteration 50, loss = 0.20962900\n",
            "Iteration 51, loss = 0.21018794\n",
            "Iteration 52, loss = 0.20854778\n",
            "Iteration 53, loss = 0.21056455\n",
            "Iteration 54, loss = 0.20801490\n",
            "Iteration 55, loss = 0.20733488\n",
            "Iteration 56, loss = 0.20705656\n",
            "Iteration 57, loss = 0.20727979\n",
            "Iteration 58, loss = 0.20685638\n",
            "Iteration 59, loss = 0.20776247\n",
            "Iteration 60, loss = 0.20715117\n",
            "Iteration 61, loss = 0.20683639\n",
            "Iteration 62, loss = 0.20668937\n",
            "Iteration 63, loss = 0.20763711\n",
            "Iteration 64, loss = 0.20604581\n",
            "Iteration 65, loss = 0.20684389\n",
            "Iteration 66, loss = 0.20807620\n",
            "Iteration 67, loss = 0.20817402\n",
            "Iteration 68, loss = 0.20561262\n",
            "Iteration 69, loss = 0.20637180\n",
            "Iteration 70, loss = 0.20740493\n",
            "Iteration 71, loss = 0.20786454\n",
            "Iteration 72, loss = 0.20693864\n",
            "Iteration 73, loss = 0.20595090\n",
            "Iteration 74, loss = 0.20625306\n",
            "Iteration 75, loss = 0.20611826\n",
            "Iteration 76, loss = 0.20782597\n",
            "Iteration 77, loss = 0.20540959\n",
            "Iteration 78, loss = 0.20669632\n",
            "Iteration 79, loss = 0.20809048\n",
            "Iteration 80, loss = 0.20684797\n",
            "Iteration 81, loss = 0.20773961\n",
            "Iteration 82, loss = 0.20549634\n",
            "Iteration 83, loss = 0.20589942\n",
            "Iteration 84, loss = 0.20739353\n",
            "Iteration 85, loss = 0.20663813\n",
            "Iteration 86, loss = 0.20609108\n",
            "Iteration 87, loss = 0.20631533\n",
            "Iteration 88, loss = 0.20530762\n",
            "Iteration 89, loss = 0.20607498\n",
            "Iteration 90, loss = 0.20551152\n",
            "Iteration 91, loss = 0.20511800\n",
            "Iteration 92, loss = 0.20697320\n",
            "Iteration 93, loss = 0.20649182\n",
            "Iteration 94, loss = 0.20600127\n",
            "Iteration 95, loss = 0.20600469\n",
            "Iteration 96, loss = 0.20572335\n",
            "Iteration 97, loss = 0.20605491\n",
            "Iteration 98, loss = 0.20619226\n",
            "Iteration 99, loss = 0.20650502\n",
            "Iteration 100, loss = 0.20602514\n",
            "Iteration 101, loss = 0.20556489\n",
            "Iteration 102, loss = 0.20496262\n",
            "Iteration 103, loss = 0.20507907\n",
            "Iteration 104, loss = 0.20783035\n",
            "Iteration 105, loss = 0.20583338\n",
            "Iteration 106, loss = 0.20818015\n",
            "Iteration 107, loss = 0.20590866\n",
            "Iteration 108, loss = 0.20585416\n",
            "Iteration 109, loss = 0.20568341\n",
            "Iteration 110, loss = 0.20787017\n",
            "Iteration 111, loss = 0.20590584\n",
            "Iteration 112, loss = 0.20514720\n",
            "Iteration 113, loss = 0.20594072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 114, loss = 0.19999998\n",
            "Iteration 115, loss = 0.19945793\n",
            "Iteration 116, loss = 0.19888312\n",
            "Iteration 117, loss = 0.19921800\n",
            "Iteration 118, loss = 0.19881103\n",
            "Iteration 119, loss = 0.19877864\n",
            "Iteration 120, loss = 0.19833323\n",
            "Iteration 121, loss = 0.19876124\n",
            "Iteration 122, loss = 0.19851532\n",
            "Iteration 123, loss = 0.19822775\n",
            "Iteration 124, loss = 0.19815198\n",
            "Iteration 125, loss = 0.19788513\n",
            "Iteration 126, loss = 0.19767316\n",
            "Iteration 127, loss = 0.19789545\n",
            "Iteration 128, loss = 0.19758250\n",
            "Iteration 129, loss = 0.19732569\n",
            "Iteration 130, loss = 0.19796408\n",
            "Iteration 131, loss = 0.19735836\n",
            "Iteration 132, loss = 0.19719764\n",
            "Iteration 133, loss = 0.19723657\n",
            "Iteration 134, loss = 0.19673618\n",
            "Iteration 135, loss = 0.19690596\n",
            "Iteration 136, loss = 0.19720406\n",
            "Iteration 137, loss = 0.19664321\n",
            "Iteration 138, loss = 0.19680919\n",
            "Iteration 139, loss = 0.19658525\n",
            "Iteration 140, loss = 0.19637149\n",
            "Iteration 141, loss = 0.19665997\n",
            "Iteration 142, loss = 0.19676289\n",
            "Iteration 143, loss = 0.19610537\n",
            "Iteration 144, loss = 0.19605705\n",
            "Iteration 145, loss = 0.19534730\n",
            "Iteration 146, loss = 0.19583755\n",
            "Iteration 147, loss = 0.19586683\n",
            "Iteration 148, loss = 0.19538878\n",
            "Iteration 149, loss = 0.19532750\n",
            "Iteration 150, loss = 0.19496877\n",
            "Iteration 151, loss = 0.19420698\n",
            "Iteration 152, loss = 0.19457788\n",
            "Iteration 153, loss = 0.19494691\n",
            "Iteration 154, loss = 0.19475446\n",
            "Iteration 155, loss = 0.19458508\n",
            "Iteration 156, loss = 0.19506576\n",
            "Iteration 157, loss = 0.19417267\n",
            "Iteration 158, loss = 0.19456728\n",
            "Iteration 159, loss = 0.19378310\n",
            "Iteration 160, loss = 0.19399651\n",
            "Iteration 161, loss = 0.19386932\n",
            "Iteration 162, loss = 0.19388956\n",
            "Iteration 163, loss = 0.19368107\n",
            "Iteration 164, loss = 0.19360208\n",
            "Iteration 165, loss = 0.19297156\n",
            "Iteration 166, loss = 0.19290260\n",
            "Iteration 167, loss = 0.19308017\n",
            "Iteration 168, loss = 0.19348123\n",
            "Iteration 169, loss = 0.19336442\n",
            "Iteration 170, loss = 0.19306071\n",
            "Iteration 171, loss = 0.19243637\n",
            "Iteration 172, loss = 0.19237688\n",
            "Iteration 173, loss = 0.19224134\n",
            "Iteration 174, loss = 0.19245539\n",
            "Iteration 175, loss = 0.19283733\n",
            "Iteration 176, loss = 0.19208650\n",
            "Iteration 177, loss = 0.19285020\n",
            "Iteration 178, loss = 0.19167539\n",
            "Iteration 179, loss = 0.19214362\n",
            "Iteration 180, loss = 0.19223122\n",
            "Iteration 181, loss = 0.19191050\n",
            "Iteration 182, loss = 0.19223804\n",
            "Iteration 183, loss = 0.19154604\n",
            "Iteration 184, loss = 0.19161542\n",
            "Iteration 185, loss = 0.19218748\n",
            "Iteration 186, loss = 0.19167630\n",
            "Iteration 187, loss = 0.19011534\n",
            "Iteration 188, loss = 0.19208591\n",
            "Iteration 189, loss = 0.19181819\n",
            "Iteration 190, loss = 0.19117404\n",
            "Iteration 191, loss = 0.19205402\n",
            "Iteration 192, loss = 0.19079016\n",
            "Iteration 193, loss = 0.19069167\n",
            "Iteration 194, loss = 0.19048708\n",
            "Iteration 195, loss = 0.19084693\n",
            "Iteration 196, loss = 0.19177593\n",
            "Iteration 197, loss = 0.19071344\n",
            "Iteration 198, loss = 0.18964917\n",
            "Iteration 199, loss = 0.19075237\n",
            "Iteration 200, loss = 0.19124959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.23329554\n",
            "Iteration 2, loss = 0.36813026\n",
            "Iteration 3, loss = 0.35301100\n",
            "Iteration 4, loss = 0.34012332\n",
            "Iteration 5, loss = 0.32835271\n",
            "Iteration 6, loss = 0.31845115\n",
            "Iteration 7, loss = 0.31054638\n",
            "Iteration 8, loss = 0.30116806\n",
            "Iteration 9, loss = 0.29387508\n",
            "Iteration 10, loss = 0.28788127\n",
            "Iteration 11, loss = 0.27979397\n",
            "Iteration 12, loss = 0.27581400\n",
            "Iteration 13, loss = 0.27101117\n",
            "Iteration 14, loss = 0.26593350\n",
            "Iteration 15, loss = 0.26033704\n",
            "Iteration 16, loss = 0.25529269\n",
            "Iteration 17, loss = 0.25380143\n",
            "Iteration 18, loss = 0.24910163\n",
            "Iteration 19, loss = 0.24671611\n",
            "Iteration 20, loss = 0.24219250\n",
            "Iteration 21, loss = 0.23967001\n",
            "Iteration 22, loss = 0.23911871\n",
            "Iteration 23, loss = 0.23660497\n",
            "Iteration 24, loss = 0.23258370\n",
            "Iteration 25, loss = 0.23055756\n",
            "Iteration 26, loss = 0.22838297\n",
            "Iteration 27, loss = 0.22949480\n",
            "Iteration 28, loss = 0.22506935\n",
            "Iteration 29, loss = 0.22427969\n",
            "Iteration 30, loss = 0.22305618\n",
            "Iteration 31, loss = 0.22213493\n",
            "Iteration 32, loss = 0.22107056\n",
            "Iteration 33, loss = 0.21853062\n",
            "Iteration 34, loss = 0.21703140\n",
            "Iteration 35, loss = 0.21726708\n",
            "Iteration 36, loss = 0.21521273\n",
            "Iteration 37, loss = 0.21804616\n",
            "Iteration 38, loss = 0.21524279\n",
            "Iteration 39, loss = 0.21471375\n",
            "Iteration 40, loss = 0.21222688\n",
            "Iteration 41, loss = 0.21301308\n",
            "Iteration 42, loss = 0.21302568\n",
            "Iteration 43, loss = 0.21040331\n",
            "Iteration 44, loss = 0.21227683\n",
            "Iteration 45, loss = 0.21260274\n",
            "Iteration 46, loss = 0.21374604\n",
            "Iteration 47, loss = 0.21002115\n",
            "Iteration 48, loss = 0.20986936\n",
            "Iteration 49, loss = 0.21031305\n",
            "Iteration 50, loss = 0.21079397\n",
            "Iteration 51, loss = 0.20797686\n",
            "Iteration 52, loss = 0.21135585\n",
            "Iteration 53, loss = 0.21007121\n",
            "Iteration 54, loss = 0.20973819\n",
            "Iteration 55, loss = 0.20856448\n",
            "Iteration 56, loss = 0.20898341\n",
            "Iteration 57, loss = 0.21038867\n",
            "Iteration 58, loss = 0.20784478\n",
            "Iteration 59, loss = 0.20987743\n",
            "Iteration 60, loss = 0.21100393\n",
            "Iteration 61, loss = 0.20665953\n",
            "Iteration 62, loss = 0.20866425\n",
            "Iteration 63, loss = 0.20779585\n",
            "Iteration 64, loss = 0.20947744\n",
            "Iteration 65, loss = 0.20703993\n",
            "Iteration 66, loss = 0.21054968\n",
            "Iteration 67, loss = 0.20892906\n",
            "Iteration 68, loss = 0.20652667\n",
            "Iteration 69, loss = 0.20693871\n",
            "Iteration 70, loss = 0.21059596\n",
            "Iteration 71, loss = 0.20709528\n",
            "Iteration 72, loss = 0.20877936\n",
            "Iteration 73, loss = 0.20910510\n",
            "Iteration 74, loss = 0.20615418\n",
            "Iteration 75, loss = 0.20890584\n",
            "Iteration 76, loss = 0.20771240\n",
            "Iteration 77, loss = 0.20678056\n",
            "Iteration 78, loss = 0.20490385\n",
            "Iteration 79, loss = 0.20825390\n",
            "Iteration 80, loss = 0.20485584\n",
            "Iteration 81, loss = 0.20950163\n",
            "Iteration 82, loss = 0.20494063\n",
            "Iteration 83, loss = 0.21218925\n",
            "Iteration 84, loss = 0.20445300\n",
            "Iteration 85, loss = 0.20657576\n",
            "Iteration 86, loss = 0.20535347\n",
            "Iteration 87, loss = 0.20892683\n",
            "Iteration 88, loss = 0.20568911\n",
            "Iteration 89, loss = 0.20697813\n",
            "Iteration 90, loss = 0.20652128\n",
            "Iteration 91, loss = 0.20629772\n",
            "Iteration 92, loss = 0.20658286\n",
            "Iteration 93, loss = 0.20721628\n",
            "Iteration 94, loss = 0.20747328\n",
            "Iteration 95, loss = 0.20821529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 96, loss = 0.19940392\n",
            "Iteration 97, loss = 0.19757662\n",
            "Iteration 98, loss = 0.19753209\n",
            "Iteration 99, loss = 0.19712787\n",
            "Iteration 100, loss = 0.19729924\n",
            "Iteration 101, loss = 0.19652355\n",
            "Iteration 102, loss = 0.19618122\n",
            "Iteration 103, loss = 0.19636395\n",
            "Iteration 104, loss = 0.19515210\n",
            "Iteration 105, loss = 0.19531387\n",
            "Iteration 106, loss = 0.19537831\n",
            "Iteration 107, loss = 0.19474217\n",
            "Iteration 108, loss = 0.19450219\n",
            "Iteration 109, loss = 0.19456561\n",
            "Iteration 110, loss = 0.19424682\n",
            "Iteration 111, loss = 0.19410918\n",
            "Iteration 112, loss = 0.19431650\n",
            "Iteration 113, loss = 0.19368413\n",
            "Iteration 114, loss = 0.19342203\n",
            "Iteration 115, loss = 0.19394926\n",
            "Iteration 116, loss = 0.19333808\n",
            "Iteration 117, loss = 0.19362811\n",
            "Iteration 118, loss = 0.19258154\n",
            "Iteration 119, loss = 0.19304041\n",
            "Iteration 120, loss = 0.19253560\n",
            "Iteration 121, loss = 0.19202083\n",
            "Iteration 122, loss = 0.19212759\n",
            "Iteration 123, loss = 0.19206475\n",
            "Iteration 124, loss = 0.19173989\n",
            "Iteration 125, loss = 0.19192719\n",
            "Iteration 126, loss = 0.19135648\n",
            "Iteration 127, loss = 0.19153477\n",
            "Iteration 128, loss = 0.19120692\n",
            "Iteration 129, loss = 0.19071767\n",
            "Iteration 130, loss = 0.19085648\n",
            "Iteration 131, loss = 0.19099640\n",
            "Iteration 132, loss = 0.19034997\n",
            "Iteration 133, loss = 0.19064336\n",
            "Iteration 134, loss = 0.19048550\n",
            "Iteration 135, loss = 0.19080265\n",
            "Iteration 136, loss = 0.19068426\n",
            "Iteration 137, loss = 0.18991819\n",
            "Iteration 138, loss = 0.18925813\n",
            "Iteration 139, loss = 0.18929699\n",
            "Iteration 140, loss = 0.18956799\n",
            "Iteration 141, loss = 0.18938180\n",
            "Iteration 142, loss = 0.18914440\n",
            "Iteration 143, loss = 0.18823972\n",
            "Iteration 144, loss = 0.18896956\n",
            "Iteration 145, loss = 0.18770104\n",
            "Iteration 146, loss = 0.18785329\n",
            "Iteration 147, loss = 0.18916527\n",
            "Iteration 148, loss = 0.18820707\n",
            "Iteration 149, loss = 0.18809565\n",
            "Iteration 150, loss = 0.18843884\n",
            "Iteration 151, loss = 0.18824027\n",
            "Iteration 152, loss = 0.18698084\n",
            "Iteration 153, loss = 0.18660796\n",
            "Iteration 154, loss = 0.18786007\n",
            "Iteration 155, loss = 0.18856402\n",
            "Iteration 156, loss = 0.18722326\n",
            "Iteration 157, loss = 0.18766396\n",
            "Iteration 158, loss = 0.18679772\n",
            "Iteration 159, loss = 0.18717780\n",
            "Iteration 160, loss = 0.18732697\n",
            "Iteration 161, loss = 0.18573896\n",
            "Iteration 162, loss = 0.18581935\n",
            "Iteration 163, loss = 0.18647292\n",
            "Iteration 164, loss = 0.18686331\n",
            "Iteration 165, loss = 0.18510052\n",
            "Iteration 166, loss = 0.18710177\n",
            "Iteration 167, loss = 0.18793601\n",
            "Iteration 168, loss = 0.18549027\n",
            "Iteration 169, loss = 0.18527096\n",
            "Iteration 170, loss = 0.18611285\n",
            "Iteration 171, loss = 0.18530629\n",
            "Iteration 172, loss = 0.18596035\n",
            "Iteration 173, loss = 0.18624838\n",
            "Iteration 174, loss = 0.18385042\n",
            "Iteration 175, loss = 0.18621719\n",
            "Iteration 176, loss = 0.18696955\n",
            "Iteration 177, loss = 0.18466738\n",
            "Iteration 178, loss = 0.18496453\n",
            "Iteration 179, loss = 0.18658295\n",
            "Iteration 180, loss = 0.18550743\n",
            "Iteration 181, loss = 0.18499610\n",
            "Iteration 182, loss = 0.18553274\n",
            "Iteration 183, loss = 0.18419140\n",
            "Iteration 184, loss = 0.18362101\n",
            "Iteration 185, loss = 0.18449762\n",
            "Iteration 186, loss = 0.18377250\n",
            "Iteration 187, loss = 0.18249167\n",
            "Iteration 188, loss = 0.18244350\n",
            "Iteration 189, loss = 0.18538938\n",
            "Iteration 190, loss = 0.18349697\n",
            "Iteration 191, loss = 0.18372178\n",
            "Iteration 192, loss = 0.18487177\n",
            "Iteration 193, loss = 0.18441871\n",
            "Iteration 194, loss = 0.18407039\n",
            "Iteration 195, loss = 0.18587814\n",
            "Iteration 196, loss = 0.18375771\n",
            "Iteration 197, loss = 0.18404137\n",
            "Iteration 198, loss = 0.18297808\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 199, loss = 0.17803248\n",
            "Iteration 200, loss = 0.17836111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.35745437\n",
            "Iteration 3, loss = 0.34418097\n",
            "Iteration 4, loss = 0.33071521\n",
            "Iteration 5, loss = 0.31956480\n",
            "Iteration 6, loss = 0.30915753\n",
            "Iteration 7, loss = 0.30064552\n",
            "Iteration 8, loss = 0.29208241\n",
            "Iteration 9, loss = 0.28571879\n",
            "Iteration 10, loss = 0.28035816\n",
            "Iteration 11, loss = 0.27337590\n",
            "Iteration 12, loss = 0.26865666\n",
            "Iteration 13, loss = 0.26323284\n",
            "Iteration 14, loss = 0.25905480\n",
            "Iteration 15, loss = 0.25490576\n",
            "Iteration 16, loss = 0.25071615\n",
            "Iteration 17, loss = 0.24791789\n",
            "Iteration 18, loss = 0.24403673\n",
            "Iteration 19, loss = 0.24162491\n",
            "Iteration 20, loss = 0.23821675\n",
            "Iteration 21, loss = 0.23594685\n",
            "Iteration 22, loss = 0.23255731\n",
            "Iteration 23, loss = 0.22977542\n",
            "Iteration 24, loss = 0.22980664\n",
            "Iteration 25, loss = 0.22730045\n",
            "Iteration 26, loss = 0.22584676\n",
            "Iteration 27, loss = 0.22386479\n",
            "Iteration 28, loss = 0.22313133\n",
            "Iteration 29, loss = 0.22043626\n",
            "Iteration 30, loss = 0.21976833\n",
            "Iteration 31, loss = 0.21959965\n",
            "Iteration 32, loss = 0.21821224\n",
            "Iteration 33, loss = 0.21697471\n",
            "Iteration 34, loss = 0.21838098\n",
            "Iteration 35, loss = 0.21316408\n",
            "Iteration 36, loss = 0.21435186\n",
            "Iteration 37, loss = 0.21505946\n",
            "Iteration 38, loss = 0.21249116\n",
            "Iteration 39, loss = 0.21116003\n",
            "Iteration 40, loss = 0.21373737\n",
            "Iteration 41, loss = 0.21143171\n",
            "Iteration 42, loss = 0.21140521\n",
            "Iteration 43, loss = 0.21004031\n",
            "Iteration 44, loss = 0.20962497\n",
            "Iteration 45, loss = 0.20981166\n",
            "Iteration 46, loss = 0.20889920\n",
            "Iteration 47, loss = 0.20919483\n",
            "Iteration 48, loss = 0.20911995\n",
            "Iteration 49, loss = 0.20856625\n",
            "Iteration 50, loss = 0.20794682\n",
            "Iteration 51, loss = 0.20962541\n",
            "Iteration 52, loss = 0.20855030\n",
            "Iteration 53, loss = 0.20581140\n",
            "Iteration 54, loss = 0.20615621\n",
            "Iteration 55, loss = 0.20821091\n",
            "Iteration 56, loss = 0.20693969\n",
            "Iteration 57, loss = 0.20838182\n",
            "Iteration 58, loss = 0.20739334\n",
            "Iteration 59, loss = 0.20696758\n",
            "Iteration 60, loss = 0.20764724\n",
            "Iteration 61, loss = 0.20544178\n",
            "Iteration 62, loss = 0.20916421\n",
            "Iteration 63, loss = 0.20547030\n",
            "Iteration 64, loss = 0.21010176\n",
            "Iteration 65, loss = 0.20531902\n",
            "Iteration 66, loss = 0.20632866\n",
            "Iteration 67, loss = 0.20784784\n",
            "Iteration 68, loss = 0.20608437\n",
            "Iteration 69, loss = 0.20770052\n",
            "Iteration 70, loss = 0.20632419\n",
            "Iteration 71, loss = 0.20700068\n",
            "Iteration 72, loss = 0.20734812\n",
            "Iteration 73, loss = 0.20476383\n",
            "Iteration 74, loss = 0.20784279\n",
            "Iteration 75, loss = 0.20684200\n",
            "Iteration 76, loss = 0.20598967\n",
            "Iteration 77, loss = 0.20758332\n",
            "Iteration 78, loss = 0.20296466\n",
            "Iteration 79, loss = 0.20598162\n",
            "Iteration 80, loss = 0.20774922\n",
            "Iteration 81, loss = 0.20664828\n",
            "Iteration 82, loss = 0.20890831\n",
            "Iteration 83, loss = 0.20701006\n",
            "Iteration 84, loss = 0.20698201\n",
            "Iteration 85, loss = 0.20345875\n",
            "Iteration 86, loss = 0.20838144\n",
            "Iteration 87, loss = 0.20822933\n",
            "Iteration 88, loss = 0.20783906\n",
            "Iteration 89, loss = 0.20499763\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 90, loss = 0.19789826\n",
            "Iteration 91, loss = 0.19689454\n",
            "Iteration 92, loss = 0.19644442\n",
            "Iteration 93, loss = 0.19612149\n",
            "Iteration 94, loss = 0.19611874\n",
            "Iteration 95, loss = 0.19595415\n",
            "Iteration 96, loss = 0.19561855\n",
            "Iteration 97, loss = 0.19510866\n",
            "Iteration 98, loss = 0.19487476\n",
            "Iteration 99, loss = 0.19496900\n",
            "Iteration 100, loss = 0.19456743\n",
            "Iteration 101, loss = 0.19406653\n",
            "Iteration 102, loss = 0.19424996\n",
            "Iteration 103, loss = 0.19347254\n",
            "Iteration 104, loss = 0.19326784\n",
            "Iteration 105, loss = 0.19311947\n",
            "Iteration 106, loss = 0.19381587\n",
            "Iteration 107, loss = 0.19332021\n",
            "Iteration 108, loss = 0.19311022\n",
            "Iteration 109, loss = 0.19230255\n",
            "Iteration 110, loss = 0.19272895\n",
            "Iteration 111, loss = 0.19268817\n",
            "Iteration 112, loss = 0.19252748\n",
            "Iteration 113, loss = 0.19204585\n",
            "Iteration 114, loss = 0.19202372\n",
            "Iteration 115, loss = 0.19246468\n",
            "Iteration 116, loss = 0.19178886\n",
            "Iteration 117, loss = 0.19205547\n",
            "Iteration 118, loss = 0.19176184\n",
            "Iteration 119, loss = 0.19098637\n",
            "Iteration 120, loss = 0.19070263\n",
            "Iteration 121, loss = 0.19045920\n",
            "Iteration 122, loss = 0.19139694\n",
            "Iteration 123, loss = 0.19085745\n",
            "Iteration 124, loss = 0.19047822\n",
            "Iteration 125, loss = 0.19040785\n",
            "Iteration 126, loss = 0.18945228\n",
            "Iteration 127, loss = 0.18977003\n",
            "Iteration 128, loss = 0.18965279\n",
            "Iteration 129, loss = 0.18952437\n",
            "Iteration 130, loss = 0.18874920\n",
            "Iteration 131, loss = 0.18938544\n",
            "Iteration 132, loss = 0.18926146\n",
            "Iteration 133, loss = 0.18903156\n",
            "Iteration 134, loss = 0.18882072\n",
            "Iteration 135, loss = 0.18815997\n",
            "Iteration 136, loss = 0.18809488\n",
            "Iteration 137, loss = 0.18844411\n",
            "Iteration 138, loss = 0.18851930\n",
            "Iteration 139, loss = 0.18904678\n",
            "Iteration 140, loss = 0.18796335\n",
            "Iteration 141, loss = 0.18735527\n",
            "Iteration 142, loss = 0.18776115\n",
            "Iteration 143, loss = 0.18809968\n",
            "Iteration 144, loss = 0.18694633\n",
            "Iteration 145, loss = 0.18703922\n",
            "Iteration 146, loss = 0.18731853\n",
            "Iteration 147, loss = 0.18695250\n",
            "Iteration 148, loss = 0.18702561\n",
            "Iteration 149, loss = 0.18730699\n",
            "Iteration 150, loss = 0.18636792\n",
            "Iteration 151, loss = 0.18619687\n",
            "Iteration 152, loss = 0.18731664\n",
            "Iteration 153, loss = 0.18633926\n",
            "Iteration 154, loss = 0.18722459\n",
            "Iteration 155, loss = 0.18722413\n",
            "Iteration 156, loss = 0.18762042\n",
            "Iteration 157, loss = 0.18740235\n",
            "Iteration 158, loss = 0.18555503\n",
            "Iteration 159, loss = 0.18577378\n",
            "Iteration 160, loss = 0.18518826\n",
            "Iteration 161, loss = 0.18503757\n",
            "Iteration 162, loss = 0.18549800\n",
            "Iteration 163, loss = 0.18538330\n",
            "Iteration 164, loss = 0.18615965\n",
            "Iteration 165, loss = 0.18587690\n",
            "Iteration 166, loss = 0.18534939\n",
            "Iteration 167, loss = 0.18482060\n",
            "Iteration 168, loss = 0.18541678\n",
            "Iteration 169, loss = 0.18610935\n",
            "Iteration 170, loss = 0.18437994\n",
            "Iteration 171, loss = 0.18461366\n",
            "Iteration 172, loss = 0.18468722\n",
            "Iteration 173, loss = 0.18385460\n",
            "Iteration 174, loss = 0.18584870\n",
            "Iteration 175, loss = 0.18391341\n",
            "Iteration 176, loss = 0.18289595\n",
            "Iteration 177, loss = 0.18498725\n",
            "Iteration 178, loss = 0.18613463\n",
            "Iteration 179, loss = 0.18581878\n",
            "Iteration 180, loss = 0.18303311\n",
            "Iteration 181, loss = 0.18339119\n",
            "Iteration 182, loss = 0.18522154\n",
            "Iteration 183, loss = 0.18646094\n",
            "Iteration 184, loss = 0.18390577\n",
            "Iteration 185, loss = 0.18484370\n",
            "Iteration 186, loss = 0.18365318\n",
            "Iteration 187, loss = 0.18563390\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 188, loss = 0.17826346\n",
            "Iteration 189, loss = 0.17804590\n",
            "Iteration 190, loss = 0.17828349\n",
            "Iteration 191, loss = 0.17824341\n",
            "Iteration 192, loss = 0.17798122\n",
            "Iteration 193, loss = 0.17797472\n",
            "Iteration 194, loss = 0.17818613\n",
            "Iteration 195, loss = 0.17789384\n",
            "Iteration 196, loss = 0.17781162\n",
            "Iteration 197, loss = 0.17794011\n",
            "Iteration 198, loss = 0.17784743\n",
            "Iteration 199, loss = 0.17801243\n",
            "Iteration 200, loss = 0.17761905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.79094605\n",
            "Iteration 2, loss = 0.35880738\n",
            "Iteration 3, loss = 0.34451764\n",
            "Iteration 4, loss = 0.33248402\n",
            "Iteration 5, loss = 0.32219956\n",
            "Iteration 6, loss = 0.31307871\n",
            "Iteration 7, loss = 0.30443752\n",
            "Iteration 8, loss = 0.29643337\n",
            "Iteration 9, loss = 0.28871763\n",
            "Iteration 10, loss = 0.28305921\n",
            "Iteration 11, loss = 0.27755592\n",
            "Iteration 12, loss = 0.27010562\n",
            "Iteration 13, loss = 0.26608428\n",
            "Iteration 14, loss = 0.26055102\n",
            "Iteration 15, loss = 0.25694621\n",
            "Iteration 16, loss = 0.25146042\n",
            "Iteration 17, loss = 0.24891060\n",
            "Iteration 18, loss = 0.24465331\n",
            "Iteration 19, loss = 0.24353147\n",
            "Iteration 20, loss = 0.24012127\n",
            "Iteration 21, loss = 0.23608680\n",
            "Iteration 22, loss = 0.23247770\n",
            "Iteration 23, loss = 0.23210360\n",
            "Iteration 24, loss = 0.23059244\n",
            "Iteration 25, loss = 0.22746231\n",
            "Iteration 26, loss = 0.22679295\n",
            "Iteration 27, loss = 0.22445636\n",
            "Iteration 28, loss = 0.22481888\n",
            "Iteration 29, loss = 0.22159371\n",
            "Iteration 30, loss = 0.22112811\n",
            "Iteration 31, loss = 0.21949447\n",
            "Iteration 32, loss = 0.21899178\n",
            "Iteration 33, loss = 0.21661881\n",
            "Iteration 34, loss = 0.21653087\n",
            "Iteration 35, loss = 0.21691911\n",
            "Iteration 36, loss = 0.21338000\n",
            "Iteration 37, loss = 0.21441454\n",
            "Iteration 38, loss = 0.21298947\n",
            "Iteration 39, loss = 0.21212214\n",
            "Iteration 40, loss = 0.21189723\n",
            "Iteration 41, loss = 0.21025531\n",
            "Iteration 42, loss = 0.21269117\n",
            "Iteration 43, loss = 0.21057810\n",
            "Iteration 44, loss = 0.21055097\n",
            "Iteration 45, loss = 0.21153128\n",
            "Iteration 46, loss = 0.21056783\n",
            "Iteration 47, loss = 0.20929776\n",
            "Iteration 48, loss = 0.20831599\n",
            "Iteration 49, loss = 0.20850298\n",
            "Iteration 50, loss = 0.20917180\n",
            "Iteration 51, loss = 0.20725606\n",
            "Iteration 52, loss = 0.20786827\n",
            "Iteration 53, loss = 0.20712630\n",
            "Iteration 54, loss = 0.20915042\n",
            "Iteration 55, loss = 0.20947102\n",
            "Iteration 56, loss = 0.20384615\n",
            "Iteration 57, loss = 0.20580932\n",
            "Iteration 58, loss = 0.20759561\n",
            "Iteration 59, loss = 0.20714097\n",
            "Iteration 60, loss = 0.20754731\n",
            "Iteration 61, loss = 0.20685449\n",
            "Iteration 62, loss = 0.20611625\n",
            "Iteration 63, loss = 0.20853881\n",
            "Iteration 64, loss = 0.20734775\n",
            "Iteration 65, loss = 0.20733112\n",
            "Iteration 66, loss = 0.20644486\n",
            "Iteration 67, loss = 0.20841070\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.20071987\n",
            "Iteration 69, loss = 0.20053370\n",
            "Iteration 70, loss = 0.19984275\n",
            "Iteration 71, loss = 0.19957784\n",
            "Iteration 72, loss = 0.19986614\n",
            "Iteration 73, loss = 0.19945428\n",
            "Iteration 74, loss = 0.19858287\n",
            "Iteration 75, loss = 0.19860334\n",
            "Iteration 76, loss = 0.19857436\n",
            "Iteration 77, loss = 0.19834056\n",
            "Iteration 78, loss = 0.19796616\n",
            "Iteration 79, loss = 0.19785638\n",
            "Iteration 80, loss = 0.19814364\n",
            "Iteration 81, loss = 0.19705238\n",
            "Iteration 82, loss = 0.19780575\n",
            "Iteration 83, loss = 0.19678482\n",
            "Iteration 84, loss = 0.19703522\n",
            "Iteration 85, loss = 0.19662946\n",
            "Iteration 86, loss = 0.19693408\n",
            "Iteration 87, loss = 0.19591947\n",
            "Iteration 88, loss = 0.19620692\n",
            "Iteration 89, loss = 0.19575314\n",
            "Iteration 90, loss = 0.19587087\n",
            "Iteration 91, loss = 0.19588186\n",
            "Iteration 92, loss = 0.19528281\n",
            "Iteration 93, loss = 0.19545556\n",
            "Iteration 94, loss = 0.19487658\n",
            "Iteration 95, loss = 0.19535905\n",
            "Iteration 96, loss = 0.19475374\n",
            "Iteration 97, loss = 0.19431884\n",
            "Iteration 98, loss = 0.19428449\n",
            "Iteration 99, loss = 0.19453540\n",
            "Iteration 100, loss = 0.19387954\n",
            "Iteration 101, loss = 0.19420099\n",
            "Iteration 102, loss = 0.19328505\n",
            "Iteration 103, loss = 0.19409496\n",
            "Iteration 104, loss = 0.19360839\n",
            "Iteration 105, loss = 0.19298952\n",
            "Iteration 106, loss = 0.19350671\n",
            "Iteration 107, loss = 0.19359159\n",
            "Iteration 108, loss = 0.19306151\n",
            "Iteration 109, loss = 0.19287357\n",
            "Iteration 110, loss = 0.19265911\n",
            "Iteration 111, loss = 0.19249162\n",
            "Iteration 112, loss = 0.19236563\n",
            "Iteration 113, loss = 0.19188046\n",
            "Iteration 114, loss = 0.19178294\n",
            "Iteration 115, loss = 0.19225954\n",
            "Iteration 116, loss = 0.19184521\n",
            "Iteration 117, loss = 0.19220449\n",
            "Iteration 118, loss = 0.19184263\n",
            "Iteration 119, loss = 0.19118824\n",
            "Iteration 120, loss = 0.19077623\n",
            "Iteration 121, loss = 0.19080056\n",
            "Iteration 122, loss = 0.19096869\n",
            "Iteration 123, loss = 0.19012501\n",
            "Iteration 124, loss = 0.19024764\n",
            "Iteration 125, loss = 0.19015303\n",
            "Iteration 126, loss = 0.18965073\n",
            "Iteration 127, loss = 0.19027056\n",
            "Iteration 128, loss = 0.18985777\n",
            "Iteration 129, loss = 0.18959131\n",
            "Iteration 130, loss = 0.18938741\n",
            "Iteration 131, loss = 0.18998552\n",
            "Iteration 132, loss = 0.18931121\n",
            "Iteration 133, loss = 0.18891727\n",
            "Iteration 134, loss = 0.19006880\n",
            "Iteration 135, loss = 0.18942999\n",
            "Iteration 136, loss = 0.18914345\n",
            "Iteration 137, loss = 0.18827087\n",
            "Iteration 138, loss = 0.18818131\n",
            "Iteration 139, loss = 0.18982896\n",
            "Iteration 140, loss = 0.18790285\n",
            "Iteration 141, loss = 0.18805274\n",
            "Iteration 142, loss = 0.18750115\n",
            "Iteration 143, loss = 0.18835846\n",
            "Iteration 144, loss = 0.18812196\n",
            "Iteration 145, loss = 0.18790005\n",
            "Iteration 146, loss = 0.18598860\n",
            "Iteration 147, loss = 0.18793846\n",
            "Iteration 148, loss = 0.18669061\n",
            "Iteration 149, loss = 0.18848181\n",
            "Iteration 150, loss = 0.18711050\n",
            "Iteration 151, loss = 0.18685031\n",
            "Iteration 152, loss = 0.18841274\n",
            "Iteration 153, loss = 0.18598582\n",
            "Iteration 154, loss = 0.18761351\n",
            "Iteration 155, loss = 0.18712043\n",
            "Iteration 156, loss = 0.18666239\n",
            "Iteration 157, loss = 0.18515358\n",
            "Iteration 158, loss = 0.18659863\n",
            "Iteration 159, loss = 0.18593222\n",
            "Iteration 160, loss = 0.18621591\n",
            "Iteration 161, loss = 0.18683641\n",
            "Iteration 162, loss = 0.18673792\n",
            "Iteration 163, loss = 0.18695742\n",
            "Iteration 164, loss = 0.18413289\n",
            "Iteration 165, loss = 0.18602320\n",
            "Iteration 166, loss = 0.18628213\n",
            "Iteration 167, loss = 0.18626815\n",
            "Iteration 168, loss = 0.18640343\n",
            "Iteration 169, loss = 0.18781389\n",
            "Iteration 170, loss = 0.18431644\n",
            "Iteration 171, loss = 0.18650683\n",
            "Iteration 172, loss = 0.18490737\n",
            "Iteration 173, loss = 0.18579602\n",
            "Iteration 174, loss = 0.18635509\n",
            "Iteration 175, loss = 0.18578682\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 176, loss = 0.17992128\n",
            "Iteration 177, loss = 0.17974617\n",
            "Iteration 178, loss = 0.17972506\n",
            "Iteration 179, loss = 0.17945384\n",
            "Iteration 180, loss = 0.17982102\n",
            "Iteration 181, loss = 0.17972656\n",
            "Iteration 182, loss = 0.17931040\n",
            "Iteration 183, loss = 0.17944920\n",
            "Iteration 184, loss = 0.17926258\n",
            "Iteration 185, loss = 0.17952508\n",
            "Iteration 186, loss = 0.17953938\n",
            "Iteration 187, loss = 0.17944435\n",
            "Iteration 188, loss = 0.17943777\n",
            "Iteration 189, loss = 0.17921222\n",
            "Iteration 190, loss = 0.17915569\n",
            "Iteration 191, loss = 0.17934082\n",
            "Iteration 192, loss = 0.17877613\n",
            "Iteration 193, loss = 0.17903560\n",
            "Iteration 194, loss = 0.17917757\n",
            "Iteration 195, loss = 0.17893116\n",
            "Iteration 196, loss = 0.17880037\n",
            "Iteration 197, loss = 0.17889282\n",
            "Iteration 198, loss = 0.17870745\n",
            "Iteration 199, loss = 0.17884606\n",
            "Iteration 200, loss = 0.17869126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.35505369\n",
            "Iteration 2, loss = 0.31973152\n",
            "Iteration 3, loss = 0.30863505\n",
            "Iteration 4, loss = 0.29886625\n",
            "Iteration 5, loss = 0.29030742\n",
            "Iteration 6, loss = 0.28291185\n",
            "Iteration 7, loss = 0.27675542\n",
            "Iteration 8, loss = 0.27107598\n",
            "Iteration 9, loss = 0.26614551\n",
            "Iteration 10, loss = 0.26032304\n",
            "Iteration 11, loss = 0.25679368\n",
            "Iteration 12, loss = 0.25250711\n",
            "Iteration 13, loss = 0.24867930\n",
            "Iteration 14, loss = 0.24651396\n",
            "Iteration 15, loss = 0.24050110\n",
            "Iteration 16, loss = 0.24215640\n",
            "Iteration 17, loss = 0.23648536\n",
            "Iteration 18, loss = 0.23440286\n",
            "Iteration 19, loss = 0.23174261\n",
            "Iteration 20, loss = 0.23068494\n",
            "Iteration 21, loss = 0.22765423\n",
            "Iteration 22, loss = 0.22803919\n",
            "Iteration 23, loss = 0.22390168\n",
            "Iteration 24, loss = 0.22366879\n",
            "Iteration 25, loss = 0.22184989\n",
            "Iteration 26, loss = 0.21963610\n",
            "Iteration 27, loss = 0.22164313\n",
            "Iteration 28, loss = 0.21907310\n",
            "Iteration 29, loss = 0.21644544\n",
            "Iteration 30, loss = 0.21665524\n",
            "Iteration 31, loss = 0.21506147\n",
            "Iteration 32, loss = 0.21620111\n",
            "Iteration 33, loss = 0.21505129\n",
            "Iteration 34, loss = 0.21241745\n",
            "Iteration 35, loss = 0.21241053\n",
            "Iteration 36, loss = 0.21141930\n",
            "Iteration 37, loss = 0.21225117\n",
            "Iteration 38, loss = 0.21000874\n",
            "Iteration 39, loss = 0.21169337\n",
            "Iteration 40, loss = 0.21202129\n",
            "Iteration 41, loss = 0.20902330\n",
            "Iteration 42, loss = 0.20966260\n",
            "Iteration 43, loss = 0.21060196\n",
            "Iteration 44, loss = 0.21048070\n",
            "Iteration 45, loss = 0.21003251\n",
            "Iteration 46, loss = 0.20732961\n",
            "Iteration 47, loss = 0.21044395\n",
            "Iteration 48, loss = 0.20895731\n",
            "Iteration 49, loss = 0.20658332\n",
            "Iteration 50, loss = 0.20828324\n",
            "Iteration 51, loss = 0.20822031\n",
            "Iteration 52, loss = 0.20619727\n",
            "Iteration 53, loss = 0.20653776\n",
            "Iteration 54, loss = 0.20792879\n",
            "Iteration 55, loss = 0.20747108\n",
            "Iteration 56, loss = 0.20902097\n",
            "Iteration 57, loss = 0.20569090\n",
            "Iteration 58, loss = 0.20576884\n",
            "Iteration 59, loss = 0.20614056\n",
            "Iteration 60, loss = 0.20605053\n",
            "Iteration 61, loss = 0.20552737\n",
            "Iteration 62, loss = 0.20506588\n",
            "Iteration 63, loss = 0.21025631\n",
            "Iteration 64, loss = 0.20727783\n",
            "Iteration 65, loss = 0.20564438\n",
            "Iteration 66, loss = 0.20622819\n",
            "Iteration 67, loss = 0.20404388\n",
            "Iteration 68, loss = 0.20685745\n",
            "Iteration 69, loss = 0.20539150\n",
            "Iteration 70, loss = 0.20587787\n",
            "Iteration 71, loss = 0.20796480\n",
            "Iteration 72, loss = 0.20705065\n",
            "Iteration 73, loss = 0.20763897\n",
            "Iteration 74, loss = 0.20493057\n",
            "Iteration 75, loss = 0.20741749\n",
            "Iteration 76, loss = 0.20597032\n",
            "Iteration 77, loss = 0.20518925\n",
            "Iteration 78, loss = 0.20829072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 79, loss = 0.20078591\n",
            "Iteration 80, loss = 0.19953753\n",
            "Iteration 81, loss = 0.19919559\n",
            "Iteration 82, loss = 0.19840800\n",
            "Iteration 83, loss = 0.19825043\n",
            "Iteration 84, loss = 0.19811546\n",
            "Iteration 85, loss = 0.19789051\n",
            "Iteration 86, loss = 0.19682205\n",
            "Iteration 87, loss = 0.19728586\n",
            "Iteration 88, loss = 0.19732782\n",
            "Iteration 89, loss = 0.19682058\n",
            "Iteration 90, loss = 0.19681834\n",
            "Iteration 91, loss = 0.19581818\n",
            "Iteration 92, loss = 0.19686892\n",
            "Iteration 93, loss = 0.19594050\n",
            "Iteration 94, loss = 0.19595084\n",
            "Iteration 95, loss = 0.19515553\n",
            "Iteration 96, loss = 0.19491011\n",
            "Iteration 97, loss = 0.19526733\n",
            "Iteration 98, loss = 0.19518918\n",
            "Iteration 99, loss = 0.19478607\n",
            "Iteration 100, loss = 0.19480148\n",
            "Iteration 101, loss = 0.19450710\n",
            "Iteration 102, loss = 0.19400361\n",
            "Iteration 103, loss = 0.19388724\n",
            "Iteration 104, loss = 0.19433613\n",
            "Iteration 105, loss = 0.19363843\n",
            "Iteration 106, loss = 0.19340302\n",
            "Iteration 107, loss = 0.19325921\n",
            "Iteration 108, loss = 0.19330555\n",
            "Iteration 109, loss = 0.19329590\n",
            "Iteration 110, loss = 0.19319040\n",
            "Iteration 111, loss = 0.19293931\n",
            "Iteration 112, loss = 0.19266882\n",
            "Iteration 113, loss = 0.19265038\n",
            "Iteration 114, loss = 0.19294327\n",
            "Iteration 115, loss = 0.19219708\n",
            "Iteration 116, loss = 0.19207589\n",
            "Iteration 117, loss = 0.19126774\n",
            "Iteration 118, loss = 0.19172094\n",
            "Iteration 119, loss = 0.19155635\n",
            "Iteration 120, loss = 0.19150627\n",
            "Iteration 121, loss = 0.19136690\n",
            "Iteration 122, loss = 0.19135000\n",
            "Iteration 123, loss = 0.19061962\n",
            "Iteration 124, loss = 0.19035815\n",
            "Iteration 125, loss = 0.19082462\n",
            "Iteration 126, loss = 0.19127505\n",
            "Iteration 127, loss = 0.18991938\n",
            "Iteration 128, loss = 0.19018730\n",
            "Iteration 129, loss = 0.19008510\n",
            "Iteration 130, loss = 0.18974284\n",
            "Iteration 131, loss = 0.18906999\n",
            "Iteration 132, loss = 0.19013091\n",
            "Iteration 133, loss = 0.18909376\n",
            "Iteration 134, loss = 0.18882814\n",
            "Iteration 135, loss = 0.19011766\n",
            "Iteration 136, loss = 0.18939834\n",
            "Iteration 137, loss = 0.18891670\n",
            "Iteration 138, loss = 0.18854319\n",
            "Iteration 139, loss = 0.18876187\n",
            "Iteration 140, loss = 0.18918929\n",
            "Iteration 141, loss = 0.18831205\n",
            "Iteration 142, loss = 0.18803386\n",
            "Iteration 143, loss = 0.18788864\n",
            "Iteration 144, loss = 0.18898192\n",
            "Iteration 145, loss = 0.18852431\n",
            "Iteration 146, loss = 0.18748344\n",
            "Iteration 147, loss = 0.18771234\n",
            "Iteration 148, loss = 0.18759294\n",
            "Iteration 149, loss = 0.18752228\n",
            "Iteration 150, loss = 0.18735951\n",
            "Iteration 151, loss = 0.18666900\n",
            "Iteration 152, loss = 0.18643512\n",
            "Iteration 153, loss = 0.18649879\n",
            "Iteration 154, loss = 0.18674855\n",
            "Iteration 155, loss = 0.18623524\n",
            "Iteration 156, loss = 0.18813648\n",
            "Iteration 157, loss = 0.18672332\n",
            "Iteration 158, loss = 0.18630369\n",
            "Iteration 159, loss = 0.18613279\n",
            "Iteration 160, loss = 0.18681985\n",
            "Iteration 161, loss = 0.18589426\n",
            "Iteration 162, loss = 0.18410348\n",
            "Iteration 163, loss = 0.18704365\n",
            "Iteration 164, loss = 0.18621676\n",
            "Iteration 165, loss = 0.18557951\n",
            "Iteration 166, loss = 0.18642356\n",
            "Iteration 167, loss = 0.18632646\n",
            "Iteration 168, loss = 0.18648222\n",
            "Iteration 169, loss = 0.18725755\n",
            "Iteration 170, loss = 0.18645172\n",
            "Iteration 171, loss = 0.18461783\n",
            "Iteration 172, loss = 0.18489947\n",
            "Iteration 173, loss = 0.18553306\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 174, loss = 0.18067254\n",
            "Iteration 175, loss = 0.18049170\n",
            "Iteration 176, loss = 0.18040376\n",
            "Iteration 177, loss = 0.18055721\n",
            "Iteration 178, loss = 0.18002136\n",
            "Iteration 179, loss = 0.18017300\n",
            "Iteration 180, loss = 0.18030187\n",
            "Iteration 181, loss = 0.18017346\n",
            "Iteration 182, loss = 0.18016857\n",
            "Iteration 183, loss = 0.18000187\n",
            "Iteration 184, loss = 0.18000483\n",
            "Iteration 185, loss = 0.17994790\n",
            "Iteration 186, loss = 0.17989232\n",
            "Iteration 187, loss = 0.17997353\n",
            "Iteration 188, loss = 0.17970994\n",
            "Iteration 189, loss = 0.17979969\n",
            "Iteration 190, loss = 0.17978651\n",
            "Iteration 191, loss = 0.17964764\n",
            "Iteration 192, loss = 0.17967609\n",
            "Iteration 193, loss = 0.17967638\n",
            "Iteration 194, loss = 0.17950058\n",
            "Iteration 195, loss = 0.17931560\n",
            "Iteration 196, loss = 0.17942187\n",
            "Iteration 197, loss = 0.17952287\n",
            "Iteration 198, loss = 0.17911745\n",
            "Iteration 199, loss = 0.17939099\n",
            "Iteration 200, loss = 0.17898775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.11266491\n",
            "Iteration 2, loss = 0.35958522\n",
            "Iteration 3, loss = 0.34551816\n",
            "Iteration 4, loss = 0.33342880\n",
            "Iteration 5, loss = 0.32271930\n",
            "Iteration 6, loss = 0.31396859\n",
            "Iteration 7, loss = 0.30525921\n",
            "Iteration 8, loss = 0.29817394\n",
            "Iteration 9, loss = 0.28995772\n",
            "Iteration 10, loss = 0.28353791\n",
            "Iteration 11, loss = 0.27631958\n",
            "Iteration 12, loss = 0.27192585\n",
            "Iteration 13, loss = 0.26668915\n",
            "Iteration 14, loss = 0.26199059\n",
            "Iteration 15, loss = 0.25741971\n",
            "Iteration 16, loss = 0.25262868\n",
            "Iteration 17, loss = 0.24945955\n",
            "Iteration 18, loss = 0.24550955\n",
            "Iteration 19, loss = 0.24328434\n",
            "Iteration 20, loss = 0.23928885\n",
            "Iteration 21, loss = 0.23696427\n",
            "Iteration 22, loss = 0.23392061\n",
            "Iteration 23, loss = 0.23196518\n",
            "Iteration 24, loss = 0.23096605\n",
            "Iteration 25, loss = 0.22778533\n",
            "Iteration 26, loss = 0.22765230\n",
            "Iteration 27, loss = 0.22386539\n",
            "Iteration 28, loss = 0.22332768\n",
            "Iteration 29, loss = 0.22163299\n",
            "Iteration 30, loss = 0.22105205\n",
            "Iteration 31, loss = 0.21761268\n",
            "Iteration 32, loss = 0.22035414\n",
            "Iteration 33, loss = 0.21724873\n",
            "Iteration 34, loss = 0.21614726\n",
            "Iteration 35, loss = 0.21683809\n",
            "Iteration 36, loss = 0.21600523\n",
            "Iteration 37, loss = 0.21356683\n",
            "Iteration 38, loss = 0.21268448\n",
            "Iteration 39, loss = 0.21363617\n",
            "Iteration 40, loss = 0.21000371\n",
            "Iteration 41, loss = 0.21117295\n",
            "Iteration 42, loss = 0.21320362\n",
            "Iteration 43, loss = 0.20904997\n",
            "Iteration 44, loss = 0.21090986\n",
            "Iteration 45, loss = 0.20964270\n",
            "Iteration 46, loss = 0.21066670\n",
            "Iteration 47, loss = 0.20891826\n",
            "Iteration 48, loss = 0.20944420\n",
            "Iteration 49, loss = 0.20948366\n",
            "Iteration 50, loss = 0.20804790\n",
            "Iteration 51, loss = 0.21001557\n",
            "Iteration 52, loss = 0.20847337\n",
            "Iteration 53, loss = 0.20649357\n",
            "Iteration 54, loss = 0.20644933\n",
            "Iteration 55, loss = 0.20779356\n",
            "Iteration 56, loss = 0.20839920\n",
            "Iteration 57, loss = 0.20671003\n",
            "Iteration 58, loss = 0.20683101\n",
            "Iteration 59, loss = 0.20715871\n",
            "Iteration 60, loss = 0.20738346\n",
            "Iteration 61, loss = 0.20630485\n",
            "Iteration 62, loss = 0.20675177\n",
            "Iteration 63, loss = 0.20658974\n",
            "Iteration 64, loss = 0.20990018\n",
            "Iteration 65, loss = 0.20749824\n",
            "Iteration 66, loss = 0.20545004\n",
            "Iteration 67, loss = 0.20570779\n",
            "Iteration 68, loss = 0.20792320\n",
            "Iteration 69, loss = 0.20675886\n",
            "Iteration 70, loss = 0.20606198\n",
            "Iteration 71, loss = 0.20620471\n",
            "Iteration 72, loss = 0.20806195\n",
            "Iteration 73, loss = 0.20880533\n",
            "Iteration 74, loss = 0.20252651\n",
            "Iteration 75, loss = 0.20825128\n",
            "Iteration 76, loss = 0.20368320\n",
            "Iteration 77, loss = 0.20600643\n",
            "Iteration 78, loss = 0.20919758\n",
            "Iteration 79, loss = 0.20532609\n",
            "Iteration 80, loss = 0.20703654\n",
            "Iteration 81, loss = 0.20519487\n",
            "Iteration 82, loss = 0.20467632\n",
            "Iteration 83, loss = 0.20758679\n",
            "Iteration 84, loss = 0.20414578\n",
            "Iteration 85, loss = 0.20529479\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 86, loss = 0.19686093\n",
            "Iteration 87, loss = 0.19608610\n",
            "Iteration 88, loss = 0.19591840\n",
            "Iteration 89, loss = 0.19553803\n",
            "Iteration 90, loss = 0.19564100\n",
            "Iteration 91, loss = 0.19547344\n",
            "Iteration 92, loss = 0.19522980\n",
            "Iteration 93, loss = 0.19503389\n",
            "Iteration 94, loss = 0.19483323\n",
            "Iteration 95, loss = 0.19453137\n",
            "Iteration 96, loss = 0.19398553\n",
            "Iteration 97, loss = 0.19430214\n",
            "Iteration 98, loss = 0.19413877\n",
            "Iteration 99, loss = 0.19417390\n",
            "Iteration 100, loss = 0.19385442\n",
            "Iteration 101, loss = 0.19381896\n",
            "Iteration 102, loss = 0.19271606\n",
            "Iteration 103, loss = 0.19328801\n",
            "Iteration 104, loss = 0.19303543\n",
            "Iteration 105, loss = 0.19307797\n",
            "Iteration 106, loss = 0.19293627\n",
            "Iteration 107, loss = 0.19236015\n",
            "Iteration 108, loss = 0.19233484\n",
            "Iteration 109, loss = 0.19229750\n",
            "Iteration 110, loss = 0.19156347\n",
            "Iteration 111, loss = 0.19224649\n",
            "Iteration 112, loss = 0.19161922\n",
            "Iteration 113, loss = 0.19162085\n",
            "Iteration 114, loss = 0.19150199\n",
            "Iteration 115, loss = 0.19108958\n",
            "Iteration 116, loss = 0.19102180\n",
            "Iteration 117, loss = 0.19120998\n",
            "Iteration 118, loss = 0.19144959\n",
            "Iteration 119, loss = 0.19011716\n",
            "Iteration 120, loss = 0.19030303\n",
            "Iteration 121, loss = 0.19046267\n",
            "Iteration 122, loss = 0.19002182\n",
            "Iteration 123, loss = 0.18993183\n",
            "Iteration 124, loss = 0.19003459\n",
            "Iteration 125, loss = 0.18994271\n",
            "Iteration 126, loss = 0.18952046\n",
            "Iteration 127, loss = 0.18982225\n",
            "Iteration 128, loss = 0.18947729\n",
            "Iteration 129, loss = 0.19016333\n",
            "Iteration 130, loss = 0.18885685\n",
            "Iteration 131, loss = 0.18825055\n",
            "Iteration 132, loss = 0.18864163\n",
            "Iteration 133, loss = 0.18821213\n",
            "Iteration 134, loss = 0.18801188\n",
            "Iteration 135, loss = 0.18872538\n",
            "Iteration 136, loss = 0.18801150\n",
            "Iteration 137, loss = 0.18801194\n",
            "Iteration 138, loss = 0.18824272\n",
            "Iteration 139, loss = 0.18845772\n",
            "Iteration 140, loss = 0.18896987\n",
            "Iteration 141, loss = 0.18721854\n",
            "Iteration 142, loss = 0.18698946\n",
            "Iteration 143, loss = 0.18854044\n",
            "Iteration 144, loss = 0.18697595\n",
            "Iteration 145, loss = 0.18739559\n",
            "Iteration 146, loss = 0.18676481\n",
            "Iteration 147, loss = 0.18663259\n",
            "Iteration 148, loss = 0.18845994\n",
            "Iteration 149, loss = 0.18712841\n",
            "Iteration 150, loss = 0.18671530\n",
            "Iteration 151, loss = 0.18701140\n",
            "Iteration 152, loss = 0.18604509\n",
            "Iteration 153, loss = 0.18598206\n",
            "Iteration 154, loss = 0.18675344\n",
            "Iteration 155, loss = 0.18524366\n",
            "Iteration 156, loss = 0.18582350\n",
            "Iteration 157, loss = 0.18609123\n",
            "Iteration 158, loss = 0.18573146\n",
            "Iteration 159, loss = 0.18672420\n",
            "Iteration 160, loss = 0.18630146\n",
            "Iteration 161, loss = 0.18413486\n",
            "Iteration 162, loss = 0.18527607\n",
            "Iteration 163, loss = 0.18585156\n",
            "Iteration 164, loss = 0.18426317\n",
            "Iteration 165, loss = 0.18491018\n",
            "Iteration 166, loss = 0.18452727\n",
            "Iteration 167, loss = 0.18584932\n",
            "Iteration 168, loss = 0.18513843\n",
            "Iteration 169, loss = 0.18458416\n",
            "Iteration 170, loss = 0.18476647\n",
            "Iteration 171, loss = 0.18553424\n",
            "Iteration 172, loss = 0.18524763\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 173, loss = 0.18000091\n",
            "Iteration 174, loss = 0.18009873\n",
            "Iteration 175, loss = 0.17995045\n",
            "Iteration 176, loss = 0.18011733\n",
            "Iteration 177, loss = 0.18028619\n",
            "Iteration 178, loss = 0.17984092\n",
            "Iteration 179, loss = 0.17953613\n",
            "Iteration 180, loss = 0.17944077\n",
            "Iteration 181, loss = 0.17968395\n",
            "Iteration 182, loss = 0.17946552\n",
            "Iteration 183, loss = 0.17962136\n",
            "Iteration 184, loss = 0.17934901\n",
            "Iteration 185, loss = 0.17945909\n",
            "Iteration 186, loss = 0.17909375\n",
            "Iteration 187, loss = 0.17952151\n",
            "Iteration 188, loss = 0.17930830\n",
            "Iteration 189, loss = 0.17904506\n",
            "Iteration 190, loss = 0.17917236\n",
            "Iteration 191, loss = 0.17899893\n",
            "Iteration 192, loss = 0.17949234\n",
            "Iteration 193, loss = 0.17954594\n",
            "Iteration 194, loss = 0.17963863\n",
            "Iteration 195, loss = 0.17887959\n",
            "Iteration 196, loss = 0.17872040\n",
            "Iteration 197, loss = 0.17876859\n",
            "Iteration 198, loss = 0.17891412\n",
            "Iteration 199, loss = 0.17898675\n",
            "Iteration 200, loss = 0.17884078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.09127831\n",
            "Iteration 2, loss = 0.37202844\n",
            "Iteration 3, loss = 0.35921934\n",
            "Iteration 4, loss = 0.34731400\n",
            "Iteration 5, loss = 0.33627598\n",
            "Iteration 6, loss = 0.32648113\n",
            "Iteration 7, loss = 0.31731257\n",
            "Iteration 8, loss = 0.30834589\n",
            "Iteration 9, loss = 0.30014750\n",
            "Iteration 10, loss = 0.29100594\n",
            "Iteration 11, loss = 0.28407713\n",
            "Iteration 12, loss = 0.27751805\n",
            "Iteration 13, loss = 0.27174216\n",
            "Iteration 14, loss = 0.26690188\n",
            "Iteration 15, loss = 0.26147208\n",
            "Iteration 16, loss = 0.25772142\n",
            "Iteration 17, loss = 0.25290520\n",
            "Iteration 18, loss = 0.24923686\n",
            "Iteration 19, loss = 0.24635770\n",
            "Iteration 20, loss = 0.24300349\n",
            "Iteration 21, loss = 0.24139118\n",
            "Iteration 22, loss = 0.23786719\n",
            "Iteration 23, loss = 0.23711355\n",
            "Iteration 24, loss = 0.23323303\n",
            "Iteration 25, loss = 0.23168657\n",
            "Iteration 26, loss = 0.22832654\n",
            "Iteration 27, loss = 0.22785156\n",
            "Iteration 28, loss = 0.22656847\n",
            "Iteration 29, loss = 0.22342070\n",
            "Iteration 30, loss = 0.22219994\n",
            "Iteration 31, loss = 0.22099318\n",
            "Iteration 32, loss = 0.21953088\n",
            "Iteration 33, loss = 0.21879068\n",
            "Iteration 34, loss = 0.21841809\n",
            "Iteration 35, loss = 0.21563310\n",
            "Iteration 36, loss = 0.21607525\n",
            "Iteration 37, loss = 0.21452346\n",
            "Iteration 38, loss = 0.21358332\n",
            "Iteration 39, loss = 0.21335701\n",
            "Iteration 40, loss = 0.21350147\n",
            "Iteration 41, loss = 0.21193380\n",
            "Iteration 42, loss = 0.21091692\n",
            "Iteration 43, loss = 0.21176322\n",
            "Iteration 44, loss = 0.21069151\n",
            "Iteration 45, loss = 0.20919705\n",
            "Iteration 46, loss = 0.21076825\n",
            "Iteration 47, loss = 0.21154663\n",
            "Iteration 48, loss = 0.20954499\n",
            "Iteration 49, loss = 0.20782723\n",
            "Iteration 50, loss = 0.20847011\n",
            "Iteration 51, loss = 0.21012099\n",
            "Iteration 52, loss = 0.20863280\n",
            "Iteration 53, loss = 0.20945674\n",
            "Iteration 54, loss = 0.20879214\n",
            "Iteration 55, loss = 0.20784303\n",
            "Iteration 56, loss = 0.20913936\n",
            "Iteration 57, loss = 0.20800006\n",
            "Iteration 58, loss = 0.20800229\n",
            "Iteration 59, loss = 0.20831877\n",
            "Iteration 60, loss = 0.20645835\n",
            "Iteration 61, loss = 0.20672153\n",
            "Iteration 62, loss = 0.20763090\n",
            "Iteration 63, loss = 0.20558914\n",
            "Iteration 64, loss = 0.20641295\n",
            "Iteration 65, loss = 0.20588638\n",
            "Iteration 66, loss = 0.20717548\n",
            "Iteration 67, loss = 0.20799443\n",
            "Iteration 68, loss = 0.20712495\n",
            "Iteration 69, loss = 0.20907415\n",
            "Iteration 70, loss = 0.20735836\n",
            "Iteration 71, loss = 0.20569028\n",
            "Iteration 72, loss = 0.20413446\n",
            "Iteration 73, loss = 0.20597391\n",
            "Iteration 74, loss = 0.20804446\n",
            "Iteration 75, loss = 0.20568804\n",
            "Iteration 76, loss = 0.20555305\n",
            "Iteration 77, loss = 0.20599833\n",
            "Iteration 78, loss = 0.20986809\n",
            "Iteration 79, loss = 0.20566257\n",
            "Iteration 80, loss = 0.20501248\n",
            "Iteration 81, loss = 0.20701978\n",
            "Iteration 82, loss = 0.20887338\n",
            "Iteration 83, loss = 0.20571706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 84, loss = 0.19786887\n",
            "Iteration 85, loss = 0.19759608\n",
            "Iteration 86, loss = 0.19696588\n",
            "Iteration 87, loss = 0.19676472\n",
            "Iteration 88, loss = 0.19624841\n",
            "Iteration 89, loss = 0.19628212\n",
            "Iteration 90, loss = 0.19597650\n",
            "Iteration 91, loss = 0.19589557\n",
            "Iteration 92, loss = 0.19580259\n",
            "Iteration 93, loss = 0.19523288\n",
            "Iteration 94, loss = 0.19553282\n",
            "Iteration 95, loss = 0.19520340\n",
            "Iteration 96, loss = 0.19536977\n",
            "Iteration 97, loss = 0.19535046\n",
            "Iteration 98, loss = 0.19420011\n",
            "Iteration 99, loss = 0.19386609\n",
            "Iteration 100, loss = 0.19428374\n",
            "Iteration 101, loss = 0.19398096\n",
            "Iteration 102, loss = 0.19436391\n",
            "Iteration 103, loss = 0.19379797\n",
            "Iteration 104, loss = 0.19347578\n",
            "Iteration 105, loss = 0.19331203\n",
            "Iteration 106, loss = 0.19327859\n",
            "Iteration 107, loss = 0.19307822\n",
            "Iteration 108, loss = 0.19279740\n",
            "Iteration 109, loss = 0.19294525\n",
            "Iteration 110, loss = 0.19236556\n",
            "Iteration 111, loss = 0.19258169\n",
            "Iteration 112, loss = 0.19261149\n",
            "Iteration 113, loss = 0.19199991\n",
            "Iteration 114, loss = 0.19160017\n",
            "Iteration 115, loss = 0.19154058\n",
            "Iteration 116, loss = 0.19187616\n",
            "Iteration 117, loss = 0.19204708\n",
            "Iteration 118, loss = 0.19190736\n",
            "Iteration 119, loss = 0.19094874\n",
            "Iteration 120, loss = 0.19107801\n",
            "Iteration 121, loss = 0.19082631\n",
            "Iteration 122, loss = 0.19106911\n",
            "Iteration 123, loss = 0.19080312\n",
            "Iteration 124, loss = 0.19025464\n",
            "Iteration 125, loss = 0.19030245\n",
            "Iteration 126, loss = 0.19078196\n",
            "Iteration 127, loss = 0.19057560\n",
            "Iteration 128, loss = 0.18947461\n",
            "Iteration 129, loss = 0.18916686\n",
            "Iteration 130, loss = 0.18924017\n",
            "Iteration 131, loss = 0.18985150\n",
            "Iteration 132, loss = 0.18979419\n",
            "Iteration 133, loss = 0.18936209\n",
            "Iteration 134, loss = 0.19001228\n",
            "Iteration 135, loss = 0.18930472\n",
            "Iteration 136, loss = 0.18931578\n",
            "Iteration 137, loss = 0.18909599\n",
            "Iteration 138, loss = 0.18868060\n",
            "Iteration 139, loss = 0.18936305\n",
            "Iteration 140, loss = 0.18823905\n",
            "Iteration 141, loss = 0.18728409\n",
            "Iteration 142, loss = 0.18855343\n",
            "Iteration 143, loss = 0.18803869\n",
            "Iteration 144, loss = 0.18755823\n",
            "Iteration 145, loss = 0.18754943\n",
            "Iteration 146, loss = 0.18757486\n",
            "Iteration 147, loss = 0.18724000\n",
            "Iteration 148, loss = 0.18748918\n",
            "Iteration 149, loss = 0.18680333\n",
            "Iteration 150, loss = 0.18697021\n",
            "Iteration 151, loss = 0.18723200\n",
            "Iteration 152, loss = 0.18649300\n",
            "Iteration 153, loss = 0.18855092\n",
            "Iteration 154, loss = 0.18669084\n",
            "Iteration 155, loss = 0.18672421\n",
            "Iteration 156, loss = 0.18641005\n",
            "Iteration 157, loss = 0.18733498\n",
            "Iteration 158, loss = 0.18712080\n",
            "Iteration 159, loss = 0.18617177\n",
            "Iteration 160, loss = 0.18650939\n",
            "Iteration 161, loss = 0.18707956\n",
            "Iteration 162, loss = 0.18663515\n",
            "Iteration 163, loss = 0.18633273\n",
            "Iteration 164, loss = 0.18495704\n",
            "Iteration 165, loss = 0.18570169\n",
            "Iteration 166, loss = 0.18619934\n",
            "Iteration 167, loss = 0.18617932\n",
            "Iteration 168, loss = 0.18646877\n",
            "Iteration 169, loss = 0.18540849\n",
            "Iteration 170, loss = 0.18612136\n",
            "Iteration 171, loss = 0.18506147\n",
            "Iteration 172, loss = 0.18472106\n",
            "Iteration 173, loss = 0.18570639\n",
            "Iteration 174, loss = 0.18552630\n",
            "Iteration 175, loss = 0.18655520\n",
            "Iteration 176, loss = 0.18610862\n",
            "Iteration 177, loss = 0.18563827\n",
            "Iteration 178, loss = 0.18634992\n",
            "Iteration 179, loss = 0.18399589\n",
            "Iteration 180, loss = 0.18682430\n",
            "Iteration 181, loss = 0.18525661\n",
            "Iteration 182, loss = 0.18562369\n",
            "Iteration 183, loss = 0.18543841\n",
            "Iteration 184, loss = 0.18482885\n",
            "Iteration 185, loss = 0.18402259\n",
            "Iteration 186, loss = 0.18524620\n",
            "Iteration 187, loss = 0.18438050\n",
            "Iteration 188, loss = 0.18503488\n",
            "Iteration 189, loss = 0.18639390\n",
            "Iteration 190, loss = 0.18488152\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 191, loss = 0.17907967\n",
            "Iteration 192, loss = 0.17865523\n",
            "Iteration 193, loss = 0.17861074\n",
            "Iteration 194, loss = 0.17868411\n",
            "Iteration 195, loss = 0.17843964\n",
            "Iteration 196, loss = 0.17835143\n",
            "Iteration 197, loss = 0.17825820\n",
            "Iteration 198, loss = 0.17794117\n",
            "Iteration 199, loss = 0.17829345\n",
            "Iteration 200, loss = 0.17805327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.20489044\n",
            "Iteration 2, loss = 0.34743921\n",
            "Iteration 3, loss = 0.33384245\n",
            "Iteration 4, loss = 0.32262559\n",
            "Iteration 5, loss = 0.31328587\n",
            "Iteration 6, loss = 0.30368095\n",
            "Iteration 7, loss = 0.29697546\n",
            "Iteration 8, loss = 0.28904851\n",
            "Iteration 9, loss = 0.28218736\n",
            "Iteration 10, loss = 0.27646379\n",
            "Iteration 11, loss = 0.27077426\n",
            "Iteration 12, loss = 0.26495170\n",
            "Iteration 13, loss = 0.26029321\n",
            "Iteration 14, loss = 0.25658325\n",
            "Iteration 15, loss = 0.25125095\n",
            "Iteration 16, loss = 0.24876716\n",
            "Iteration 17, loss = 0.24583212\n",
            "Iteration 18, loss = 0.24207728\n",
            "Iteration 19, loss = 0.23943305\n",
            "Iteration 20, loss = 0.23728714\n",
            "Iteration 21, loss = 0.23417882\n",
            "Iteration 22, loss = 0.23245954\n",
            "Iteration 23, loss = 0.22961292\n",
            "Iteration 24, loss = 0.22624255\n",
            "Iteration 25, loss = 0.22405206\n",
            "Iteration 26, loss = 0.22440544\n",
            "Iteration 27, loss = 0.22373013\n",
            "Iteration 28, loss = 0.22349569\n",
            "Iteration 29, loss = 0.22008082\n",
            "Iteration 30, loss = 0.22020380\n",
            "Iteration 31, loss = 0.21615864\n",
            "Iteration 32, loss = 0.21766593\n",
            "Iteration 33, loss = 0.21695242\n",
            "Iteration 34, loss = 0.21421885\n",
            "Iteration 35, loss = 0.21336280\n",
            "Iteration 36, loss = 0.21229949\n",
            "Iteration 37, loss = 0.21350170\n",
            "Iteration 38, loss = 0.21320703\n",
            "Iteration 39, loss = 0.21466672\n",
            "Iteration 40, loss = 0.21254939\n",
            "Iteration 41, loss = 0.21029047\n",
            "Iteration 42, loss = 0.21099033\n",
            "Iteration 43, loss = 0.21302031\n",
            "Iteration 44, loss = 0.20977618\n",
            "Iteration 45, loss = 0.20873539\n",
            "Iteration 46, loss = 0.20902319\n",
            "Iteration 47, loss = 0.20750904\n",
            "Iteration 48, loss = 0.20831234\n",
            "Iteration 49, loss = 0.20898477\n",
            "Iteration 50, loss = 0.20854314\n",
            "Iteration 51, loss = 0.20813327\n",
            "Iteration 52, loss = 0.20711897\n",
            "Iteration 53, loss = 0.21034519\n",
            "Iteration 54, loss = 0.20699714\n",
            "Iteration 55, loss = 0.20661407\n",
            "Iteration 56, loss = 0.21024759\n",
            "Iteration 57, loss = 0.20682279\n",
            "Iteration 58, loss = 0.20581423\n",
            "Iteration 59, loss = 0.20558095\n",
            "Iteration 60, loss = 0.20677294\n",
            "Iteration 61, loss = 0.20869898\n",
            "Iteration 62, loss = 0.20779621\n",
            "Iteration 63, loss = 0.20864963\n",
            "Iteration 64, loss = 0.20627725\n",
            "Iteration 65, loss = 0.20561357\n",
            "Iteration 66, loss = 0.20918309\n",
            "Iteration 67, loss = 0.20480625\n",
            "Iteration 68, loss = 0.20483981\n",
            "Iteration 69, loss = 0.20621647\n",
            "Iteration 70, loss = 0.20637991\n",
            "Iteration 71, loss = 0.20596514\n",
            "Iteration 72, loss = 0.20292024\n",
            "Iteration 73, loss = 0.20981780\n",
            "Iteration 74, loss = 0.20624333\n",
            "Iteration 75, loss = 0.20446482\n",
            "Iteration 76, loss = 0.20625339\n",
            "Iteration 77, loss = 0.20598981\n",
            "Iteration 78, loss = 0.20515866\n",
            "Iteration 79, loss = 0.20617728\n",
            "Iteration 80, loss = 0.20585377\n",
            "Iteration 81, loss = 0.20615897\n",
            "Iteration 82, loss = 0.20729777\n",
            "Iteration 83, loss = 0.20485033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 84, loss = 0.19886670\n",
            "Iteration 85, loss = 0.19638472\n",
            "Iteration 86, loss = 0.19697563\n",
            "Iteration 87, loss = 0.19613360\n",
            "Iteration 88, loss = 0.19621802\n",
            "Iteration 89, loss = 0.19538630\n",
            "Iteration 90, loss = 0.19511074\n",
            "Iteration 91, loss = 0.19479321\n",
            "Iteration 92, loss = 0.19468681\n",
            "Iteration 93, loss = 0.19478015\n",
            "Iteration 94, loss = 0.19457352\n",
            "Iteration 95, loss = 0.19410141\n",
            "Iteration 96, loss = 0.19395744\n",
            "Iteration 97, loss = 0.19385295\n",
            "Iteration 98, loss = 0.19404260\n",
            "Iteration 99, loss = 0.19371669\n",
            "Iteration 100, loss = 0.19366564\n",
            "Iteration 101, loss = 0.19313565\n",
            "Iteration 102, loss = 0.19313055\n",
            "Iteration 103, loss = 0.19302225\n",
            "Iteration 104, loss = 0.19235925\n",
            "Iteration 105, loss = 0.19243239\n",
            "Iteration 106, loss = 0.19266681\n",
            "Iteration 107, loss = 0.19217688\n",
            "Iteration 108, loss = 0.19208633\n",
            "Iteration 109, loss = 0.19213003\n",
            "Iteration 110, loss = 0.19164966\n",
            "Iteration 111, loss = 0.19153378\n",
            "Iteration 112, loss = 0.19142031\n",
            "Iteration 113, loss = 0.19102365\n",
            "Iteration 114, loss = 0.19101638\n",
            "Iteration 115, loss = 0.19129752\n",
            "Iteration 116, loss = 0.19109851\n",
            "Iteration 117, loss = 0.19069872\n",
            "Iteration 118, loss = 0.18984647\n",
            "Iteration 119, loss = 0.19015905\n",
            "Iteration 120, loss = 0.18984176\n",
            "Iteration 121, loss = 0.18959575\n",
            "Iteration 122, loss = 0.19005852\n",
            "Iteration 123, loss = 0.19029711\n",
            "Iteration 124, loss = 0.18961916\n",
            "Iteration 125, loss = 0.18974667\n",
            "Iteration 126, loss = 0.18892090\n",
            "Iteration 127, loss = 0.18922538\n",
            "Iteration 128, loss = 0.18900851\n",
            "Iteration 129, loss = 0.18906812\n",
            "Iteration 130, loss = 0.18927165\n",
            "Iteration 131, loss = 0.18891087\n",
            "Iteration 132, loss = 0.18837614\n",
            "Iteration 133, loss = 0.18864863\n",
            "Iteration 134, loss = 0.18770534\n",
            "Iteration 135, loss = 0.18777332\n",
            "Iteration 136, loss = 0.18803446\n",
            "Iteration 137, loss = 0.18767158\n",
            "Iteration 138, loss = 0.18763040\n",
            "Iteration 139, loss = 0.18681649\n",
            "Iteration 140, loss = 0.18797148\n",
            "Iteration 141, loss = 0.18789776\n",
            "Iteration 142, loss = 0.18685156\n",
            "Iteration 143, loss = 0.18764567\n",
            "Iteration 144, loss = 0.18712230\n",
            "Iteration 145, loss = 0.18732022\n",
            "Iteration 146, loss = 0.18636306\n",
            "Iteration 147, loss = 0.18582815\n",
            "Iteration 148, loss = 0.18599320\n",
            "Iteration 149, loss = 0.18653574\n",
            "Iteration 150, loss = 0.18599125\n",
            "Iteration 151, loss = 0.18615183\n",
            "Iteration 152, loss = 0.18688692\n",
            "Iteration 153, loss = 0.18562869\n",
            "Iteration 154, loss = 0.18720467\n",
            "Iteration 155, loss = 0.18660862\n",
            "Iteration 156, loss = 0.18543822\n",
            "Iteration 157, loss = 0.18642588\n",
            "Iteration 158, loss = 0.18563589\n",
            "Iteration 159, loss = 0.18617245\n",
            "Iteration 160, loss = 0.18468610\n",
            "Iteration 161, loss = 0.18642674\n",
            "Iteration 162, loss = 0.18483002\n",
            "Iteration 163, loss = 0.18432544\n",
            "Iteration 164, loss = 0.18497347\n",
            "Iteration 165, loss = 0.18508185\n",
            "Iteration 166, loss = 0.18654292\n",
            "Iteration 167, loss = 0.18553327\n",
            "Iteration 168, loss = 0.18459824\n",
            "Iteration 169, loss = 0.18546246\n",
            "Iteration 170, loss = 0.18442132\n",
            "Iteration 171, loss = 0.18494429\n",
            "Iteration 172, loss = 0.18411332\n",
            "Iteration 173, loss = 0.18464290\n",
            "Iteration 174, loss = 0.18472300\n",
            "Iteration 175, loss = 0.18320371\n",
            "Iteration 176, loss = 0.18483055\n",
            "Iteration 177, loss = 0.18679168\n",
            "Iteration 178, loss = 0.18640174\n",
            "Iteration 179, loss = 0.18393339\n",
            "Iteration 180, loss = 0.18363438\n",
            "Iteration 181, loss = 0.18807064\n",
            "Iteration 182, loss = 0.18397016\n",
            "Iteration 183, loss = 0.18428502\n",
            "Iteration 184, loss = 0.18276549\n",
            "Iteration 185, loss = 0.18354772\n",
            "Iteration 186, loss = 0.18243156\n",
            "Iteration 187, loss = 0.18485617\n",
            "Iteration 188, loss = 0.18348145\n",
            "Iteration 189, loss = 0.18198098\n",
            "Iteration 190, loss = 0.18641794\n",
            "Iteration 191, loss = 0.18433319\n",
            "Iteration 192, loss = 0.18435778\n",
            "Iteration 193, loss = 0.18628117\n",
            "Iteration 194, loss = 0.18706383\n",
            "Iteration 195, loss = 0.18462171\n",
            "Iteration 196, loss = 0.18352197\n",
            "Iteration 197, loss = 0.18534564\n",
            "Iteration 198, loss = 0.18461934\n",
            "Iteration 199, loss = 0.18305970\n",
            "Iteration 200, loss = 0.18202278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.22761415\n",
            "Iteration 2, loss = 0.30834858\n",
            "Iteration 3, loss = 0.29650972\n",
            "Iteration 4, loss = 0.28813262\n",
            "Iteration 5, loss = 0.28082929\n",
            "Iteration 6, loss = 0.27409847\n",
            "Iteration 7, loss = 0.26858014\n",
            "Iteration 8, loss = 0.26331582\n",
            "Iteration 9, loss = 0.25936638\n",
            "Iteration 10, loss = 0.25508368\n",
            "Iteration 11, loss = 0.25190475\n",
            "Iteration 12, loss = 0.24731183\n",
            "Iteration 13, loss = 0.24234353\n",
            "Iteration 14, loss = 0.24235780\n",
            "Iteration 15, loss = 0.24123809\n",
            "Iteration 16, loss = 0.23543696\n",
            "Iteration 17, loss = 0.23396786\n",
            "Iteration 18, loss = 0.23113690\n",
            "Iteration 19, loss = 0.23234379\n",
            "Iteration 20, loss = 0.22712420\n",
            "Iteration 21, loss = 0.22772435\n",
            "Iteration 22, loss = 0.22673280\n",
            "Iteration 23, loss = 0.22237701\n",
            "Iteration 24, loss = 0.22175598\n",
            "Iteration 25, loss = 0.22170873\n",
            "Iteration 26, loss = 0.21815430\n",
            "Iteration 27, loss = 0.21829436\n",
            "Iteration 28, loss = 0.21741467\n",
            "Iteration 29, loss = 0.21775151\n",
            "Iteration 30, loss = 0.21589435\n",
            "Iteration 31, loss = 0.21777123\n",
            "Iteration 32, loss = 0.21402840\n",
            "Iteration 33, loss = 0.21305970\n",
            "Iteration 34, loss = 0.21223720\n",
            "Iteration 35, loss = 0.21181738\n",
            "Iteration 36, loss = 0.21640289\n",
            "Iteration 37, loss = 0.21331301\n",
            "Iteration 38, loss = 0.21028178\n",
            "Iteration 39, loss = 0.21185491\n",
            "Iteration 40, loss = 0.21004007\n",
            "Iteration 41, loss = 0.21031942\n",
            "Iteration 42, loss = 0.21116516\n",
            "Iteration 43, loss = 0.20775670\n",
            "Iteration 44, loss = 0.20919770\n",
            "Iteration 45, loss = 0.21017361\n",
            "Iteration 46, loss = 0.20791133\n",
            "Iteration 47, loss = 0.20839456\n",
            "Iteration 48, loss = 0.20959358\n",
            "Iteration 49, loss = 0.20903457\n",
            "Iteration 50, loss = 0.20824159\n",
            "Iteration 51, loss = 0.21047267\n",
            "Iteration 52, loss = 0.20697919\n",
            "Iteration 53, loss = 0.20863555\n",
            "Iteration 54, loss = 0.20732557\n",
            "Iteration 55, loss = 0.20732780\n",
            "Iteration 56, loss = 0.20522972\n",
            "Iteration 57, loss = 0.20628196\n",
            "Iteration 58, loss = 0.20641776\n",
            "Iteration 59, loss = 0.20603192\n",
            "Iteration 60, loss = 0.21021880\n",
            "Iteration 61, loss = 0.20867231\n",
            "Iteration 62, loss = 0.20747508\n",
            "Iteration 63, loss = 0.20838268\n",
            "Iteration 64, loss = 0.20589075\n",
            "Iteration 65, loss = 0.20539808\n",
            "Iteration 66, loss = 0.20562680\n",
            "Iteration 67, loss = 0.20497107\n",
            "Iteration 68, loss = 0.21062380\n",
            "Iteration 69, loss = 0.20684783\n",
            "Iteration 70, loss = 0.20906045\n",
            "Iteration 71, loss = 0.20744939\n",
            "Iteration 72, loss = 0.20740208\n",
            "Iteration 73, loss = 0.20563467\n",
            "Iteration 74, loss = 0.20989695\n",
            "Iteration 75, loss = 0.20658923\n",
            "Iteration 76, loss = 0.20825345\n",
            "Iteration 77, loss = 0.20819669\n",
            "Iteration 78, loss = 0.20383126\n",
            "Iteration 79, loss = 0.20658645\n",
            "Iteration 80, loss = 0.20689849\n",
            "Iteration 81, loss = 0.20419920\n",
            "Iteration 82, loss = 0.20438456\n",
            "Iteration 83, loss = 0.20850466\n",
            "Iteration 84, loss = 0.20551618\n",
            "Iteration 85, loss = 0.20801541\n",
            "Iteration 86, loss = 0.20534165\n",
            "Iteration 87, loss = 0.20733443\n",
            "Iteration 88, loss = 0.20493915\n",
            "Iteration 89, loss = 0.20957281\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 90, loss = 0.19749045\n",
            "Iteration 91, loss = 0.19667231\n",
            "Iteration 92, loss = 0.19624838\n",
            "Iteration 93, loss = 0.19596210\n",
            "Iteration 94, loss = 0.19630618\n",
            "Iteration 95, loss = 0.19561754\n",
            "Iteration 96, loss = 0.19479548\n",
            "Iteration 97, loss = 0.19491793\n",
            "Iteration 98, loss = 0.19536387\n",
            "Iteration 99, loss = 0.19436245\n",
            "Iteration 100, loss = 0.19386655\n",
            "Iteration 101, loss = 0.19375804\n",
            "Iteration 102, loss = 0.19366066\n",
            "Iteration 103, loss = 0.19342568\n",
            "Iteration 104, loss = 0.19328346\n",
            "Iteration 105, loss = 0.19343040\n",
            "Iteration 106, loss = 0.19340742\n",
            "Iteration 107, loss = 0.19358858\n",
            "Iteration 108, loss = 0.19297763\n",
            "Iteration 109, loss = 0.19197704\n",
            "Iteration 110, loss = 0.19213390\n",
            "Iteration 111, loss = 0.19239353\n",
            "Iteration 112, loss = 0.19258359\n",
            "Iteration 113, loss = 0.19168173\n",
            "Iteration 114, loss = 0.19181361\n",
            "Iteration 115, loss = 0.19176777\n",
            "Iteration 116, loss = 0.19192964\n",
            "Iteration 117, loss = 0.19124622\n",
            "Iteration 118, loss = 0.19091938\n",
            "Iteration 119, loss = 0.19135322\n",
            "Iteration 120, loss = 0.19036524\n",
            "Iteration 121, loss = 0.19051855\n",
            "Iteration 122, loss = 0.19011266\n",
            "Iteration 123, loss = 0.18991853\n",
            "Iteration 124, loss = 0.18968281\n",
            "Iteration 125, loss = 0.18960759\n",
            "Iteration 126, loss = 0.19042434\n",
            "Iteration 127, loss = 0.18978997\n",
            "Iteration 128, loss = 0.18933663\n",
            "Iteration 129, loss = 0.18922539\n",
            "Iteration 130, loss = 0.18933815\n",
            "Iteration 131, loss = 0.18953813\n",
            "Iteration 132, loss = 0.18844768\n",
            "Iteration 133, loss = 0.18906108\n",
            "Iteration 134, loss = 0.18902876\n",
            "Iteration 135, loss = 0.18835145\n",
            "Iteration 136, loss = 0.18866303\n",
            "Iteration 137, loss = 0.18798687\n",
            "Iteration 138, loss = 0.18857042\n",
            "Iteration 139, loss = 0.18815089\n",
            "Iteration 140, loss = 0.18763857\n",
            "Iteration 141, loss = 0.18815498\n",
            "Iteration 142, loss = 0.18690289\n",
            "Iteration 143, loss = 0.18725046\n",
            "Iteration 144, loss = 0.18785996\n",
            "Iteration 145, loss = 0.18744712\n",
            "Iteration 146, loss = 0.18649870\n",
            "Iteration 147, loss = 0.18737329\n",
            "Iteration 148, loss = 0.18644438\n",
            "Iteration 149, loss = 0.18686608\n",
            "Iteration 150, loss = 0.18610492\n",
            "Iteration 151, loss = 0.18682505\n",
            "Iteration 152, loss = 0.18717977\n",
            "Iteration 153, loss = 0.18566667\n",
            "Iteration 154, loss = 0.18554335\n",
            "Iteration 155, loss = 0.18536020\n",
            "Iteration 156, loss = 0.18485437\n",
            "Iteration 157, loss = 0.18553054\n",
            "Iteration 158, loss = 0.18562194\n",
            "Iteration 159, loss = 0.18521054\n",
            "Iteration 160, loss = 0.18421027\n",
            "Iteration 161, loss = 0.18537430\n",
            "Iteration 162, loss = 0.18592616\n",
            "Iteration 163, loss = 0.18525960\n",
            "Iteration 164, loss = 0.18507722\n",
            "Iteration 165, loss = 0.18529140\n",
            "Iteration 166, loss = 0.18513069\n",
            "Iteration 167, loss = 0.18641681\n",
            "Iteration 168, loss = 0.18682848\n",
            "Iteration 169, loss = 0.18414072\n",
            "Iteration 170, loss = 0.18452607\n",
            "Iteration 171, loss = 0.18658701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 172, loss = 0.18045180\n",
            "Iteration 173, loss = 0.17984644\n",
            "Iteration 174, loss = 0.17983230\n",
            "Iteration 175, loss = 0.18004134\n",
            "Iteration 176, loss = 0.17961339\n",
            "Iteration 177, loss = 0.17995357\n",
            "Iteration 178, loss = 0.17968409\n",
            "Iteration 179, loss = 0.17992132\n",
            "Iteration 180, loss = 0.17945503\n",
            "Iteration 181, loss = 0.17969503\n",
            "Iteration 182, loss = 0.17977730\n",
            "Iteration 183, loss = 0.17985324\n",
            "Iteration 184, loss = 0.17941448\n",
            "Iteration 185, loss = 0.17933445\n",
            "Iteration 186, loss = 0.17940514\n",
            "Iteration 187, loss = 0.17960053\n",
            "Iteration 188, loss = 0.17949275\n",
            "Iteration 189, loss = 0.17920135\n",
            "Iteration 190, loss = 0.17947959\n",
            "Iteration 191, loss = 0.17911389\n",
            "Iteration 192, loss = 0.17922981\n",
            "Iteration 193, loss = 0.17914582\n",
            "Iteration 194, loss = 0.17952608\n",
            "Iteration 195, loss = 0.17908264\n",
            "Iteration 196, loss = 0.17922107\n",
            "Iteration 197, loss = 0.17891355\n",
            "Iteration 198, loss = 0.17887605\n",
            "Iteration 199, loss = 0.17894342\n",
            "Iteration 200, loss = 0.17873962\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 10.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[33825    22]\n",
            " [ 1323  2451]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     33847\n",
            "           1       0.99      0.65      0.78      3774\n",
            "\n",
            "    accuracy                           0.96     37621\n",
            "   macro avg       0.98      0.82      0.88     37621\n",
            "weighted avg       0.97      0.96      0.96     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 96.800% (0.925%)\n",
            "Prediction Accuracy (Shuffle Split) : 94.064% (0.207%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.40710507\n",
            "Iteration 3, loss = 0.38088928\n",
            "Iteration 4, loss = 0.36132144\n",
            "Iteration 5, loss = 0.34786335\n",
            "Iteration 6, loss = 0.33828425\n",
            "Iteration 7, loss = 0.33116107\n",
            "Iteration 8, loss = 0.32501575\n",
            "Iteration 9, loss = 0.32129595\n",
            "Iteration 10, loss = 0.32076772\n",
            "Iteration 11, loss = 0.31925565\n",
            "Iteration 12, loss = 0.31700667\n",
            "Iteration 13, loss = 0.31534136\n",
            "Iteration 14, loss = 0.31442179\n",
            "Iteration 15, loss = 0.31360093\n",
            "Iteration 16, loss = 0.31321225\n",
            "Iteration 17, loss = 0.31216382\n",
            "Iteration 18, loss = 0.31239278\n",
            "Iteration 19, loss = 0.31273602\n",
            "Iteration 20, loss = 0.31920991\n",
            "Iteration 21, loss = 0.31668362\n",
            "Iteration 22, loss = 0.31472518\n",
            "Iteration 23, loss = 0.31352106\n",
            "Iteration 24, loss = 0.31250977\n",
            "Iteration 25, loss = 0.31232816\n",
            "Iteration 26, loss = 0.31254829\n",
            "Iteration 27, loss = 0.31187460\n",
            "Iteration 28, loss = 0.31187415\n",
            "Iteration 29, loss = 0.31109285\n",
            "Iteration 30, loss = 0.31172466\n",
            "Iteration 31, loss = 0.31197678\n",
            "Iteration 32, loss = 0.31092445\n",
            "Iteration 33, loss = 0.31155353\n",
            "Iteration 34, loss = 0.31259248\n",
            "Iteration 35, loss = 0.31241944\n",
            "Iteration 36, loss = 0.31181415\n",
            "Iteration 37, loss = 0.31211241\n",
            "Iteration 38, loss = 0.31227913\n",
            "Iteration 39, loss = 0.31224662\n",
            "Iteration 40, loss = 0.31182394\n",
            "Iteration 41, loss = 0.31212520\n",
            "Iteration 42, loss = 0.31186885\n",
            "Iteration 43, loss = 0.31099397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.30299123\n",
            "Iteration 45, loss = 0.30064930\n",
            "Iteration 46, loss = 0.29904922\n",
            "Iteration 47, loss = 0.29789665\n",
            "Iteration 48, loss = 0.29619157\n",
            "Iteration 49, loss = 0.29457207\n",
            "Iteration 50, loss = 0.29342330\n",
            "Iteration 51, loss = 0.29241994\n",
            "Iteration 52, loss = 0.29060918\n",
            "Iteration 53, loss = 0.29012561\n",
            "Iteration 54, loss = 0.28990224\n",
            "Iteration 55, loss = 0.28908786\n",
            "Iteration 56, loss = 0.28849280\n",
            "Iteration 57, loss = 0.29019583\n",
            "Iteration 58, loss = 0.29281633\n",
            "Iteration 59, loss = 0.28957959\n",
            "Iteration 60, loss = 0.29046581\n",
            "Iteration 61, loss = 0.29347205\n",
            "Iteration 62, loss = 0.29353012\n",
            "Iteration 63, loss = 0.29258113\n",
            "Iteration 64, loss = 0.29215997\n",
            "Iteration 65, loss = 0.29236401\n",
            "Iteration 66, loss = 0.29511562\n",
            "Iteration 67, loss = 0.29558068\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.27069326\n",
            "Iteration 69, loss = 0.27013730\n",
            "Iteration 70, loss = 0.26929726\n",
            "Iteration 71, loss = 0.26868490\n",
            "Iteration 72, loss = 0.26808436\n",
            "Iteration 73, loss = 0.26756023\n",
            "Iteration 74, loss = 0.26678648\n",
            "Iteration 75, loss = 0.26616411\n",
            "Iteration 76, loss = 0.26573507\n",
            "Iteration 77, loss = 0.26519782\n",
            "Iteration 78, loss = 0.26443445\n",
            "Iteration 79, loss = 0.26375222\n",
            "Iteration 80, loss = 0.26333490\n",
            "Iteration 81, loss = 0.26314137\n",
            "Iteration 82, loss = 0.26230148\n",
            "Iteration 83, loss = 0.26186702\n",
            "Iteration 84, loss = 0.26128840\n",
            "Iteration 85, loss = 0.26050020\n",
            "Iteration 86, loss = 0.26008305\n",
            "Iteration 87, loss = 0.25951700\n",
            "Iteration 88, loss = 0.25933864\n",
            "Iteration 89, loss = 0.25891313\n",
            "Iteration 90, loss = 0.25806811\n",
            "Iteration 91, loss = 0.25746581\n",
            "Iteration 92, loss = 0.25687191\n",
            "Iteration 93, loss = 0.25679512\n",
            "Iteration 94, loss = 0.25641126\n",
            "Iteration 95, loss = 0.25581744\n",
            "Iteration 96, loss = 0.25518555\n",
            "Iteration 97, loss = 0.25490174\n",
            "Iteration 98, loss = 0.25440100\n",
            "Iteration 99, loss = 0.25379413\n",
            "Iteration 100, loss = 0.25368490\n",
            "Iteration 101, loss = 0.25335814\n",
            "Iteration 102, loss = 0.25230981\n",
            "Iteration 103, loss = 0.25227841\n",
            "Iteration 104, loss = 0.25180532\n",
            "Iteration 105, loss = 0.25169765\n",
            "Iteration 106, loss = 0.25087082\n",
            "Iteration 107, loss = 0.25065206\n",
            "Iteration 108, loss = 0.25047354\n",
            "Iteration 109, loss = 0.24962139\n",
            "Iteration 110, loss = 0.24938441\n",
            "Iteration 111, loss = 0.24894907\n",
            "Iteration 112, loss = 0.24855927\n",
            "Iteration 113, loss = 0.24823204\n",
            "Iteration 114, loss = 0.24781024\n",
            "Iteration 115, loss = 0.24784563\n",
            "Iteration 116, loss = 0.24717302\n",
            "Iteration 117, loss = 0.24674614\n",
            "Iteration 118, loss = 0.24651234\n",
            "Iteration 119, loss = 0.24629915\n",
            "Iteration 120, loss = 0.24598561\n",
            "Iteration 121, loss = 0.24569842\n",
            "Iteration 122, loss = 0.24558695\n",
            "Iteration 123, loss = 0.24515512\n",
            "Iteration 124, loss = 0.24478122\n",
            "Iteration 125, loss = 0.24466126\n",
            "Iteration 126, loss = 0.24391476\n",
            "Iteration 127, loss = 0.24414038\n",
            "Iteration 128, loss = 0.24364848\n",
            "Iteration 129, loss = 0.24345402\n",
            "Iteration 130, loss = 0.24305165\n",
            "Iteration 131, loss = 0.24260839\n",
            "Iteration 132, loss = 0.24254818\n",
            "Iteration 133, loss = 0.24251334\n",
            "Iteration 134, loss = 0.24216512\n",
            "Iteration 135, loss = 0.24188595\n",
            "Iteration 136, loss = 0.24175889\n",
            "Iteration 137, loss = 0.24115964\n",
            "Iteration 138, loss = 0.24124542\n",
            "Iteration 139, loss = 0.24097962\n",
            "Iteration 140, loss = 0.24084825\n",
            "Iteration 141, loss = 0.24053892\n",
            "Iteration 142, loss = 0.24018276\n",
            "Iteration 143, loss = 0.24020268\n",
            "Iteration 144, loss = 0.24016314\n",
            "Iteration 145, loss = 0.23994543\n",
            "Iteration 146, loss = 0.23976132\n",
            "Iteration 147, loss = 0.23904992\n",
            "Iteration 148, loss = 0.23904420\n",
            "Iteration 149, loss = 0.23919612\n",
            "Iteration 150, loss = 0.23873768\n",
            "Iteration 151, loss = 0.23884897\n",
            "Iteration 152, loss = 0.23813125\n",
            "Iteration 153, loss = 0.23833136\n",
            "Iteration 154, loss = 0.23839301\n",
            "Iteration 155, loss = 0.23819335\n",
            "Iteration 156, loss = 0.23796772\n",
            "Iteration 157, loss = 0.23806955\n",
            "Iteration 158, loss = 0.23776129\n",
            "Iteration 159, loss = 0.23741181\n",
            "Iteration 160, loss = 0.23786167\n",
            "Iteration 161, loss = 0.23741385\n",
            "Iteration 162, loss = 0.23665623\n",
            "Iteration 163, loss = 0.23691786\n",
            "Iteration 164, loss = 0.23707113\n",
            "Iteration 165, loss = 0.23703398\n",
            "Iteration 166, loss = 0.23655455\n",
            "Iteration 167, loss = 0.23651014\n",
            "Iteration 168, loss = 0.23622537\n",
            "Iteration 169, loss = 0.23620741\n",
            "Iteration 170, loss = 0.23614442\n",
            "Iteration 171, loss = 0.23599194\n",
            "Iteration 172, loss = 0.23562067\n",
            "Iteration 173, loss = 0.23589470\n",
            "Iteration 174, loss = 0.23597312\n",
            "Iteration 175, loss = 0.23602850\n",
            "Iteration 176, loss = 0.23577916\n",
            "Iteration 177, loss = 0.23528213\n",
            "Iteration 178, loss = 0.23565235\n",
            "Iteration 179, loss = 0.23573241\n",
            "Iteration 180, loss = 0.23527956\n",
            "Iteration 181, loss = 0.23556427\n",
            "Iteration 182, loss = 0.23519118\n",
            "Iteration 183, loss = 0.23521564\n",
            "Iteration 184, loss = 0.23519397\n",
            "Iteration 185, loss = 0.23490161\n",
            "Iteration 186, loss = 0.23457287\n",
            "Iteration 187, loss = 0.23478197\n",
            "Iteration 188, loss = 0.23504901\n",
            "Iteration 189, loss = 0.23461169\n",
            "Iteration 190, loss = 0.23435504\n",
            "Iteration 191, loss = 0.23417759\n",
            "Iteration 192, loss = 0.23461514\n",
            "Iteration 193, loss = 0.23442018\n",
            "Iteration 194, loss = 0.23473242\n",
            "Iteration 195, loss = 0.23419390\n",
            "Iteration 196, loss = 0.23442158\n",
            "Iteration 197, loss = 0.23430412\n",
            "Iteration 198, loss = 0.23467218\n",
            "Iteration 199, loss = 0.23411762\n",
            "Iteration 200, loss = 0.23426015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.63224108\n",
            "Iteration 2, loss = 0.28985929\n",
            "Iteration 3, loss = 0.27031207\n",
            "Iteration 4, loss = 0.25446399\n",
            "Iteration 5, loss = 0.24311772\n",
            "Iteration 6, loss = 0.23361864\n",
            "Iteration 7, loss = 0.22723271\n",
            "Iteration 8, loss = 0.22187605\n",
            "Iteration 9, loss = 0.21710955\n",
            "Iteration 10, loss = 0.21586328\n",
            "Iteration 11, loss = 0.21563276\n",
            "Iteration 12, loss = 0.21242809\n",
            "Iteration 13, loss = 0.20989510\n",
            "Iteration 14, loss = 0.20922483\n",
            "Iteration 15, loss = 0.20765612\n",
            "Iteration 16, loss = 0.20675302\n",
            "Iteration 17, loss = 0.20633748\n",
            "Iteration 18, loss = 0.20648692\n",
            "Iteration 19, loss = 0.20545551\n",
            "Iteration 20, loss = 0.20565734\n",
            "Iteration 21, loss = 0.20604502\n",
            "Iteration 22, loss = 0.20487291\n",
            "Iteration 23, loss = 0.20487095\n",
            "Iteration 24, loss = 0.20475344\n",
            "Iteration 25, loss = 0.20491525\n",
            "Iteration 26, loss = 0.20403420\n",
            "Iteration 27, loss = 0.20492343\n",
            "Iteration 28, loss = 0.20429019\n",
            "Iteration 29, loss = 0.20476289\n",
            "Iteration 30, loss = 0.20559608\n",
            "Iteration 31, loss = 0.20367257\n",
            "Iteration 32, loss = 0.20509642\n",
            "Iteration 33, loss = 0.20427847\n",
            "Iteration 34, loss = 0.20477009\n",
            "Iteration 35, loss = 0.20485402\n",
            "Iteration 36, loss = 0.20439151\n",
            "Iteration 37, loss = 0.20527036\n",
            "Iteration 38, loss = 0.20444028\n",
            "Iteration 39, loss = 0.20480950\n",
            "Iteration 40, loss = 0.20580990\n",
            "Iteration 41, loss = 0.20484181\n",
            "Iteration 42, loss = 0.20397032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.19752916\n",
            "Iteration 44, loss = 0.19637374\n",
            "Iteration 45, loss = 0.19572489\n",
            "Iteration 46, loss = 0.19506897\n",
            "Iteration 47, loss = 0.19434532\n",
            "Iteration 48, loss = 0.19379136\n",
            "Iteration 49, loss = 0.19333610\n",
            "Iteration 50, loss = 0.19334325\n",
            "Iteration 51, loss = 0.19271342\n",
            "Iteration 52, loss = 0.19172063\n",
            "Iteration 53, loss = 0.19138936\n",
            "Iteration 54, loss = 0.19129944\n",
            "Iteration 55, loss = 0.19058547\n",
            "Iteration 56, loss = 0.19007036\n",
            "Iteration 57, loss = 0.18946045\n",
            "Iteration 58, loss = 0.18917424\n",
            "Iteration 59, loss = 0.18856673\n",
            "Iteration 60, loss = 0.18897828\n",
            "Iteration 61, loss = 0.18817031\n",
            "Iteration 62, loss = 0.18746381\n",
            "Iteration 63, loss = 0.18688360\n",
            "Iteration 64, loss = 0.18674674\n",
            "Iteration 65, loss = 0.18688638\n",
            "Iteration 66, loss = 0.18598253\n",
            "Iteration 67, loss = 0.18639853\n",
            "Iteration 68, loss = 0.18601044\n",
            "Iteration 69, loss = 0.18529389\n",
            "Iteration 70, loss = 0.18492767\n",
            "Iteration 71, loss = 0.18608832\n",
            "Iteration 72, loss = 0.18558602\n",
            "Iteration 73, loss = 0.18532397\n",
            "Iteration 74, loss = 0.18478078\n",
            "Iteration 75, loss = 0.18535570\n",
            "Iteration 76, loss = 0.18580625\n",
            "Iteration 77, loss = 0.18495459\n",
            "Iteration 78, loss = 0.18504821\n",
            "Iteration 79, loss = 0.18429406\n",
            "Iteration 80, loss = 0.18566491\n",
            "Iteration 81, loss = 0.18644896\n",
            "Iteration 82, loss = 0.18602672\n",
            "Iteration 83, loss = 0.18374595\n",
            "Iteration 84, loss = 0.18395610\n",
            "Iteration 85, loss = 0.18675401\n",
            "Iteration 86, loss = 0.18527336\n",
            "Iteration 87, loss = 0.18316617\n",
            "Iteration 88, loss = 0.18873250\n",
            "Iteration 89, loss = 0.19275779\n",
            "Iteration 90, loss = 0.18699996\n",
            "Iteration 91, loss = 0.18740369\n",
            "Iteration 92, loss = 0.18419624\n",
            "Iteration 93, loss = 0.18527800\n",
            "Iteration 94, loss = 0.18675401\n",
            "Iteration 95, loss = 0.18835454\n",
            "Iteration 96, loss = 0.19130341\n",
            "Iteration 97, loss = 0.18534969\n",
            "Iteration 98, loss = 0.18677128\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 99, loss = 0.17434327\n",
            "Iteration 100, loss = 0.17371949\n",
            "Iteration 101, loss = 0.17352188\n",
            "Iteration 102, loss = 0.17337552\n",
            "Iteration 103, loss = 0.17335467\n",
            "Iteration 104, loss = 0.17307448\n",
            "Iteration 105, loss = 0.17318952\n",
            "Iteration 106, loss = 0.17294714\n",
            "Iteration 107, loss = 0.17275254\n",
            "Iteration 108, loss = 0.17258041\n",
            "Iteration 109, loss = 0.17267259\n",
            "Iteration 110, loss = 0.17252346\n",
            "Iteration 111, loss = 0.17244972\n",
            "Iteration 112, loss = 0.17221120\n",
            "Iteration 113, loss = 0.17216341\n",
            "Iteration 114, loss = 0.17211471\n",
            "Iteration 115, loss = 0.17185785\n",
            "Iteration 116, loss = 0.17188093\n",
            "Iteration 117, loss = 0.17184811\n",
            "Iteration 118, loss = 0.17183407\n",
            "Iteration 119, loss = 0.17150200\n",
            "Iteration 120, loss = 0.17139540\n",
            "Iteration 121, loss = 0.17146795\n",
            "Iteration 122, loss = 0.17127497\n",
            "Iteration 123, loss = 0.17124104\n",
            "Iteration 124, loss = 0.17111282\n",
            "Iteration 125, loss = 0.17094802\n",
            "Iteration 126, loss = 0.17082962\n",
            "Iteration 127, loss = 0.17076748\n",
            "Iteration 128, loss = 0.17070908\n",
            "Iteration 129, loss = 0.17060281\n",
            "Iteration 130, loss = 0.17069129\n",
            "Iteration 131, loss = 0.17051275\n",
            "Iteration 132, loss = 0.17032985\n",
            "Iteration 133, loss = 0.17040897\n",
            "Iteration 134, loss = 0.17021748\n",
            "Iteration 135, loss = 0.17035017\n",
            "Iteration 136, loss = 0.17008217\n",
            "Iteration 137, loss = 0.16985707\n",
            "Iteration 138, loss = 0.16992601\n",
            "Iteration 139, loss = 0.16985669\n",
            "Iteration 140, loss = 0.16988551\n",
            "Iteration 141, loss = 0.16984980\n",
            "Iteration 142, loss = 0.16955797\n",
            "Iteration 143, loss = 0.16958254\n",
            "Iteration 144, loss = 0.16956837\n",
            "Iteration 145, loss = 0.16941485\n",
            "Iteration 146, loss = 0.16945353\n",
            "Iteration 147, loss = 0.16930289\n",
            "Iteration 148, loss = 0.16922497\n",
            "Iteration 149, loss = 0.16907029\n",
            "Iteration 150, loss = 0.16914327\n",
            "Iteration 151, loss = 0.16921475\n",
            "Iteration 152, loss = 0.16898265\n",
            "Iteration 153, loss = 0.16905532\n",
            "Iteration 154, loss = 0.16881334\n",
            "Iteration 155, loss = 0.16876610\n",
            "Iteration 156, loss = 0.16855754\n",
            "Iteration 157, loss = 0.16870082\n",
            "Iteration 158, loss = 0.16861464\n",
            "Iteration 159, loss = 0.16846636\n",
            "Iteration 160, loss = 0.16835476\n",
            "Iteration 161, loss = 0.16844503\n",
            "Iteration 162, loss = 0.16819034\n",
            "Iteration 163, loss = 0.16852994\n",
            "Iteration 164, loss = 0.16838286\n",
            "Iteration 165, loss = 0.16796649\n",
            "Iteration 166, loss = 0.16822177\n",
            "Iteration 167, loss = 0.16801297\n",
            "Iteration 168, loss = 0.16801189\n",
            "Iteration 169, loss = 0.16790984\n",
            "Iteration 170, loss = 0.16803797\n",
            "Iteration 171, loss = 0.16766967\n",
            "Iteration 172, loss = 0.16780746\n",
            "Iteration 173, loss = 0.16768855\n",
            "Iteration 174, loss = 0.16774472\n",
            "Iteration 175, loss = 0.16754316\n",
            "Iteration 176, loss = 0.16768889\n",
            "Iteration 177, loss = 0.16743308\n",
            "Iteration 178, loss = 0.16756731\n",
            "Iteration 179, loss = 0.16738908\n",
            "Iteration 180, loss = 0.16746648\n",
            "Iteration 181, loss = 0.16733571\n",
            "Iteration 182, loss = 0.16714164\n",
            "Iteration 183, loss = 0.16717736\n",
            "Iteration 184, loss = 0.16723132\n",
            "Iteration 185, loss = 0.16720541\n",
            "Iteration 186, loss = 0.16727531\n",
            "Iteration 187, loss = 0.16720191\n",
            "Iteration 188, loss = 0.16685450\n",
            "Iteration 189, loss = 0.16707980\n",
            "Iteration 190, loss = 0.16694977\n",
            "Iteration 191, loss = 0.16704739\n",
            "Iteration 192, loss = 0.16708793\n",
            "Iteration 193, loss = 0.16687303\n",
            "Iteration 194, loss = 0.16689607\n",
            "Iteration 195, loss = 0.16678225\n",
            "Iteration 196, loss = 0.16656914\n",
            "Iteration 197, loss = 0.16677040\n",
            "Iteration 198, loss = 0.16644240\n",
            "Iteration 199, loss = 0.16663432\n",
            "Iteration 200, loss = 0.16658044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.87449438\n",
            "Iteration 2, loss = 0.31291680\n",
            "Iteration 3, loss = 0.28577007\n",
            "Iteration 4, loss = 0.26585190\n",
            "Iteration 5, loss = 0.25113189\n",
            "Iteration 6, loss = 0.23975488\n",
            "Iteration 7, loss = 0.23156516\n",
            "Iteration 8, loss = 0.22505307\n",
            "Iteration 9, loss = 0.22060049\n",
            "Iteration 10, loss = 0.21624952\n",
            "Iteration 11, loss = 0.21346996\n",
            "Iteration 12, loss = 0.21148971\n",
            "Iteration 13, loss = 0.20986868\n",
            "Iteration 14, loss = 0.20857541\n",
            "Iteration 15, loss = 0.20680288\n",
            "Iteration 16, loss = 0.20718858\n",
            "Iteration 17, loss = 0.20673349\n",
            "Iteration 18, loss = 0.20623123\n",
            "Iteration 19, loss = 0.20652167\n",
            "Iteration 20, loss = 0.20470234\n",
            "Iteration 21, loss = 0.20574168\n",
            "Iteration 22, loss = 0.20533789\n",
            "Iteration 23, loss = 0.20485933\n",
            "Iteration 24, loss = 0.20471530\n",
            "Iteration 25, loss = 0.20538219\n",
            "Iteration 26, loss = 0.20563902\n",
            "Iteration 27, loss = 0.20488214\n",
            "Iteration 28, loss = 0.20556399\n",
            "Iteration 29, loss = 0.20591836\n",
            "Iteration 30, loss = 0.20486562\n",
            "Iteration 31, loss = 0.20480865\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.19739201\n",
            "Iteration 33, loss = 0.19642935\n",
            "Iteration 34, loss = 0.19571356\n",
            "Iteration 35, loss = 0.19536247\n",
            "Iteration 36, loss = 0.19455688\n",
            "Iteration 37, loss = 0.19442534\n",
            "Iteration 38, loss = 0.19368051\n",
            "Iteration 39, loss = 0.19318528\n",
            "Iteration 40, loss = 0.19258922\n",
            "Iteration 41, loss = 0.19217137\n",
            "Iteration 42, loss = 0.19177259\n",
            "Iteration 43, loss = 0.19135097\n",
            "Iteration 44, loss = 0.19063182\n",
            "Iteration 45, loss = 0.19010599\n",
            "Iteration 46, loss = 0.19002450\n",
            "Iteration 47, loss = 0.18940412\n",
            "Iteration 48, loss = 0.18860077\n",
            "Iteration 49, loss = 0.18839858\n",
            "Iteration 50, loss = 0.18792231\n",
            "Iteration 51, loss = 0.18770405\n",
            "Iteration 52, loss = 0.18760243\n",
            "Iteration 53, loss = 0.18655015\n",
            "Iteration 54, loss = 0.18706731\n",
            "Iteration 55, loss = 0.18753705\n",
            "Iteration 56, loss = 0.18720753\n",
            "Iteration 57, loss = 0.18628231\n",
            "Iteration 58, loss = 0.18740838\n",
            "Iteration 59, loss = 0.18650367\n",
            "Iteration 60, loss = 0.18653157\n",
            "Iteration 61, loss = 0.18514588\n",
            "Iteration 62, loss = 0.18562145\n",
            "Iteration 63, loss = 0.18515807\n",
            "Iteration 64, loss = 0.18467325\n",
            "Iteration 65, loss = 0.18538188\n",
            "Iteration 66, loss = 0.18547806\n",
            "Iteration 67, loss = 0.18520820\n",
            "Iteration 68, loss = 0.18643579\n",
            "Iteration 69, loss = 0.18519938\n",
            "Iteration 70, loss = 0.18704631\n",
            "Iteration 71, loss = 0.18621982\n",
            "Iteration 72, loss = 0.18518907\n",
            "Iteration 73, loss = 0.18618775\n",
            "Iteration 74, loss = 0.18614443\n",
            "Iteration 75, loss = 0.18565462\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.17600136\n",
            "Iteration 77, loss = 0.17581649\n",
            "Iteration 78, loss = 0.17553518\n",
            "Iteration 79, loss = 0.17545133\n",
            "Iteration 80, loss = 0.17543068\n",
            "Iteration 81, loss = 0.17533240\n",
            "Iteration 82, loss = 0.17516846\n",
            "Iteration 83, loss = 0.17519420\n",
            "Iteration 84, loss = 0.17488379\n",
            "Iteration 85, loss = 0.17464440\n",
            "Iteration 86, loss = 0.17454653\n",
            "Iteration 87, loss = 0.17465421\n",
            "Iteration 88, loss = 0.17447830\n",
            "Iteration 89, loss = 0.17438664\n",
            "Iteration 90, loss = 0.17431755\n",
            "Iteration 91, loss = 0.17415865\n",
            "Iteration 92, loss = 0.17408920\n",
            "Iteration 93, loss = 0.17396875\n",
            "Iteration 94, loss = 0.17385587\n",
            "Iteration 95, loss = 0.17373922\n",
            "Iteration 96, loss = 0.17358384\n",
            "Iteration 97, loss = 0.17357845\n",
            "Iteration 98, loss = 0.17324964\n",
            "Iteration 99, loss = 0.17347104\n",
            "Iteration 100, loss = 0.17335212\n",
            "Iteration 101, loss = 0.17310703\n",
            "Iteration 102, loss = 0.17289648\n",
            "Iteration 103, loss = 0.17292704\n",
            "Iteration 104, loss = 0.17276013\n",
            "Iteration 105, loss = 0.17266060\n",
            "Iteration 106, loss = 0.17265684\n",
            "Iteration 107, loss = 0.17246000\n",
            "Iteration 108, loss = 0.17245229\n",
            "Iteration 109, loss = 0.17239458\n",
            "Iteration 110, loss = 0.17219799\n",
            "Iteration 111, loss = 0.17213253\n",
            "Iteration 112, loss = 0.17216133\n",
            "Iteration 113, loss = 0.17208489\n",
            "Iteration 114, loss = 0.17196217\n",
            "Iteration 115, loss = 0.17183269\n",
            "Iteration 116, loss = 0.17165316\n",
            "Iteration 117, loss = 0.17171722\n",
            "Iteration 118, loss = 0.17156282\n",
            "Iteration 119, loss = 0.17148054\n",
            "Iteration 120, loss = 0.17156557\n",
            "Iteration 121, loss = 0.17133398\n",
            "Iteration 122, loss = 0.17132794\n",
            "Iteration 123, loss = 0.17130314\n",
            "Iteration 124, loss = 0.17113783\n",
            "Iteration 125, loss = 0.17109311\n",
            "Iteration 126, loss = 0.17108580\n",
            "Iteration 127, loss = 0.17079104\n",
            "Iteration 128, loss = 0.17107383\n",
            "Iteration 129, loss = 0.17083010\n",
            "Iteration 130, loss = 0.17056426\n",
            "Iteration 131, loss = 0.17061217\n",
            "Iteration 132, loss = 0.17065675\n",
            "Iteration 133, loss = 0.17051772\n",
            "Iteration 134, loss = 0.17028210\n",
            "Iteration 135, loss = 0.17011767\n",
            "Iteration 136, loss = 0.17005339\n",
            "Iteration 137, loss = 0.17000176\n",
            "Iteration 138, loss = 0.17013790\n",
            "Iteration 139, loss = 0.17000980\n",
            "Iteration 140, loss = 0.17006336\n",
            "Iteration 141, loss = 0.17003733\n",
            "Iteration 142, loss = 0.16973829\n",
            "Iteration 143, loss = 0.16975218\n",
            "Iteration 144, loss = 0.16956866\n",
            "Iteration 145, loss = 0.16993074\n",
            "Iteration 146, loss = 0.16937068\n",
            "Iteration 147, loss = 0.16967774\n",
            "Iteration 148, loss = 0.16941589\n",
            "Iteration 149, loss = 0.16944424\n",
            "Iteration 150, loss = 0.16922532\n",
            "Iteration 151, loss = 0.16905768\n",
            "Iteration 152, loss = 0.16914120\n",
            "Iteration 153, loss = 0.16917180\n",
            "Iteration 154, loss = 0.16906058\n",
            "Iteration 155, loss = 0.16902120\n",
            "Iteration 156, loss = 0.16893962\n",
            "Iteration 157, loss = 0.16908350\n",
            "Iteration 158, loss = 0.16890687\n",
            "Iteration 159, loss = 0.16888175\n",
            "Iteration 160, loss = 0.16872346\n",
            "Iteration 161, loss = 0.16853819\n",
            "Iteration 162, loss = 0.16867818\n",
            "Iteration 163, loss = 0.16875899\n",
            "Iteration 164, loss = 0.16846781\n",
            "Iteration 165, loss = 0.16845587\n",
            "Iteration 166, loss = 0.16853419\n",
            "Iteration 167, loss = 0.16832958\n",
            "Iteration 168, loss = 0.16836264\n",
            "Iteration 169, loss = 0.16837746\n",
            "Iteration 170, loss = 0.16835033\n",
            "Iteration 171, loss = 0.16819191\n",
            "Iteration 172, loss = 0.16844542\n",
            "Iteration 173, loss = 0.16808553\n",
            "Iteration 174, loss = 0.16788852\n",
            "Iteration 175, loss = 0.16788273\n",
            "Iteration 176, loss = 0.16827362\n",
            "Iteration 177, loss = 0.16783145\n",
            "Iteration 178, loss = 0.16795460\n",
            "Iteration 179, loss = 0.16788717\n",
            "Iteration 180, loss = 0.16771783\n",
            "Iteration 181, loss = 0.16768268\n",
            "Iteration 182, loss = 0.16768476\n",
            "Iteration 183, loss = 0.16756286\n",
            "Iteration 184, loss = 0.16776774\n",
            "Iteration 185, loss = 0.16757999\n",
            "Iteration 186, loss = 0.16736772\n",
            "Iteration 187, loss = 0.16741488\n",
            "Iteration 188, loss = 0.16731408\n",
            "Iteration 189, loss = 0.16737551\n",
            "Iteration 190, loss = 0.16745968\n",
            "Iteration 191, loss = 0.16737920\n",
            "Iteration 192, loss = 0.16718109\n",
            "Iteration 193, loss = 0.16703806\n",
            "Iteration 194, loss = 0.16702751\n",
            "Iteration 195, loss = 0.16727224\n",
            "Iteration 196, loss = 0.16721049\n",
            "Iteration 197, loss = 0.16727085\n",
            "Iteration 198, loss = 0.16703593\n",
            "Iteration 199, loss = 0.16701090\n",
            "Iteration 200, loss = 0.16706790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.80856594\n",
            "Iteration 2, loss = 0.30460602\n",
            "Iteration 3, loss = 0.28053606\n",
            "Iteration 4, loss = 0.26186410\n",
            "Iteration 5, loss = 0.24801643\n",
            "Iteration 6, loss = 0.23777031\n",
            "Iteration 7, loss = 0.22997182\n",
            "Iteration 8, loss = 0.22359085\n",
            "Iteration 9, loss = 0.21889257\n",
            "Iteration 10, loss = 0.21526516\n",
            "Iteration 11, loss = 0.21346514\n",
            "Iteration 12, loss = 0.21124110\n",
            "Iteration 13, loss = 0.20911207\n",
            "Iteration 14, loss = 0.20792832\n",
            "Iteration 15, loss = 0.20764980\n",
            "Iteration 16, loss = 0.20729353\n",
            "Iteration 17, loss = 0.20697410\n",
            "Iteration 18, loss = 0.20632677\n",
            "Iteration 19, loss = 0.20531093\n",
            "Iteration 20, loss = 0.20626805\n",
            "Iteration 21, loss = 0.20560513\n",
            "Iteration 22, loss = 0.20541913\n",
            "Iteration 23, loss = 0.20565484\n",
            "Iteration 24, loss = 0.20689518\n",
            "Iteration 25, loss = 0.20408414\n",
            "Iteration 26, loss = 0.20798440\n",
            "Iteration 27, loss = 0.20717955\n",
            "Iteration 28, loss = 0.20566677\n",
            "Iteration 29, loss = 0.20582524\n",
            "Iteration 30, loss = 0.20569321\n",
            "Iteration 31, loss = 0.20626043\n",
            "Iteration 32, loss = 0.20663346\n",
            "Iteration 33, loss = 0.20562334\n",
            "Iteration 34, loss = 0.20476248\n",
            "Iteration 35, loss = 0.20408433\n",
            "Iteration 36, loss = 0.20411033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.19700293\n",
            "Iteration 38, loss = 0.19618805\n",
            "Iteration 39, loss = 0.19555409\n",
            "Iteration 40, loss = 0.19484008\n",
            "Iteration 41, loss = 0.19450432\n",
            "Iteration 42, loss = 0.19378057\n",
            "Iteration 43, loss = 0.19348325\n",
            "Iteration 44, loss = 0.19271138\n",
            "Iteration 45, loss = 0.19182120\n",
            "Iteration 46, loss = 0.19163807\n",
            "Iteration 47, loss = 0.19146366\n",
            "Iteration 48, loss = 0.19096441\n",
            "Iteration 49, loss = 0.19042439\n",
            "Iteration 50, loss = 0.18975991\n",
            "Iteration 51, loss = 0.18920600\n",
            "Iteration 52, loss = 0.18930965\n",
            "Iteration 53, loss = 0.18872174\n",
            "Iteration 54, loss = 0.18806392\n",
            "Iteration 55, loss = 0.18781326\n",
            "Iteration 56, loss = 0.18782743\n",
            "Iteration 57, loss = 0.18702228\n",
            "Iteration 58, loss = 0.18675670\n",
            "Iteration 59, loss = 0.18621067\n",
            "Iteration 60, loss = 0.18658654\n",
            "Iteration 61, loss = 0.18583495\n",
            "Iteration 62, loss = 0.18523387\n",
            "Iteration 63, loss = 0.18608871\n",
            "Iteration 64, loss = 0.18498137\n",
            "Iteration 65, loss = 0.18500100\n",
            "Iteration 66, loss = 0.18544329\n",
            "Iteration 67, loss = 0.18498234\n",
            "Iteration 68, loss = 0.18449321\n",
            "Iteration 69, loss = 0.18427413\n",
            "Iteration 70, loss = 0.18659235\n",
            "Iteration 71, loss = 0.18457519\n",
            "Iteration 72, loss = 0.18581428\n",
            "Iteration 73, loss = 0.18507269\n",
            "Iteration 74, loss = 0.18653916\n",
            "Iteration 75, loss = 0.18490624\n",
            "Iteration 76, loss = 0.18457979\n",
            "Iteration 77, loss = 0.18425662\n",
            "Iteration 78, loss = 0.18564905\n",
            "Iteration 79, loss = 0.18469083\n",
            "Iteration 80, loss = 0.18513522\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.17558713\n",
            "Iteration 82, loss = 0.17535421\n",
            "Iteration 83, loss = 0.17517745\n",
            "Iteration 84, loss = 0.17525354\n",
            "Iteration 85, loss = 0.17486970\n",
            "Iteration 86, loss = 0.17485776\n",
            "Iteration 87, loss = 0.17502315\n",
            "Iteration 88, loss = 0.17469083\n",
            "Iteration 89, loss = 0.17452134\n",
            "Iteration 90, loss = 0.17428442\n",
            "Iteration 91, loss = 0.17433591\n",
            "Iteration 92, loss = 0.17427503\n",
            "Iteration 93, loss = 0.17404200\n",
            "Iteration 94, loss = 0.17409439\n",
            "Iteration 95, loss = 0.17391556\n",
            "Iteration 96, loss = 0.17370205\n",
            "Iteration 97, loss = 0.17353077\n",
            "Iteration 98, loss = 0.17366443\n",
            "Iteration 99, loss = 0.17353030\n",
            "Iteration 100, loss = 0.17341837\n",
            "Iteration 101, loss = 0.17324284\n",
            "Iteration 102, loss = 0.17312187\n",
            "Iteration 103, loss = 0.17296528\n",
            "Iteration 104, loss = 0.17299034\n",
            "Iteration 105, loss = 0.17299062\n",
            "Iteration 106, loss = 0.17272086\n",
            "Iteration 107, loss = 0.17293157\n",
            "Iteration 108, loss = 0.17258687\n",
            "Iteration 109, loss = 0.17254913\n",
            "Iteration 110, loss = 0.17235782\n",
            "Iteration 111, loss = 0.17239137\n",
            "Iteration 112, loss = 0.17229739\n",
            "Iteration 113, loss = 0.17219076\n",
            "Iteration 114, loss = 0.17218709\n",
            "Iteration 115, loss = 0.17200053\n",
            "Iteration 116, loss = 0.17174659\n",
            "Iteration 117, loss = 0.17190518\n",
            "Iteration 118, loss = 0.17167068\n",
            "Iteration 119, loss = 0.17149834\n",
            "Iteration 120, loss = 0.17156952\n",
            "Iteration 121, loss = 0.17147136\n",
            "Iteration 122, loss = 0.17145144\n",
            "Iteration 123, loss = 0.17120002\n",
            "Iteration 124, loss = 0.17151914\n",
            "Iteration 125, loss = 0.17117721\n",
            "Iteration 126, loss = 0.17116080\n",
            "Iteration 127, loss = 0.17103305\n",
            "Iteration 128, loss = 0.17095057\n",
            "Iteration 129, loss = 0.17092823\n",
            "Iteration 130, loss = 0.17061319\n",
            "Iteration 131, loss = 0.17071562\n",
            "Iteration 132, loss = 0.17071522\n",
            "Iteration 133, loss = 0.17054999\n",
            "Iteration 134, loss = 0.17034028\n",
            "Iteration 135, loss = 0.17056447\n",
            "Iteration 136, loss = 0.17037891\n",
            "Iteration 137, loss = 0.17014840\n",
            "Iteration 138, loss = 0.17012092\n",
            "Iteration 139, loss = 0.17012792\n",
            "Iteration 140, loss = 0.16983838\n",
            "Iteration 141, loss = 0.16996976\n",
            "Iteration 142, loss = 0.17006134\n",
            "Iteration 143, loss = 0.16993688\n",
            "Iteration 144, loss = 0.16976464\n",
            "Iteration 145, loss = 0.16975874\n",
            "Iteration 146, loss = 0.16974037\n",
            "Iteration 147, loss = 0.16963695\n",
            "Iteration 148, loss = 0.16960844\n",
            "Iteration 149, loss = 0.16944221\n",
            "Iteration 150, loss = 0.16941390\n",
            "Iteration 151, loss = 0.16926053\n",
            "Iteration 152, loss = 0.16928047\n",
            "Iteration 153, loss = 0.16914184\n",
            "Iteration 154, loss = 0.16895150\n",
            "Iteration 155, loss = 0.16918588\n",
            "Iteration 156, loss = 0.16905288\n",
            "Iteration 157, loss = 0.16886207\n",
            "Iteration 158, loss = 0.16878960\n",
            "Iteration 159, loss = 0.16881120\n",
            "Iteration 160, loss = 0.16899571\n",
            "Iteration 161, loss = 0.16874311\n",
            "Iteration 162, loss = 0.16845693\n",
            "Iteration 163, loss = 0.16859772\n",
            "Iteration 164, loss = 0.16857111\n",
            "Iteration 165, loss = 0.16853070\n",
            "Iteration 166, loss = 0.16836226\n",
            "Iteration 167, loss = 0.16841845\n",
            "Iteration 168, loss = 0.16842117\n",
            "Iteration 169, loss = 0.16834193\n",
            "Iteration 170, loss = 0.16837715\n",
            "Iteration 171, loss = 0.16813195\n",
            "Iteration 172, loss = 0.16826483\n",
            "Iteration 173, loss = 0.16827083\n",
            "Iteration 174, loss = 0.16803509\n",
            "Iteration 175, loss = 0.16786680\n",
            "Iteration 176, loss = 0.16799911\n",
            "Iteration 177, loss = 0.16775936\n",
            "Iteration 178, loss = 0.16786974\n",
            "Iteration 179, loss = 0.16783872\n",
            "Iteration 180, loss = 0.16785408\n",
            "Iteration 181, loss = 0.16762309\n",
            "Iteration 182, loss = 0.16776489\n",
            "Iteration 183, loss = 0.16773682\n",
            "Iteration 184, loss = 0.16749293\n",
            "Iteration 185, loss = 0.16758355\n",
            "Iteration 186, loss = 0.16752569\n",
            "Iteration 187, loss = 0.16736137\n",
            "Iteration 188, loss = 0.16746580\n",
            "Iteration 189, loss = 0.16726762\n",
            "Iteration 190, loss = 0.16722535\n",
            "Iteration 191, loss = 0.16732225\n",
            "Iteration 192, loss = 0.16712029\n",
            "Iteration 193, loss = 0.16723949\n",
            "Iteration 194, loss = 0.16710239\n",
            "Iteration 195, loss = 0.16728221\n",
            "Iteration 196, loss = 0.16687354\n",
            "Iteration 197, loss = 0.16701038\n",
            "Iteration 198, loss = 0.16705135\n",
            "Iteration 199, loss = 0.16692221\n",
            "Iteration 200, loss = 0.16688838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32361075\n",
            "Iteration 3, loss = 0.29427550\n",
            "Iteration 4, loss = 0.27289328\n",
            "Iteration 5, loss = 0.25652973\n",
            "Iteration 6, loss = 0.24358161\n",
            "Iteration 7, loss = 0.23339894\n",
            "Iteration 8, loss = 0.22748578\n",
            "Iteration 9, loss = 0.22161832\n",
            "Iteration 10, loss = 0.21753254\n",
            "Iteration 11, loss = 0.21532299\n",
            "Iteration 12, loss = 0.21253630\n",
            "Iteration 13, loss = 0.21010453\n",
            "Iteration 14, loss = 0.20829449\n",
            "Iteration 15, loss = 0.20744101\n",
            "Iteration 16, loss = 0.20746552\n",
            "Iteration 17, loss = 0.20683662\n",
            "Iteration 18, loss = 0.20698863\n",
            "Iteration 19, loss = 0.20472099\n",
            "Iteration 20, loss = 0.20678345\n",
            "Iteration 21, loss = 0.20529897\n",
            "Iteration 22, loss = 0.20491277\n",
            "Iteration 23, loss = 0.20654315\n",
            "Iteration 24, loss = 0.20477440\n",
            "Iteration 25, loss = 0.20436359\n",
            "Iteration 26, loss = 0.20432734\n",
            "Iteration 27, loss = 0.20504919\n",
            "Iteration 28, loss = 0.20518460\n",
            "Iteration 29, loss = 0.20498125\n",
            "Iteration 30, loss = 0.20550050\n",
            "Iteration 31, loss = 0.20649401\n",
            "Iteration 32, loss = 0.20351569\n",
            "Iteration 33, loss = 0.20603977\n",
            "Iteration 34, loss = 0.20528082\n",
            "Iteration 35, loss = 0.20568131\n",
            "Iteration 36, loss = 0.20446623\n",
            "Iteration 37, loss = 0.20547876\n",
            "Iteration 38, loss = 0.20501917\n",
            "Iteration 39, loss = 0.20524001\n",
            "Iteration 40, loss = 0.20500277\n",
            "Iteration 41, loss = 0.20492554\n",
            "Iteration 42, loss = 0.20497360\n",
            "Iteration 43, loss = 0.20562727\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.19676822\n",
            "Iteration 45, loss = 0.19575203\n",
            "Iteration 46, loss = 0.19502431\n",
            "Iteration 47, loss = 0.19437092\n",
            "Iteration 48, loss = 0.19362216\n",
            "Iteration 49, loss = 0.19324438\n",
            "Iteration 50, loss = 0.19214894\n",
            "Iteration 51, loss = 0.19151839\n",
            "Iteration 52, loss = 0.19145394\n",
            "Iteration 53, loss = 0.19093919\n",
            "Iteration 54, loss = 0.19016744\n",
            "Iteration 55, loss = 0.18999581\n",
            "Iteration 56, loss = 0.18945598\n",
            "Iteration 57, loss = 0.18913882\n",
            "Iteration 58, loss = 0.18843361\n",
            "Iteration 59, loss = 0.18803059\n",
            "Iteration 60, loss = 0.18734975\n",
            "Iteration 61, loss = 0.18703525\n",
            "Iteration 62, loss = 0.18686373\n",
            "Iteration 63, loss = 0.18619180\n",
            "Iteration 64, loss = 0.18679851\n",
            "Iteration 65, loss = 0.18595693\n",
            "Iteration 66, loss = 0.18560748\n",
            "Iteration 67, loss = 0.18594890\n",
            "Iteration 68, loss = 0.18560167\n",
            "Iteration 69, loss = 0.18542790\n",
            "Iteration 70, loss = 0.18447831\n",
            "Iteration 71, loss = 0.18515694\n",
            "Iteration 72, loss = 0.18583278\n",
            "Iteration 73, loss = 0.18444477\n",
            "Iteration 74, loss = 0.18477787\n",
            "Iteration 75, loss = 0.18497743\n",
            "Iteration 76, loss = 0.18372454\n",
            "Iteration 77, loss = 0.18325453\n",
            "Iteration 78, loss = 0.18463892\n",
            "Iteration 79, loss = 0.18445750\n",
            "Iteration 80, loss = 0.18699955\n",
            "Iteration 81, loss = 0.18454696\n",
            "Iteration 82, loss = 0.18455533\n",
            "Iteration 83, loss = 0.18311392\n",
            "Iteration 84, loss = 0.18507713\n",
            "Iteration 85, loss = 0.18366053\n",
            "Iteration 86, loss = 0.18495507\n",
            "Iteration 87, loss = 0.18594991\n",
            "Iteration 88, loss = 0.18561891\n",
            "Iteration 89, loss = 0.18609201\n",
            "Iteration 90, loss = 0.19005478\n",
            "Iteration 91, loss = 0.18481020\n",
            "Iteration 92, loss = 0.18408816\n",
            "Iteration 93, loss = 0.18309906\n",
            "Iteration 94, loss = 0.18800299\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 95, loss = 0.17349524\n",
            "Iteration 96, loss = 0.17338801\n",
            "Iteration 97, loss = 0.17321355\n",
            "Iteration 98, loss = 0.17296587\n",
            "Iteration 99, loss = 0.17320695\n",
            "Iteration 100, loss = 0.17259883\n",
            "Iteration 101, loss = 0.17251445\n",
            "Iteration 102, loss = 0.17258771\n",
            "Iteration 103, loss = 0.17239289\n",
            "Iteration 104, loss = 0.17250198\n",
            "Iteration 105, loss = 0.17253345\n",
            "Iteration 106, loss = 0.17208970\n",
            "Iteration 107, loss = 0.17206960\n",
            "Iteration 108, loss = 0.17192611\n",
            "Iteration 109, loss = 0.17192399\n",
            "Iteration 110, loss = 0.17168947\n",
            "Iteration 111, loss = 0.17150308\n",
            "Iteration 112, loss = 0.17149408\n",
            "Iteration 113, loss = 0.17163401\n",
            "Iteration 114, loss = 0.17147156\n",
            "Iteration 115, loss = 0.17107403\n",
            "Iteration 116, loss = 0.17106037\n",
            "Iteration 117, loss = 0.17091072\n",
            "Iteration 118, loss = 0.17110680\n",
            "Iteration 119, loss = 0.17084846\n",
            "Iteration 120, loss = 0.17069061\n",
            "Iteration 121, loss = 0.17059345\n",
            "Iteration 122, loss = 0.17063102\n",
            "Iteration 123, loss = 0.17046053\n",
            "Iteration 124, loss = 0.17050635\n",
            "Iteration 125, loss = 0.17023240\n",
            "Iteration 126, loss = 0.17041138\n",
            "Iteration 127, loss = 0.17001670\n",
            "Iteration 128, loss = 0.16987121\n",
            "Iteration 129, loss = 0.16985168\n",
            "Iteration 130, loss = 0.16982573\n",
            "Iteration 131, loss = 0.16990938\n",
            "Iteration 132, loss = 0.16975027\n",
            "Iteration 133, loss = 0.16950746\n",
            "Iteration 134, loss = 0.16959920\n",
            "Iteration 135, loss = 0.16945333\n",
            "Iteration 136, loss = 0.16928514\n",
            "Iteration 137, loss = 0.16946692\n",
            "Iteration 138, loss = 0.16933724\n",
            "Iteration 139, loss = 0.16912191\n",
            "Iteration 140, loss = 0.16912142\n",
            "Iteration 141, loss = 0.16909750\n",
            "Iteration 142, loss = 0.16888522\n",
            "Iteration 143, loss = 0.16866096\n",
            "Iteration 144, loss = 0.16894523\n",
            "Iteration 145, loss = 0.16902123\n",
            "Iteration 146, loss = 0.16871445\n",
            "Iteration 147, loss = 0.16861166\n",
            "Iteration 148, loss = 0.16851547\n",
            "Iteration 149, loss = 0.16851586\n",
            "Iteration 150, loss = 0.16822690\n",
            "Iteration 151, loss = 0.16837050\n",
            "Iteration 152, loss = 0.16834538\n",
            "Iteration 153, loss = 0.16835307\n",
            "Iteration 154, loss = 0.16812764\n",
            "Iteration 155, loss = 0.16842159\n",
            "Iteration 156, loss = 0.16803988\n",
            "Iteration 157, loss = 0.16809012\n",
            "Iteration 158, loss = 0.16780056\n",
            "Iteration 159, loss = 0.16793359\n",
            "Iteration 160, loss = 0.16771411\n",
            "Iteration 161, loss = 0.16770017\n",
            "Iteration 162, loss = 0.16766892\n",
            "Iteration 163, loss = 0.16757371\n",
            "Iteration 164, loss = 0.16766844\n",
            "Iteration 165, loss = 0.16750452\n",
            "Iteration 166, loss = 0.16734618\n",
            "Iteration 167, loss = 0.16744661\n",
            "Iteration 168, loss = 0.16719162\n",
            "Iteration 169, loss = 0.16730263\n",
            "Iteration 170, loss = 0.16711400\n",
            "Iteration 171, loss = 0.16751535\n",
            "Iteration 172, loss = 0.16726737\n",
            "Iteration 173, loss = 0.16712655\n",
            "Iteration 174, loss = 0.16705428\n",
            "Iteration 175, loss = 0.16712937\n",
            "Iteration 176, loss = 0.16693861\n",
            "Iteration 177, loss = 0.16700636\n",
            "Iteration 178, loss = 0.16684971\n",
            "Iteration 179, loss = 0.16682685\n",
            "Iteration 180, loss = 0.16659026\n",
            "Iteration 181, loss = 0.16686993\n",
            "Iteration 182, loss = 0.16679342\n",
            "Iteration 183, loss = 0.16665255\n",
            "Iteration 184, loss = 0.16652056\n",
            "Iteration 185, loss = 0.16659066\n",
            "Iteration 186, loss = 0.16654794\n",
            "Iteration 187, loss = 0.16642289\n",
            "Iteration 188, loss = 0.16652231\n",
            "Iteration 189, loss = 0.16640199\n",
            "Iteration 190, loss = 0.16640133\n",
            "Iteration 191, loss = 0.16640369\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 192, loss = 0.16544330\n",
            "Iteration 193, loss = 0.16538568\n",
            "Iteration 194, loss = 0.16533176\n",
            "Iteration 195, loss = 0.16529136\n",
            "Iteration 196, loss = 0.16531808\n",
            "Iteration 197, loss = 0.16518634\n",
            "Iteration 198, loss = 0.16528477\n",
            "Iteration 199, loss = 0.16549005\n",
            "Iteration 200, loss = 0.16527293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32459513\n",
            "Iteration 3, loss = 0.29515147\n",
            "Iteration 4, loss = 0.27287765\n",
            "Iteration 5, loss = 0.25683433\n",
            "Iteration 6, loss = 0.24421533\n",
            "Iteration 7, loss = 0.23460494\n",
            "Iteration 8, loss = 0.22753317\n",
            "Iteration 9, loss = 0.22221474\n",
            "Iteration 10, loss = 0.21783364\n",
            "Iteration 11, loss = 0.21459398\n",
            "Iteration 12, loss = 0.21203143\n",
            "Iteration 13, loss = 0.21069661\n",
            "Iteration 14, loss = 0.20831180\n",
            "Iteration 15, loss = 0.20752125\n",
            "Iteration 16, loss = 0.20604516\n",
            "Iteration 17, loss = 0.20582380\n",
            "Iteration 18, loss = 0.20489646\n",
            "Iteration 19, loss = 0.20478295\n",
            "Iteration 20, loss = 0.20592589\n",
            "Iteration 21, loss = 0.20419189\n",
            "Iteration 22, loss = 0.20504367\n",
            "Iteration 23, loss = 0.20454049\n",
            "Iteration 24, loss = 0.20417011\n",
            "Iteration 25, loss = 0.20370988\n",
            "Iteration 26, loss = 0.20521788\n",
            "Iteration 27, loss = 0.20481180\n",
            "Iteration 28, loss = 0.20383673\n",
            "Iteration 29, loss = 0.20501125\n",
            "Iteration 30, loss = 0.20481680\n",
            "Iteration 31, loss = 0.20443494\n",
            "Iteration 32, loss = 0.20439534\n",
            "Iteration 33, loss = 0.20469925\n",
            "Iteration 34, loss = 0.20481946\n",
            "Iteration 35, loss = 0.20377788\n",
            "Iteration 36, loss = 0.20451433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.19707093\n",
            "Iteration 38, loss = 0.19612796\n",
            "Iteration 39, loss = 0.19554603\n",
            "Iteration 40, loss = 0.19474637\n",
            "Iteration 41, loss = 0.19392725\n",
            "Iteration 42, loss = 0.19333067\n",
            "Iteration 43, loss = 0.19343656\n",
            "Iteration 44, loss = 0.19279631\n",
            "Iteration 45, loss = 0.19205056\n",
            "Iteration 46, loss = 0.19155956\n",
            "Iteration 47, loss = 0.19098018\n",
            "Iteration 48, loss = 0.19068093\n",
            "Iteration 49, loss = 0.19020074\n",
            "Iteration 50, loss = 0.18966694\n",
            "Iteration 51, loss = 0.18953148\n",
            "Iteration 52, loss = 0.18906428\n",
            "Iteration 53, loss = 0.18851496\n",
            "Iteration 54, loss = 0.18775192\n",
            "Iteration 55, loss = 0.18745648\n",
            "Iteration 56, loss = 0.18705210\n",
            "Iteration 57, loss = 0.18708967\n",
            "Iteration 58, loss = 0.18659436\n",
            "Iteration 59, loss = 0.18689907\n",
            "Iteration 60, loss = 0.18591446\n",
            "Iteration 61, loss = 0.18618194\n",
            "Iteration 62, loss = 0.18510576\n",
            "Iteration 63, loss = 0.18530569\n",
            "Iteration 64, loss = 0.18499086\n",
            "Iteration 65, loss = 0.18549236\n",
            "Iteration 66, loss = 0.18371388\n",
            "Iteration 67, loss = 0.18592625\n",
            "Iteration 68, loss = 0.18614654\n",
            "Iteration 69, loss = 0.18421500\n",
            "Iteration 70, loss = 0.18548364\n",
            "Iteration 71, loss = 0.18460847\n",
            "Iteration 72, loss = 0.18688405\n",
            "Iteration 73, loss = 0.18555810\n",
            "Iteration 74, loss = 0.18558345\n",
            "Iteration 75, loss = 0.18607377\n",
            "Iteration 76, loss = 0.18528954\n",
            "Iteration 77, loss = 0.18531103\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 78, loss = 0.17605366\n",
            "Iteration 79, loss = 0.17608454\n",
            "Iteration 80, loss = 0.17593346\n",
            "Iteration 81, loss = 0.17593955\n",
            "Iteration 82, loss = 0.17582329\n",
            "Iteration 83, loss = 0.17577614\n",
            "Iteration 84, loss = 0.17571498\n",
            "Iteration 85, loss = 0.17533901\n",
            "Iteration 86, loss = 0.17523267\n",
            "Iteration 87, loss = 0.17508786\n",
            "Iteration 88, loss = 0.17505076\n",
            "Iteration 89, loss = 0.17468869\n",
            "Iteration 90, loss = 0.17482993\n",
            "Iteration 91, loss = 0.17490248\n",
            "Iteration 92, loss = 0.17450288\n",
            "Iteration 93, loss = 0.17434563\n",
            "Iteration 94, loss = 0.17428069\n",
            "Iteration 95, loss = 0.17438868\n",
            "Iteration 96, loss = 0.17424646\n",
            "Iteration 97, loss = 0.17405104\n",
            "Iteration 98, loss = 0.17373037\n",
            "Iteration 99, loss = 0.17390338\n",
            "Iteration 100, loss = 0.17389949\n",
            "Iteration 101, loss = 0.17361071\n",
            "Iteration 102, loss = 0.17360519\n",
            "Iteration 103, loss = 0.17334750\n",
            "Iteration 104, loss = 0.17332440\n",
            "Iteration 105, loss = 0.17312786\n",
            "Iteration 106, loss = 0.17297397\n",
            "Iteration 107, loss = 0.17298898\n",
            "Iteration 108, loss = 0.17297032\n",
            "Iteration 109, loss = 0.17306780\n",
            "Iteration 110, loss = 0.17264109\n",
            "Iteration 111, loss = 0.17258124\n",
            "Iteration 112, loss = 0.17255867\n",
            "Iteration 113, loss = 0.17241137\n",
            "Iteration 114, loss = 0.17223667\n",
            "Iteration 115, loss = 0.17234994\n",
            "Iteration 116, loss = 0.17215299\n",
            "Iteration 117, loss = 0.17198794\n",
            "Iteration 118, loss = 0.17188157\n",
            "Iteration 119, loss = 0.17191257\n",
            "Iteration 120, loss = 0.17188863\n",
            "Iteration 121, loss = 0.17197890\n",
            "Iteration 122, loss = 0.17178828\n",
            "Iteration 123, loss = 0.17154901\n",
            "Iteration 124, loss = 0.17134983\n",
            "Iteration 125, loss = 0.17138748\n",
            "Iteration 126, loss = 0.17125870\n",
            "Iteration 127, loss = 0.17126520\n",
            "Iteration 128, loss = 0.17121894\n",
            "Iteration 129, loss = 0.17124915\n",
            "Iteration 130, loss = 0.17113919\n",
            "Iteration 131, loss = 0.17096295\n",
            "Iteration 132, loss = 0.17078809\n",
            "Iteration 133, loss = 0.17086653\n",
            "Iteration 134, loss = 0.17085995\n",
            "Iteration 135, loss = 0.17035548\n",
            "Iteration 136, loss = 0.17059266\n",
            "Iteration 137, loss = 0.17032337\n",
            "Iteration 138, loss = 0.17045582\n",
            "Iteration 139, loss = 0.17033746\n",
            "Iteration 140, loss = 0.17044496\n",
            "Iteration 141, loss = 0.17027434\n",
            "Iteration 142, loss = 0.17000100\n",
            "Iteration 143, loss = 0.17003260\n",
            "Iteration 144, loss = 0.16993530\n",
            "Iteration 145, loss = 0.16973092\n",
            "Iteration 146, loss = 0.16993608\n",
            "Iteration 147, loss = 0.16992410\n",
            "Iteration 148, loss = 0.16975420\n",
            "Iteration 149, loss = 0.16987599\n",
            "Iteration 150, loss = 0.16962956\n",
            "Iteration 151, loss = 0.16969517\n",
            "Iteration 152, loss = 0.16943627\n",
            "Iteration 153, loss = 0.16938746\n",
            "Iteration 154, loss = 0.16927290\n",
            "Iteration 155, loss = 0.16937119\n",
            "Iteration 156, loss = 0.16919599\n",
            "Iteration 157, loss = 0.16928312\n",
            "Iteration 158, loss = 0.16903588\n",
            "Iteration 159, loss = 0.16895099\n",
            "Iteration 160, loss = 0.16884424\n",
            "Iteration 161, loss = 0.16875226\n",
            "Iteration 162, loss = 0.16900920\n",
            "Iteration 163, loss = 0.16885354\n",
            "Iteration 164, loss = 0.16868249\n",
            "Iteration 165, loss = 0.16882295\n",
            "Iteration 166, loss = 0.16879362\n",
            "Iteration 167, loss = 0.16854679\n",
            "Iteration 168, loss = 0.16844027\n",
            "Iteration 169, loss = 0.16857664\n",
            "Iteration 170, loss = 0.16835003\n",
            "Iteration 171, loss = 0.16832274\n",
            "Iteration 172, loss = 0.16814422\n",
            "Iteration 173, loss = 0.16814309\n",
            "Iteration 174, loss = 0.16823337\n",
            "Iteration 175, loss = 0.16829736\n",
            "Iteration 176, loss = 0.16790770\n",
            "Iteration 177, loss = 0.16807908\n",
            "Iteration 178, loss = 0.16793808\n",
            "Iteration 179, loss = 0.16779901\n",
            "Iteration 180, loss = 0.16782473\n",
            "Iteration 181, loss = 0.16788348\n",
            "Iteration 182, loss = 0.16783769\n",
            "Iteration 183, loss = 0.16767699\n",
            "Iteration 184, loss = 0.16775643\n",
            "Iteration 185, loss = 0.16764782\n",
            "Iteration 186, loss = 0.16752416\n",
            "Iteration 187, loss = 0.16769697\n",
            "Iteration 188, loss = 0.16756874\n",
            "Iteration 189, loss = 0.16734260\n",
            "Iteration 190, loss = 0.16749530\n",
            "Iteration 191, loss = 0.16738135\n",
            "Iteration 192, loss = 0.16732734\n",
            "Iteration 193, loss = 0.16739915\n",
            "Iteration 194, loss = 0.16740677\n",
            "Iteration 195, loss = 0.16717738\n",
            "Iteration 196, loss = 0.16712631\n",
            "Iteration 197, loss = 0.16721998\n",
            "Iteration 198, loss = 0.16693012\n",
            "Iteration 199, loss = 0.16717356\n",
            "Iteration 200, loss = 0.16730891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.64262123\n",
            "Iteration 2, loss = 0.29570518\n",
            "Iteration 3, loss = 0.27309472\n",
            "Iteration 4, loss = 0.25666485\n",
            "Iteration 5, loss = 0.24415860\n",
            "Iteration 6, loss = 0.23464440\n",
            "Iteration 7, loss = 0.22717142\n",
            "Iteration 8, loss = 0.22157180\n",
            "Iteration 9, loss = 0.21646736\n",
            "Iteration 10, loss = 0.21404714\n",
            "Iteration 11, loss = 0.21181209\n",
            "Iteration 12, loss = 0.20984264\n",
            "Iteration 13, loss = 0.20891653\n",
            "Iteration 14, loss = 0.20728654\n",
            "Iteration 15, loss = 0.20731587\n",
            "Iteration 16, loss = 0.20589608\n",
            "Iteration 17, loss = 0.20604573\n",
            "Iteration 18, loss = 0.20528632\n",
            "Iteration 19, loss = 0.20576151\n",
            "Iteration 20, loss = 0.20438788\n",
            "Iteration 21, loss = 0.20598370\n",
            "Iteration 22, loss = 0.20485391\n",
            "Iteration 23, loss = 0.20476661\n",
            "Iteration 24, loss = 0.20520901\n",
            "Iteration 25, loss = 0.20432634\n",
            "Iteration 26, loss = 0.20493620\n",
            "Iteration 27, loss = 0.20534524\n",
            "Iteration 28, loss = 0.20571435\n",
            "Iteration 29, loss = 0.20478706\n",
            "Iteration 30, loss = 0.20470008\n",
            "Iteration 31, loss = 0.20487882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.19731785\n",
            "Iteration 33, loss = 0.19648734\n",
            "Iteration 34, loss = 0.19569720\n",
            "Iteration 35, loss = 0.19491201\n",
            "Iteration 36, loss = 0.19420157\n",
            "Iteration 37, loss = 0.19377534\n",
            "Iteration 38, loss = 0.19303669\n",
            "Iteration 39, loss = 0.19255810\n",
            "Iteration 40, loss = 0.19199382\n",
            "Iteration 41, loss = 0.19160062\n",
            "Iteration 42, loss = 0.19143576\n",
            "Iteration 43, loss = 0.19091232\n",
            "Iteration 44, loss = 0.19047756\n",
            "Iteration 45, loss = 0.18969906\n",
            "Iteration 46, loss = 0.18924904\n",
            "Iteration 47, loss = 0.18871569\n",
            "Iteration 48, loss = 0.18849116\n",
            "Iteration 49, loss = 0.18774139\n",
            "Iteration 50, loss = 0.18796861\n",
            "Iteration 51, loss = 0.18803572\n",
            "Iteration 52, loss = 0.18749690\n",
            "Iteration 53, loss = 0.18624679\n",
            "Iteration 54, loss = 0.18715277\n",
            "Iteration 55, loss = 0.18636119\n",
            "Iteration 56, loss = 0.18628251\n",
            "Iteration 57, loss = 0.18589570\n",
            "Iteration 58, loss = 0.18597854\n",
            "Iteration 59, loss = 0.18595837\n",
            "Iteration 60, loss = 0.18581037\n",
            "Iteration 61, loss = 0.18591638\n",
            "Iteration 62, loss = 0.18681970\n",
            "Iteration 63, loss = 0.18459521\n",
            "Iteration 64, loss = 0.18483834\n",
            "Iteration 65, loss = 0.18548712\n",
            "Iteration 66, loss = 0.18523901\n",
            "Iteration 67, loss = 0.18456004\n",
            "Iteration 68, loss = 0.18401524\n",
            "Iteration 69, loss = 0.18478684\n",
            "Iteration 70, loss = 0.18558826\n",
            "Iteration 71, loss = 0.18515871\n",
            "Iteration 72, loss = 0.18390069\n",
            "Iteration 73, loss = 0.18455896\n",
            "Iteration 74, loss = 0.18565586\n",
            "Iteration 75, loss = 0.18620873\n",
            "Iteration 76, loss = 0.18734370\n",
            "Iteration 77, loss = 0.18624942\n",
            "Iteration 78, loss = 0.18625189\n",
            "Iteration 79, loss = 0.18646149\n",
            "Iteration 80, loss = 0.18654004\n",
            "Iteration 81, loss = 0.18561007\n",
            "Iteration 82, loss = 0.18273452\n",
            "Iteration 83, loss = 0.18568628\n",
            "Iteration 84, loss = 0.18945937\n",
            "Iteration 85, loss = 0.18654588\n",
            "Iteration 86, loss = 0.18656446\n",
            "Iteration 87, loss = 0.18793592\n",
            "Iteration 88, loss = 0.18855336\n",
            "Iteration 89, loss = 0.18463258\n",
            "Iteration 90, loss = 0.18412352\n",
            "Iteration 91, loss = 0.18765360\n",
            "Iteration 92, loss = 0.18868145\n",
            "Iteration 93, loss = 0.18971529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 94, loss = 0.17335834\n",
            "Iteration 95, loss = 0.17347831\n",
            "Iteration 96, loss = 0.17309914\n",
            "Iteration 97, loss = 0.17319858\n",
            "Iteration 98, loss = 0.17298519\n",
            "Iteration 99, loss = 0.17262605\n",
            "Iteration 100, loss = 0.17278536\n",
            "Iteration 101, loss = 0.17292544\n",
            "Iteration 102, loss = 0.17235223\n",
            "Iteration 103, loss = 0.17241255\n",
            "Iteration 104, loss = 0.17235717\n",
            "Iteration 105, loss = 0.17201769\n",
            "Iteration 106, loss = 0.17199682\n",
            "Iteration 107, loss = 0.17180740\n",
            "Iteration 108, loss = 0.17199213\n",
            "Iteration 109, loss = 0.17162977\n",
            "Iteration 110, loss = 0.17165200\n",
            "Iteration 111, loss = 0.17145281\n",
            "Iteration 112, loss = 0.17135283\n",
            "Iteration 113, loss = 0.17112090\n",
            "Iteration 114, loss = 0.17127248\n",
            "Iteration 115, loss = 0.17111215\n",
            "Iteration 116, loss = 0.17122051\n",
            "Iteration 117, loss = 0.17102799\n",
            "Iteration 118, loss = 0.17086229\n",
            "Iteration 119, loss = 0.17064313\n",
            "Iteration 120, loss = 0.17073852\n",
            "Iteration 121, loss = 0.17043799\n",
            "Iteration 122, loss = 0.17061977\n",
            "Iteration 123, loss = 0.17043815\n",
            "Iteration 124, loss = 0.17035844\n",
            "Iteration 125, loss = 0.17030165\n",
            "Iteration 126, loss = 0.17014618\n",
            "Iteration 127, loss = 0.17002591\n",
            "Iteration 128, loss = 0.16993580\n",
            "Iteration 129, loss = 0.17014521\n",
            "Iteration 130, loss = 0.16986790\n",
            "Iteration 131, loss = 0.16973466\n",
            "Iteration 132, loss = 0.16977492\n",
            "Iteration 133, loss = 0.16974202\n",
            "Iteration 134, loss = 0.16971792\n",
            "Iteration 135, loss = 0.16961418\n",
            "Iteration 136, loss = 0.16953651\n",
            "Iteration 137, loss = 0.16928789\n",
            "Iteration 138, loss = 0.16927842\n",
            "Iteration 139, loss = 0.16931863\n",
            "Iteration 140, loss = 0.16939790\n",
            "Iteration 141, loss = 0.16910159\n",
            "Iteration 142, loss = 0.16903974\n",
            "Iteration 143, loss = 0.16890793\n",
            "Iteration 144, loss = 0.16907327\n",
            "Iteration 145, loss = 0.16891731\n",
            "Iteration 146, loss = 0.16893629\n",
            "Iteration 147, loss = 0.16863651\n",
            "Iteration 148, loss = 0.16877292\n",
            "Iteration 149, loss = 0.16864548\n",
            "Iteration 150, loss = 0.16871317\n",
            "Iteration 151, loss = 0.16850065\n",
            "Iteration 152, loss = 0.16855585\n",
            "Iteration 153, loss = 0.16844894\n",
            "Iteration 154, loss = 0.16850398\n",
            "Iteration 155, loss = 0.16818083\n",
            "Iteration 156, loss = 0.16849635\n",
            "Iteration 157, loss = 0.16825228\n",
            "Iteration 158, loss = 0.16821619\n",
            "Iteration 159, loss = 0.16810615\n",
            "Iteration 160, loss = 0.16819049\n",
            "Iteration 161, loss = 0.16798416\n",
            "Iteration 162, loss = 0.16775019\n",
            "Iteration 163, loss = 0.16795737\n",
            "Iteration 164, loss = 0.16786168\n",
            "Iteration 165, loss = 0.16777081\n",
            "Iteration 166, loss = 0.16789136\n",
            "Iteration 167, loss = 0.16762985\n",
            "Iteration 168, loss = 0.16763009\n",
            "Iteration 169, loss = 0.16760752\n",
            "Iteration 170, loss = 0.16745734\n",
            "Iteration 171, loss = 0.16748411\n",
            "Iteration 172, loss = 0.16744573\n",
            "Iteration 173, loss = 0.16756848\n",
            "Iteration 174, loss = 0.16744623\n",
            "Iteration 175, loss = 0.16722428\n",
            "Iteration 176, loss = 0.16731773\n",
            "Iteration 177, loss = 0.16700906\n",
            "Iteration 178, loss = 0.16723390\n",
            "Iteration 179, loss = 0.16718308\n",
            "Iteration 180, loss = 0.16723160\n",
            "Iteration 181, loss = 0.16719586\n",
            "Iteration 182, loss = 0.16726713\n",
            "Iteration 183, loss = 0.16691396\n",
            "Iteration 184, loss = 0.16700220\n",
            "Iteration 185, loss = 0.16708880\n",
            "Iteration 186, loss = 0.16686314\n",
            "Iteration 187, loss = 0.16693345\n",
            "Iteration 188, loss = 0.16685565\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 189, loss = 0.16592566\n",
            "Iteration 190, loss = 0.16599017\n",
            "Iteration 191, loss = 0.16592996\n",
            "Iteration 192, loss = 0.16589627\n",
            "Iteration 193, loss = 0.16584838\n",
            "Iteration 194, loss = 0.16586904\n",
            "Iteration 195, loss = 0.16591701\n",
            "Iteration 196, loss = 0.16596685\n",
            "Iteration 197, loss = 0.16600226\n",
            "Iteration 198, loss = 0.16592484\n",
            "Iteration 199, loss = 0.16605036\n",
            "Iteration 200, loss = 0.16593174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32376450\n",
            "Iteration 3, loss = 0.29427970\n",
            "Iteration 4, loss = 0.27285555\n",
            "Iteration 5, loss = 0.25679576\n",
            "Iteration 6, loss = 0.24400930\n",
            "Iteration 7, loss = 0.23454475\n",
            "Iteration 8, loss = 0.22724365\n",
            "Iteration 9, loss = 0.22093987\n",
            "Iteration 10, loss = 0.21696867\n",
            "Iteration 11, loss = 0.21405522\n",
            "Iteration 12, loss = 0.21193145\n",
            "Iteration 13, loss = 0.20982411\n",
            "Iteration 14, loss = 0.20925317\n",
            "Iteration 15, loss = 0.20735339\n",
            "Iteration 16, loss = 0.20693082\n",
            "Iteration 17, loss = 0.20649758\n",
            "Iteration 18, loss = 0.20605832\n",
            "Iteration 19, loss = 0.20651438\n",
            "Iteration 20, loss = 0.20534451\n",
            "Iteration 21, loss = 0.20506961\n",
            "Iteration 22, loss = 0.20492248\n",
            "Iteration 23, loss = 0.20525423\n",
            "Iteration 24, loss = 0.20439439\n",
            "Iteration 25, loss = 0.20430794\n",
            "Iteration 26, loss = 0.20498317\n",
            "Iteration 27, loss = 0.20508902\n",
            "Iteration 28, loss = 0.20474188\n",
            "Iteration 29, loss = 0.20492051\n",
            "Iteration 30, loss = 0.20495053\n",
            "Iteration 31, loss = 0.20484274\n",
            "Iteration 32, loss = 0.20501420\n",
            "Iteration 33, loss = 0.20459965\n",
            "Iteration 34, loss = 0.20478832\n",
            "Iteration 35, loss = 0.20568251\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.19884739\n",
            "Iteration 37, loss = 0.19785604\n",
            "Iteration 38, loss = 0.19690572\n",
            "Iteration 39, loss = 0.19669426\n",
            "Iteration 40, loss = 0.19589069\n",
            "Iteration 41, loss = 0.19571267\n",
            "Iteration 42, loss = 0.19501970\n",
            "Iteration 43, loss = 0.19478913\n",
            "Iteration 44, loss = 0.19419060\n",
            "Iteration 45, loss = 0.19401959\n",
            "Iteration 46, loss = 0.19323681\n",
            "Iteration 47, loss = 0.19238765\n",
            "Iteration 48, loss = 0.19236663\n",
            "Iteration 49, loss = 0.19176871\n",
            "Iteration 50, loss = 0.19143926\n",
            "Iteration 51, loss = 0.19083575\n",
            "Iteration 52, loss = 0.19053701\n",
            "Iteration 53, loss = 0.19033461\n",
            "Iteration 54, loss = 0.18976724\n",
            "Iteration 55, loss = 0.18934360\n",
            "Iteration 56, loss = 0.18883762\n",
            "Iteration 57, loss = 0.18863027\n",
            "Iteration 58, loss = 0.18828983\n",
            "Iteration 59, loss = 0.18768336\n",
            "Iteration 60, loss = 0.18739172\n",
            "Iteration 61, loss = 0.18747855\n",
            "Iteration 62, loss = 0.18716416\n",
            "Iteration 63, loss = 0.18681580\n",
            "Iteration 64, loss = 0.18593289\n",
            "Iteration 65, loss = 0.18725310\n",
            "Iteration 66, loss = 0.18494451\n",
            "Iteration 67, loss = 0.18574074\n",
            "Iteration 68, loss = 0.18549804\n",
            "Iteration 69, loss = 0.18691798\n",
            "Iteration 70, loss = 0.18619823\n",
            "Iteration 71, loss = 0.18627984\n",
            "Iteration 72, loss = 0.18540219\n",
            "Iteration 73, loss = 0.18520841\n",
            "Iteration 74, loss = 0.18714556\n",
            "Iteration 75, loss = 0.18581158\n",
            "Iteration 76, loss = 0.18469152\n",
            "Iteration 77, loss = 0.18705795\n",
            "Iteration 78, loss = 0.18546989\n",
            "Iteration 79, loss = 0.18560443\n",
            "Iteration 80, loss = 0.18579450\n",
            "Iteration 81, loss = 0.18494529\n",
            "Iteration 82, loss = 0.18681464\n",
            "Iteration 83, loss = 0.18585797\n",
            "Iteration 84, loss = 0.18390190\n",
            "Iteration 85, loss = 0.18531148\n",
            "Iteration 86, loss = 0.18582370\n",
            "Iteration 87, loss = 0.18876740\n",
            "Iteration 88, loss = 0.18423821\n",
            "Iteration 89, loss = 0.18652374\n",
            "Iteration 90, loss = 0.18702344\n",
            "Iteration 91, loss = 0.18664070\n",
            "Iteration 92, loss = 0.18701598\n",
            "Iteration 93, loss = 0.18484548\n",
            "Iteration 94, loss = 0.18725685\n",
            "Iteration 95, loss = 0.18834111\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.17423212\n",
            "Iteration 97, loss = 0.17397158\n",
            "Iteration 98, loss = 0.17373020\n",
            "Iteration 99, loss = 0.17377539\n",
            "Iteration 100, loss = 0.17383303\n",
            "Iteration 101, loss = 0.17342280\n",
            "Iteration 102, loss = 0.17340715\n",
            "Iteration 103, loss = 0.17320464\n",
            "Iteration 104, loss = 0.17313390\n",
            "Iteration 105, loss = 0.17301985\n",
            "Iteration 106, loss = 0.17286870\n",
            "Iteration 107, loss = 0.17278933\n",
            "Iteration 108, loss = 0.17274992\n",
            "Iteration 109, loss = 0.17269603\n",
            "Iteration 110, loss = 0.17271871\n",
            "Iteration 111, loss = 0.17252029\n",
            "Iteration 112, loss = 0.17250685\n",
            "Iteration 113, loss = 0.17232816\n",
            "Iteration 114, loss = 0.17223440\n",
            "Iteration 115, loss = 0.17203378\n",
            "Iteration 116, loss = 0.17196289\n",
            "Iteration 117, loss = 0.17174000\n",
            "Iteration 118, loss = 0.17189596\n",
            "Iteration 119, loss = 0.17187205\n",
            "Iteration 120, loss = 0.17166008\n",
            "Iteration 121, loss = 0.17151109\n",
            "Iteration 122, loss = 0.17148998\n",
            "Iteration 123, loss = 0.17143064\n",
            "Iteration 124, loss = 0.17156561\n",
            "Iteration 125, loss = 0.17106999\n",
            "Iteration 126, loss = 0.17119998\n",
            "Iteration 127, loss = 0.17110175\n",
            "Iteration 128, loss = 0.17097418\n",
            "Iteration 129, loss = 0.17093717\n",
            "Iteration 130, loss = 0.17084105\n",
            "Iteration 131, loss = 0.17074509\n",
            "Iteration 132, loss = 0.17068373\n",
            "Iteration 133, loss = 0.17062870\n",
            "Iteration 134, loss = 0.17061945\n",
            "Iteration 135, loss = 0.17053223\n",
            "Iteration 136, loss = 0.17037671\n",
            "Iteration 137, loss = 0.17029186\n",
            "Iteration 138, loss = 0.17011247\n",
            "Iteration 139, loss = 0.17008513\n",
            "Iteration 140, loss = 0.17010864\n",
            "Iteration 141, loss = 0.17010476\n",
            "Iteration 142, loss = 0.17006007\n",
            "Iteration 143, loss = 0.16974012\n",
            "Iteration 144, loss = 0.16976201\n",
            "Iteration 145, loss = 0.16971102\n",
            "Iteration 146, loss = 0.16965645\n",
            "Iteration 147, loss = 0.16953467\n",
            "Iteration 148, loss = 0.16946153\n",
            "Iteration 149, loss = 0.16947089\n",
            "Iteration 150, loss = 0.16937526\n",
            "Iteration 151, loss = 0.16928532\n",
            "Iteration 152, loss = 0.16924158\n",
            "Iteration 153, loss = 0.16926042\n",
            "Iteration 154, loss = 0.16934069\n",
            "Iteration 155, loss = 0.16917581\n",
            "Iteration 156, loss = 0.16904088\n",
            "Iteration 157, loss = 0.16908170\n",
            "Iteration 158, loss = 0.16918844\n",
            "Iteration 159, loss = 0.16888813\n",
            "Iteration 160, loss = 0.16877988\n",
            "Iteration 161, loss = 0.16903644\n",
            "Iteration 162, loss = 0.16879712\n",
            "Iteration 163, loss = 0.16868303\n",
            "Iteration 164, loss = 0.16872478\n",
            "Iteration 165, loss = 0.16858064\n",
            "Iteration 166, loss = 0.16867731\n",
            "Iteration 167, loss = 0.16855666\n",
            "Iteration 168, loss = 0.16863952\n",
            "Iteration 169, loss = 0.16839523\n",
            "Iteration 170, loss = 0.16837065\n",
            "Iteration 171, loss = 0.16840321\n",
            "Iteration 172, loss = 0.16830589\n",
            "Iteration 173, loss = 0.16835084\n",
            "Iteration 174, loss = 0.16828706\n",
            "Iteration 175, loss = 0.16834843\n",
            "Iteration 176, loss = 0.16817342\n",
            "Iteration 177, loss = 0.16814008\n",
            "Iteration 178, loss = 0.16799670\n",
            "Iteration 179, loss = 0.16818799\n",
            "Iteration 180, loss = 0.16809116\n",
            "Iteration 181, loss = 0.16780793\n",
            "Iteration 182, loss = 0.16777927\n",
            "Iteration 183, loss = 0.16781983\n",
            "Iteration 184, loss = 0.16771873\n",
            "Iteration 185, loss = 0.16760089\n",
            "Iteration 186, loss = 0.16781302\n",
            "Iteration 187, loss = 0.16795385\n",
            "Iteration 188, loss = 0.16802133\n",
            "Iteration 189, loss = 0.16762853\n",
            "Iteration 190, loss = 0.16754504\n",
            "Iteration 191, loss = 0.16749606\n",
            "Iteration 192, loss = 0.16739224\n",
            "Iteration 193, loss = 0.16742069\n",
            "Iteration 194, loss = 0.16731932\n",
            "Iteration 195, loss = 0.16724444\n",
            "Iteration 196, loss = 0.16734905\n",
            "Iteration 197, loss = 0.16756284\n",
            "Iteration 198, loss = 0.16746176\n",
            "Iteration 199, loss = 0.16710085\n",
            "Iteration 200, loss = 0.16717523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.52267992\n",
            "Iteration 2, loss = 0.27394776\n",
            "Iteration 3, loss = 0.25609730\n",
            "Iteration 4, loss = 0.24404275\n",
            "Iteration 5, loss = 0.23345548\n",
            "Iteration 6, loss = 0.22828867\n",
            "Iteration 7, loss = 0.22239484\n",
            "Iteration 8, loss = 0.21832852\n",
            "Iteration 9, loss = 0.21536094\n",
            "Iteration 10, loss = 0.21355118\n",
            "Iteration 11, loss = 0.21199994\n",
            "Iteration 12, loss = 0.20889965\n",
            "Iteration 13, loss = 0.20843478\n",
            "Iteration 14, loss = 0.20795342\n",
            "Iteration 15, loss = 0.20724150\n",
            "Iteration 16, loss = 0.20784633\n",
            "Iteration 17, loss = 0.20570878\n",
            "Iteration 18, loss = 0.20603764\n",
            "Iteration 19, loss = 0.20563978\n",
            "Iteration 20, loss = 0.20620630\n",
            "Iteration 21, loss = 0.20612644\n",
            "Iteration 22, loss = 0.20649095\n",
            "Iteration 23, loss = 0.20561067\n",
            "Iteration 24, loss = 0.20450792\n",
            "Iteration 25, loss = 0.20698345\n",
            "Iteration 26, loss = 0.20456870\n",
            "Iteration 27, loss = 0.20543516\n",
            "Iteration 28, loss = 0.20534504\n",
            "Iteration 29, loss = 0.20628959\n",
            "Iteration 30, loss = 0.20556424\n",
            "Iteration 31, loss = 0.20506208\n",
            "Iteration 32, loss = 0.20542670\n",
            "Iteration 33, loss = 0.20621189\n",
            "Iteration 34, loss = 0.20493591\n",
            "Iteration 35, loss = 0.20501139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.19741007\n",
            "Iteration 37, loss = 0.19615483\n",
            "Iteration 38, loss = 0.19551873\n",
            "Iteration 39, loss = 0.19493828\n",
            "Iteration 40, loss = 0.19399387\n",
            "Iteration 41, loss = 0.19389155\n",
            "Iteration 42, loss = 0.19297116\n",
            "Iteration 43, loss = 0.19220718\n",
            "Iteration 44, loss = 0.19200581\n",
            "Iteration 45, loss = 0.19134306\n",
            "Iteration 46, loss = 0.19091662\n",
            "Iteration 47, loss = 0.19033732\n",
            "Iteration 48, loss = 0.18958453\n",
            "Iteration 49, loss = 0.18974402\n",
            "Iteration 50, loss = 0.18861415\n",
            "Iteration 51, loss = 0.18854875\n",
            "Iteration 52, loss = 0.18796546\n",
            "Iteration 53, loss = 0.18747713\n",
            "Iteration 54, loss = 0.18729753\n",
            "Iteration 55, loss = 0.18763095\n",
            "Iteration 56, loss = 0.18714558\n",
            "Iteration 57, loss = 0.18606197\n",
            "Iteration 58, loss = 0.18595006\n",
            "Iteration 59, loss = 0.18665039\n",
            "Iteration 60, loss = 0.18690388\n",
            "Iteration 61, loss = 0.18512460\n",
            "Iteration 62, loss = 0.18595899\n",
            "Iteration 63, loss = 0.18531139\n",
            "Iteration 64, loss = 0.18505216\n",
            "Iteration 65, loss = 0.18601873\n",
            "Iteration 66, loss = 0.18517162\n",
            "Iteration 67, loss = 0.18478008\n",
            "Iteration 68, loss = 0.18541266\n",
            "Iteration 69, loss = 0.18529977\n",
            "Iteration 70, loss = 0.18493257\n",
            "Iteration 71, loss = 0.18766503\n",
            "Iteration 72, loss = 0.18393724\n",
            "Iteration 73, loss = 0.18420298\n",
            "Iteration 74, loss = 0.18391560\n",
            "Iteration 75, loss = 0.18481616\n",
            "Iteration 76, loss = 0.18520605\n",
            "Iteration 77, loss = 0.18737680\n",
            "Iteration 78, loss = 0.18591220\n",
            "Iteration 79, loss = 0.18361042\n",
            "Iteration 80, loss = 0.18646621\n",
            "Iteration 81, loss = 0.18681581\n",
            "Iteration 82, loss = 0.18734087\n",
            "Iteration 83, loss = 0.18390303\n",
            "Iteration 84, loss = 0.18748786\n",
            "Iteration 85, loss = 0.18633973\n",
            "Iteration 86, loss = 0.18614357\n",
            "Iteration 87, loss = 0.18434631\n",
            "Iteration 88, loss = 0.18563265\n",
            "Iteration 89, loss = 0.18718722\n",
            "Iteration 90, loss = 0.18928605\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 91, loss = 0.17387597\n",
            "Iteration 92, loss = 0.17348406\n",
            "Iteration 93, loss = 0.17320030\n",
            "Iteration 94, loss = 0.17322724\n",
            "Iteration 95, loss = 0.17328008\n",
            "Iteration 96, loss = 0.17294654\n",
            "Iteration 97, loss = 0.17278969\n",
            "Iteration 98, loss = 0.17272837\n",
            "Iteration 99, loss = 0.17259207\n",
            "Iteration 100, loss = 0.17261505\n",
            "Iteration 101, loss = 0.17245451\n",
            "Iteration 102, loss = 0.17239488\n",
            "Iteration 103, loss = 0.17221373\n",
            "Iteration 104, loss = 0.17206552\n",
            "Iteration 105, loss = 0.17211306\n",
            "Iteration 106, loss = 0.17177775\n",
            "Iteration 107, loss = 0.17196733\n",
            "Iteration 108, loss = 0.17153705\n",
            "Iteration 109, loss = 0.17154485\n",
            "Iteration 110, loss = 0.17149562\n",
            "Iteration 111, loss = 0.17142373\n",
            "Iteration 112, loss = 0.17139863\n",
            "Iteration 113, loss = 0.17110341\n",
            "Iteration 114, loss = 0.17116038\n",
            "Iteration 115, loss = 0.17104833\n",
            "Iteration 116, loss = 0.17108867\n",
            "Iteration 117, loss = 0.17098400\n",
            "Iteration 118, loss = 0.17081047\n",
            "Iteration 119, loss = 0.17081209\n",
            "Iteration 120, loss = 0.17057756\n",
            "Iteration 121, loss = 0.17055180\n",
            "Iteration 122, loss = 0.17040863\n",
            "Iteration 123, loss = 0.17033564\n",
            "Iteration 124, loss = 0.17042321\n",
            "Iteration 125, loss = 0.17002400\n",
            "Iteration 126, loss = 0.17022806\n",
            "Iteration 127, loss = 0.17020565\n",
            "Iteration 128, loss = 0.16998713\n",
            "Iteration 129, loss = 0.16989453\n",
            "Iteration 130, loss = 0.17003436\n",
            "Iteration 131, loss = 0.16979370\n",
            "Iteration 132, loss = 0.16962001\n",
            "Iteration 133, loss = 0.16972297\n",
            "Iteration 134, loss = 0.16964019\n",
            "Iteration 135, loss = 0.16947337\n",
            "Iteration 136, loss = 0.16948687\n",
            "Iteration 137, loss = 0.16933619\n",
            "Iteration 138, loss = 0.16942961\n",
            "Iteration 139, loss = 0.16913805\n",
            "Iteration 140, loss = 0.16904224\n",
            "Iteration 141, loss = 0.16914055\n",
            "Iteration 142, loss = 0.16895508\n",
            "Iteration 143, loss = 0.16915549\n",
            "Iteration 144, loss = 0.16882787\n",
            "Iteration 145, loss = 0.16887920\n",
            "Iteration 146, loss = 0.16880820\n",
            "Iteration 147, loss = 0.16877174\n",
            "Iteration 148, loss = 0.16854752\n",
            "Iteration 149, loss = 0.16864994\n",
            "Iteration 150, loss = 0.16864018\n",
            "Iteration 151, loss = 0.16854722\n",
            "Iteration 152, loss = 0.16836119\n",
            "Iteration 153, loss = 0.16830682\n",
            "Iteration 154, loss = 0.16823448\n",
            "Iteration 155, loss = 0.16817635\n",
            "Iteration 156, loss = 0.16818242\n",
            "Iteration 157, loss = 0.16809319\n",
            "Iteration 158, loss = 0.16814977\n",
            "Iteration 159, loss = 0.16804494\n",
            "Iteration 160, loss = 0.16785324\n",
            "Iteration 161, loss = 0.16794680\n",
            "Iteration 162, loss = 0.16789742\n",
            "Iteration 163, loss = 0.16790790\n",
            "Iteration 164, loss = 0.16792142\n",
            "Iteration 165, loss = 0.16791510\n",
            "Iteration 166, loss = 0.16754479\n",
            "Iteration 167, loss = 0.16763683\n",
            "Iteration 168, loss = 0.16755002\n",
            "Iteration 169, loss = 0.16755047\n",
            "Iteration 170, loss = 0.16776732\n",
            "Iteration 171, loss = 0.16771458\n",
            "Iteration 172, loss = 0.16752657\n",
            "Iteration 173, loss = 0.16747054\n",
            "Iteration 174, loss = 0.16739052\n",
            "Iteration 175, loss = 0.16721762\n",
            "Iteration 176, loss = 0.16747757\n",
            "Iteration 177, loss = 0.16721010\n",
            "Iteration 178, loss = 0.16702872\n",
            "Iteration 179, loss = 0.16710236\n",
            "Iteration 180, loss = 0.16744691\n",
            "Iteration 181, loss = 0.16709982\n",
            "Iteration 182, loss = 0.16698279\n",
            "Iteration 183, loss = 0.16701049\n",
            "Iteration 184, loss = 0.16693530\n",
            "Iteration 185, loss = 0.16698772\n",
            "Iteration 186, loss = 0.16675422\n",
            "Iteration 187, loss = 0.16683731\n",
            "Iteration 188, loss = 0.16679927\n",
            "Iteration 189, loss = 0.16672041\n",
            "Iteration 190, loss = 0.16656137\n",
            "Iteration 191, loss = 0.16672621\n",
            "Iteration 192, loss = 0.16671211\n",
            "Iteration 193, loss = 0.16664387\n",
            "Iteration 194, loss = 0.16664435\n",
            "Iteration 195, loss = 0.16661914\n",
            "Iteration 196, loss = 0.16637523\n",
            "Iteration 197, loss = 0.16682586\n",
            "Iteration 198, loss = 0.16643874\n",
            "Iteration 199, loss = 0.16638130\n",
            "Iteration 200, loss = 0.16643809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.31714462\n",
            "Iteration 3, loss = 0.28960117\n",
            "Iteration 4, loss = 0.26926678\n",
            "Iteration 5, loss = 0.25327577\n",
            "Iteration 6, loss = 0.24145723\n",
            "Iteration 7, loss = 0.23256018\n",
            "Iteration 8, loss = 0.22551195\n",
            "Iteration 9, loss = 0.22065373\n",
            "Iteration 10, loss = 0.21595283\n",
            "Iteration 11, loss = 0.21408225\n",
            "Iteration 12, loss = 0.21219845\n",
            "Iteration 13, loss = 0.21068728\n",
            "Iteration 14, loss = 0.20938409\n",
            "Iteration 15, loss = 0.20797340\n",
            "Iteration 16, loss = 0.20680622\n",
            "Iteration 17, loss = 0.20734324\n",
            "Iteration 18, loss = 0.20651970\n",
            "Iteration 19, loss = 0.20655297\n",
            "Iteration 20, loss = 0.20536309\n",
            "Iteration 21, loss = 0.20565642\n",
            "Iteration 22, loss = 0.20548442\n",
            "Iteration 23, loss = 0.20625141\n",
            "Iteration 24, loss = 0.20551807\n",
            "Iteration 25, loss = 0.20528980\n",
            "Iteration 26, loss = 0.20504763\n",
            "Iteration 27, loss = 0.20561840\n",
            "Iteration 28, loss = 0.20488223\n",
            "Iteration 29, loss = 0.20602174\n",
            "Iteration 30, loss = 0.20482824\n",
            "Iteration 31, loss = 0.20545760\n",
            "Iteration 32, loss = 0.20516710\n",
            "Iteration 33, loss = 0.20513777\n",
            "Iteration 34, loss = 0.20492491\n",
            "Iteration 35, loss = 0.20379890\n",
            "Iteration 36, loss = 0.20598734\n",
            "Iteration 37, loss = 0.20594750\n",
            "Iteration 38, loss = 0.20543309\n",
            "Iteration 39, loss = 0.20484447\n",
            "Iteration 40, loss = 0.20646515\n",
            "Iteration 41, loss = 0.20528477\n",
            "Iteration 42, loss = 0.20482555\n",
            "Iteration 43, loss = 0.20570211\n",
            "Iteration 44, loss = 0.20514208\n",
            "Iteration 45, loss = 0.20521267\n",
            "Iteration 46, loss = 0.20515054\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.19684480\n",
            "Iteration 48, loss = 0.19566203\n",
            "Iteration 49, loss = 0.19475833\n",
            "Iteration 50, loss = 0.19432608\n",
            "Iteration 51, loss = 0.19360065\n",
            "Iteration 52, loss = 0.19277045\n",
            "Iteration 53, loss = 0.19274676\n",
            "Iteration 54, loss = 0.19185575\n",
            "Iteration 55, loss = 0.19142386\n",
            "Iteration 56, loss = 0.19076002\n",
            "Iteration 57, loss = 0.19052584\n",
            "Iteration 58, loss = 0.18992580\n",
            "Iteration 59, loss = 0.18939603\n",
            "Iteration 60, loss = 0.18878908\n",
            "Iteration 61, loss = 0.18832833\n",
            "Iteration 62, loss = 0.18850567\n",
            "Iteration 63, loss = 0.18786586\n",
            "Iteration 64, loss = 0.18753839\n",
            "Iteration 65, loss = 0.18693031\n",
            "Iteration 66, loss = 0.18690575\n",
            "Iteration 67, loss = 0.18705934\n",
            "Iteration 68, loss = 0.18571971\n",
            "Iteration 69, loss = 0.18570082\n",
            "Iteration 70, loss = 0.18548391\n",
            "Iteration 71, loss = 0.18495971\n",
            "Iteration 72, loss = 0.18599089\n",
            "Iteration 73, loss = 0.18508692\n",
            "Iteration 74, loss = 0.18417109\n",
            "Iteration 75, loss = 0.18506855\n",
            "Iteration 76, loss = 0.18418917\n",
            "Iteration 77, loss = 0.18376699\n",
            "Iteration 78, loss = 0.18672349\n",
            "Iteration 79, loss = 0.18411613\n",
            "Iteration 80, loss = 0.18386274\n",
            "Iteration 81, loss = 0.18496562\n",
            "Iteration 82, loss = 0.18445663\n",
            "Iteration 83, loss = 0.18513946\n",
            "Iteration 84, loss = 0.18423307\n",
            "Iteration 85, loss = 0.18570086\n",
            "Iteration 86, loss = 0.18677242\n",
            "Iteration 87, loss = 0.18251815\n",
            "Iteration 88, loss = 0.18736200\n",
            "Iteration 89, loss = 0.18773636\n",
            "Iteration 90, loss = 0.18552720\n",
            "Iteration 91, loss = 0.18567104\n",
            "Iteration 92, loss = 0.18542891\n",
            "Iteration 93, loss = 0.18880391\n",
            "Iteration 94, loss = 0.18620908\n",
            "Iteration 95, loss = 0.18403550\n",
            "Iteration 96, loss = 0.18519425\n",
            "Iteration 97, loss = 0.18498455\n",
            "Iteration 98, loss = 0.18668158\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 99, loss = 0.17476948\n",
            "Iteration 100, loss = 0.17364870\n",
            "Iteration 101, loss = 0.17329068\n",
            "Iteration 102, loss = 0.17317252\n",
            "Iteration 103, loss = 0.17306841\n",
            "Iteration 104, loss = 0.17308645\n",
            "Iteration 105, loss = 0.17295353\n",
            "Iteration 106, loss = 0.17273466\n",
            "Iteration 107, loss = 0.17269550\n",
            "Iteration 108, loss = 0.17244346\n",
            "Iteration 109, loss = 0.17248924\n",
            "Iteration 110, loss = 0.17213570\n",
            "Iteration 111, loss = 0.17224415\n",
            "Iteration 112, loss = 0.17215452\n",
            "Iteration 113, loss = 0.17208641\n",
            "Iteration 114, loss = 0.17180894\n",
            "Iteration 115, loss = 0.17171344\n",
            "Iteration 116, loss = 0.17159157\n",
            "Iteration 117, loss = 0.17139290\n",
            "Iteration 118, loss = 0.17139013\n",
            "Iteration 119, loss = 0.17134367\n",
            "Iteration 120, loss = 0.17130888\n",
            "Iteration 121, loss = 0.17121832\n",
            "Iteration 122, loss = 0.17102852\n",
            "Iteration 123, loss = 0.17100220\n",
            "Iteration 124, loss = 0.17090928\n",
            "Iteration 125, loss = 0.17077898\n",
            "Iteration 126, loss = 0.17062612\n",
            "Iteration 127, loss = 0.17062007\n",
            "Iteration 128, loss = 0.17060342\n",
            "Iteration 129, loss = 0.17046374\n",
            "Iteration 130, loss = 0.17049479\n",
            "Iteration 131, loss = 0.17023584\n",
            "Iteration 132, loss = 0.17026111\n",
            "Iteration 133, loss = 0.17022749\n",
            "Iteration 134, loss = 0.17014250\n",
            "Iteration 135, loss = 0.16996794\n",
            "Iteration 136, loss = 0.16982254\n",
            "Iteration 137, loss = 0.16975910\n",
            "Iteration 138, loss = 0.16951967\n",
            "Iteration 139, loss = 0.16967204\n",
            "Iteration 140, loss = 0.16953534\n",
            "Iteration 141, loss = 0.16960099\n",
            "Iteration 142, loss = 0.16962647\n",
            "Iteration 143, loss = 0.16936283\n",
            "Iteration 144, loss = 0.16938204\n",
            "Iteration 145, loss = 0.16927169\n",
            "Iteration 146, loss = 0.16909633\n",
            "Iteration 147, loss = 0.16904086\n",
            "Iteration 148, loss = 0.16897912\n",
            "Iteration 149, loss = 0.16885131\n",
            "Iteration 150, loss = 0.16899792\n",
            "Iteration 151, loss = 0.16872736\n",
            "Iteration 152, loss = 0.16859767\n",
            "Iteration 153, loss = 0.16868794\n",
            "Iteration 154, loss = 0.16869106\n",
            "Iteration 155, loss = 0.16835618\n",
            "Iteration 156, loss = 0.16845081\n",
            "Iteration 157, loss = 0.16838586\n",
            "Iteration 158, loss = 0.16840679\n",
            "Iteration 159, loss = 0.16832012\n",
            "Iteration 160, loss = 0.16842671\n",
            "Iteration 161, loss = 0.16830537\n",
            "Iteration 162, loss = 0.16795633\n",
            "Iteration 163, loss = 0.16799861\n",
            "Iteration 164, loss = 0.16822212\n",
            "Iteration 165, loss = 0.16780318\n",
            "Iteration 166, loss = 0.16782334\n",
            "Iteration 167, loss = 0.16794680\n",
            "Iteration 168, loss = 0.16787696\n",
            "Iteration 169, loss = 0.16770515\n",
            "Iteration 170, loss = 0.16763433\n",
            "Iteration 171, loss = 0.16776723\n",
            "Iteration 172, loss = 0.16774556\n",
            "Iteration 173, loss = 0.16751231\n",
            "Iteration 174, loss = 0.16746776\n",
            "Iteration 175, loss = 0.16755647\n",
            "Iteration 176, loss = 0.16734968\n",
            "Iteration 177, loss = 0.16723708\n",
            "Iteration 178, loss = 0.16727913\n",
            "Iteration 179, loss = 0.16717068\n",
            "Iteration 180, loss = 0.16722211\n",
            "Iteration 181, loss = 0.16719074\n",
            "Iteration 182, loss = 0.16711662\n",
            "Iteration 183, loss = 0.16706553\n",
            "Iteration 184, loss = 0.16703175\n",
            "Iteration 185, loss = 0.16687993\n",
            "Iteration 186, loss = 0.16698899\n",
            "Iteration 187, loss = 0.16698301\n",
            "Iteration 188, loss = 0.16690619\n",
            "Iteration 189, loss = 0.16687239\n",
            "Iteration 190, loss = 0.16680390\n",
            "Iteration 191, loss = 0.16648414\n",
            "Iteration 192, loss = 0.16665846\n",
            "Iteration 193, loss = 0.16656453\n",
            "Iteration 194, loss = 0.16668108\n",
            "Iteration 195, loss = 0.16669105\n",
            "Iteration 196, loss = 0.16635383\n",
            "Iteration 197, loss = 0.16652126\n",
            "Iteration 198, loss = 0.16653483\n",
            "Iteration 199, loss = 0.16648876\n",
            "Iteration 200, loss = 0.16640922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.88824997\n",
            "Iteration 2, loss = 0.32224986\n",
            "Iteration 3, loss = 0.29338767\n",
            "Iteration 4, loss = 0.27145968\n",
            "Iteration 5, loss = 0.25517382\n",
            "Iteration 6, loss = 0.24290632\n",
            "Iteration 7, loss = 0.23374374\n",
            "Iteration 8, loss = 0.22628913\n",
            "Iteration 9, loss = 0.22160214\n",
            "Iteration 10, loss = 0.21704667\n",
            "Iteration 11, loss = 0.21406373\n",
            "Iteration 12, loss = 0.21176709\n",
            "Iteration 13, loss = 0.21029036\n",
            "Iteration 14, loss = 0.20810637\n",
            "Iteration 15, loss = 0.20822657\n",
            "Iteration 16, loss = 0.20673523\n",
            "Iteration 17, loss = 0.20631100\n",
            "Iteration 18, loss = 0.20556243\n",
            "Iteration 19, loss = 0.20502619\n",
            "Iteration 20, loss = 0.20535578\n",
            "Iteration 21, loss = 0.20465167\n",
            "Iteration 22, loss = 0.20493515\n",
            "Iteration 23, loss = 0.20448965\n",
            "Iteration 24, loss = 0.20420641\n",
            "Iteration 25, loss = 0.20424986\n",
            "Iteration 26, loss = 0.20461176\n",
            "Iteration 27, loss = 0.20333505\n",
            "Iteration 28, loss = 0.20469846\n",
            "Iteration 29, loss = 0.20435265\n",
            "Iteration 30, loss = 0.20487052\n",
            "Iteration 31, loss = 0.20440783\n",
            "Iteration 32, loss = 0.20388560\n",
            "Iteration 33, loss = 0.20537056\n",
            "Iteration 34, loss = 0.20395062\n",
            "Iteration 35, loss = 0.20416324\n",
            "Iteration 36, loss = 0.20407459\n",
            "Iteration 37, loss = 0.20404556\n",
            "Iteration 38, loss = 0.20496531\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.19905806\n",
            "Iteration 40, loss = 0.19838891\n",
            "Iteration 41, loss = 0.19805967\n",
            "Iteration 42, loss = 0.19780679\n",
            "Iteration 43, loss = 0.19748702\n",
            "Iteration 44, loss = 0.19708731\n",
            "Iteration 45, loss = 0.19725178\n",
            "Iteration 46, loss = 0.19687114\n",
            "Iteration 47, loss = 0.19652120\n",
            "Iteration 48, loss = 0.19655071\n",
            "Iteration 49, loss = 0.19630280\n",
            "Iteration 50, loss = 0.19572624\n",
            "Iteration 51, loss = 0.19554807\n",
            "Iteration 52, loss = 0.19554340\n",
            "Iteration 53, loss = 0.19507921\n",
            "Iteration 54, loss = 0.19518006\n",
            "Iteration 55, loss = 0.19472052\n",
            "Iteration 56, loss = 0.19449057\n",
            "Iteration 57, loss = 0.19424043\n",
            "Iteration 58, loss = 0.19444540\n",
            "Iteration 59, loss = 0.19415096\n",
            "Iteration 60, loss = 0.19351159\n",
            "Iteration 61, loss = 0.19380765\n",
            "Iteration 62, loss = 0.19319731\n",
            "Iteration 63, loss = 0.19311133\n",
            "Iteration 64, loss = 0.19291278\n",
            "Iteration 65, loss = 0.19263965\n",
            "Iteration 66, loss = 0.19248587\n",
            "Iteration 67, loss = 0.19244603\n",
            "Iteration 68, loss = 0.19176605\n",
            "Iteration 69, loss = 0.19104977\n",
            "Iteration 70, loss = 0.19115040\n",
            "Iteration 71, loss = 0.19134079\n",
            "Iteration 72, loss = 0.19068629\n",
            "Iteration 73, loss = 0.19032857\n",
            "Iteration 74, loss = 0.19038499\n",
            "Iteration 75, loss = 0.19012183\n",
            "Iteration 76, loss = 0.18957160\n",
            "Iteration 77, loss = 0.19017485\n",
            "Iteration 78, loss = 0.19042707\n",
            "Iteration 79, loss = 0.19024544\n",
            "Iteration 80, loss = 0.19022916\n",
            "Iteration 81, loss = 0.18948932\n",
            "Iteration 82, loss = 0.19043833\n",
            "Iteration 83, loss = 0.18980446\n",
            "Iteration 84, loss = 0.18817082\n",
            "Iteration 85, loss = 0.18987074\n",
            "Iteration 86, loss = 0.18938289\n",
            "Iteration 87, loss = 0.18955672\n",
            "Iteration 88, loss = 0.18957802\n",
            "Iteration 89, loss = 0.18980692\n",
            "Iteration 90, loss = 0.19122235\n",
            "Iteration 91, loss = 0.19778873\n",
            "Iteration 92, loss = 0.19740951\n",
            "Iteration 93, loss = 0.19689797\n",
            "Iteration 94, loss = 0.19539831\n",
            "Iteration 95, loss = 0.19561349\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.19017933\n",
            "Iteration 97, loss = 0.18989550\n",
            "Iteration 98, loss = 0.18958157\n",
            "Iteration 99, loss = 0.18953950\n",
            "Iteration 100, loss = 0.18922019\n",
            "Iteration 101, loss = 0.18912498\n",
            "Iteration 102, loss = 0.18907501\n",
            "Iteration 103, loss = 0.18865169\n",
            "Iteration 104, loss = 0.18872783\n",
            "Iteration 105, loss = 0.18848448\n",
            "Iteration 106, loss = 0.18827538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 107, loss = 0.18745242\n",
            "Iteration 108, loss = 0.18741160\n",
            "Iteration 109, loss = 0.18742269\n",
            "Iteration 110, loss = 0.18733307\n",
            "Iteration 111, loss = 0.18728545\n",
            "Iteration 112, loss = 0.18738319\n",
            "Iteration 113, loss = 0.18724383\n",
            "Iteration 114, loss = 0.18726190\n",
            "Iteration 115, loss = 0.18713171\n",
            "Iteration 116, loss = 0.18717592\n",
            "Iteration 117, loss = 0.18710860\n",
            "Iteration 118, loss = 0.18709233\n",
            "Iteration 119, loss = 0.18713766\n",
            "Iteration 120, loss = 0.18705129\n",
            "Iteration 121, loss = 0.18693201\n",
            "Iteration 122, loss = 0.18688055\n",
            "Iteration 123, loss = 0.18680336\n",
            "Iteration 124, loss = 0.18686523\n",
            "Iteration 125, loss = 0.18680239\n",
            "Iteration 126, loss = 0.18685075\n",
            "Iteration 127, loss = 0.18685642\n",
            "Iteration 128, loss = 0.18671940\n",
            "Iteration 129, loss = 0.18673672\n",
            "Iteration 130, loss = 0.18667522\n",
            "Iteration 131, loss = 0.18662539\n",
            "Iteration 132, loss = 0.18661880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 133, loss = 0.18634152\n",
            "Iteration 134, loss = 0.18636352\n",
            "Iteration 135, loss = 0.18632531\n",
            "Iteration 136, loss = 0.18635073\n",
            "Iteration 137, loss = 0.18635260\n",
            "Iteration 138, loss = 0.18634698\n",
            "Iteration 139, loss = 0.18633165\n",
            "Iteration 140, loss = 0.18630236\n",
            "Iteration 141, loss = 0.18632102\n",
            "Iteration 142, loss = 0.18630267\n",
            "Iteration 143, loss = 0.18631736\n",
            "Iteration 144, loss = 0.18631007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 145, loss = 0.18624834\n",
            "Iteration 146, loss = 0.18623616\n",
            "Iteration 147, loss = 0.18623966\n",
            "Iteration 148, loss = 0.18623331\n",
            "Iteration 149, loss = 0.18624276\n",
            "Iteration 150, loss = 0.18623538\n",
            "Iteration 151, loss = 0.18622692\n",
            "Iteration 152, loss = 0.18623681\n",
            "Iteration 153, loss = 0.18623177\n",
            "Iteration 154, loss = 0.18622417\n",
            "Iteration 155, loss = 0.18621893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 30.7min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.12360712\n",
            "Iteration 2, loss = 0.35740363\n",
            "Iteration 3, loss = 0.34440150\n",
            "Iteration 4, loss = 0.33199428\n",
            "Iteration 5, loss = 0.32190488\n",
            "Iteration 6, loss = 0.31296967\n",
            "Iteration 7, loss = 0.30256455\n",
            "Iteration 8, loss = 0.29461896\n",
            "Iteration 9, loss = 0.28779664\n",
            "Iteration 10, loss = 0.28209439\n",
            "Iteration 11, loss = 0.27511159\n",
            "Iteration 12, loss = 0.26900638\n",
            "Iteration 13, loss = 0.26487329\n",
            "Iteration 14, loss = 0.25977293\n",
            "Iteration 15, loss = 0.25571296\n",
            "Iteration 16, loss = 0.25125681\n",
            "Iteration 17, loss = 0.24593286\n",
            "Iteration 18, loss = 0.24422532\n",
            "Iteration 19, loss = 0.24110450\n",
            "Iteration 20, loss = 0.23925732\n",
            "Iteration 21, loss = 0.23468322\n",
            "Iteration 22, loss = 0.23259550\n",
            "Iteration 23, loss = 0.23152848\n",
            "Iteration 24, loss = 0.22891182\n",
            "Iteration 25, loss = 0.22590731\n",
            "Iteration 26, loss = 0.22478170\n",
            "Iteration 27, loss = 0.22206868\n",
            "Iteration 28, loss = 0.22204570\n",
            "Iteration 29, loss = 0.22063897\n",
            "Iteration 30, loss = 0.21914396\n",
            "Iteration 31, loss = 0.21903970\n",
            "Iteration 32, loss = 0.21561559\n",
            "Iteration 33, loss = 0.21329596\n",
            "Iteration 34, loss = 0.21338835\n",
            "Iteration 35, loss = 0.21371948\n",
            "Iteration 36, loss = 0.21117334\n",
            "Iteration 37, loss = 0.21315151\n",
            "Iteration 38, loss = 0.21234134\n",
            "Iteration 39, loss = 0.20862161\n",
            "Iteration 40, loss = 0.21029941\n",
            "Iteration 41, loss = 0.21071325\n",
            "Iteration 42, loss = 0.20809396\n",
            "Iteration 43, loss = 0.20723290\n",
            "Iteration 44, loss = 0.20816314\n",
            "Iteration 45, loss = 0.20722649\n",
            "Iteration 46, loss = 0.20754801\n",
            "Iteration 47, loss = 0.20868393\n",
            "Iteration 48, loss = 0.20760064\n",
            "Iteration 49, loss = 0.20496830\n",
            "Iteration 50, loss = 0.20538055\n",
            "Iteration 51, loss = 0.20579224\n",
            "Iteration 52, loss = 0.20633620\n",
            "Iteration 53, loss = 0.20701539\n",
            "Iteration 54, loss = 0.20619957\n",
            "Iteration 55, loss = 0.20396151\n",
            "Iteration 56, loss = 0.20532855\n",
            "Iteration 57, loss = 0.20536325\n",
            "Iteration 58, loss = 0.20383136\n",
            "Iteration 59, loss = 0.20677027\n",
            "Iteration 60, loss = 0.20484069\n",
            "Iteration 61, loss = 0.20499975\n",
            "Iteration 62, loss = 0.20330141\n",
            "Iteration 63, loss = 0.20355342\n",
            "Iteration 64, loss = 0.20520916\n",
            "Iteration 65, loss = 0.20531160\n",
            "Iteration 66, loss = 0.20479432\n",
            "Iteration 67, loss = 0.20333652\n",
            "Iteration 68, loss = 0.20270564\n",
            "Iteration 69, loss = 0.20608295\n",
            "Iteration 70, loss = 0.20519565\n",
            "Iteration 71, loss = 0.20315970\n",
            "Iteration 72, loss = 0.20329834\n",
            "Iteration 73, loss = 0.20454347\n",
            "Iteration 74, loss = 0.20429200\n",
            "Iteration 75, loss = 0.20303428\n",
            "Iteration 76, loss = 0.20457785\n",
            "Iteration 77, loss = 0.20385737\n",
            "Iteration 78, loss = 0.20377049\n",
            "Iteration 79, loss = 0.20396433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 80, loss = 0.19676640\n",
            "Iteration 81, loss = 0.19546388\n",
            "Iteration 82, loss = 0.19549263\n",
            "Iteration 83, loss = 0.19515203\n",
            "Iteration 84, loss = 0.19497001\n",
            "Iteration 85, loss = 0.19467812\n",
            "Iteration 86, loss = 0.19420062\n",
            "Iteration 87, loss = 0.19424705\n",
            "Iteration 88, loss = 0.19415022\n",
            "Iteration 89, loss = 0.19397267\n",
            "Iteration 90, loss = 0.19438605\n",
            "Iteration 91, loss = 0.19351919\n",
            "Iteration 92, loss = 0.19333901\n",
            "Iteration 93, loss = 0.19302601\n",
            "Iteration 94, loss = 0.19304012\n",
            "Iteration 95, loss = 0.19293749\n",
            "Iteration 96, loss = 0.19285427\n",
            "Iteration 97, loss = 0.19259764\n",
            "Iteration 98, loss = 0.19239429\n",
            "Iteration 99, loss = 0.19212200\n",
            "Iteration 100, loss = 0.19199092\n",
            "Iteration 101, loss = 0.19159079\n",
            "Iteration 102, loss = 0.19164287\n",
            "Iteration 103, loss = 0.19146012\n",
            "Iteration 104, loss = 0.19146507\n",
            "Iteration 105, loss = 0.19083962\n",
            "Iteration 106, loss = 0.19104233\n",
            "Iteration 107, loss = 0.19081747\n",
            "Iteration 108, loss = 0.19074929\n",
            "Iteration 109, loss = 0.19099590\n",
            "Iteration 110, loss = 0.19083676\n",
            "Iteration 111, loss = 0.19057423\n",
            "Iteration 112, loss = 0.19098856\n",
            "Iteration 113, loss = 0.18999494\n",
            "Iteration 114, loss = 0.18994931\n",
            "Iteration 115, loss = 0.18974575\n",
            "Iteration 116, loss = 0.18998431\n",
            "Iteration 117, loss = 0.18976690\n",
            "Iteration 118, loss = 0.18939494\n",
            "Iteration 119, loss = 0.18973626\n",
            "Iteration 120, loss = 0.18943660\n",
            "Iteration 121, loss = 0.18923298\n",
            "Iteration 122, loss = 0.18890719\n",
            "Iteration 123, loss = 0.18886426\n",
            "Iteration 124, loss = 0.18865306\n",
            "Iteration 125, loss = 0.18851795\n",
            "Iteration 126, loss = 0.18844620\n",
            "Iteration 127, loss = 0.18882024\n",
            "Iteration 128, loss = 0.18887304\n",
            "Iteration 129, loss = 0.18780823\n",
            "Iteration 130, loss = 0.18799170\n",
            "Iteration 131, loss = 0.18794193\n",
            "Iteration 132, loss = 0.18703364\n",
            "Iteration 133, loss = 0.18831119\n",
            "Iteration 134, loss = 0.18820912\n",
            "Iteration 135, loss = 0.18834477\n",
            "Iteration 136, loss = 0.18671752\n",
            "Iteration 137, loss = 0.18721803\n",
            "Iteration 138, loss = 0.18768940\n",
            "Iteration 139, loss = 0.18684114\n",
            "Iteration 140, loss = 0.18698346\n",
            "Iteration 141, loss = 0.18724940\n",
            "Iteration 142, loss = 0.18634451\n",
            "Iteration 143, loss = 0.18672905\n",
            "Iteration 144, loss = 0.18665142\n",
            "Iteration 145, loss = 0.18629191\n",
            "Iteration 146, loss = 0.18643671\n",
            "Iteration 147, loss = 0.18622195\n",
            "Iteration 148, loss = 0.18668858\n",
            "Iteration 149, loss = 0.18700991\n",
            "Iteration 150, loss = 0.18608870\n",
            "Iteration 151, loss = 0.18581988\n",
            "Iteration 152, loss = 0.18668112\n",
            "Iteration 153, loss = 0.18645980\n",
            "Iteration 154, loss = 0.18557928\n",
            "Iteration 155, loss = 0.18622178\n",
            "Iteration 156, loss = 0.18539366\n",
            "Iteration 157, loss = 0.18437871\n",
            "Iteration 158, loss = 0.18533674\n",
            "Iteration 159, loss = 0.18502322\n",
            "Iteration 160, loss = 0.18437045\n",
            "Iteration 161, loss = 0.18479047\n",
            "Iteration 162, loss = 0.18464547\n",
            "Iteration 163, loss = 0.18557023\n",
            "Iteration 164, loss = 0.18481180\n",
            "Iteration 165, loss = 0.18443072\n",
            "Iteration 166, loss = 0.18447280\n",
            "Iteration 167, loss = 0.18462123\n",
            "Iteration 168, loss = 0.18511057\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 169, loss = 0.18115185\n",
            "Iteration 170, loss = 0.18065438\n",
            "Iteration 171, loss = 0.18047941\n",
            "Iteration 172, loss = 0.18058824\n",
            "Iteration 173, loss = 0.18012636\n",
            "Iteration 174, loss = 0.18044723\n",
            "Iteration 175, loss = 0.18013078\n",
            "Iteration 176, loss = 0.18021955\n",
            "Iteration 177, loss = 0.18006875\n",
            "Iteration 178, loss = 0.18023718\n",
            "Iteration 179, loss = 0.18026602\n",
            "Iteration 180, loss = 0.17995380\n",
            "Iteration 181, loss = 0.18022205\n",
            "Iteration 182, loss = 0.17981371\n",
            "Iteration 183, loss = 0.17968293\n",
            "Iteration 184, loss = 0.17986284\n",
            "Iteration 185, loss = 0.17977917\n",
            "Iteration 186, loss = 0.17991439\n",
            "Iteration 187, loss = 0.17979537\n",
            "Iteration 188, loss = 0.17958803\n",
            "Iteration 189, loss = 0.17979813\n",
            "Iteration 190, loss = 0.17961241\n",
            "Iteration 191, loss = 0.17956614\n",
            "Iteration 192, loss = 0.17939360\n",
            "Iteration 193, loss = 0.17949585\n",
            "Iteration 194, loss = 0.17947606\n",
            "Iteration 195, loss = 0.17978826\n",
            "Iteration 196, loss = 0.18004663\n",
            "Iteration 197, loss = 0.17954597\n",
            "Iteration 198, loss = 0.17920380\n",
            "Iteration 199, loss = 0.17936537\n",
            "Iteration 200, loss = 0.17968062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.38489996\n",
            "Iteration 3, loss = 0.36780039\n",
            "Iteration 4, loss = 0.35298257\n",
            "Iteration 5, loss = 0.33997097\n",
            "Iteration 6, loss = 0.32847658\n",
            "Iteration 7, loss = 0.31910991\n",
            "Iteration 8, loss = 0.30908047\n",
            "Iteration 9, loss = 0.30130217\n",
            "Iteration 10, loss = 0.29333731\n",
            "Iteration 11, loss = 0.28591217\n",
            "Iteration 12, loss = 0.27981761\n",
            "Iteration 13, loss = 0.27632217\n",
            "Iteration 14, loss = 0.26875130\n",
            "Iteration 15, loss = 0.26442520\n",
            "Iteration 16, loss = 0.25967536\n",
            "Iteration 17, loss = 0.25635762\n",
            "Iteration 18, loss = 0.25066148\n",
            "Iteration 19, loss = 0.24688597\n",
            "Iteration 20, loss = 0.24466199\n",
            "Iteration 21, loss = 0.24019735\n",
            "Iteration 22, loss = 0.23958148\n",
            "Iteration 23, loss = 0.23465522\n",
            "Iteration 24, loss = 0.23311930\n",
            "Iteration 25, loss = 0.23081153\n",
            "Iteration 26, loss = 0.22809972\n",
            "Iteration 27, loss = 0.22799335\n",
            "Iteration 28, loss = 0.22517464\n",
            "Iteration 29, loss = 0.22284160\n",
            "Iteration 30, loss = 0.22296632\n",
            "Iteration 31, loss = 0.22072495\n",
            "Iteration 32, loss = 0.21917559\n",
            "Iteration 33, loss = 0.22007358\n",
            "Iteration 34, loss = 0.21504736\n",
            "Iteration 35, loss = 0.21633856\n",
            "Iteration 36, loss = 0.21627927\n",
            "Iteration 37, loss = 0.21433785\n",
            "Iteration 38, loss = 0.21338111\n",
            "Iteration 39, loss = 0.21252938\n",
            "Iteration 40, loss = 0.21161734\n",
            "Iteration 41, loss = 0.21335148\n",
            "Iteration 42, loss = 0.21023837\n",
            "Iteration 43, loss = 0.21100449\n",
            "Iteration 44, loss = 0.20819971\n",
            "Iteration 45, loss = 0.20810952\n",
            "Iteration 46, loss = 0.20842721\n",
            "Iteration 47, loss = 0.20992183\n",
            "Iteration 48, loss = 0.20886483\n",
            "Iteration 49, loss = 0.20714687\n",
            "Iteration 50, loss = 0.20672680\n",
            "Iteration 51, loss = 0.20650284\n",
            "Iteration 52, loss = 0.20760381\n",
            "Iteration 53, loss = 0.20606487\n",
            "Iteration 54, loss = 0.20752400\n",
            "Iteration 55, loss = 0.20608668\n",
            "Iteration 56, loss = 0.20340564\n",
            "Iteration 57, loss = 0.20739877\n",
            "Iteration 58, loss = 0.20644230\n",
            "Iteration 59, loss = 0.20506394\n",
            "Iteration 60, loss = 0.20819720\n",
            "Iteration 61, loss = 0.20418355\n",
            "Iteration 62, loss = 0.20667607\n",
            "Iteration 63, loss = 0.20435348\n",
            "Iteration 64, loss = 0.20983801\n",
            "Iteration 65, loss = 0.20437186\n",
            "Iteration 66, loss = 0.20442415\n",
            "Iteration 67, loss = 0.20701996\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.19809697\n",
            "Iteration 69, loss = 0.19755676\n",
            "Iteration 70, loss = 0.19759930\n",
            "Iteration 71, loss = 0.19692537\n",
            "Iteration 72, loss = 0.19689305\n",
            "Iteration 73, loss = 0.19714751\n",
            "Iteration 74, loss = 0.19623306\n",
            "Iteration 75, loss = 0.19621589\n",
            "Iteration 76, loss = 0.19645663\n",
            "Iteration 77, loss = 0.19592864\n",
            "Iteration 78, loss = 0.19582301\n",
            "Iteration 79, loss = 0.19563802\n",
            "Iteration 80, loss = 0.19534352\n",
            "Iteration 81, loss = 0.19497489\n",
            "Iteration 82, loss = 0.19583848\n",
            "Iteration 83, loss = 0.19452006\n",
            "Iteration 84, loss = 0.19438775\n",
            "Iteration 85, loss = 0.19451887\n",
            "Iteration 86, loss = 0.19407278\n",
            "Iteration 87, loss = 0.19405683\n",
            "Iteration 88, loss = 0.19401782\n",
            "Iteration 89, loss = 0.19427141\n",
            "Iteration 90, loss = 0.19437920\n",
            "Iteration 91, loss = 0.19391329\n",
            "Iteration 92, loss = 0.19355313\n",
            "Iteration 93, loss = 0.19341253\n",
            "Iteration 94, loss = 0.19306988\n",
            "Iteration 95, loss = 0.19293192\n",
            "Iteration 96, loss = 0.19305296\n",
            "Iteration 97, loss = 0.19334590\n",
            "Iteration 98, loss = 0.19248405\n",
            "Iteration 99, loss = 0.19187515\n",
            "Iteration 100, loss = 0.19233487\n",
            "Iteration 101, loss = 0.19217571\n",
            "Iteration 102, loss = 0.19163335\n",
            "Iteration 103, loss = 0.19243790\n",
            "Iteration 104, loss = 0.19192927\n",
            "Iteration 105, loss = 0.19196173\n",
            "Iteration 106, loss = 0.19139510\n",
            "Iteration 107, loss = 0.19245907\n",
            "Iteration 108, loss = 0.19193964\n",
            "Iteration 109, loss = 0.19131806\n",
            "Iteration 110, loss = 0.19128772\n",
            "Iteration 111, loss = 0.19115319\n",
            "Iteration 112, loss = 0.19098076\n",
            "Iteration 113, loss = 0.19046145\n",
            "Iteration 114, loss = 0.19097590\n",
            "Iteration 115, loss = 0.19002226\n",
            "Iteration 116, loss = 0.19005297\n",
            "Iteration 117, loss = 0.19118951\n",
            "Iteration 118, loss = 0.19059732\n",
            "Iteration 119, loss = 0.18979885\n",
            "Iteration 120, loss = 0.18957144\n",
            "Iteration 121, loss = 0.19014847\n",
            "Iteration 122, loss = 0.19008930\n",
            "Iteration 123, loss = 0.18972580\n",
            "Iteration 124, loss = 0.19013227\n",
            "Iteration 125, loss = 0.18949467\n",
            "Iteration 126, loss = 0.18950215\n",
            "Iteration 127, loss = 0.18856197\n",
            "Iteration 128, loss = 0.18859221\n",
            "Iteration 129, loss = 0.18900461\n",
            "Iteration 130, loss = 0.18863350\n",
            "Iteration 131, loss = 0.18869749\n",
            "Iteration 132, loss = 0.18922225\n",
            "Iteration 133, loss = 0.18802273\n",
            "Iteration 134, loss = 0.18813838\n",
            "Iteration 135, loss = 0.18798424\n",
            "Iteration 136, loss = 0.18793820\n",
            "Iteration 137, loss = 0.18738199\n",
            "Iteration 138, loss = 0.18867419\n",
            "Iteration 139, loss = 0.18796645\n",
            "Iteration 140, loss = 0.18736347\n",
            "Iteration 141, loss = 0.18776159\n",
            "Iteration 142, loss = 0.18929091\n",
            "Iteration 143, loss = 0.18843096\n",
            "Iteration 144, loss = 0.18692641\n",
            "Iteration 145, loss = 0.18628705\n",
            "Iteration 146, loss = 0.18875643\n",
            "Iteration 147, loss = 0.18681091\n",
            "Iteration 148, loss = 0.18716766\n",
            "Iteration 149, loss = 0.18728194\n",
            "Iteration 150, loss = 0.18684040\n",
            "Iteration 151, loss = 0.18757750\n",
            "Iteration 152, loss = 0.18660367\n",
            "Iteration 153, loss = 0.18673200\n",
            "Iteration 154, loss = 0.18685159\n",
            "Iteration 155, loss = 0.18553456\n",
            "Iteration 156, loss = 0.18746963\n",
            "Iteration 157, loss = 0.18696426\n",
            "Iteration 158, loss = 0.18616219\n",
            "Iteration 159, loss = 0.18746390\n",
            "Iteration 160, loss = 0.18648601\n",
            "Iteration 161, loss = 0.18599943\n",
            "Iteration 162, loss = 0.18508401\n",
            "Iteration 163, loss = 0.18513830\n",
            "Iteration 164, loss = 0.18612936\n",
            "Iteration 165, loss = 0.18414712\n",
            "Iteration 166, loss = 0.18563874\n",
            "Iteration 167, loss = 0.18501913\n",
            "Iteration 168, loss = 0.18659121\n",
            "Iteration 169, loss = 0.18608319\n",
            "Iteration 170, loss = 0.18701945\n",
            "Iteration 171, loss = 0.18654250\n",
            "Iteration 172, loss = 0.18666663\n",
            "Iteration 173, loss = 0.18541980\n",
            "Iteration 174, loss = 0.18410916\n",
            "Iteration 175, loss = 0.18499662\n",
            "Iteration 176, loss = 0.18285360\n",
            "Iteration 177, loss = 0.18477808\n",
            "Iteration 178, loss = 0.18622899\n",
            "Iteration 179, loss = 0.18862771\n",
            "Iteration 180, loss = 0.18788975\n",
            "Iteration 181, loss = 0.18469998\n",
            "Iteration 182, loss = 0.18450118\n",
            "Iteration 183, loss = 0.18551082\n",
            "Iteration 184, loss = 0.18539926\n",
            "Iteration 185, loss = 0.18523207\n",
            "Iteration 186, loss = 0.18594507\n",
            "Iteration 187, loss = 0.18554584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 188, loss = 0.17889652\n",
            "Iteration 189, loss = 0.17894895\n",
            "Iteration 190, loss = 0.17905785\n",
            "Iteration 191, loss = 0.17853910\n",
            "Iteration 192, loss = 0.17886697\n",
            "Iteration 193, loss = 0.17864524\n",
            "Iteration 194, loss = 0.17888025\n",
            "Iteration 195, loss = 0.17870978\n",
            "Iteration 196, loss = 0.17863029\n",
            "Iteration 197, loss = 0.17852755\n",
            "Iteration 198, loss = 0.17840955\n",
            "Iteration 199, loss = 0.17860300\n",
            "Iteration 200, loss = 0.17869134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.16384453\n",
            "Iteration 2, loss = 0.36144102\n",
            "Iteration 3, loss = 0.34619160\n",
            "Iteration 4, loss = 0.33401337\n",
            "Iteration 5, loss = 0.32238321\n",
            "Iteration 6, loss = 0.31223038\n",
            "Iteration 7, loss = 0.30361141\n",
            "Iteration 8, loss = 0.29654725\n",
            "Iteration 9, loss = 0.28875361\n",
            "Iteration 10, loss = 0.28195401\n",
            "Iteration 11, loss = 0.27599430\n",
            "Iteration 12, loss = 0.27047235\n",
            "Iteration 13, loss = 0.26531816\n",
            "Iteration 14, loss = 0.26031601\n",
            "Iteration 15, loss = 0.25503777\n",
            "Iteration 16, loss = 0.25191091\n",
            "Iteration 17, loss = 0.24936936\n",
            "Iteration 18, loss = 0.24419736\n",
            "Iteration 19, loss = 0.24106608\n",
            "Iteration 20, loss = 0.23736580\n",
            "Iteration 21, loss = 0.23629301\n",
            "Iteration 22, loss = 0.23436561\n",
            "Iteration 23, loss = 0.23087169\n",
            "Iteration 24, loss = 0.22864904\n",
            "Iteration 25, loss = 0.22675257\n",
            "Iteration 26, loss = 0.22439123\n",
            "Iteration 27, loss = 0.22372779\n",
            "Iteration 28, loss = 0.22059766\n",
            "Iteration 29, loss = 0.22117138\n",
            "Iteration 30, loss = 0.22357147\n",
            "Iteration 31, loss = 0.21725241\n",
            "Iteration 32, loss = 0.21639501\n",
            "Iteration 33, loss = 0.21634080\n",
            "Iteration 34, loss = 0.21399352\n",
            "Iteration 35, loss = 0.21576294\n",
            "Iteration 36, loss = 0.21232977\n",
            "Iteration 37, loss = 0.21158152\n",
            "Iteration 38, loss = 0.21045022\n",
            "Iteration 39, loss = 0.21309764\n",
            "Iteration 40, loss = 0.21029263\n",
            "Iteration 41, loss = 0.21209932\n",
            "Iteration 42, loss = 0.20995941\n",
            "Iteration 43, loss = 0.20739463\n",
            "Iteration 44, loss = 0.20919900\n",
            "Iteration 45, loss = 0.20675245\n",
            "Iteration 46, loss = 0.20899387\n",
            "Iteration 47, loss = 0.20693156\n",
            "Iteration 48, loss = 0.20701575\n",
            "Iteration 49, loss = 0.20867475\n",
            "Iteration 50, loss = 0.20755127\n",
            "Iteration 51, loss = 0.20792056\n",
            "Iteration 52, loss = 0.20450561\n",
            "Iteration 53, loss = 0.20671542\n",
            "Iteration 54, loss = 0.20610283\n",
            "Iteration 55, loss = 0.20680781\n",
            "Iteration 56, loss = 0.20380701\n",
            "Iteration 57, loss = 0.20601350\n",
            "Iteration 58, loss = 0.20388419\n",
            "Iteration 59, loss = 0.20687909\n",
            "Iteration 60, loss = 0.20608247\n",
            "Iteration 61, loss = 0.20482796\n",
            "Iteration 62, loss = 0.20542960\n",
            "Iteration 63, loss = 0.20581826\n",
            "Iteration 64, loss = 0.20343548\n",
            "Iteration 65, loss = 0.20474399\n",
            "Iteration 66, loss = 0.20435698\n",
            "Iteration 67, loss = 0.20404195\n",
            "Iteration 68, loss = 0.20505224\n",
            "Iteration 69, loss = 0.20381373\n",
            "Iteration 70, loss = 0.20519505\n",
            "Iteration 71, loss = 0.20605616\n",
            "Iteration 72, loss = 0.20484896\n",
            "Iteration 73, loss = 0.20133999\n",
            "Iteration 74, loss = 0.20221876\n",
            "Iteration 75, loss = 0.20134799\n",
            "Iteration 76, loss = 0.20742233\n",
            "Iteration 77, loss = 0.20581815\n",
            "Iteration 78, loss = 0.20758755\n",
            "Iteration 79, loss = 0.20505786\n",
            "Iteration 80, loss = 0.20400430\n",
            "Iteration 81, loss = 0.20495503\n",
            "Iteration 82, loss = 0.20443820\n",
            "Iteration 83, loss = 0.20510891\n",
            "Iteration 84, loss = 0.20547604\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 85, loss = 0.19642908\n",
            "Iteration 86, loss = 0.19558433\n",
            "Iteration 87, loss = 0.19512967\n",
            "Iteration 88, loss = 0.19478849\n",
            "Iteration 89, loss = 0.19471766\n",
            "Iteration 90, loss = 0.19431555\n",
            "Iteration 91, loss = 0.19478045\n",
            "Iteration 92, loss = 0.19365141\n",
            "Iteration 93, loss = 0.19341865\n",
            "Iteration 94, loss = 0.19370715\n",
            "Iteration 95, loss = 0.19264256\n",
            "Iteration 96, loss = 0.19298573\n",
            "Iteration 97, loss = 0.19321451\n",
            "Iteration 98, loss = 0.19230631\n",
            "Iteration 99, loss = 0.19257638\n",
            "Iteration 100, loss = 0.19291181\n",
            "Iteration 101, loss = 0.19230690\n",
            "Iteration 102, loss = 0.19140769\n",
            "Iteration 103, loss = 0.19155626\n",
            "Iteration 104, loss = 0.19151991\n",
            "Iteration 105, loss = 0.19160744\n",
            "Iteration 106, loss = 0.19106430\n",
            "Iteration 107, loss = 0.19150087\n",
            "Iteration 108, loss = 0.19108681\n",
            "Iteration 109, loss = 0.19124350\n",
            "Iteration 110, loss = 0.19059913\n",
            "Iteration 111, loss = 0.19075606\n",
            "Iteration 112, loss = 0.19112631\n",
            "Iteration 113, loss = 0.19078864\n",
            "Iteration 114, loss = 0.19038170\n",
            "Iteration 115, loss = 0.19004670\n",
            "Iteration 116, loss = 0.18988173\n",
            "Iteration 117, loss = 0.19000962\n",
            "Iteration 118, loss = 0.19039209\n",
            "Iteration 119, loss = 0.19000162\n",
            "Iteration 120, loss = 0.18855045\n",
            "Iteration 121, loss = 0.18956791\n",
            "Iteration 122, loss = 0.18888265\n",
            "Iteration 123, loss = 0.18852286\n",
            "Iteration 124, loss = 0.18867061\n",
            "Iteration 125, loss = 0.18878832\n",
            "Iteration 126, loss = 0.18989569\n",
            "Iteration 127, loss = 0.18804882\n",
            "Iteration 128, loss = 0.18839327\n",
            "Iteration 129, loss = 0.18871994\n",
            "Iteration 130, loss = 0.18767606\n",
            "Iteration 131, loss = 0.18796714\n",
            "Iteration 132, loss = 0.18728864\n",
            "Iteration 133, loss = 0.18774295\n",
            "Iteration 134, loss = 0.18764947\n",
            "Iteration 135, loss = 0.18753983\n",
            "Iteration 136, loss = 0.18793662\n",
            "Iteration 137, loss = 0.18750734\n",
            "Iteration 138, loss = 0.18683433\n",
            "Iteration 139, loss = 0.18683010\n",
            "Iteration 140, loss = 0.18655670\n",
            "Iteration 141, loss = 0.18628608\n",
            "Iteration 142, loss = 0.18601196\n",
            "Iteration 143, loss = 0.18598711\n",
            "Iteration 144, loss = 0.18679527\n",
            "Iteration 145, loss = 0.18653850\n",
            "Iteration 146, loss = 0.18609811\n",
            "Iteration 147, loss = 0.18582374\n",
            "Iteration 148, loss = 0.18532512\n",
            "Iteration 149, loss = 0.18606232\n",
            "Iteration 150, loss = 0.18577138\n",
            "Iteration 151, loss = 0.18515105\n",
            "Iteration 152, loss = 0.18640171\n",
            "Iteration 153, loss = 0.18444232\n",
            "Iteration 154, loss = 0.18441836\n",
            "Iteration 155, loss = 0.18483779\n",
            "Iteration 156, loss = 0.18562401\n",
            "Iteration 157, loss = 0.18427401\n",
            "Iteration 158, loss = 0.18513358\n",
            "Iteration 159, loss = 0.18531742\n",
            "Iteration 160, loss = 0.18495339\n",
            "Iteration 161, loss = 0.18575323\n",
            "Iteration 162, loss = 0.18390539\n",
            "Iteration 163, loss = 0.18371585\n",
            "Iteration 164, loss = 0.18456119\n",
            "Iteration 165, loss = 0.18393537\n",
            "Iteration 166, loss = 0.18424614\n",
            "Iteration 167, loss = 0.18471344\n",
            "Iteration 168, loss = 0.18415061\n",
            "Iteration 169, loss = 0.18489616\n",
            "Iteration 170, loss = 0.18319742\n",
            "Iteration 171, loss = 0.18558004\n",
            "Iteration 172, loss = 0.18293658\n",
            "Iteration 173, loss = 0.18306027\n",
            "Iteration 174, loss = 0.18385660\n",
            "Iteration 175, loss = 0.18240863\n",
            "Iteration 176, loss = 0.18383136\n",
            "Iteration 177, loss = 0.18290457\n",
            "Iteration 178, loss = 0.18405447\n",
            "Iteration 179, loss = 0.18274147\n",
            "Iteration 180, loss = 0.18376531\n",
            "Iteration 181, loss = 0.18389401\n",
            "Iteration 182, loss = 0.18443797\n",
            "Iteration 183, loss = 0.18345481\n",
            "Iteration 184, loss = 0.18354830\n",
            "Iteration 185, loss = 0.18373216\n",
            "Iteration 186, loss = 0.18172575\n",
            "Iteration 187, loss = 0.18495677\n",
            "Iteration 188, loss = 0.18217029\n",
            "Iteration 189, loss = 0.18516902\n",
            "Iteration 190, loss = 0.18499921\n",
            "Iteration 191, loss = 0.18195662\n",
            "Iteration 192, loss = 0.18155814\n",
            "Iteration 193, loss = 0.18286376\n",
            "Iteration 194, loss = 0.18245687\n",
            "Iteration 195, loss = 0.18364511\n",
            "Iteration 196, loss = 0.18097343\n",
            "Iteration 197, loss = 0.18204093\n",
            "Iteration 198, loss = 0.18224995\n",
            "Iteration 199, loss = 0.18595370\n",
            "Iteration 200, loss = 0.18401697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.37236225\n",
            "Iteration 3, loss = 0.35733369\n",
            "Iteration 4, loss = 0.34451459\n",
            "Iteration 5, loss = 0.33376089\n",
            "Iteration 6, loss = 0.32228624\n",
            "Iteration 7, loss = 0.31395610\n",
            "Iteration 8, loss = 0.30561996\n",
            "Iteration 9, loss = 0.29829400\n",
            "Iteration 10, loss = 0.29060996\n",
            "Iteration 11, loss = 0.28396788\n",
            "Iteration 12, loss = 0.27619141\n",
            "Iteration 13, loss = 0.27076000\n",
            "Iteration 14, loss = 0.26550562\n",
            "Iteration 15, loss = 0.26053448\n",
            "Iteration 16, loss = 0.25590717\n",
            "Iteration 17, loss = 0.25400702\n",
            "Iteration 18, loss = 0.24866154\n",
            "Iteration 19, loss = 0.24481225\n",
            "Iteration 20, loss = 0.24209275\n",
            "Iteration 21, loss = 0.23945092\n",
            "Iteration 22, loss = 0.23524625\n",
            "Iteration 23, loss = 0.23316923\n",
            "Iteration 24, loss = 0.22899384\n",
            "Iteration 25, loss = 0.23025775\n",
            "Iteration 26, loss = 0.22727040\n",
            "Iteration 27, loss = 0.22517191\n",
            "Iteration 28, loss = 0.22271969\n",
            "Iteration 29, loss = 0.22243355\n",
            "Iteration 30, loss = 0.22127720\n",
            "Iteration 31, loss = 0.22172585\n",
            "Iteration 32, loss = 0.21561141\n",
            "Iteration 33, loss = 0.21628278\n",
            "Iteration 34, loss = 0.21597713\n",
            "Iteration 35, loss = 0.21441958\n",
            "Iteration 36, loss = 0.21397604\n",
            "Iteration 37, loss = 0.21081400\n",
            "Iteration 38, loss = 0.21143349\n",
            "Iteration 39, loss = 0.21170048\n",
            "Iteration 40, loss = 0.20861657\n",
            "Iteration 41, loss = 0.20938610\n",
            "Iteration 42, loss = 0.21091604\n",
            "Iteration 43, loss = 0.21018182\n",
            "Iteration 44, loss = 0.20950110\n",
            "Iteration 45, loss = 0.20677017\n",
            "Iteration 46, loss = 0.20975906\n",
            "Iteration 47, loss = 0.20854110\n",
            "Iteration 48, loss = 0.20742265\n",
            "Iteration 49, loss = 0.20638135\n",
            "Iteration 50, loss = 0.20568715\n",
            "Iteration 51, loss = 0.20524375\n",
            "Iteration 52, loss = 0.20489784\n",
            "Iteration 53, loss = 0.20736188\n",
            "Iteration 54, loss = 0.20608691\n",
            "Iteration 55, loss = 0.20535748\n",
            "Iteration 56, loss = 0.20616226\n",
            "Iteration 57, loss = 0.20544186\n",
            "Iteration 58, loss = 0.20552739\n",
            "Iteration 59, loss = 0.20461800\n",
            "Iteration 60, loss = 0.20422079\n",
            "Iteration 61, loss = 0.20386787\n",
            "Iteration 62, loss = 0.20609141\n",
            "Iteration 63, loss = 0.20474765\n",
            "Iteration 64, loss = 0.20363458\n",
            "Iteration 65, loss = 0.20770597\n",
            "Iteration 66, loss = 0.20528967\n",
            "Iteration 67, loss = 0.20335890\n",
            "Iteration 68, loss = 0.20477988\n",
            "Iteration 69, loss = 0.20471384\n",
            "Iteration 70, loss = 0.20296186\n",
            "Iteration 71, loss = 0.20661916\n",
            "Iteration 72, loss = 0.20385209\n",
            "Iteration 73, loss = 0.20428648\n",
            "Iteration 74, loss = 0.20323060\n",
            "Iteration 75, loss = 0.20404320\n",
            "Iteration 76, loss = 0.20261031\n",
            "Iteration 77, loss = 0.20678004\n",
            "Iteration 78, loss = 0.20275258\n",
            "Iteration 79, loss = 0.20294509\n",
            "Iteration 80, loss = 0.20511937\n",
            "Iteration 81, loss = 0.20513515\n",
            "Iteration 82, loss = 0.20394747\n",
            "Iteration 83, loss = 0.20414384\n",
            "Iteration 84, loss = 0.20143782\n",
            "Iteration 85, loss = 0.20776833\n",
            "Iteration 86, loss = 0.20171594\n",
            "Iteration 87, loss = 0.20319016\n",
            "Iteration 88, loss = 0.20294367\n",
            "Iteration 89, loss = 0.20640988\n",
            "Iteration 90, loss = 0.20096516\n",
            "Iteration 91, loss = 0.20724526\n",
            "Iteration 92, loss = 0.20163799\n",
            "Iteration 93, loss = 0.20289090\n",
            "Iteration 94, loss = 0.20435332\n",
            "Iteration 95, loss = 0.20032136\n",
            "Iteration 96, loss = 0.20409924\n",
            "Iteration 97, loss = 0.20160081\n",
            "Iteration 98, loss = 0.20392429\n",
            "Iteration 99, loss = 0.20566869\n",
            "Iteration 100, loss = 0.20323365\n",
            "Iteration 101, loss = 0.20047864\n",
            "Iteration 102, loss = 0.20460854\n",
            "Iteration 103, loss = 0.20184283\n",
            "Iteration 104, loss = 0.20388611\n",
            "Iteration 105, loss = 0.20382461\n",
            "Iteration 106, loss = 0.20811206\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 107, loss = 0.19494254\n",
            "Iteration 108, loss = 0.19469767\n",
            "Iteration 109, loss = 0.19464628\n",
            "Iteration 110, loss = 0.19397884\n",
            "Iteration 111, loss = 0.19373392\n",
            "Iteration 112, loss = 0.19322596\n",
            "Iteration 113, loss = 0.19309487\n",
            "Iteration 114, loss = 0.19259396\n",
            "Iteration 115, loss = 0.19227370\n",
            "Iteration 116, loss = 0.19252265\n",
            "Iteration 117, loss = 0.19230332\n",
            "Iteration 118, loss = 0.19214044\n",
            "Iteration 119, loss = 0.19165739\n",
            "Iteration 120, loss = 0.19146817\n",
            "Iteration 121, loss = 0.19128592\n",
            "Iteration 122, loss = 0.19095738\n",
            "Iteration 123, loss = 0.19084749\n",
            "Iteration 124, loss = 0.19032171\n",
            "Iteration 125, loss = 0.19057563\n",
            "Iteration 126, loss = 0.19047375\n",
            "Iteration 127, loss = 0.19021543\n",
            "Iteration 128, loss = 0.19019881\n",
            "Iteration 129, loss = 0.18995956\n",
            "Iteration 130, loss = 0.18933259\n",
            "Iteration 131, loss = 0.18937414\n",
            "Iteration 132, loss = 0.18940912\n",
            "Iteration 133, loss = 0.18948243\n",
            "Iteration 134, loss = 0.18896126\n",
            "Iteration 135, loss = 0.18927720\n",
            "Iteration 136, loss = 0.18830097\n",
            "Iteration 137, loss = 0.18883303\n",
            "Iteration 138, loss = 0.18914376\n",
            "Iteration 139, loss = 0.18845696\n",
            "Iteration 140, loss = 0.18844419\n",
            "Iteration 141, loss = 0.18867715\n",
            "Iteration 142, loss = 0.18772015\n",
            "Iteration 143, loss = 0.18793782\n",
            "Iteration 144, loss = 0.18771819\n",
            "Iteration 145, loss = 0.18782682\n",
            "Iteration 146, loss = 0.18741479\n",
            "Iteration 147, loss = 0.18673084\n",
            "Iteration 148, loss = 0.18736816\n",
            "Iteration 149, loss = 0.18698668\n",
            "Iteration 150, loss = 0.18737827\n",
            "Iteration 151, loss = 0.18657587\n",
            "Iteration 152, loss = 0.18648913\n",
            "Iteration 153, loss = 0.18648397\n",
            "Iteration 154, loss = 0.18676927\n",
            "Iteration 155, loss = 0.18651390\n",
            "Iteration 156, loss = 0.18634065\n",
            "Iteration 157, loss = 0.18573123\n",
            "Iteration 158, loss = 0.18569570\n",
            "Iteration 159, loss = 0.18696360\n",
            "Iteration 160, loss = 0.18551589\n",
            "Iteration 161, loss = 0.18590275\n",
            "Iteration 162, loss = 0.18586625\n",
            "Iteration 163, loss = 0.18500830\n",
            "Iteration 164, loss = 0.18509444\n",
            "Iteration 165, loss = 0.18577528\n",
            "Iteration 166, loss = 0.18492957\n",
            "Iteration 167, loss = 0.18516116\n",
            "Iteration 168, loss = 0.18516285\n",
            "Iteration 169, loss = 0.18493187\n",
            "Iteration 170, loss = 0.18464948\n",
            "Iteration 171, loss = 0.18383525\n",
            "Iteration 172, loss = 0.18400577\n",
            "Iteration 173, loss = 0.18384658\n",
            "Iteration 174, loss = 0.18538407\n",
            "Iteration 175, loss = 0.18395417\n",
            "Iteration 176, loss = 0.18410983\n",
            "Iteration 177, loss = 0.18341173\n",
            "Iteration 178, loss = 0.18378098\n",
            "Iteration 179, loss = 0.18372916\n",
            "Iteration 180, loss = 0.18374845\n",
            "Iteration 181, loss = 0.18423378\n",
            "Iteration 182, loss = 0.18275930\n",
            "Iteration 183, loss = 0.18346617\n",
            "Iteration 184, loss = 0.18378513\n",
            "Iteration 185, loss = 0.18307206\n",
            "Iteration 186, loss = 0.18343543\n",
            "Iteration 187, loss = 0.18246324\n",
            "Iteration 188, loss = 0.18331624\n",
            "Iteration 189, loss = 0.18450909\n",
            "Iteration 190, loss = 0.18274277\n",
            "Iteration 191, loss = 0.18375695\n",
            "Iteration 192, loss = 0.18353579\n",
            "Iteration 193, loss = 0.18187074\n",
            "Iteration 194, loss = 0.18395953\n",
            "Iteration 195, loss = 0.18189068\n",
            "Iteration 196, loss = 0.18480621\n",
            "Iteration 197, loss = 0.18217036\n",
            "Iteration 198, loss = 0.18202205\n",
            "Iteration 199, loss = 0.18351648\n",
            "Iteration 200, loss = 0.18334381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.69095647\n",
            "Iteration 2, loss = 0.34208318\n",
            "Iteration 3, loss = 0.32738499\n",
            "Iteration 4, loss = 0.31589405\n",
            "Iteration 5, loss = 0.30680749\n",
            "Iteration 6, loss = 0.29816957\n",
            "Iteration 7, loss = 0.29199241\n",
            "Iteration 8, loss = 0.28257982\n",
            "Iteration 9, loss = 0.27753488\n",
            "Iteration 10, loss = 0.27155989\n",
            "Iteration 11, loss = 0.26756357\n",
            "Iteration 12, loss = 0.26266503\n",
            "Iteration 13, loss = 0.25721009\n",
            "Iteration 14, loss = 0.25300153\n",
            "Iteration 15, loss = 0.24791028\n",
            "Iteration 16, loss = 0.24669181\n",
            "Iteration 17, loss = 0.24289695\n",
            "Iteration 18, loss = 0.23806997\n",
            "Iteration 19, loss = 0.23801989\n",
            "Iteration 20, loss = 0.23598669\n",
            "Iteration 21, loss = 0.23031086\n",
            "Iteration 22, loss = 0.23191240\n",
            "Iteration 23, loss = 0.22717110\n",
            "Iteration 24, loss = 0.22654920\n",
            "Iteration 25, loss = 0.22466533\n",
            "Iteration 26, loss = 0.22293677\n",
            "Iteration 27, loss = 0.22466623\n",
            "Iteration 28, loss = 0.22007523\n",
            "Iteration 29, loss = 0.22063638\n",
            "Iteration 30, loss = 0.22029160\n",
            "Iteration 31, loss = 0.21673507\n",
            "Iteration 32, loss = 0.22194994\n",
            "Iteration 33, loss = 0.22001477\n",
            "Iteration 34, loss = 0.21741349\n",
            "Iteration 35, loss = 0.21443334\n",
            "Iteration 36, loss = 0.21474411\n",
            "Iteration 37, loss = 0.21572480\n",
            "Iteration 38, loss = 0.21267751\n",
            "Iteration 39, loss = 0.21421208\n",
            "Iteration 40, loss = 0.21321518\n",
            "Iteration 41, loss = 0.21316057\n",
            "Iteration 42, loss = 0.20943723\n",
            "Iteration 43, loss = 0.21029933\n",
            "Iteration 44, loss = 0.20972421\n",
            "Iteration 45, loss = 0.21039694\n",
            "Iteration 46, loss = 0.21037177\n",
            "Iteration 47, loss = 0.20822604\n",
            "Iteration 48, loss = 0.21117505\n",
            "Iteration 49, loss = 0.20758413\n",
            "Iteration 50, loss = 0.20919946\n",
            "Iteration 51, loss = 0.20670255\n",
            "Iteration 52, loss = 0.20754572\n",
            "Iteration 53, loss = 0.20670204\n",
            "Iteration 54, loss = 0.20884642\n",
            "Iteration 55, loss = 0.20808684\n",
            "Iteration 56, loss = 0.20702968\n",
            "Iteration 57, loss = 0.20691951\n",
            "Iteration 58, loss = 0.20931541\n",
            "Iteration 59, loss = 0.20674286\n",
            "Iteration 60, loss = 0.20594698\n",
            "Iteration 61, loss = 0.20632653\n",
            "Iteration 62, loss = 0.20596856\n",
            "Iteration 63, loss = 0.20786850\n",
            "Iteration 64, loss = 0.20628364\n",
            "Iteration 65, loss = 0.20773520\n",
            "Iteration 66, loss = 0.20598707\n",
            "Iteration 67, loss = 0.20573074\n",
            "Iteration 68, loss = 0.20454059\n",
            "Iteration 69, loss = 0.20737545\n",
            "Iteration 70, loss = 0.20536033\n",
            "Iteration 71, loss = 0.20421643\n",
            "Iteration 72, loss = 0.20420805\n",
            "Iteration 73, loss = 0.20521239\n",
            "Iteration 74, loss = 0.20482603\n",
            "Iteration 75, loss = 0.20811097\n",
            "Iteration 76, loss = 0.20662623\n",
            "Iteration 77, loss = 0.20446766\n",
            "Iteration 78, loss = 0.20559184\n",
            "Iteration 79, loss = 0.20538516\n",
            "Iteration 80, loss = 0.20704640\n",
            "Iteration 81, loss = 0.20411675\n",
            "Iteration 82, loss = 0.20697830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 83, loss = 0.19812311\n",
            "Iteration 84, loss = 0.19736769\n",
            "Iteration 85, loss = 0.19722079\n",
            "Iteration 86, loss = 0.19675359\n",
            "Iteration 87, loss = 0.19653097\n",
            "Iteration 88, loss = 0.19645414\n",
            "Iteration 89, loss = 0.19630236\n",
            "Iteration 90, loss = 0.19613630\n",
            "Iteration 91, loss = 0.19572692\n",
            "Iteration 92, loss = 0.19507265\n",
            "Iteration 93, loss = 0.19544921\n",
            "Iteration 94, loss = 0.19560317\n",
            "Iteration 95, loss = 0.19491100\n",
            "Iteration 96, loss = 0.19485717\n",
            "Iteration 97, loss = 0.19447526\n",
            "Iteration 98, loss = 0.19464964\n",
            "Iteration 99, loss = 0.19434023\n",
            "Iteration 100, loss = 0.19428128\n",
            "Iteration 101, loss = 0.19463156\n",
            "Iteration 102, loss = 0.19419075\n",
            "Iteration 103, loss = 0.19386055\n",
            "Iteration 104, loss = 0.19389299\n",
            "Iteration 105, loss = 0.19304095\n",
            "Iteration 106, loss = 0.19330053\n",
            "Iteration 107, loss = 0.19295644\n",
            "Iteration 108, loss = 0.19272666\n",
            "Iteration 109, loss = 0.19253333\n",
            "Iteration 110, loss = 0.19260284\n",
            "Iteration 111, loss = 0.19289624\n",
            "Iteration 112, loss = 0.19202730\n",
            "Iteration 113, loss = 0.19253746\n",
            "Iteration 114, loss = 0.19221957\n",
            "Iteration 115, loss = 0.19225684\n",
            "Iteration 116, loss = 0.19248990\n",
            "Iteration 117, loss = 0.19107013\n",
            "Iteration 118, loss = 0.19142246\n",
            "Iteration 119, loss = 0.19113677\n",
            "Iteration 120, loss = 0.19134572\n",
            "Iteration 121, loss = 0.19114158\n",
            "Iteration 122, loss = 0.19140998\n",
            "Iteration 123, loss = 0.19084017\n",
            "Iteration 124, loss = 0.19032585\n",
            "Iteration 125, loss = 0.19058767\n",
            "Iteration 126, loss = 0.19024488\n",
            "Iteration 127, loss = 0.18970648\n",
            "Iteration 128, loss = 0.18992824\n",
            "Iteration 129, loss = 0.18916217\n",
            "Iteration 130, loss = 0.19004852\n",
            "Iteration 131, loss = 0.18986298\n",
            "Iteration 132, loss = 0.18961396\n",
            "Iteration 133, loss = 0.18915017\n",
            "Iteration 134, loss = 0.18902196\n",
            "Iteration 135, loss = 0.18909013\n",
            "Iteration 136, loss = 0.18931922\n",
            "Iteration 137, loss = 0.18888153\n",
            "Iteration 138, loss = 0.18899181\n",
            "Iteration 139, loss = 0.18870698\n",
            "Iteration 140, loss = 0.18800941\n",
            "Iteration 141, loss = 0.18854252\n",
            "Iteration 142, loss = 0.18733832\n",
            "Iteration 143, loss = 0.18789887\n",
            "Iteration 144, loss = 0.18882431\n",
            "Iteration 145, loss = 0.18795947\n",
            "Iteration 146, loss = 0.18794738\n",
            "Iteration 147, loss = 0.18714330\n",
            "Iteration 148, loss = 0.18793466\n",
            "Iteration 149, loss = 0.18752452\n",
            "Iteration 150, loss = 0.18656937\n",
            "Iteration 151, loss = 0.18747959\n",
            "Iteration 152, loss = 0.18629449\n",
            "Iteration 153, loss = 0.18674221\n",
            "Iteration 154, loss = 0.18770339\n",
            "Iteration 155, loss = 0.18692791\n",
            "Iteration 156, loss = 0.18750486\n",
            "Iteration 157, loss = 0.18609337\n",
            "Iteration 158, loss = 0.18696982\n",
            "Iteration 159, loss = 0.18738894\n",
            "Iteration 160, loss = 0.18549415\n",
            "Iteration 161, loss = 0.18553982\n",
            "Iteration 162, loss = 0.18472204\n",
            "Iteration 163, loss = 0.18582711\n",
            "Iteration 164, loss = 0.18620121\n",
            "Iteration 165, loss = 0.18518580\n",
            "Iteration 166, loss = 0.18606685\n",
            "Iteration 167, loss = 0.18562482\n",
            "Iteration 168, loss = 0.18442009\n",
            "Iteration 169, loss = 0.18517302\n",
            "Iteration 170, loss = 0.18494198\n",
            "Iteration 171, loss = 0.18433533\n",
            "Iteration 172, loss = 0.18562415\n",
            "Iteration 173, loss = 0.18523232\n",
            "Iteration 174, loss = 0.18368359\n",
            "Iteration 175, loss = 0.18664886\n",
            "Iteration 176, loss = 0.18544721\n",
            "Iteration 177, loss = 0.18372209\n",
            "Iteration 178, loss = 0.18590748\n",
            "Iteration 179, loss = 0.18557968\n",
            "Iteration 180, loss = 0.18401656\n",
            "Iteration 181, loss = 0.18395978\n",
            "Iteration 182, loss = 0.18609520\n",
            "Iteration 183, loss = 0.18536530\n",
            "Iteration 184, loss = 0.18367858\n",
            "Iteration 185, loss = 0.18645575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 186, loss = 0.17975424\n",
            "Iteration 187, loss = 0.17901021\n",
            "Iteration 188, loss = 0.17875848\n",
            "Iteration 189, loss = 0.17901197\n",
            "Iteration 190, loss = 0.17903418\n",
            "Iteration 191, loss = 0.17908280\n",
            "Iteration 192, loss = 0.17903864\n",
            "Iteration 193, loss = 0.17873362\n",
            "Iteration 194, loss = 0.17857769\n",
            "Iteration 195, loss = 0.17896681\n",
            "Iteration 196, loss = 0.17887161\n",
            "Iteration 197, loss = 0.17850855\n",
            "Iteration 198, loss = 0.17835464\n",
            "Iteration 199, loss = 0.17836939\n",
            "Iteration 200, loss = 0.17857505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.44140711\n",
            "Iteration 2, loss = 0.36814000\n",
            "Iteration 3, loss = 0.35564278\n",
            "Iteration 4, loss = 0.34460863\n",
            "Iteration 5, loss = 0.33382561\n",
            "Iteration 6, loss = 0.32332757\n",
            "Iteration 7, loss = 0.31466456\n",
            "Iteration 8, loss = 0.30567548\n",
            "Iteration 9, loss = 0.29729862\n",
            "Iteration 10, loss = 0.28977244\n",
            "Iteration 11, loss = 0.28259589\n",
            "Iteration 12, loss = 0.27597215\n",
            "Iteration 13, loss = 0.27059582\n",
            "Iteration 14, loss = 0.26553006\n",
            "Iteration 15, loss = 0.26152851\n",
            "Iteration 16, loss = 0.25600534\n",
            "Iteration 17, loss = 0.25254886\n",
            "Iteration 18, loss = 0.24819037\n",
            "Iteration 19, loss = 0.24478122\n",
            "Iteration 20, loss = 0.24259474\n",
            "Iteration 21, loss = 0.23874237\n",
            "Iteration 22, loss = 0.23547483\n",
            "Iteration 23, loss = 0.23401359\n",
            "Iteration 24, loss = 0.23182636\n",
            "Iteration 25, loss = 0.23069997\n",
            "Iteration 26, loss = 0.22657586\n",
            "Iteration 27, loss = 0.22644267\n",
            "Iteration 28, loss = 0.22321981\n",
            "Iteration 29, loss = 0.22291856\n",
            "Iteration 30, loss = 0.22072505\n",
            "Iteration 31, loss = 0.21853210\n",
            "Iteration 32, loss = 0.21816032\n",
            "Iteration 33, loss = 0.21739200\n",
            "Iteration 34, loss = 0.21550347\n",
            "Iteration 35, loss = 0.21587507\n",
            "Iteration 36, loss = 0.21315659\n",
            "Iteration 37, loss = 0.21363497\n",
            "Iteration 38, loss = 0.21336114\n",
            "Iteration 39, loss = 0.21319307\n",
            "Iteration 40, loss = 0.21203100\n",
            "Iteration 41, loss = 0.21029382\n",
            "Iteration 42, loss = 0.21107652\n",
            "Iteration 43, loss = 0.20953461\n",
            "Iteration 44, loss = 0.21045912\n",
            "Iteration 45, loss = 0.20927953\n",
            "Iteration 46, loss = 0.21054841\n",
            "Iteration 47, loss = 0.20706400\n",
            "Iteration 48, loss = 0.20759264\n",
            "Iteration 49, loss = 0.20781589\n",
            "Iteration 50, loss = 0.20818954\n",
            "Iteration 51, loss = 0.20867589\n",
            "Iteration 52, loss = 0.20680396\n",
            "Iteration 53, loss = 0.20639530\n",
            "Iteration 54, loss = 0.20868240\n",
            "Iteration 55, loss = 0.20755040\n",
            "Iteration 56, loss = 0.20740837\n",
            "Iteration 57, loss = 0.20586460\n",
            "Iteration 58, loss = 0.20715683\n",
            "Iteration 59, loss = 0.20782700\n",
            "Iteration 60, loss = 0.20673777\n",
            "Iteration 61, loss = 0.20691665\n",
            "Iteration 62, loss = 0.20531008\n",
            "Iteration 63, loss = 0.20583604\n",
            "Iteration 64, loss = 0.20322755\n",
            "Iteration 65, loss = 0.20582757\n",
            "Iteration 66, loss = 0.20556065\n",
            "Iteration 67, loss = 0.20562501\n",
            "Iteration 68, loss = 0.20650311\n",
            "Iteration 69, loss = 0.20453274\n",
            "Iteration 70, loss = 0.20406500\n",
            "Iteration 71, loss = 0.20649512\n",
            "Iteration 72, loss = 0.20546604\n",
            "Iteration 73, loss = 0.20532643\n",
            "Iteration 74, loss = 0.20538384\n",
            "Iteration 75, loss = 0.20715547\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.19929466\n",
            "Iteration 77, loss = 0.19823328\n",
            "Iteration 78, loss = 0.19795961\n",
            "Iteration 79, loss = 0.19751813\n",
            "Iteration 80, loss = 0.19740114\n",
            "Iteration 81, loss = 0.19724050\n",
            "Iteration 82, loss = 0.19711710\n",
            "Iteration 83, loss = 0.19719977\n",
            "Iteration 84, loss = 0.19702447\n",
            "Iteration 85, loss = 0.19660619\n",
            "Iteration 86, loss = 0.19608544\n",
            "Iteration 87, loss = 0.19651246\n",
            "Iteration 88, loss = 0.19598275\n",
            "Iteration 89, loss = 0.19562191\n",
            "Iteration 90, loss = 0.19594239\n",
            "Iteration 91, loss = 0.19532831\n",
            "Iteration 92, loss = 0.19525517\n",
            "Iteration 93, loss = 0.19531931\n",
            "Iteration 94, loss = 0.19487725\n",
            "Iteration 95, loss = 0.19540238\n",
            "Iteration 96, loss = 0.19508737\n",
            "Iteration 97, loss = 0.19521388\n",
            "Iteration 98, loss = 0.19488636\n",
            "Iteration 99, loss = 0.19402966\n",
            "Iteration 100, loss = 0.19421885\n",
            "Iteration 101, loss = 0.19437214\n",
            "Iteration 102, loss = 0.19421866\n",
            "Iteration 103, loss = 0.19392185\n",
            "Iteration 104, loss = 0.19462240\n",
            "Iteration 105, loss = 0.19458522\n",
            "Iteration 106, loss = 0.19321708\n",
            "Iteration 107, loss = 0.19316241\n",
            "Iteration 108, loss = 0.19377816\n",
            "Iteration 109, loss = 0.19320227\n",
            "Iteration 110, loss = 0.19344328\n",
            "Iteration 111, loss = 0.19360676\n",
            "Iteration 112, loss = 0.19305398\n",
            "Iteration 113, loss = 0.19259816\n",
            "Iteration 114, loss = 0.19246879\n",
            "Iteration 115, loss = 0.19293645\n",
            "Iteration 116, loss = 0.19297952\n",
            "Iteration 117, loss = 0.19215712\n",
            "Iteration 118, loss = 0.19267126\n",
            "Iteration 119, loss = 0.19211672\n",
            "Iteration 120, loss = 0.19187278\n",
            "Iteration 121, loss = 0.19196168\n",
            "Iteration 122, loss = 0.19232908\n",
            "Iteration 123, loss = 0.19147070\n",
            "Iteration 124, loss = 0.19198121\n",
            "Iteration 125, loss = 0.19214276\n",
            "Iteration 126, loss = 0.19119257\n",
            "Iteration 127, loss = 0.19173110\n",
            "Iteration 128, loss = 0.19173092\n",
            "Iteration 129, loss = 0.19151505\n",
            "Iteration 130, loss = 0.19155345\n",
            "Iteration 131, loss = 0.19137064\n",
            "Iteration 132, loss = 0.19085342\n",
            "Iteration 133, loss = 0.19052740\n",
            "Iteration 134, loss = 0.19101716\n",
            "Iteration 135, loss = 0.19111247\n",
            "Iteration 136, loss = 0.19087629\n",
            "Iteration 137, loss = 0.19068586\n",
            "Iteration 138, loss = 0.19127487\n",
            "Iteration 139, loss = 0.19044173\n",
            "Iteration 140, loss = 0.19063774\n",
            "Iteration 141, loss = 0.19033802\n",
            "Iteration 142, loss = 0.18993439\n",
            "Iteration 143, loss = 0.18947353\n",
            "Iteration 144, loss = 0.19001301\n",
            "Iteration 145, loss = 0.18957353\n",
            "Iteration 146, loss = 0.18969984\n",
            "Iteration 147, loss = 0.19018374\n",
            "Iteration 148, loss = 0.19008110\n",
            "Iteration 149, loss = 0.18981695\n",
            "Iteration 150, loss = 0.18977284\n",
            "Iteration 151, loss = 0.18966012\n",
            "Iteration 152, loss = 0.18938122\n",
            "Iteration 153, loss = 0.18944592\n",
            "Iteration 154, loss = 0.18927603\n",
            "Iteration 155, loss = 0.18866368\n",
            "Iteration 156, loss = 0.18953452\n",
            "Iteration 157, loss = 0.18847649\n",
            "Iteration 158, loss = 0.18941035\n",
            "Iteration 159, loss = 0.18910512\n",
            "Iteration 160, loss = 0.18823186\n",
            "Iteration 161, loss = 0.18907693\n",
            "Iteration 162, loss = 0.18837766\n",
            "Iteration 163, loss = 0.18821668\n",
            "Iteration 164, loss = 0.18921277\n",
            "Iteration 165, loss = 0.18929392\n",
            "Iteration 166, loss = 0.18793993\n",
            "Iteration 167, loss = 0.18854610\n",
            "Iteration 168, loss = 0.18788000\n",
            "Iteration 169, loss = 0.18830013\n",
            "Iteration 170, loss = 0.18860505\n",
            "Iteration 171, loss = 0.18847870\n",
            "Iteration 172, loss = 0.18814538\n",
            "Iteration 173, loss = 0.18896253\n",
            "Iteration 174, loss = 0.18872988\n",
            "Iteration 175, loss = 0.18846623\n",
            "Iteration 176, loss = 0.18903467\n",
            "Iteration 177, loss = 0.18722531\n",
            "Iteration 178, loss = 0.18677458\n",
            "Iteration 179, loss = 0.18763543\n",
            "Iteration 180, loss = 0.18766157\n",
            "Iteration 181, loss = 0.18899025\n",
            "Iteration 182, loss = 0.18834937\n",
            "Iteration 183, loss = 0.18684835\n",
            "Iteration 184, loss = 0.18731102\n",
            "Iteration 185, loss = 0.18792729\n",
            "Iteration 186, loss = 0.18686782\n",
            "Iteration 187, loss = 0.18704037\n",
            "Iteration 188, loss = 0.18696421\n",
            "Iteration 189, loss = 0.18834431\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 190, loss = 0.18325601\n",
            "Iteration 191, loss = 0.18314136\n",
            "Iteration 192, loss = 0.18304395\n",
            "Iteration 193, loss = 0.18288940\n",
            "Iteration 194, loss = 0.18251428\n",
            "Iteration 195, loss = 0.18316506\n",
            "Iteration 196, loss = 0.18266998\n",
            "Iteration 197, loss = 0.18263685\n",
            "Iteration 198, loss = 0.18283487\n",
            "Iteration 199, loss = 0.18239497\n",
            "Iteration 200, loss = 0.18247765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.40131394\n",
            "Iteration 2, loss = 0.34016164\n",
            "Iteration 3, loss = 0.32703615\n",
            "Iteration 4, loss = 0.31638916\n",
            "Iteration 5, loss = 0.30714142\n",
            "Iteration 6, loss = 0.29812805\n",
            "Iteration 7, loss = 0.29148917\n",
            "Iteration 8, loss = 0.28415298\n",
            "Iteration 9, loss = 0.27862691\n",
            "Iteration 10, loss = 0.27280574\n",
            "Iteration 11, loss = 0.26811149\n",
            "Iteration 12, loss = 0.26182032\n",
            "Iteration 13, loss = 0.25862587\n",
            "Iteration 14, loss = 0.25430141\n",
            "Iteration 15, loss = 0.25068725\n",
            "Iteration 16, loss = 0.24683805\n",
            "Iteration 17, loss = 0.24288181\n",
            "Iteration 18, loss = 0.24126570\n",
            "Iteration 19, loss = 0.23748125\n",
            "Iteration 20, loss = 0.23423683\n",
            "Iteration 21, loss = 0.23230211\n",
            "Iteration 22, loss = 0.22962987\n",
            "Iteration 23, loss = 0.22668991\n",
            "Iteration 24, loss = 0.22423081\n",
            "Iteration 25, loss = 0.22393394\n",
            "Iteration 26, loss = 0.22393968\n",
            "Iteration 27, loss = 0.22145167\n",
            "Iteration 28, loss = 0.21987770\n",
            "Iteration 29, loss = 0.21936455\n",
            "Iteration 30, loss = 0.21778986\n",
            "Iteration 31, loss = 0.21667693\n",
            "Iteration 32, loss = 0.21726750\n",
            "Iteration 33, loss = 0.21615367\n",
            "Iteration 34, loss = 0.21457819\n",
            "Iteration 35, loss = 0.21406514\n",
            "Iteration 36, loss = 0.21429843\n",
            "Iteration 37, loss = 0.21327733\n",
            "Iteration 38, loss = 0.21236735\n",
            "Iteration 39, loss = 0.21275642\n",
            "Iteration 40, loss = 0.21055782\n",
            "Iteration 41, loss = 0.21116537\n",
            "Iteration 42, loss = 0.21100143\n",
            "Iteration 43, loss = 0.21017169\n",
            "Iteration 44, loss = 0.21079205\n",
            "Iteration 45, loss = 0.20787245\n",
            "Iteration 46, loss = 0.20796132\n",
            "Iteration 47, loss = 0.20714414\n",
            "Iteration 48, loss = 0.20966256\n",
            "Iteration 49, loss = 0.20795124\n",
            "Iteration 50, loss = 0.20634062\n",
            "Iteration 51, loss = 0.20734554\n",
            "Iteration 52, loss = 0.20558072\n",
            "Iteration 53, loss = 0.20658431\n",
            "Iteration 54, loss = 0.20526023\n",
            "Iteration 55, loss = 0.20514498\n",
            "Iteration 56, loss = 0.20404920\n",
            "Iteration 57, loss = 0.20620117\n",
            "Iteration 58, loss = 0.20552935\n",
            "Iteration 59, loss = 0.20556877\n",
            "Iteration 60, loss = 0.20661625\n",
            "Iteration 61, loss = 0.20465595\n",
            "Iteration 62, loss = 0.20444643\n",
            "Iteration 63, loss = 0.20477164\n",
            "Iteration 64, loss = 0.20387461\n",
            "Iteration 65, loss = 0.20429052\n",
            "Iteration 66, loss = 0.20763730\n",
            "Iteration 67, loss = 0.20525389\n",
            "Iteration 68, loss = 0.20632914\n",
            "Iteration 69, loss = 0.20323900\n",
            "Iteration 70, loss = 0.20524710\n",
            "Iteration 71, loss = 0.20128015\n",
            "Iteration 72, loss = 0.20593902\n",
            "Iteration 73, loss = 0.20648254\n",
            "Iteration 74, loss = 0.20546797\n",
            "Iteration 75, loss = 0.20399300\n",
            "Iteration 76, loss = 0.20374334\n",
            "Iteration 77, loss = 0.20722190\n",
            "Iteration 78, loss = 0.20325726\n",
            "Iteration 79, loss = 0.20958570\n",
            "Iteration 80, loss = 0.20409552\n",
            "Iteration 81, loss = 0.20531145\n",
            "Iteration 82, loss = 0.20485532\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 83, loss = 0.19720409\n",
            "Iteration 84, loss = 0.19627191\n",
            "Iteration 85, loss = 0.19634445\n",
            "Iteration 86, loss = 0.19554267\n",
            "Iteration 87, loss = 0.19506761\n",
            "Iteration 88, loss = 0.19537643\n",
            "Iteration 89, loss = 0.19476141\n",
            "Iteration 90, loss = 0.19471000\n",
            "Iteration 91, loss = 0.19438931\n",
            "Iteration 92, loss = 0.19413925\n",
            "Iteration 93, loss = 0.19445403\n",
            "Iteration 94, loss = 0.19414337\n",
            "Iteration 95, loss = 0.19389382\n",
            "Iteration 96, loss = 0.19365035\n",
            "Iteration 97, loss = 0.19333549\n",
            "Iteration 98, loss = 0.19314711\n",
            "Iteration 99, loss = 0.19314990\n",
            "Iteration 100, loss = 0.19302367\n",
            "Iteration 101, loss = 0.19264115\n",
            "Iteration 102, loss = 0.19252460\n",
            "Iteration 103, loss = 0.19264978\n",
            "Iteration 104, loss = 0.19251860\n",
            "Iteration 105, loss = 0.19215958\n",
            "Iteration 106, loss = 0.19251388\n",
            "Iteration 107, loss = 0.19267522\n",
            "Iteration 108, loss = 0.19167776\n",
            "Iteration 109, loss = 0.19108528\n",
            "Iteration 110, loss = 0.19148199\n",
            "Iteration 111, loss = 0.19071608\n",
            "Iteration 112, loss = 0.19147681\n",
            "Iteration 113, loss = 0.19060183\n",
            "Iteration 114, loss = 0.19069385\n",
            "Iteration 115, loss = 0.19063540\n",
            "Iteration 116, loss = 0.19023411\n",
            "Iteration 117, loss = 0.19023196\n",
            "Iteration 118, loss = 0.19025601\n",
            "Iteration 119, loss = 0.19021062\n",
            "Iteration 120, loss = 0.18989892\n",
            "Iteration 121, loss = 0.18966874\n",
            "Iteration 122, loss = 0.19052245\n",
            "Iteration 123, loss = 0.18978654\n",
            "Iteration 124, loss = 0.18934851\n",
            "Iteration 125, loss = 0.18916108\n",
            "Iteration 126, loss = 0.18895118\n",
            "Iteration 127, loss = 0.18906349\n",
            "Iteration 128, loss = 0.18937225\n",
            "Iteration 129, loss = 0.18907175\n",
            "Iteration 130, loss = 0.18898226\n",
            "Iteration 131, loss = 0.18808639\n",
            "Iteration 132, loss = 0.18924791\n",
            "Iteration 133, loss = 0.18816236\n",
            "Iteration 134, loss = 0.18790897\n",
            "Iteration 135, loss = 0.18816007\n",
            "Iteration 136, loss = 0.18773121\n",
            "Iteration 137, loss = 0.18813970\n",
            "Iteration 138, loss = 0.18717974\n",
            "Iteration 139, loss = 0.18751797\n",
            "Iteration 140, loss = 0.18861047\n",
            "Iteration 141, loss = 0.18762156\n",
            "Iteration 142, loss = 0.18726943\n",
            "Iteration 143, loss = 0.18736523\n",
            "Iteration 144, loss = 0.18722124\n",
            "Iteration 145, loss = 0.18658839\n",
            "Iteration 146, loss = 0.18712914\n",
            "Iteration 147, loss = 0.18786548\n",
            "Iteration 148, loss = 0.18682821\n",
            "Iteration 149, loss = 0.18711274\n",
            "Iteration 150, loss = 0.18625681\n",
            "Iteration 151, loss = 0.18628887\n",
            "Iteration 152, loss = 0.18589001\n",
            "Iteration 153, loss = 0.18662711\n",
            "Iteration 154, loss = 0.18629896\n",
            "Iteration 155, loss = 0.18532086\n",
            "Iteration 156, loss = 0.18672355\n",
            "Iteration 157, loss = 0.18536203\n",
            "Iteration 158, loss = 0.18550783\n",
            "Iteration 159, loss = 0.18709700\n",
            "Iteration 160, loss = 0.18529755\n",
            "Iteration 161, loss = 0.18599629\n",
            "Iteration 162, loss = 0.18512785\n",
            "Iteration 163, loss = 0.18473971\n",
            "Iteration 164, loss = 0.18497348\n",
            "Iteration 165, loss = 0.18359261\n",
            "Iteration 166, loss = 0.18549821\n",
            "Iteration 167, loss = 0.18512667\n",
            "Iteration 168, loss = 0.18384430\n",
            "Iteration 169, loss = 0.18548506\n",
            "Iteration 170, loss = 0.18497491\n",
            "Iteration 171, loss = 0.18477454\n",
            "Iteration 172, loss = 0.18460348\n",
            "Iteration 173, loss = 0.18520751\n",
            "Iteration 174, loss = 0.18562423\n",
            "Iteration 175, loss = 0.18391464\n",
            "Iteration 176, loss = 0.18505570\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 177, loss = 0.18036841\n",
            "Iteration 178, loss = 0.18000443\n",
            "Iteration 179, loss = 0.17981254\n",
            "Iteration 180, loss = 0.18019226\n",
            "Iteration 181, loss = 0.18026802\n",
            "Iteration 182, loss = 0.17979601\n",
            "Iteration 183, loss = 0.17959046\n",
            "Iteration 184, loss = 0.17993574\n",
            "Iteration 185, loss = 0.17957718\n",
            "Iteration 186, loss = 0.17981240\n",
            "Iteration 187, loss = 0.17966328\n",
            "Iteration 188, loss = 0.17971111\n",
            "Iteration 189, loss = 0.17985185\n",
            "Iteration 190, loss = 0.17953731\n",
            "Iteration 191, loss = 0.17976827\n",
            "Iteration 192, loss = 0.17985578\n",
            "Iteration 193, loss = 0.17937754\n",
            "Iteration 194, loss = 0.17931277\n",
            "Iteration 195, loss = 0.17937573\n",
            "Iteration 196, loss = 0.17921431\n",
            "Iteration 197, loss = 0.17938134\n",
            "Iteration 198, loss = 0.17923147\n",
            "Iteration 199, loss = 0.17940521\n",
            "Iteration 200, loss = 0.17954887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.24345288\n",
            "Iteration 2, loss = 0.31894657\n",
            "Iteration 3, loss = 0.30790845\n",
            "Iteration 4, loss = 0.29807886\n",
            "Iteration 5, loss = 0.28969232\n",
            "Iteration 6, loss = 0.28219473\n",
            "Iteration 7, loss = 0.27572424\n",
            "Iteration 8, loss = 0.26949759\n",
            "Iteration 9, loss = 0.26496376\n",
            "Iteration 10, loss = 0.25992582\n",
            "Iteration 11, loss = 0.25563987\n",
            "Iteration 12, loss = 0.25144145\n",
            "Iteration 13, loss = 0.24706064\n",
            "Iteration 14, loss = 0.24551682\n",
            "Iteration 15, loss = 0.24173703\n",
            "Iteration 16, loss = 0.23794871\n",
            "Iteration 17, loss = 0.23501871\n",
            "Iteration 18, loss = 0.23235188\n",
            "Iteration 19, loss = 0.22934510\n",
            "Iteration 20, loss = 0.22914493\n",
            "Iteration 21, loss = 0.22607371\n",
            "Iteration 22, loss = 0.22329077\n",
            "Iteration 23, loss = 0.22355630\n",
            "Iteration 24, loss = 0.22264968\n",
            "Iteration 25, loss = 0.21929078\n",
            "Iteration 26, loss = 0.21901629\n",
            "Iteration 27, loss = 0.21807698\n",
            "Iteration 28, loss = 0.21687259\n",
            "Iteration 29, loss = 0.21485306\n",
            "Iteration 30, loss = 0.21424265\n",
            "Iteration 31, loss = 0.21337541\n",
            "Iteration 32, loss = 0.21295593\n",
            "Iteration 33, loss = 0.21093046\n",
            "Iteration 34, loss = 0.21232117\n",
            "Iteration 35, loss = 0.21162687\n",
            "Iteration 36, loss = 0.21182119\n",
            "Iteration 37, loss = 0.20808623\n",
            "Iteration 38, loss = 0.20983994\n",
            "Iteration 39, loss = 0.20931311\n",
            "Iteration 40, loss = 0.21004866\n",
            "Iteration 41, loss = 0.20718966\n",
            "Iteration 42, loss = 0.20614165\n",
            "Iteration 43, loss = 0.20906943\n",
            "Iteration 44, loss = 0.20699425\n",
            "Iteration 45, loss = 0.20677960\n",
            "Iteration 46, loss = 0.20718347\n",
            "Iteration 47, loss = 0.20476099\n",
            "Iteration 48, loss = 0.20662870\n",
            "Iteration 49, loss = 0.20649222\n",
            "Iteration 50, loss = 0.20555382\n",
            "Iteration 51, loss = 0.20305887\n",
            "Iteration 52, loss = 0.20424693\n",
            "Iteration 53, loss = 0.20721039\n",
            "Iteration 54, loss = 0.20578269\n",
            "Iteration 55, loss = 0.20483910\n",
            "Iteration 56, loss = 0.20347117\n",
            "Iteration 57, loss = 0.20519864\n",
            "Iteration 58, loss = 0.20488845\n",
            "Iteration 59, loss = 0.20430110\n",
            "Iteration 60, loss = 0.20360521\n",
            "Iteration 61, loss = 0.20457555\n",
            "Iteration 62, loss = 0.20401934\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 63, loss = 0.19658644\n",
            "Iteration 64, loss = 0.19616510\n",
            "Iteration 65, loss = 0.19546669\n",
            "Iteration 66, loss = 0.19508715\n",
            "Iteration 67, loss = 0.19542923\n",
            "Iteration 68, loss = 0.19528595\n",
            "Iteration 69, loss = 0.19475367\n",
            "Iteration 70, loss = 0.19463265\n",
            "Iteration 71, loss = 0.19402573\n",
            "Iteration 72, loss = 0.19380973\n",
            "Iteration 73, loss = 0.19393202\n",
            "Iteration 74, loss = 0.19339454\n",
            "Iteration 75, loss = 0.19377720\n",
            "Iteration 76, loss = 0.19261554\n",
            "Iteration 77, loss = 0.19296599\n",
            "Iteration 78, loss = 0.19285566\n",
            "Iteration 79, loss = 0.19231582\n",
            "Iteration 80, loss = 0.19285088\n",
            "Iteration 81, loss = 0.19266489\n",
            "Iteration 82, loss = 0.19227424\n",
            "Iteration 83, loss = 0.19254872\n",
            "Iteration 84, loss = 0.19142763\n",
            "Iteration 85, loss = 0.19181119\n",
            "Iteration 86, loss = 0.19200693\n",
            "Iteration 87, loss = 0.19184912\n",
            "Iteration 88, loss = 0.19115398\n",
            "Iteration 89, loss = 0.19168794\n",
            "Iteration 90, loss = 0.19075906\n",
            "Iteration 91, loss = 0.19103735\n",
            "Iteration 92, loss = 0.19076283\n",
            "Iteration 93, loss = 0.19058887\n",
            "Iteration 94, loss = 0.19051436\n",
            "Iteration 95, loss = 0.19052991\n",
            "Iteration 96, loss = 0.19032170\n",
            "Iteration 97, loss = 0.18990706\n",
            "Iteration 98, loss = 0.19018898\n",
            "Iteration 99, loss = 0.18995976\n",
            "Iteration 100, loss = 0.19016265\n",
            "Iteration 101, loss = 0.18966951\n",
            "Iteration 102, loss = 0.18885307\n",
            "Iteration 103, loss = 0.18907141\n",
            "Iteration 104, loss = 0.18938812\n",
            "Iteration 105, loss = 0.18938777\n",
            "Iteration 106, loss = 0.18848136\n",
            "Iteration 107, loss = 0.18864009\n",
            "Iteration 108, loss = 0.18868183\n",
            "Iteration 109, loss = 0.18865530\n",
            "Iteration 110, loss = 0.18799714\n",
            "Iteration 111, loss = 0.18822394\n",
            "Iteration 112, loss = 0.18801105\n",
            "Iteration 113, loss = 0.18781600\n",
            "Iteration 114, loss = 0.18777121\n",
            "Iteration 115, loss = 0.18862793\n",
            "Iteration 116, loss = 0.18800567\n",
            "Iteration 117, loss = 0.18735057\n",
            "Iteration 118, loss = 0.18654527\n",
            "Iteration 119, loss = 0.18666057\n",
            "Iteration 120, loss = 0.18773799\n",
            "Iteration 121, loss = 0.18756425\n",
            "Iteration 122, loss = 0.18714467\n",
            "Iteration 123, loss = 0.18667919\n",
            "Iteration 124, loss = 0.18583553\n",
            "Iteration 125, loss = 0.18646064\n",
            "Iteration 126, loss = 0.18573239\n",
            "Iteration 127, loss = 0.18611370\n",
            "Iteration 128, loss = 0.18689452\n",
            "Iteration 129, loss = 0.18648834\n",
            "Iteration 130, loss = 0.18591727\n",
            "Iteration 131, loss = 0.18566527\n",
            "Iteration 132, loss = 0.18608157\n",
            "Iteration 133, loss = 0.18600401\n",
            "Iteration 134, loss = 0.18494078\n",
            "Iteration 135, loss = 0.18538275\n",
            "Iteration 136, loss = 0.18456094\n",
            "Iteration 137, loss = 0.18577919\n",
            "Iteration 138, loss = 0.18406747\n",
            "Iteration 139, loss = 0.18444887\n",
            "Iteration 140, loss = 0.18558178\n",
            "Iteration 141, loss = 0.18425066\n",
            "Iteration 142, loss = 0.18594193\n",
            "Iteration 143, loss = 0.18446861\n",
            "Iteration 144, loss = 0.18348155\n",
            "Iteration 145, loss = 0.18437432\n",
            "Iteration 146, loss = 0.18407484\n",
            "Iteration 147, loss = 0.18497707\n",
            "Iteration 148, loss = 0.18477528\n",
            "Iteration 149, loss = 0.18428825\n",
            "Iteration 150, loss = 0.18287607\n",
            "Iteration 151, loss = 0.18624184\n",
            "Iteration 152, loss = 0.18267722\n",
            "Iteration 153, loss = 0.18443137\n",
            "Iteration 154, loss = 0.18223546\n",
            "Iteration 155, loss = 0.18256812\n",
            "Iteration 156, loss = 0.18251077\n",
            "Iteration 157, loss = 0.18465263\n",
            "Iteration 158, loss = 0.18242884\n",
            "Iteration 159, loss = 0.18278382\n",
            "Iteration 160, loss = 0.18405475\n",
            "Iteration 161, loss = 0.18398893\n",
            "Iteration 162, loss = 0.18412389\n",
            "Iteration 163, loss = 0.18346313\n",
            "Iteration 164, loss = 0.18252312\n",
            "Iteration 165, loss = 0.18323244\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 166, loss = 0.17852380\n",
            "Iteration 167, loss = 0.17764707\n",
            "Iteration 168, loss = 0.17808191\n",
            "Iteration 169, loss = 0.17796660\n",
            "Iteration 170, loss = 0.17789353\n",
            "Iteration 171, loss = 0.17781902\n",
            "Iteration 172, loss = 0.17764187\n",
            "Iteration 173, loss = 0.17775998\n",
            "Iteration 174, loss = 0.17771924\n",
            "Iteration 175, loss = 0.17739150\n",
            "Iteration 176, loss = 0.17759718\n",
            "Iteration 177, loss = 0.17727205\n",
            "Iteration 178, loss = 0.17760732\n",
            "Iteration 179, loss = 0.17729941\n",
            "Iteration 180, loss = 0.17713975\n",
            "Iteration 181, loss = 0.17721012\n",
            "Iteration 182, loss = 0.17747039\n",
            "Iteration 183, loss = 0.17777906\n",
            "Iteration 184, loss = 0.17736840\n",
            "Iteration 185, loss = 0.17745214\n",
            "Iteration 186, loss = 0.17719760\n",
            "Iteration 187, loss = 0.17700206\n",
            "Iteration 188, loss = 0.17712185\n",
            "Iteration 189, loss = 0.17721269\n",
            "Iteration 190, loss = 0.17703502\n",
            "Iteration 191, loss = 0.17698997\n",
            "Iteration 192, loss = 0.17698783\n",
            "Iteration 193, loss = 0.17652719\n",
            "Iteration 194, loss = 0.17701493\n",
            "Iteration 195, loss = 0.17662465\n",
            "Iteration 196, loss = 0.17675764\n",
            "Iteration 197, loss = 0.17661126\n",
            "Iteration 198, loss = 0.17667315\n",
            "Iteration 199, loss = 0.17663043\n",
            "Iteration 200, loss = 0.17671372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.14013795\n",
            "Iteration 2, loss = 0.36835872\n",
            "Iteration 3, loss = 0.35383767\n",
            "Iteration 4, loss = 0.34219531\n",
            "Iteration 5, loss = 0.33043733\n",
            "Iteration 6, loss = 0.32114600\n",
            "Iteration 7, loss = 0.31209734\n",
            "Iteration 8, loss = 0.30326671\n",
            "Iteration 9, loss = 0.29552593\n",
            "Iteration 10, loss = 0.28770625\n",
            "Iteration 11, loss = 0.28116477\n",
            "Iteration 12, loss = 0.27677917\n",
            "Iteration 13, loss = 0.26930776\n",
            "Iteration 14, loss = 0.26572647\n",
            "Iteration 15, loss = 0.26055229\n",
            "Iteration 16, loss = 0.25616046\n",
            "Iteration 17, loss = 0.25196806\n",
            "Iteration 18, loss = 0.24825420\n",
            "Iteration 19, loss = 0.24457908\n",
            "Iteration 20, loss = 0.24219909\n",
            "Iteration 21, loss = 0.23801206\n",
            "Iteration 22, loss = 0.23643240\n",
            "Iteration 23, loss = 0.23359852\n",
            "Iteration 24, loss = 0.23180937\n",
            "Iteration 25, loss = 0.22920838\n",
            "Iteration 26, loss = 0.22704771\n",
            "Iteration 27, loss = 0.22579095\n",
            "Iteration 28, loss = 0.22459358\n",
            "Iteration 29, loss = 0.22182655\n",
            "Iteration 30, loss = 0.22082744\n",
            "Iteration 31, loss = 0.21985060\n",
            "Iteration 32, loss = 0.21949335\n",
            "Iteration 33, loss = 0.21950776\n",
            "Iteration 34, loss = 0.21674922\n",
            "Iteration 35, loss = 0.21695303\n",
            "Iteration 36, loss = 0.21444752\n",
            "Iteration 37, loss = 0.21373903\n",
            "Iteration 38, loss = 0.21437802\n",
            "Iteration 39, loss = 0.21468628\n",
            "Iteration 40, loss = 0.21187650\n",
            "Iteration 41, loss = 0.20956971\n",
            "Iteration 42, loss = 0.20978063\n",
            "Iteration 43, loss = 0.20910178\n",
            "Iteration 44, loss = 0.20839305\n",
            "Iteration 45, loss = 0.20934188\n",
            "Iteration 46, loss = 0.20939301\n",
            "Iteration 47, loss = 0.21133230\n",
            "Iteration 48, loss = 0.21129632\n",
            "Iteration 49, loss = 0.20777628\n",
            "Iteration 50, loss = 0.20752064\n",
            "Iteration 51, loss = 0.20676689\n",
            "Iteration 52, loss = 0.20856337\n",
            "Iteration 53, loss = 0.20511207\n",
            "Iteration 54, loss = 0.20584255\n",
            "Iteration 55, loss = 0.20585031\n",
            "Iteration 56, loss = 0.20761462\n",
            "Iteration 57, loss = 0.20750726\n",
            "Iteration 58, loss = 0.20656914\n",
            "Iteration 59, loss = 0.20567722\n",
            "Iteration 60, loss = 0.20338218\n",
            "Iteration 61, loss = 0.20891890\n",
            "Iteration 62, loss = 0.20580467\n",
            "Iteration 63, loss = 0.20488471\n",
            "Iteration 64, loss = 0.20530655\n",
            "Iteration 65, loss = 0.20474781\n",
            "Iteration 66, loss = 0.20905243\n",
            "Iteration 67, loss = 0.20432448\n",
            "Iteration 68, loss = 0.20633894\n",
            "Iteration 69, loss = 0.20580706\n",
            "Iteration 70, loss = 0.20702625\n",
            "Iteration 71, loss = 0.20371464\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 72, loss = 0.20021327\n",
            "Iteration 73, loss = 0.19687556\n",
            "Iteration 74, loss = 0.19728073\n",
            "Iteration 75, loss = 0.19706265\n",
            "Iteration 76, loss = 0.19676104\n",
            "Iteration 77, loss = 0.19588569\n",
            "Iteration 78, loss = 0.19601657\n",
            "Iteration 79, loss = 0.19606922\n",
            "Iteration 80, loss = 0.19555395\n",
            "Iteration 81, loss = 0.19581964\n",
            "Iteration 82, loss = 0.19507375\n",
            "Iteration 83, loss = 0.19548272\n",
            "Iteration 84, loss = 0.19519521\n",
            "Iteration 85, loss = 0.19502232\n",
            "Iteration 86, loss = 0.19494277\n",
            "Iteration 87, loss = 0.19461783\n",
            "Iteration 88, loss = 0.19406007\n",
            "Iteration 89, loss = 0.19408065\n",
            "Iteration 90, loss = 0.19365312\n",
            "Iteration 91, loss = 0.19407072\n",
            "Iteration 92, loss = 0.19398742\n",
            "Iteration 93, loss = 0.19381857\n",
            "Iteration 94, loss = 0.19318587\n",
            "Iteration 95, loss = 0.19270146\n",
            "Iteration 96, loss = 0.19331322\n",
            "Iteration 97, loss = 0.19290576\n",
            "Iteration 98, loss = 0.19256676\n",
            "Iteration 99, loss = 0.19271174\n",
            "Iteration 100, loss = 0.19234181\n",
            "Iteration 101, loss = 0.19197154\n",
            "Iteration 102, loss = 0.19276324\n",
            "Iteration 103, loss = 0.19194680\n",
            "Iteration 104, loss = 0.19137854\n",
            "Iteration 105, loss = 0.19212143\n",
            "Iteration 106, loss = 0.19163324\n",
            "Iteration 107, loss = 0.19122203\n",
            "Iteration 108, loss = 0.19158707\n",
            "Iteration 109, loss = 0.19055600\n",
            "Iteration 110, loss = 0.19057872\n",
            "Iteration 111, loss = 0.19147609\n",
            "Iteration 112, loss = 0.19051026\n",
            "Iteration 113, loss = 0.19066549\n",
            "Iteration 114, loss = 0.19105829\n",
            "Iteration 115, loss = 0.19080963\n",
            "Iteration 116, loss = 0.19030443\n",
            "Iteration 117, loss = 0.18969724\n",
            "Iteration 118, loss = 0.18956851\n",
            "Iteration 119, loss = 0.18925742\n",
            "Iteration 120, loss = 0.18959292\n",
            "Iteration 121, loss = 0.18901155\n",
            "Iteration 122, loss = 0.18973615\n",
            "Iteration 123, loss = 0.18954966\n",
            "Iteration 124, loss = 0.18863693\n",
            "Iteration 125, loss = 0.18933836\n",
            "Iteration 126, loss = 0.18847701\n",
            "Iteration 127, loss = 0.18833787\n",
            "Iteration 128, loss = 0.18832829\n",
            "Iteration 129, loss = 0.18910677\n",
            "Iteration 130, loss = 0.18847796\n",
            "Iteration 131, loss = 0.18825130\n",
            "Iteration 132, loss = 0.18837325\n",
            "Iteration 133, loss = 0.18810388\n",
            "Iteration 134, loss = 0.18900424\n",
            "Iteration 135, loss = 0.18782559\n",
            "Iteration 136, loss = 0.18772588\n",
            "Iteration 137, loss = 0.18836550\n",
            "Iteration 138, loss = 0.18807876\n",
            "Iteration 139, loss = 0.18755568\n",
            "Iteration 140, loss = 0.18712044\n",
            "Iteration 141, loss = 0.18660967\n",
            "Iteration 142, loss = 0.18719655\n",
            "Iteration 143, loss = 0.18732354\n",
            "Iteration 144, loss = 0.18617525\n",
            "Iteration 145, loss = 0.18654179\n",
            "Iteration 146, loss = 0.18683749\n",
            "Iteration 147, loss = 0.18625801\n",
            "Iteration 148, loss = 0.18523507\n",
            "Iteration 149, loss = 0.18697883\n",
            "Iteration 150, loss = 0.18632683\n",
            "Iteration 151, loss = 0.18602618\n",
            "Iteration 152, loss = 0.18621317\n",
            "Iteration 153, loss = 0.18462694\n",
            "Iteration 154, loss = 0.18607310\n",
            "Iteration 155, loss = 0.18483579\n",
            "Iteration 156, loss = 0.18460478\n",
            "Iteration 157, loss = 0.18685001\n",
            "Iteration 158, loss = 0.18491611\n",
            "Iteration 159, loss = 0.18505919\n",
            "Iteration 160, loss = 0.18409424\n",
            "Iteration 161, loss = 0.18528448\n",
            "Iteration 162, loss = 0.18705334\n",
            "Iteration 163, loss = 0.18516362\n",
            "Iteration 164, loss = 0.18439318\n",
            "Iteration 165, loss = 0.18507719\n",
            "Iteration 166, loss = 0.18532448\n",
            "Iteration 167, loss = 0.18560256\n",
            "Iteration 168, loss = 0.18338824\n",
            "Iteration 169, loss = 0.18410807\n",
            "Iteration 170, loss = 0.18362157\n",
            "Iteration 171, loss = 0.18461072\n",
            "Iteration 172, loss = 0.18381591\n",
            "Iteration 173, loss = 0.18479764\n",
            "Iteration 174, loss = 0.18478570\n",
            "Iteration 175, loss = 0.18542209\n",
            "Iteration 176, loss = 0.18471086\n",
            "Iteration 177, loss = 0.18469027\n",
            "Iteration 178, loss = 0.18288520\n",
            "Iteration 179, loss = 0.18476594\n",
            "Iteration 180, loss = 0.18410815\n",
            "Iteration 181, loss = 0.18774381\n",
            "Iteration 182, loss = 0.18423264\n",
            "Iteration 183, loss = 0.18359938\n",
            "Iteration 184, loss = 0.18586143\n",
            "Iteration 185, loss = 0.18277444\n",
            "Iteration 186, loss = 0.18355874\n",
            "Iteration 187, loss = 0.18468368\n",
            "Iteration 188, loss = 0.18380684\n",
            "Iteration 189, loss = 0.18492398\n",
            "Iteration 190, loss = 0.18310918\n",
            "Iteration 191, loss = 0.18565849\n",
            "Iteration 192, loss = 0.18322467\n",
            "Iteration 193, loss = 0.18354742\n",
            "Iteration 194, loss = 0.18404687\n",
            "Iteration 195, loss = 0.18398286\n",
            "Iteration 196, loss = 0.18189936\n",
            "Iteration 197, loss = 0.18354427\n",
            "Iteration 198, loss = 0.18208731\n",
            "Iteration 199, loss = 0.18380941\n",
            "Iteration 200, loss = 0.18424720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.05318396\n",
            "Iteration 2, loss = 0.33992248\n",
            "Iteration 3, loss = 0.32609263\n",
            "Iteration 4, loss = 0.31479236\n",
            "Iteration 5, loss = 0.30585843\n",
            "Iteration 6, loss = 0.29624352\n",
            "Iteration 7, loss = 0.28852094\n",
            "Iteration 8, loss = 0.28203441\n",
            "Iteration 9, loss = 0.27638743\n",
            "Iteration 10, loss = 0.26992364\n",
            "Iteration 11, loss = 0.26519654\n",
            "Iteration 12, loss = 0.26044806\n",
            "Iteration 13, loss = 0.25691839\n",
            "Iteration 14, loss = 0.25039244\n",
            "Iteration 15, loss = 0.24853970\n",
            "Iteration 16, loss = 0.24631384\n",
            "Iteration 17, loss = 0.24108899\n",
            "Iteration 18, loss = 0.23922405\n",
            "Iteration 19, loss = 0.23686639\n",
            "Iteration 20, loss = 0.23480060\n",
            "Iteration 21, loss = 0.23325653\n",
            "Iteration 22, loss = 0.22759671\n",
            "Iteration 23, loss = 0.22755539\n",
            "Iteration 24, loss = 0.22525023\n",
            "Iteration 25, loss = 0.22527950\n",
            "Iteration 26, loss = 0.22053152\n",
            "Iteration 27, loss = 0.22177470\n",
            "Iteration 28, loss = 0.21944941\n",
            "Iteration 29, loss = 0.21897532\n",
            "Iteration 30, loss = 0.21724545\n",
            "Iteration 31, loss = 0.21715585\n",
            "Iteration 32, loss = 0.21665152\n",
            "Iteration 33, loss = 0.21393242\n",
            "Iteration 34, loss = 0.21413237\n",
            "Iteration 35, loss = 0.21521998\n",
            "Iteration 36, loss = 0.21227266\n",
            "Iteration 37, loss = 0.21386909\n",
            "Iteration 38, loss = 0.20975575\n",
            "Iteration 39, loss = 0.20910488\n",
            "Iteration 40, loss = 0.21242026\n",
            "Iteration 41, loss = 0.20896844\n",
            "Iteration 42, loss = 0.21142021\n",
            "Iteration 43, loss = 0.20685285\n",
            "Iteration 44, loss = 0.20978867\n",
            "Iteration 45, loss = 0.20752501\n",
            "Iteration 46, loss = 0.20922115\n",
            "Iteration 47, loss = 0.20579733\n",
            "Iteration 48, loss = 0.20573287\n",
            "Iteration 49, loss = 0.20823023\n",
            "Iteration 50, loss = 0.20490756\n",
            "Iteration 51, loss = 0.20564277\n",
            "Iteration 52, loss = 0.20656871\n",
            "Iteration 53, loss = 0.20852617\n",
            "Iteration 54, loss = 0.20725975\n",
            "Iteration 55, loss = 0.20370596\n",
            "Iteration 56, loss = 0.20496719\n",
            "Iteration 57, loss = 0.20797618\n",
            "Iteration 58, loss = 0.20470340\n",
            "Iteration 59, loss = 0.20528452\n",
            "Iteration 60, loss = 0.20572548\n",
            "Iteration 61, loss = 0.20444396\n",
            "Iteration 62, loss = 0.20614667\n",
            "Iteration 63, loss = 0.20607110\n",
            "Iteration 64, loss = 0.20736152\n",
            "Iteration 65, loss = 0.20560344\n",
            "Iteration 66, loss = 0.20302011\n",
            "Iteration 67, loss = 0.20522844\n",
            "Iteration 68, loss = 0.20338674\n",
            "Iteration 69, loss = 0.20552287\n",
            "Iteration 70, loss = 0.20667205\n",
            "Iteration 71, loss = 0.20491464\n",
            "Iteration 72, loss = 0.20441989\n",
            "Iteration 73, loss = 0.20405547\n",
            "Iteration 74, loss = 0.20400842\n",
            "Iteration 75, loss = 0.20656184\n",
            "Iteration 76, loss = 0.20504877\n",
            "Iteration 77, loss = 0.20491199\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.19745374\n",
            "Iteration 79, loss = 0.19670441\n",
            "Iteration 80, loss = 0.19655476\n",
            "Iteration 81, loss = 0.19607351\n",
            "Iteration 82, loss = 0.19594490\n",
            "Iteration 83, loss = 0.19525428\n",
            "Iteration 84, loss = 0.19539343\n",
            "Iteration 85, loss = 0.19509982\n",
            "Iteration 86, loss = 0.19467468\n",
            "Iteration 87, loss = 0.19470530\n",
            "Iteration 88, loss = 0.19421060\n",
            "Iteration 89, loss = 0.19422378\n",
            "Iteration 90, loss = 0.19416680\n",
            "Iteration 91, loss = 0.19393652\n",
            "Iteration 92, loss = 0.19367308\n",
            "Iteration 93, loss = 0.19340112\n",
            "Iteration 94, loss = 0.19355619\n",
            "Iteration 95, loss = 0.19293322\n",
            "Iteration 96, loss = 0.19305919\n",
            "Iteration 97, loss = 0.19232062\n",
            "Iteration 98, loss = 0.19299009\n",
            "Iteration 99, loss = 0.19216877\n",
            "Iteration 100, loss = 0.19250462\n",
            "Iteration 101, loss = 0.19205738\n",
            "Iteration 102, loss = 0.19188489\n",
            "Iteration 103, loss = 0.19169616\n",
            "Iteration 104, loss = 0.19176380\n",
            "Iteration 105, loss = 0.19197321\n",
            "Iteration 106, loss = 0.19158707\n",
            "Iteration 107, loss = 0.19182596\n",
            "Iteration 108, loss = 0.19140698\n",
            "Iteration 109, loss = 0.19111957\n",
            "Iteration 110, loss = 0.19100291\n",
            "Iteration 111, loss = 0.19127826\n",
            "Iteration 112, loss = 0.19091033\n",
            "Iteration 113, loss = 0.19076565\n",
            "Iteration 114, loss = 0.19028223\n",
            "Iteration 115, loss = 0.19080720\n",
            "Iteration 116, loss = 0.19029068\n",
            "Iteration 117, loss = 0.19013387\n",
            "Iteration 118, loss = 0.19042808\n",
            "Iteration 119, loss = 0.18941689\n",
            "Iteration 120, loss = 0.18942122\n",
            "Iteration 121, loss = 0.18951707\n",
            "Iteration 122, loss = 0.18942453\n",
            "Iteration 123, loss = 0.18934583\n",
            "Iteration 124, loss = 0.18852189\n",
            "Iteration 125, loss = 0.18912763\n",
            "Iteration 126, loss = 0.18869495\n",
            "Iteration 127, loss = 0.18819200\n",
            "Iteration 128, loss = 0.18829238\n",
            "Iteration 129, loss = 0.18836756\n",
            "Iteration 130, loss = 0.18823877\n",
            "Iteration 131, loss = 0.18846877\n",
            "Iteration 132, loss = 0.18744019\n",
            "Iteration 133, loss = 0.18807142\n",
            "Iteration 134, loss = 0.18798262\n",
            "Iteration 135, loss = 0.18772500\n",
            "Iteration 136, loss = 0.18728321\n",
            "Iteration 137, loss = 0.18712473\n",
            "Iteration 138, loss = 0.18813194\n",
            "Iteration 139, loss = 0.18739984\n",
            "Iteration 140, loss = 0.18780773\n",
            "Iteration 141, loss = 0.18647259\n",
            "Iteration 142, loss = 0.18567526\n",
            "Iteration 143, loss = 0.18616130\n",
            "Iteration 144, loss = 0.18653036\n",
            "Iteration 145, loss = 0.18618158\n",
            "Iteration 146, loss = 0.18631912\n",
            "Iteration 147, loss = 0.18650171\n",
            "Iteration 148, loss = 0.18757096\n",
            "Iteration 149, loss = 0.18715061\n",
            "Iteration 150, loss = 0.18544639\n",
            "Iteration 151, loss = 0.18602897\n",
            "Iteration 152, loss = 0.18616591\n",
            "Iteration 153, loss = 0.18541732\n",
            "Iteration 154, loss = 0.18588688\n",
            "Iteration 155, loss = 0.18449475\n",
            "Iteration 156, loss = 0.18629422\n",
            "Iteration 157, loss = 0.18476018\n",
            "Iteration 158, loss = 0.18586141\n",
            "Iteration 159, loss = 0.18524632\n",
            "Iteration 160, loss = 0.18649630\n",
            "Iteration 161, loss = 0.18556338\n",
            "Iteration 162, loss = 0.18460362\n",
            "Iteration 163, loss = 0.18406077\n",
            "Iteration 164, loss = 0.18556617\n",
            "Iteration 165, loss = 0.18362723\n",
            "Iteration 166, loss = 0.18702143\n",
            "Iteration 167, loss = 0.18419728\n",
            "Iteration 168, loss = 0.18578696\n",
            "Iteration 169, loss = 0.18430609\n",
            "Iteration 170, loss = 0.18543116\n",
            "Iteration 171, loss = 0.18511022\n",
            "Iteration 172, loss = 0.18592562\n",
            "Iteration 173, loss = 0.18417314\n",
            "Iteration 174, loss = 0.18460097\n",
            "Iteration 175, loss = 0.18317138\n",
            "Iteration 176, loss = 0.18203254\n",
            "Iteration 177, loss = 0.18274162\n",
            "Iteration 178, loss = 0.18422506\n",
            "Iteration 179, loss = 0.18556404\n",
            "Iteration 180, loss = 0.18458293\n",
            "Iteration 181, loss = 0.18380051\n",
            "Iteration 182, loss = 0.18207365\n",
            "Iteration 183, loss = 0.18288685\n",
            "Iteration 184, loss = 0.18356169\n",
            "Iteration 185, loss = 0.18358294\n",
            "Iteration 186, loss = 0.18345680\n",
            "Iteration 187, loss = 0.18187512\n",
            "Iteration 188, loss = 0.18303569\n",
            "Iteration 189, loss = 0.18440827\n",
            "Iteration 190, loss = 0.18278472\n",
            "Iteration 191, loss = 0.18149714\n",
            "Iteration 192, loss = 0.18337213\n",
            "Iteration 193, loss = 0.18361700\n",
            "Iteration 194, loss = 0.18492591\n",
            "Iteration 195, loss = 0.18253191\n",
            "Iteration 196, loss = 0.18305091\n",
            "Iteration 197, loss = 0.18352517\n",
            "Iteration 198, loss = 0.18190589\n",
            "Iteration 199, loss = 0.18345599\n",
            "Iteration 200, loss = 0.18229951\n",
            "----------------------------------\n",
            "[[33826    21]\n",
            " [ 1364  2410]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  8.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     33847\n",
            "           1       0.99      0.64      0.78      3774\n",
            "\n",
            "    accuracy                           0.96     37621\n",
            "   macro avg       0.98      0.82      0.88     37621\n",
            "weighted avg       0.96      0.96      0.96     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 95.179% (0.495%)\n",
            "Prediction Accuracy (Shuffle Split) : 96.102% (0.757%)\n",
            "Iteration 1, loss = 0.80484632\n",
            "Iteration 2, loss = 0.41660248\n",
            "Iteration 3, loss = 0.38704361\n",
            "Iteration 4, loss = 0.36581753\n",
            "Iteration 5, loss = 0.35127570\n",
            "Iteration 6, loss = 0.33982860\n",
            "Iteration 7, loss = 0.33195404\n",
            "Iteration 8, loss = 0.32566245\n",
            "Iteration 9, loss = 0.32173585\n",
            "Iteration 10, loss = 0.31937162\n",
            "Iteration 11, loss = 0.31666811\n",
            "Iteration 12, loss = 0.31423535\n",
            "Iteration 13, loss = 0.31411590\n",
            "Iteration 14, loss = 0.31324210\n",
            "Iteration 15, loss = 0.31227396\n",
            "Iteration 16, loss = 0.31225658\n",
            "Iteration 17, loss = 0.31152331\n",
            "Iteration 18, loss = 0.31187048\n",
            "Iteration 19, loss = 0.31063379\n",
            "Iteration 20, loss = 0.31237509\n",
            "Iteration 21, loss = 0.31117535\n",
            "Iteration 22, loss = 0.31045126\n",
            "Iteration 23, loss = 0.31151843\n",
            "Iteration 24, loss = 0.31096267\n",
            "Iteration 25, loss = 0.31046586\n",
            "Iteration 26, loss = 0.31114240\n",
            "Iteration 27, loss = 0.31126632\n",
            "Iteration 28, loss = 0.31098581\n",
            "Iteration 29, loss = 0.31093498\n",
            "Iteration 30, loss = 0.31061955\n",
            "Iteration 31, loss = 0.31034829\n",
            "Iteration 32, loss = 0.31102834\n",
            "Iteration 33, loss = 0.31004369\n",
            "Iteration 34, loss = 0.31055231\n",
            "Iteration 35, loss = 0.31023905\n",
            "Iteration 36, loss = 0.31031841\n",
            "Iteration 37, loss = 0.31094179\n",
            "Iteration 38, loss = 0.31055063\n",
            "Iteration 39, loss = 0.30903527\n",
            "Iteration 40, loss = 0.31110412\n",
            "Iteration 41, loss = 0.30897192\n",
            "Iteration 42, loss = 0.32393380\n",
            "Iteration 43, loss = 0.32233880\n",
            "Iteration 44, loss = 0.31839523\n",
            "Iteration 45, loss = 0.31556290\n",
            "Iteration 46, loss = 0.31368495\n",
            "Iteration 47, loss = 0.31223911\n",
            "Iteration 48, loss = 0.31084077\n",
            "Iteration 49, loss = 0.31033854\n",
            "Iteration 50, loss = 0.31044969\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 51, loss = 0.30756428\n",
            "Iteration 52, loss = 0.30664736\n",
            "Iteration 53, loss = 0.30657944\n",
            "Iteration 54, loss = 0.30593061\n",
            "Iteration 55, loss = 0.30586389\n",
            "Iteration 56, loss = 0.30535180\n",
            "Iteration 57, loss = 0.30493734\n",
            "Iteration 58, loss = 0.30468996\n",
            "Iteration 59, loss = 0.30388020\n",
            "Iteration 60, loss = 0.30368603\n",
            "Iteration 61, loss = 0.30317025\n",
            "Iteration 62, loss = 0.30268570\n",
            "Iteration 63, loss = 0.30189014\n",
            "Iteration 64, loss = 0.30127754\n",
            "Iteration 65, loss = 0.30085583\n",
            "Iteration 66, loss = 0.30004709\n",
            "Iteration 67, loss = 0.29969012\n",
            "Iteration 68, loss = 0.29871721\n",
            "Iteration 69, loss = 0.29859379\n",
            "Iteration 70, loss = 0.29810118\n",
            "Iteration 71, loss = 0.29762446\n",
            "Iteration 72, loss = 0.29630554\n",
            "Iteration 73, loss = 0.29556649\n",
            "Iteration 74, loss = 0.29558807\n",
            "Iteration 75, loss = 0.29555627\n",
            "Iteration 76, loss = 0.29534017\n",
            "Iteration 77, loss = 0.29471919\n",
            "Iteration 78, loss = 0.29425887\n",
            "Iteration 79, loss = 0.29416216\n",
            "Iteration 80, loss = 0.29377306\n",
            "Iteration 81, loss = 0.29268656\n",
            "Iteration 82, loss = 0.29438297\n",
            "Iteration 83, loss = 0.29391255\n",
            "Iteration 84, loss = 0.29399834\n",
            "Iteration 85, loss = 0.29401695\n",
            "Iteration 86, loss = 0.29439375\n",
            "Iteration 87, loss = 0.29328673\n",
            "Iteration 88, loss = 0.29458314\n",
            "Iteration 89, loss = 0.29453570\n",
            "Iteration 90, loss = 0.29484251\n",
            "Iteration 91, loss = 0.29571627\n",
            "Iteration 92, loss = 0.29497866\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 93, loss = 0.28196812\n",
            "Iteration 94, loss = 0.28086715\n",
            "Iteration 95, loss = 0.28052193\n",
            "Iteration 96, loss = 0.28018925\n",
            "Iteration 97, loss = 0.27979369\n",
            "Iteration 98, loss = 0.27942000\n",
            "Iteration 99, loss = 0.27914560\n",
            "Iteration 100, loss = 0.27868031\n",
            "Iteration 101, loss = 0.27840985\n",
            "Iteration 102, loss = 0.27793522\n",
            "Iteration 103, loss = 0.27756219\n",
            "Iteration 104, loss = 0.27746237\n",
            "Iteration 105, loss = 0.27687686\n",
            "Iteration 106, loss = 0.27665055\n",
            "Iteration 107, loss = 0.27629051\n",
            "Iteration 108, loss = 0.27581974\n",
            "Iteration 109, loss = 0.27572782\n",
            "Iteration 110, loss = 0.27538599\n",
            "Iteration 111, loss = 0.27484939\n",
            "Iteration 112, loss = 0.27480393\n",
            "Iteration 113, loss = 0.27427856\n",
            "Iteration 114, loss = 0.27404523\n",
            "Iteration 115, loss = 0.27361188\n",
            "Iteration 116, loss = 0.27339201\n",
            "Iteration 117, loss = 0.27326827\n",
            "Iteration 118, loss = 0.27293469\n",
            "Iteration 119, loss = 0.27253623\n",
            "Iteration 120, loss = 0.27223396\n",
            "Iteration 121, loss = 0.27203256\n",
            "Iteration 122, loss = 0.27174488\n",
            "Iteration 123, loss = 0.27148382\n",
            "Iteration 124, loss = 0.27139260\n",
            "Iteration 125, loss = 0.27105606\n",
            "Iteration 126, loss = 0.27074170\n",
            "Iteration 127, loss = 0.27030011\n",
            "Iteration 128, loss = 0.27046430\n",
            "Iteration 129, loss = 0.26980686\n",
            "Iteration 130, loss = 0.26966729\n",
            "Iteration 131, loss = 0.26929451\n",
            "Iteration 132, loss = 0.26908900\n",
            "Iteration 133, loss = 0.26885972\n",
            "Iteration 134, loss = 0.26869038\n",
            "Iteration 135, loss = 0.26848074\n",
            "Iteration 136, loss = 0.26835401\n",
            "Iteration 137, loss = 0.26793058\n",
            "Iteration 138, loss = 0.26756747\n",
            "Iteration 139, loss = 0.26758225\n",
            "Iteration 140, loss = 0.26717439\n",
            "Iteration 141, loss = 0.26708227\n",
            "Iteration 142, loss = 0.26707616\n",
            "Iteration 143, loss = 0.26647306\n",
            "Iteration 144, loss = 0.26651657\n",
            "Iteration 145, loss = 0.26635223\n",
            "Iteration 146, loss = 0.26596651\n",
            "Iteration 147, loss = 0.26564990\n",
            "Iteration 148, loss = 0.26530175\n",
            "Iteration 149, loss = 0.26538672\n",
            "Iteration 150, loss = 0.26524956\n",
            "Iteration 151, loss = 0.26493999\n",
            "Iteration 152, loss = 0.26462464\n",
            "Iteration 153, loss = 0.26470656\n",
            "Iteration 154, loss = 0.26437230\n",
            "Iteration 155, loss = 0.26461675\n",
            "Iteration 156, loss = 0.26414377\n",
            "Iteration 157, loss = 0.26418570\n",
            "Iteration 158, loss = 0.26378122\n",
            "Iteration 159, loss = 0.26383831\n",
            "Iteration 160, loss = 0.26326824\n",
            "Iteration 161, loss = 0.26307806\n",
            "Iteration 162, loss = 0.26335216\n",
            "Iteration 163, loss = 0.26301635\n",
            "Iteration 164, loss = 0.26278943\n",
            "Iteration 165, loss = 0.26266027\n",
            "Iteration 166, loss = 0.26273496\n",
            "Iteration 167, loss = 0.26239513\n",
            "Iteration 168, loss = 0.26227407\n",
            "Iteration 169, loss = 0.26227275\n",
            "Iteration 170, loss = 0.26186978\n",
            "Iteration 171, loss = 0.26153281\n",
            "Iteration 172, loss = 0.26164176\n",
            "Iteration 173, loss = 0.26133509\n",
            "Iteration 174, loss = 0.26116873\n",
            "Iteration 175, loss = 0.26094748\n",
            "Iteration 176, loss = 0.26076910\n",
            "Iteration 177, loss = 0.26082674\n",
            "Iteration 178, loss = 0.26057243\n",
            "Iteration 179, loss = 0.26067598\n",
            "Iteration 180, loss = 0.26067360\n",
            "Iteration 181, loss = 0.26027919\n",
            "Iteration 182, loss = 0.25994467\n",
            "Iteration 183, loss = 0.25979263\n",
            "Iteration 184, loss = 0.25966055\n",
            "Iteration 185, loss = 0.25968771\n",
            "Iteration 186, loss = 0.25943446\n",
            "Iteration 187, loss = 0.25925619\n",
            "Iteration 188, loss = 0.25877816\n",
            "Iteration 189, loss = 0.25903226\n",
            "Iteration 190, loss = 0.25930639\n",
            "Iteration 191, loss = 0.25891898\n",
            "Iteration 192, loss = 0.25866571\n",
            "Iteration 193, loss = 0.25869843\n",
            "Iteration 194, loss = 0.25833329\n",
            "Iteration 195, loss = 0.25829352\n",
            "Iteration 196, loss = 0.25802919\n",
            "Iteration 197, loss = 0.25804640\n",
            "Iteration 198, loss = 0.25792638\n",
            "Iteration 199, loss = 0.25772191\n",
            "Iteration 200, loss = 0.25816898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.13132301\n",
            "Iteration 2, loss = 0.09996655\n",
            "Iteration 3, loss = 0.07609027\n",
            "Iteration 4, loss = 0.05791705\n",
            "Iteration 5, loss = 0.04409112\n",
            "Iteration 6, loss = 0.03358916\n",
            "Iteration 7, loss = 0.02561128\n",
            "Iteration 8, loss = 0.01955094\n",
            "Iteration 9, loss = 0.01495221\n",
            "Iteration 10, loss = 0.01146847\n",
            "Iteration 11, loss = 0.00883557\n",
            "Iteration 12, loss = 0.00685083\n",
            "Iteration 13, loss = 0.00535934\n",
            "Iteration 14, loss = 0.00424240\n",
            "Iteration 15, loss = 0.00340711\n",
            "Iteration 16, loss = 0.00278219\n",
            "Iteration 17, loss = 0.00231393\n",
            "Iteration 18, loss = 0.00196224\n",
            "Iteration 19, loss = 0.00169748\n",
            "Iteration 20, loss = 0.00149773\n",
            "Iteration 21, loss = 0.00134677\n",
            "Iteration 22, loss = 0.00123246\n",
            "Iteration 23, loss = 0.00114577\n",
            "Iteration 24, loss = 0.00107996\n",
            "Iteration 25, loss = 0.00102998\n",
            "Iteration 26, loss = 0.00099199\n",
            "Iteration 27, loss = 0.00096311\n",
            "Iteration 28, loss = 0.00094113\n",
            "Iteration 29, loss = 0.00092441\n",
            "Iteration 30, loss = 0.00091167\n",
            "Iteration 31, loss = 0.00090197\n",
            "Iteration 32, loss = 0.00089457\n",
            "Iteration 33, loss = 0.00088893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00088611\n",
            "Iteration 35, loss = 0.00088530\n",
            "Iteration 36, loss = 0.00088453\n",
            "Iteration 37, loss = 0.00088380\n",
            "Iteration 38, loss = 0.00088312\n",
            "Iteration 39, loss = 0.00088246\n",
            "Iteration 40, loss = 0.00088185\n",
            "Iteration 41, loss = 0.00088126\n",
            "Iteration 42, loss = 0.00088071\n",
            "Iteration 43, loss = 0.00088018\n",
            "Iteration 44, loss = 0.00087968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00087939\n",
            "Iteration 46, loss = 0.00087929\n",
            "Iteration 47, loss = 0.00087920\n",
            "Iteration 48, loss = 0.00087911\n",
            "Iteration 49, loss = 0.00087902\n",
            "Iteration 50, loss = 0.00087893\n",
            "Iteration 51, loss = 0.00087884\n",
            "Iteration 52, loss = 0.00087875\n",
            "Iteration 53, loss = 0.00087867\n",
            "Iteration 54, loss = 0.00087858\n",
            "Iteration 55, loss = 0.00087850\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00087844\n",
            "Iteration 57, loss = 0.00087843\n",
            "Iteration 58, loss = 0.00087841\n",
            "Iteration 59, loss = 0.00087839\n",
            "Iteration 60, loss = 0.00087838\n",
            "Iteration 61, loss = 0.00087836\n",
            "Iteration 62, loss = 0.00087834\n",
            "Iteration 63, loss = 0.00087833\n",
            "Iteration 64, loss = 0.00087831\n",
            "Iteration 65, loss = 0.00087829\n",
            "Iteration 66, loss = 0.00087828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00087827\n",
            "Iteration 68, loss = 0.00087826\n",
            "Iteration 69, loss = 0.00087826\n",
            "Iteration 70, loss = 0.00087826\n",
            "Iteration 71, loss = 0.00087825\n",
            "Iteration 72, loss = 0.00087825\n",
            "Iteration 73, loss = 0.00087825\n",
            "Iteration 74, loss = 0.00087824\n",
            "Iteration 75, loss = 0.00087824\n",
            "Iteration 76, loss = 0.00087824\n",
            "Iteration 77, loss = 0.00087823\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00087823\n",
            "Iteration 79, loss = 0.00087823\n",
            "Iteration 80, loss = 0.00087823\n",
            "Iteration 81, loss = 0.00087823\n",
            "Iteration 82, loss = 0.00087823\n",
            "Iteration 83, loss = 0.00087823\n",
            "Iteration 84, loss = 0.00087823\n",
            "Iteration 85, loss = 0.00087823\n",
            "Iteration 86, loss = 0.00087823\n",
            "Iteration 87, loss = 0.00087823\n",
            "Iteration 88, loss = 0.00087823\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.24446534\n",
            "Iteration 2, loss = 0.17579119\n",
            "Iteration 3, loss = 0.13380474\n",
            "Iteration 4, loss = 0.10184644\n",
            "Iteration 5, loss = 0.07752116\n",
            "Iteration 6, loss = 0.05900579\n",
            "Iteration 7, loss = 0.04491268\n",
            "Iteration 8, loss = 0.03418561\n",
            "Iteration 9, loss = 0.02602063\n",
            "Iteration 10, loss = 0.01980579\n",
            "Iteration 11, loss = 0.01507532\n",
            "Iteration 12, loss = 0.01147469\n",
            "Iteration 13, loss = 0.00873404\n",
            "Iteration 14, loss = 0.00664798\n",
            "Iteration 15, loss = 0.00506016\n",
            "Iteration 16, loss = 0.00385159\n",
            "Iteration 17, loss = 0.00293262\n",
            "Iteration 18, loss = 0.00225432\n",
            "Iteration 19, loss = 0.00183954\n",
            "Iteration 20, loss = 0.00159925\n",
            "Iteration 21, loss = 0.00142515\n",
            "Iteration 22, loss = 0.00129415\n",
            "Iteration 23, loss = 0.00119527\n",
            "Iteration 24, loss = 0.00112050\n",
            "Iteration 25, loss = 0.00106386\n",
            "Iteration 26, loss = 0.00102091\n",
            "Iteration 27, loss = 0.00098830\n",
            "Iteration 28, loss = 0.00096353\n",
            "Iteration 29, loss = 0.00094469\n",
            "Iteration 30, loss = 0.00093037\n",
            "Iteration 31, loss = 0.00091946\n",
            "Iteration 32, loss = 0.00091115\n",
            "Iteration 33, loss = 0.00090482\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00090165\n",
            "Iteration 35, loss = 0.00090075\n",
            "Iteration 36, loss = 0.00089989\n",
            "Iteration 37, loss = 0.00089908\n",
            "Iteration 38, loss = 0.00089831\n",
            "Iteration 39, loss = 0.00089758\n",
            "Iteration 40, loss = 0.00089689\n",
            "Iteration 41, loss = 0.00089623\n",
            "Iteration 42, loss = 0.00089561\n",
            "Iteration 43, loss = 0.00089502\n",
            "Iteration 44, loss = 0.00089446\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00089413\n",
            "Iteration 46, loss = 0.00089403\n",
            "Iteration 47, loss = 0.00089392\n",
            "Iteration 48, loss = 0.00089382\n",
            "Iteration 49, loss = 0.00089372\n",
            "Iteration 50, loss = 0.00089362\n",
            "Iteration 51, loss = 0.00089352\n",
            "Iteration 52, loss = 0.00089342\n",
            "Iteration 53, loss = 0.00089332\n",
            "Iteration 54, loss = 0.00089323\n",
            "Iteration 55, loss = 0.00089313\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089308\n",
            "Iteration 57, loss = 0.00089306\n",
            "Iteration 58, loss = 0.00089304\n",
            "Iteration 59, loss = 0.00089302\n",
            "Iteration 60, loss = 0.00089300\n",
            "Iteration 61, loss = 0.00089298\n",
            "Iteration 62, loss = 0.00089296\n",
            "Iteration 63, loss = 0.00089294\n",
            "Iteration 64, loss = 0.00089293\n",
            "Iteration 65, loss = 0.00089291\n",
            "Iteration 66, loss = 0.00089289\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089288\n",
            "Iteration 68, loss = 0.00089287\n",
            "Iteration 69, loss = 0.00089287\n",
            "Iteration 70, loss = 0.00089287\n",
            "Iteration 71, loss = 0.00089286\n",
            "Iteration 72, loss = 0.00089286\n",
            "Iteration 73, loss = 0.00089286\n",
            "Iteration 74, loss = 0.00089285\n",
            "Iteration 75, loss = 0.00089285\n",
            "Iteration 76, loss = 0.00089284\n",
            "Iteration 77, loss = 0.00089284\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089284\n",
            "Iteration 79, loss = 0.00089284\n",
            "Iteration 80, loss = 0.00089284\n",
            "Iteration 81, loss = 0.00089284\n",
            "Iteration 82, loss = 0.00089284\n",
            "Iteration 83, loss = 0.00089284\n",
            "Iteration 84, loss = 0.00089283\n",
            "Iteration 85, loss = 0.00089283\n",
            "Iteration 86, loss = 0.00089283\n",
            "Iteration 87, loss = 0.00089283\n",
            "Iteration 88, loss = 0.00089283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12827510\n",
            "Iteration 2, loss = 0.09764640\n",
            "Iteration 3, loss = 0.07432426\n",
            "Iteration 4, loss = 0.05657245\n",
            "Iteration 5, loss = 0.04306053\n",
            "Iteration 6, loss = 0.03277583\n",
            "Iteration 7, loss = 0.02494758\n",
            "Iteration 8, loss = 0.01899028\n",
            "Iteration 9, loss = 0.01447730\n",
            "Iteration 10, loss = 0.01110434\n",
            "Iteration 11, loss = 0.00857223\n",
            "Iteration 12, loss = 0.00666417\n",
            "Iteration 13, loss = 0.00523052\n",
            "Iteration 14, loss = 0.00415632\n",
            "Iteration 15, loss = 0.00335281\n",
            "Iteration 16, loss = 0.00275114\n",
            "Iteration 17, loss = 0.00229872\n",
            "Iteration 18, loss = 0.00195771\n",
            "Iteration 19, loss = 0.00170017\n",
            "Iteration 20, loss = 0.00150534\n",
            "Iteration 21, loss = 0.00135773\n",
            "Iteration 22, loss = 0.00124578\n",
            "Iteration 23, loss = 0.00116079\n",
            "Iteration 24, loss = 0.00109622\n",
            "Iteration 25, loss = 0.00104714\n",
            "Iteration 26, loss = 0.00100980\n",
            "Iteration 27, loss = 0.00098136\n",
            "Iteration 28, loss = 0.00095972\n",
            "Iteration 29, loss = 0.00094325\n",
            "Iteration 30, loss = 0.00093071\n",
            "Iteration 31, loss = 0.00092115\n",
            "Iteration 32, loss = 0.00091387\n",
            "Iteration 33, loss = 0.00090833\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00090555\n",
            "Iteration 35, loss = 0.00090476\n",
            "Iteration 36, loss = 0.00090401\n",
            "Iteration 37, loss = 0.00090330\n",
            "Iteration 38, loss = 0.00090262\n",
            "Iteration 39, loss = 0.00090198\n",
            "Iteration 40, loss = 0.00090138\n",
            "Iteration 41, loss = 0.00090080\n",
            "Iteration 42, loss = 0.00090026\n",
            "Iteration 43, loss = 0.00089974\n",
            "Iteration 44, loss = 0.00089925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00089896\n",
            "Iteration 46, loss = 0.00089887\n",
            "Iteration 47, loss = 0.00089878\n",
            "Iteration 48, loss = 0.00089869\n",
            "Iteration 49, loss = 0.00089860\n",
            "Iteration 50, loss = 0.00089851\n",
            "Iteration 51, loss = 0.00089843\n",
            "Iteration 52, loss = 0.00089834\n",
            "Iteration 53, loss = 0.00089826\n",
            "Iteration 54, loss = 0.00089817\n",
            "Iteration 55, loss = 0.00089809\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089804\n",
            "Iteration 57, loss = 0.00089802\n",
            "Iteration 58, loss = 0.00089800\n",
            "Iteration 59, loss = 0.00089799\n",
            "Iteration 60, loss = 0.00089797\n",
            "Iteration 61, loss = 0.00089796\n",
            "Iteration 62, loss = 0.00089794\n",
            "Iteration 63, loss = 0.00089792\n",
            "Iteration 64, loss = 0.00089791\n",
            "Iteration 65, loss = 0.00089789\n",
            "Iteration 66, loss = 0.00089787\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089786\n",
            "Iteration 68, loss = 0.00089786\n",
            "Iteration 69, loss = 0.00089786\n",
            "Iteration 70, loss = 0.00089785\n",
            "Iteration 71, loss = 0.00089785\n",
            "Iteration 72, loss = 0.00089785\n",
            "Iteration 73, loss = 0.00089785\n",
            "Iteration 74, loss = 0.00089784\n",
            "Iteration 75, loss = 0.00089784\n",
            "Iteration 76, loss = 0.00089784\n",
            "Iteration 77, loss = 0.00089783\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089783\n",
            "Iteration 79, loss = 0.00089783\n",
            "Iteration 80, loss = 0.00089783\n",
            "Iteration 81, loss = 0.00089783\n",
            "Iteration 82, loss = 0.00089783\n",
            "Iteration 83, loss = 0.00089783\n",
            "Iteration 84, loss = 0.00089783\n",
            "Iteration 85, loss = 0.00089783\n",
            "Iteration 86, loss = 0.00089783\n",
            "Iteration 87, loss = 0.00089782\n",
            "Iteration 88, loss = 0.00089782\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.20186745\n",
            "Iteration 3, loss = 0.15365287\n",
            "Iteration 4, loss = 0.11695399\n",
            "Iteration 5, loss = 0.08902038\n",
            "Iteration 6, loss = 0.06775850\n",
            "Iteration 7, loss = 0.05157487\n",
            "Iteration 8, loss = 0.03925659\n",
            "Iteration 9, loss = 0.02988044\n",
            "Iteration 10, loss = 0.02274371\n",
            "Iteration 11, loss = 0.01731154\n",
            "Iteration 12, loss = 0.01317680\n",
            "Iteration 13, loss = 0.01002962\n",
            "Iteration 14, loss = 0.00763412\n",
            "Iteration 15, loss = 0.00581076\n",
            "Iteration 16, loss = 0.00442296\n",
            "Iteration 17, loss = 0.00336937\n",
            "Iteration 18, loss = 0.00260668\n",
            "Iteration 19, loss = 0.00212891\n",
            "Iteration 20, loss = 0.00180973\n",
            "Iteration 21, loss = 0.00157543\n",
            "Iteration 22, loss = 0.00140170\n",
            "Iteration 23, loss = 0.00127229\n",
            "Iteration 24, loss = 0.00117542\n",
            "Iteration 25, loss = 0.00110272\n",
            "Iteration 26, loss = 0.00104801\n",
            "Iteration 27, loss = 0.00100673\n",
            "Iteration 28, loss = 0.00097551\n",
            "Iteration 29, loss = 0.00095186\n",
            "Iteration 30, loss = 0.00093392\n",
            "Iteration 31, loss = 0.00092028\n",
            "Iteration 32, loss = 0.00090991\n",
            "Iteration 33, loss = 0.00090201\n",
            "Iteration 34, loss = 0.00089599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00089299\n",
            "Iteration 36, loss = 0.00089212\n",
            "Iteration 37, loss = 0.00089130\n",
            "Iteration 38, loss = 0.00089053\n",
            "Iteration 39, loss = 0.00088980\n",
            "Iteration 40, loss = 0.00088910\n",
            "Iteration 41, loss = 0.00088844\n",
            "Iteration 42, loss = 0.00088781\n",
            "Iteration 43, loss = 0.00088722\n",
            "Iteration 44, loss = 0.00088666\n",
            "Iteration 45, loss = 0.00088613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00088582\n",
            "Iteration 47, loss = 0.00088572\n",
            "Iteration 48, loss = 0.00088562\n",
            "Iteration 49, loss = 0.00088552\n",
            "Iteration 50, loss = 0.00088543\n",
            "Iteration 51, loss = 0.00088533\n",
            "Iteration 52, loss = 0.00088524\n",
            "Iteration 53, loss = 0.00088514\n",
            "Iteration 54, loss = 0.00088505\n",
            "Iteration 55, loss = 0.00088496\n",
            "Iteration 56, loss = 0.00088487\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00088482\n",
            "Iteration 58, loss = 0.00088480\n",
            "Iteration 59, loss = 0.00088478\n",
            "Iteration 60, loss = 0.00088476\n",
            "Iteration 61, loss = 0.00088475\n",
            "Iteration 62, loss = 0.00088473\n",
            "Iteration 63, loss = 0.00088471\n",
            "Iteration 64, loss = 0.00088469\n",
            "Iteration 65, loss = 0.00088467\n",
            "Iteration 66, loss = 0.00088466\n",
            "Iteration 67, loss = 0.00088464\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00088463\n",
            "Iteration 69, loss = 0.00088463\n",
            "Iteration 70, loss = 0.00088462\n",
            "Iteration 71, loss = 0.00088462\n",
            "Iteration 72, loss = 0.00088462\n",
            "Iteration 73, loss = 0.00088461\n",
            "Iteration 74, loss = 0.00088461\n",
            "Iteration 75, loss = 0.00088460\n",
            "Iteration 76, loss = 0.00088460\n",
            "Iteration 77, loss = 0.00088460\n",
            "Iteration 78, loss = 0.00088459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00088459\n",
            "Iteration 80, loss = 0.00088459\n",
            "Iteration 81, loss = 0.00088459\n",
            "Iteration 82, loss = 0.00088459\n",
            "Iteration 83, loss = 0.00088459\n",
            "Iteration 84, loss = 0.00088459\n",
            "Iteration 85, loss = 0.00088459\n",
            "Iteration 86, loss = 0.00088459\n",
            "Iteration 87, loss = 0.00088459\n",
            "Iteration 88, loss = 0.00088459\n",
            "Iteration 89, loss = 0.00088459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.29363653\n",
            "Iteration 2, loss = 0.20038346\n",
            "Iteration 3, loss = 0.15252332\n",
            "Iteration 4, loss = 0.11609422\n",
            "Iteration 5, loss = 0.08836596\n",
            "Iteration 6, loss = 0.06726039\n",
            "Iteration 7, loss = 0.05119573\n",
            "Iteration 8, loss = 0.03896800\n",
            "Iteration 9, loss = 0.02966077\n",
            "Iteration 10, loss = 0.02257651\n",
            "Iteration 11, loss = 0.01718428\n",
            "Iteration 12, loss = 0.01307994\n",
            "Iteration 13, loss = 0.00995589\n",
            "Iteration 14, loss = 0.00757800\n",
            "Iteration 15, loss = 0.00576805\n",
            "Iteration 16, loss = 0.00439040\n",
            "Iteration 17, loss = 0.00334235\n",
            "Iteration 18, loss = 0.00255918\n",
            "Iteration 19, loss = 0.00204850\n",
            "Iteration 20, loss = 0.00174152\n",
            "Iteration 21, loss = 0.00152342\n",
            "Iteration 22, loss = 0.00136203\n",
            "Iteration 23, loss = 0.00124199\n",
            "Iteration 24, loss = 0.00115238\n",
            "Iteration 25, loss = 0.00108526\n",
            "Iteration 26, loss = 0.00103481\n",
            "Iteration 27, loss = 0.00099678\n",
            "Iteration 28, loss = 0.00096804\n",
            "Iteration 29, loss = 0.00094629\n",
            "Iteration 30, loss = 0.00092979\n",
            "Iteration 31, loss = 0.00091725\n",
            "Iteration 32, loss = 0.00090770\n",
            "Iteration 33, loss = 0.00090042\n",
            "Iteration 34, loss = 0.00089490\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00089214\n",
            "Iteration 36, loss = 0.00089135\n",
            "Iteration 37, loss = 0.00089060\n",
            "Iteration 38, loss = 0.00088990\n",
            "Iteration 39, loss = 0.00088923\n",
            "Iteration 40, loss = 0.00088859\n",
            "Iteration 41, loss = 0.00088799\n",
            "Iteration 42, loss = 0.00088742\n",
            "Iteration 43, loss = 0.00088688\n",
            "Iteration 44, loss = 0.00088637\n",
            "Iteration 45, loss = 0.00088588\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00088560\n",
            "Iteration 47, loss = 0.00088551\n",
            "Iteration 48, loss = 0.00088542\n",
            "Iteration 49, loss = 0.00088533\n",
            "Iteration 50, loss = 0.00088524\n",
            "Iteration 51, loss = 0.00088515\n",
            "Iteration 52, loss = 0.00088507\n",
            "Iteration 53, loss = 0.00088498\n",
            "Iteration 54, loss = 0.00088490\n",
            "Iteration 55, loss = 0.00088481\n",
            "Iteration 56, loss = 0.00088473\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00088468\n",
            "Iteration 58, loss = 0.00088466\n",
            "Iteration 59, loss = 0.00088465\n",
            "Iteration 60, loss = 0.00088463\n",
            "Iteration 61, loss = 0.00088461\n",
            "Iteration 62, loss = 0.00088460\n",
            "Iteration 63, loss = 0.00088458\n",
            "Iteration 64, loss = 0.00088457\n",
            "Iteration 65, loss = 0.00088455\n",
            "Iteration 66, loss = 0.00088453\n",
            "Iteration 67, loss = 0.00088452\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00088451\n",
            "Iteration 69, loss = 0.00088450\n",
            "Iteration 70, loss = 0.00088450\n",
            "Iteration 71, loss = 0.00088450\n",
            "Iteration 72, loss = 0.00088449\n",
            "Iteration 73, loss = 0.00088449\n",
            "Iteration 74, loss = 0.00088449\n",
            "Iteration 75, loss = 0.00088449\n",
            "Iteration 76, loss = 0.00088448\n",
            "Iteration 77, loss = 0.00088448\n",
            "Iteration 78, loss = 0.00088448\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00088447\n",
            "Iteration 80, loss = 0.00088447\n",
            "Iteration 81, loss = 0.00088447\n",
            "Iteration 82, loss = 0.00088447\n",
            "Iteration 83, loss = 0.00088447\n",
            "Iteration 84, loss = 0.00088447\n",
            "Iteration 85, loss = 0.00088447\n",
            "Iteration 86, loss = 0.00088447\n",
            "Iteration 87, loss = 0.00088447\n",
            "Iteration 88, loss = 0.00088447\n",
            "Iteration 89, loss = 0.00088447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12147583\n",
            "Iteration 2, loss = 0.09248275\n",
            "Iteration 3, loss = 0.07041217\n",
            "Iteration 4, loss = 0.05361902\n",
            "Iteration 5, loss = 0.04084440\n",
            "Iteration 6, loss = 0.03113032\n",
            "Iteration 7, loss = 0.02374775\n",
            "Iteration 8, loss = 0.01814196\n",
            "Iteration 9, loss = 0.01389088\n",
            "Iteration 10, loss = 0.01067312\n",
            "Iteration 11, loss = 0.00824346\n",
            "Iteration 12, loss = 0.00641423\n",
            "Iteration 13, loss = 0.00504119\n",
            "Iteration 14, loss = 0.00401326\n",
            "Iteration 15, loss = 0.00324429\n",
            "Iteration 16, loss = 0.00266861\n",
            "Iteration 17, loss = 0.00223615\n",
            "Iteration 18, loss = 0.00190997\n",
            "Iteration 19, loss = 0.00166327\n",
            "Iteration 20, loss = 0.00147634\n",
            "Iteration 21, loss = 0.00133457\n",
            "Iteration 22, loss = 0.00122695\n",
            "Iteration 23, loss = 0.00114517\n",
            "Iteration 24, loss = 0.00108300\n",
            "Iteration 25, loss = 0.00103571\n",
            "Iteration 26, loss = 0.00099973\n",
            "Iteration 27, loss = 0.00097235\n",
            "Iteration 28, loss = 0.00095150\n",
            "Iteration 29, loss = 0.00093563\n",
            "Iteration 30, loss = 0.00092354\n",
            "Iteration 31, loss = 0.00091433\n",
            "Iteration 32, loss = 0.00090730\n",
            "Iteration 33, loss = 0.00090194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089926\n",
            "Iteration 35, loss = 0.00089850\n",
            "Iteration 36, loss = 0.00089777\n",
            "Iteration 37, loss = 0.00089708\n",
            "Iteration 38, loss = 0.00089643\n",
            "Iteration 39, loss = 0.00089581\n",
            "Iteration 40, loss = 0.00089522\n",
            "Iteration 41, loss = 0.00089466\n",
            "Iteration 42, loss = 0.00089414\n",
            "Iteration 43, loss = 0.00089363\n",
            "Iteration 44, loss = 0.00089316\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00089288\n",
            "Iteration 46, loss = 0.00089279\n",
            "Iteration 47, loss = 0.00089270\n",
            "Iteration 48, loss = 0.00089261\n",
            "Iteration 49, loss = 0.00089253\n",
            "Iteration 50, loss = 0.00089244\n",
            "Iteration 51, loss = 0.00089236\n",
            "Iteration 52, loss = 0.00089227\n",
            "Iteration 53, loss = 0.00089219\n",
            "Iteration 54, loss = 0.00089211\n",
            "Iteration 55, loss = 0.00089203\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089198\n",
            "Iteration 57, loss = 0.00089196\n",
            "Iteration 58, loss = 0.00089195\n",
            "Iteration 59, loss = 0.00089193\n",
            "Iteration 60, loss = 0.00089192\n",
            "Iteration 61, loss = 0.00089190\n",
            "Iteration 62, loss = 0.00089188\n",
            "Iteration 63, loss = 0.00089187\n",
            "Iteration 64, loss = 0.00089185\n",
            "Iteration 65, loss = 0.00089184\n",
            "Iteration 66, loss = 0.00089182\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089181\n",
            "Iteration 68, loss = 0.00089181\n",
            "Iteration 69, loss = 0.00089181\n",
            "Iteration 70, loss = 0.00089180\n",
            "Iteration 71, loss = 0.00089180\n",
            "Iteration 72, loss = 0.00089180\n",
            "Iteration 73, loss = 0.00089179\n",
            "Iteration 74, loss = 0.00089179\n",
            "Iteration 75, loss = 0.00089179\n",
            "Iteration 76, loss = 0.00089178\n",
            "Iteration 77, loss = 0.00089178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089178\n",
            "Iteration 79, loss = 0.00089178\n",
            "Iteration 80, loss = 0.00089178\n",
            "Iteration 81, loss = 0.00089178\n",
            "Iteration 82, loss = 0.00089178\n",
            "Iteration 83, loss = 0.00089178\n",
            "Iteration 84, loss = 0.00089177\n",
            "Iteration 85, loss = 0.00089177\n",
            "Iteration 86, loss = 0.00089177\n",
            "Iteration 87, loss = 0.00089177\n",
            "Iteration 88, loss = 0.00089177\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.20392754\n",
            "Iteration 3, loss = 0.15522092\n",
            "Iteration 4, loss = 0.11814752\n",
            "Iteration 5, loss = 0.08992884\n",
            "Iteration 6, loss = 0.06844999\n",
            "Iteration 7, loss = 0.05210120\n",
            "Iteration 8, loss = 0.03965721\n",
            "Iteration 9, loss = 0.03018537\n",
            "Iteration 10, loss = 0.02297581\n",
            "Iteration 11, loss = 0.01748821\n",
            "Iteration 12, loss = 0.01331127\n",
            "Iteration 13, loss = 0.01013197\n",
            "Iteration 14, loss = 0.00771202\n",
            "Iteration 15, loss = 0.00587006\n",
            "Iteration 16, loss = 0.00446807\n",
            "Iteration 17, loss = 0.00340269\n",
            "Iteration 18, loss = 0.00262319\n",
            "Iteration 19, loss = 0.00213649\n",
            "Iteration 20, loss = 0.00182002\n",
            "Iteration 21, loss = 0.00158654\n",
            "Iteration 22, loss = 0.00141115\n",
            "Iteration 23, loss = 0.00128085\n",
            "Iteration 24, loss = 0.00118389\n",
            "Iteration 25, loss = 0.00111141\n",
            "Iteration 26, loss = 0.00105700\n",
            "Iteration 27, loss = 0.00101602\n",
            "Iteration 28, loss = 0.00098510\n",
            "Iteration 29, loss = 0.00096172\n",
            "Iteration 30, loss = 0.00094400\n",
            "Iteration 31, loss = 0.00093054\n",
            "Iteration 32, loss = 0.00092030\n",
            "Iteration 33, loss = 0.00091241\n",
            "Iteration 34, loss = 0.00090644\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00090348\n",
            "Iteration 36, loss = 0.00090263\n",
            "Iteration 37, loss = 0.00090184\n",
            "Iteration 38, loss = 0.00090108\n",
            "Iteration 39, loss = 0.00090037\n",
            "Iteration 40, loss = 0.00089969\n",
            "Iteration 41, loss = 0.00089906\n",
            "Iteration 42, loss = 0.00089845\n",
            "Iteration 43, loss = 0.00089788\n",
            "Iteration 44, loss = 0.00089734\n",
            "Iteration 45, loss = 0.00089682\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00089652\n",
            "Iteration 47, loss = 0.00089642\n",
            "Iteration 48, loss = 0.00089633\n",
            "Iteration 49, loss = 0.00089624\n",
            "Iteration 50, loss = 0.00089614\n",
            "Iteration 51, loss = 0.00089605\n",
            "Iteration 52, loss = 0.00089596\n",
            "Iteration 53, loss = 0.00089587\n",
            "Iteration 54, loss = 0.00089578\n",
            "Iteration 55, loss = 0.00089569\n",
            "Iteration 56, loss = 0.00089561\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00089555\n",
            "Iteration 58, loss = 0.00089554\n",
            "Iteration 59, loss = 0.00089552\n",
            "Iteration 60, loss = 0.00089550\n",
            "Iteration 61, loss = 0.00089548\n",
            "Iteration 62, loss = 0.00089547\n",
            "Iteration 63, loss = 0.00089545\n",
            "Iteration 64, loss = 0.00089543\n",
            "Iteration 65, loss = 0.00089542\n",
            "Iteration 66, loss = 0.00089540\n",
            "Iteration 67, loss = 0.00089538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00089537\n",
            "Iteration 69, loss = 0.00089537\n",
            "Iteration 70, loss = 0.00089537\n",
            "Iteration 71, loss = 0.00089536\n",
            "Iteration 72, loss = 0.00089536\n",
            "Iteration 73, loss = 0.00089536\n",
            "Iteration 74, loss = 0.00089535\n",
            "Iteration 75, loss = 0.00089535\n",
            "Iteration 76, loss = 0.00089535\n",
            "Iteration 77, loss = 0.00089534\n",
            "Iteration 78, loss = 0.00089534\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00089534\n",
            "Iteration 80, loss = 0.00089534\n",
            "Iteration 81, loss = 0.00089534\n",
            "Iteration 82, loss = 0.00089533\n",
            "Iteration 83, loss = 0.00089533\n",
            "Iteration 84, loss = 0.00089533\n",
            "Iteration 85, loss = 0.00089533\n",
            "Iteration 86, loss = 0.00089533\n",
            "Iteration 87, loss = 0.00089533\n",
            "Iteration 88, loss = 0.00089533\n",
            "Iteration 89, loss = 0.00089533\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12509884\n",
            "Iteration 2, loss = 0.09523148\n",
            "Iteration 3, loss = 0.07249826\n",
            "Iteration 4, loss = 0.05519910\n",
            "Iteration 5, loss = 0.04203804\n",
            "Iteration 6, loss = 0.03202902\n",
            "Iteration 7, loss = 0.02442168\n",
            "Iteration 8, loss = 0.01864480\n",
            "Iteration 9, loss = 0.01426361\n",
            "Iteration 10, loss = 0.01094734\n",
            "Iteration 11, loss = 0.00844408\n",
            "Iteration 12, loss = 0.00656076\n",
            "Iteration 13, loss = 0.00514773\n",
            "Iteration 14, loss = 0.00408877\n",
            "Iteration 15, loss = 0.00329648\n",
            "Iteration 16, loss = 0.00270317\n",
            "Iteration 17, loss = 0.00225791\n",
            "Iteration 18, loss = 0.00192318\n",
            "Iteration 19, loss = 0.00167110\n",
            "Iteration 20, loss = 0.00148081\n",
            "Iteration 21, loss = 0.00133680\n",
            "Iteration 22, loss = 0.00122774\n",
            "Iteration 23, loss = 0.00114510\n",
            "Iteration 24, loss = 0.00108241\n",
            "Iteration 25, loss = 0.00103480\n",
            "Iteration 26, loss = 0.00099863\n",
            "Iteration 27, loss = 0.00097113\n",
            "Iteration 28, loss = 0.00095020\n",
            "Iteration 29, loss = 0.00093427\n",
            "Iteration 30, loss = 0.00092215\n",
            "Iteration 31, loss = 0.00091291\n",
            "Iteration 32, loss = 0.00090587\n",
            "Iteration 33, loss = 0.00090049\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089781\n",
            "Iteration 35, loss = 0.00089704\n",
            "Iteration 36, loss = 0.00089631\n",
            "Iteration 37, loss = 0.00089562\n",
            "Iteration 38, loss = 0.00089497\n",
            "Iteration 39, loss = 0.00089435\n",
            "Iteration 40, loss = 0.00089376\n",
            "Iteration 41, loss = 0.00089320\n",
            "Iteration 42, loss = 0.00089267\n",
            "Iteration 43, loss = 0.00089217\n",
            "Iteration 44, loss = 0.00089170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00089142\n",
            "Iteration 46, loss = 0.00089133\n",
            "Iteration 47, loss = 0.00089124\n",
            "Iteration 48, loss = 0.00089115\n",
            "Iteration 49, loss = 0.00089107\n",
            "Iteration 50, loss = 0.00089098\n",
            "Iteration 51, loss = 0.00089090\n",
            "Iteration 52, loss = 0.00089081\n",
            "Iteration 53, loss = 0.00089073\n",
            "Iteration 54, loss = 0.00089065\n",
            "Iteration 55, loss = 0.00089057\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00089052\n",
            "Iteration 57, loss = 0.00089050\n",
            "Iteration 58, loss = 0.00089049\n",
            "Iteration 59, loss = 0.00089047\n",
            "Iteration 60, loss = 0.00089046\n",
            "Iteration 61, loss = 0.00089044\n",
            "Iteration 62, loss = 0.00089042\n",
            "Iteration 63, loss = 0.00089041\n",
            "Iteration 64, loss = 0.00089039\n",
            "Iteration 65, loss = 0.00089038\n",
            "Iteration 66, loss = 0.00089036\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00089035\n",
            "Iteration 68, loss = 0.00089035\n",
            "Iteration 69, loss = 0.00089035\n",
            "Iteration 70, loss = 0.00089034\n",
            "Iteration 71, loss = 0.00089034\n",
            "Iteration 72, loss = 0.00089034\n",
            "Iteration 73, loss = 0.00089033\n",
            "Iteration 74, loss = 0.00089033\n",
            "Iteration 75, loss = 0.00089033\n",
            "Iteration 76, loss = 0.00089032\n",
            "Iteration 77, loss = 0.00089032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00089032\n",
            "Iteration 79, loss = 0.00089032\n",
            "Iteration 80, loss = 0.00089032\n",
            "Iteration 81, loss = 0.00089032\n",
            "Iteration 82, loss = 0.00089032\n",
            "Iteration 83, loss = 0.00089032\n",
            "Iteration 84, loss = 0.00089032\n",
            "Iteration 85, loss = 0.00089031\n",
            "Iteration 86, loss = 0.00089031\n",
            "Iteration 87, loss = 0.00089031\n",
            "Iteration 88, loss = 0.00089031\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13774756\n",
            "Iteration 2, loss = 0.10485709\n",
            "Iteration 3, loss = 0.07981273\n",
            "Iteration 4, loss = 0.06075004\n",
            "Iteration 5, loss = 0.04624033\n",
            "Iteration 6, loss = 0.03519616\n",
            "Iteration 7, loss = 0.02678981\n",
            "Iteration 8, loss = 0.02039129\n",
            "Iteration 9, loss = 0.01552267\n",
            "Iteration 10, loss = 0.01184303\n",
            "Iteration 11, loss = 0.00911604\n",
            "Iteration 12, loss = 0.00708080\n",
            "Iteration 13, loss = 0.00555039\n",
            "Iteration 14, loss = 0.00440090\n",
            "Iteration 15, loss = 0.00353777\n",
            "Iteration 16, loss = 0.00288908\n",
            "Iteration 17, loss = 0.00240059\n",
            "Iteration 18, loss = 0.00203190\n",
            "Iteration 19, loss = 0.00175310\n",
            "Iteration 20, loss = 0.00154205\n",
            "Iteration 21, loss = 0.00138210\n",
            "Iteration 22, loss = 0.00126075\n",
            "Iteration 23, loss = 0.00116860\n",
            "Iteration 24, loss = 0.00109858\n",
            "Iteration 25, loss = 0.00104535\n",
            "Iteration 26, loss = 0.00100487\n",
            "Iteration 27, loss = 0.00097406\n",
            "Iteration 28, loss = 0.00095062\n",
            "Iteration 29, loss = 0.00093277\n",
            "Iteration 30, loss = 0.00091918\n",
            "Iteration 31, loss = 0.00090882\n",
            "Iteration 32, loss = 0.00090093\n",
            "Iteration 33, loss = 0.00089491\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089190\n",
            "Iteration 35, loss = 0.00089104\n",
            "Iteration 36, loss = 0.00089022\n",
            "Iteration 37, loss = 0.00088945\n",
            "Iteration 38, loss = 0.00088871\n",
            "Iteration 39, loss = 0.00088802\n",
            "Iteration 40, loss = 0.00088736\n",
            "Iteration 41, loss = 0.00088674\n",
            "Iteration 42, loss = 0.00088615\n",
            "Iteration 43, loss = 0.00088559\n",
            "Iteration 44, loss = 0.00088505\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00088474\n",
            "Iteration 46, loss = 0.00088464\n",
            "Iteration 47, loss = 0.00088454\n",
            "Iteration 48, loss = 0.00088445\n",
            "Iteration 49, loss = 0.00088435\n",
            "Iteration 50, loss = 0.00088425\n",
            "Iteration 51, loss = 0.00088416\n",
            "Iteration 52, loss = 0.00088407\n",
            "Iteration 53, loss = 0.00088397\n",
            "Iteration 54, loss = 0.00088388\n",
            "Iteration 55, loss = 0.00088379\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00088374\n",
            "Iteration 57, loss = 0.00088372\n",
            "Iteration 58, loss = 0.00088370\n",
            "Iteration 59, loss = 0.00088368\n",
            "Iteration 60, loss = 0.00088367\n",
            "Iteration 61, loss = 0.00088365\n",
            "Iteration 62, loss = 0.00088363\n",
            "Iteration 63, loss = 0.00088361\n",
            "Iteration 64, loss = 0.00088360\n",
            "Iteration 65, loss = 0.00088358\n",
            "Iteration 66, loss = 0.00088356\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00088355\n",
            "Iteration 68, loss = 0.00088355\n",
            "Iteration 69, loss = 0.00088354\n",
            "Iteration 70, loss = 0.00088354\n",
            "Iteration 71, loss = 0.00088354\n",
            "Iteration 72, loss = 0.00088353\n",
            "Iteration 73, loss = 0.00088353\n",
            "Iteration 74, loss = 0.00088352\n",
            "Iteration 75, loss = 0.00088352\n",
            "Iteration 76, loss = 0.00088352\n",
            "Iteration 77, loss = 0.00088351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00088351\n",
            "Iteration 79, loss = 0.00088351\n",
            "Iteration 80, loss = 0.00088351\n",
            "Iteration 81, loss = 0.00088351\n",
            "Iteration 82, loss = 0.00088351\n",
            "Iteration 83, loss = 0.00088351\n",
            "Iteration 84, loss = 0.00088351\n",
            "Iteration 85, loss = 0.00088351\n",
            "Iteration 86, loss = 0.00088351\n",
            "Iteration 87, loss = 0.00088351\n",
            "Iteration 88, loss = 0.00088351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13461896\n",
            "Iteration 2, loss = 0.10248144\n",
            "Iteration 3, loss = 0.07801275\n",
            "Iteration 4, loss = 0.05939147\n",
            "Iteration 5, loss = 0.04522262\n",
            "Iteration 6, loss = 0.03444549\n",
            "Iteration 7, loss = 0.02625347\n",
            "Iteration 8, loss = 0.02003217\n",
            "Iteration 9, loss = 0.01531323\n",
            "Iteration 10, loss = 0.01173992\n",
            "Iteration 11, loss = 0.00904068\n",
            "Iteration 12, loss = 0.00700791\n",
            "Iteration 13, loss = 0.00548179\n",
            "Iteration 14, loss = 0.00433900\n",
            "Iteration 15, loss = 0.00348476\n",
            "Iteration 16, loss = 0.00284654\n",
            "Iteration 17, loss = 0.00236873\n",
            "Iteration 18, loss = 0.00200976\n",
            "Iteration 19, loss = 0.00173919\n",
            "Iteration 20, loss = 0.00153475\n",
            "Iteration 21, loss = 0.00138022\n",
            "Iteration 22, loss = 0.00126326\n",
            "Iteration 23, loss = 0.00117462\n",
            "Iteration 24, loss = 0.00110738\n",
            "Iteration 25, loss = 0.00105632\n",
            "Iteration 26, loss = 0.00101752\n",
            "Iteration 27, loss = 0.00098803\n",
            "Iteration 28, loss = 0.00096559\n",
            "Iteration 29, loss = 0.00094852\n",
            "Iteration 30, loss = 0.00093552\n",
            "Iteration 31, loss = 0.00092561\n",
            "Iteration 32, loss = 0.00091807\n",
            "Iteration 33, loss = 0.00091231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00090943\n",
            "Iteration 35, loss = 0.00090861\n",
            "Iteration 36, loss = 0.00090782\n",
            "Iteration 37, loss = 0.00090708\n",
            "Iteration 38, loss = 0.00090638\n",
            "Iteration 39, loss = 0.00090572\n",
            "Iteration 40, loss = 0.00090509\n",
            "Iteration 41, loss = 0.00090449\n",
            "Iteration 42, loss = 0.00090392\n",
            "Iteration 43, loss = 0.00090339\n",
            "Iteration 44, loss = 0.00090288\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00090258\n",
            "Iteration 46, loss = 0.00090248\n",
            "Iteration 47, loss = 0.00090239\n",
            "Iteration 48, loss = 0.00090230\n",
            "Iteration 49, loss = 0.00090220\n",
            "Iteration 50, loss = 0.00090211\n",
            "Iteration 51, loss = 0.00090202\n",
            "Iteration 52, loss = 0.00090193\n",
            "Iteration 53, loss = 0.00090184\n",
            "Iteration 54, loss = 0.00090176\n",
            "Iteration 55, loss = 0.00090167\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00090162\n",
            "Iteration 57, loss = 0.00090160\n",
            "Iteration 58, loss = 0.00090158\n",
            "Iteration 59, loss = 0.00090156\n",
            "Iteration 60, loss = 0.00090155\n",
            "Iteration 61, loss = 0.00090153\n",
            "Iteration 62, loss = 0.00090151\n",
            "Iteration 63, loss = 0.00090150\n",
            "Iteration 64, loss = 0.00090148\n",
            "Iteration 65, loss = 0.00090146\n",
            "Iteration 66, loss = 0.00090145\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00090144\n",
            "Iteration 68, loss = 0.00090143\n",
            "Iteration 69, loss = 0.00090143\n",
            "Iteration 70, loss = 0.00090143\n",
            "Iteration 71, loss = 0.00090142\n",
            "Iteration 72, loss = 0.00090142\n",
            "Iteration 73, loss = 0.00090142\n",
            "Iteration 74, loss = 0.00090141\n",
            "Iteration 75, loss = 0.00090141\n",
            "Iteration 76, loss = 0.00090141\n",
            "Iteration 77, loss = 0.00090140\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00090140\n",
            "Iteration 79, loss = 0.00090140\n",
            "Iteration 80, loss = 0.00090140\n",
            "Iteration 81, loss = 0.00090140\n",
            "Iteration 82, loss = 0.00090140\n",
            "Iteration 83, loss = 0.00090140\n",
            "Iteration 84, loss = 0.00090140\n",
            "Iteration 85, loss = 0.00090140\n",
            "Iteration 86, loss = 0.00090140\n",
            "Iteration 87, loss = 0.00090139\n",
            "Iteration 88, loss = 0.00090139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 12.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.29980096\n",
            "Iteration 2, loss = 0.21915411\n",
            "Iteration 3, loss = 0.20225659\n",
            "Iteration 4, loss = 0.18666193\n",
            "Iteration 5, loss = 0.17226966\n",
            "Iteration 6, loss = 0.15898709\n",
            "Iteration 7, loss = 0.14672866\n",
            "Iteration 8, loss = 0.13541538\n",
            "Iteration 9, loss = 0.12497440\n",
            "Iteration 10, loss = 0.11533846\n",
            "Iteration 11, loss = 0.10644547\n",
            "Iteration 12, loss = 0.09823817\n",
            "Iteration 13, loss = 0.09066368\n",
            "Iteration 14, loss = 0.08367320\n",
            "Iteration 15, loss = 0.07722172\n",
            "Iteration 16, loss = 0.07126766\n",
            "Iteration 17, loss = 0.06577269\n",
            "Iteration 18, loss = 0.06070139\n",
            "Iteration 19, loss = 0.05602111\n",
            "Iteration 20, loss = 0.05170169\n",
            "Iteration 21, loss = 0.04771532\n",
            "Iteration 22, loss = 0.04403631\n",
            "Iteration 23, loss = 0.04064096\n",
            "Iteration 24, loss = 0.03750741\n",
            "Iteration 25, loss = 0.03461546\n",
            "Iteration 26, loss = 0.03194649\n",
            "Iteration 27, loss = 0.02948331\n",
            "Iteration 28, loss = 0.02721005\n",
            "Iteration 29, loss = 0.02511206\n",
            "Iteration 30, loss = 0.02317584\n",
            "Iteration 31, loss = 0.02138890\n",
            "Iteration 32, loss = 0.01973975\n",
            "Iteration 33, loss = 0.01821775\n",
            "Iteration 34, loss = 0.01681310\n",
            "Iteration 35, loss = 0.01551675\n",
            "Iteration 36, loss = 0.01432036\n",
            "Iteration 37, loss = 0.01321621\n",
            "Iteration 38, loss = 0.01219719\n",
            "Iteration 39, loss = 0.01125675\n",
            "Iteration 40, loss = 0.01038882\n",
            "Iteration 41, loss = 0.00958780\n",
            "Iteration 42, loss = 0.00884855\n",
            "Iteration 43, loss = 0.00816630\n",
            "Iteration 44, loss = 0.00753665\n",
            "Iteration 45, loss = 0.00695555\n",
            "Iteration 46, loss = 0.00641925\n",
            "Iteration 47, loss = 0.00592431\n",
            "Iteration 48, loss = 0.00546752\n",
            "Iteration 49, loss = 0.00504596\n",
            "Iteration 50, loss = 0.00465690\n",
            "Iteration 51, loss = 0.00429784\n",
            "Iteration 52, loss = 0.00396648\n",
            "Iteration 53, loss = 0.00366072\n",
            "Iteration 54, loss = 0.00337870\n",
            "Iteration 55, loss = 0.00311891\n",
            "Iteration 56, loss = 0.00288047\n",
            "Iteration 57, loss = 0.00266355\n",
            "Iteration 58, loss = 0.00246988\n",
            "Iteration 59, loss = 0.00230234\n",
            "Iteration 60, loss = 0.00216230\n",
            "Iteration 61, loss = 0.00204647\n",
            "Iteration 62, loss = 0.00194808\n",
            "Iteration 63, loss = 0.00186119\n",
            "Iteration 64, loss = 0.00178252\n",
            "Iteration 65, loss = 0.00171057\n",
            "Iteration 66, loss = 0.00164455\n",
            "Iteration 67, loss = 0.00158390\n",
            "Iteration 68, loss = 0.00152815\n",
            "Iteration 69, loss = 0.00147689\n",
            "Iteration 70, loss = 0.00142974\n",
            "Iteration 71, loss = 0.00138635\n",
            "Iteration 72, loss = 0.00134643\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 73, loss = 0.00132241\n",
            "Iteration 74, loss = 0.00131512\n",
            "Iteration 75, loss = 0.00130805\n",
            "Iteration 76, loss = 0.00130109\n",
            "Iteration 77, loss = 0.00129425\n",
            "Iteration 78, loss = 0.00128751\n",
            "Iteration 79, loss = 0.00128089\n",
            "Iteration 80, loss = 0.00127438\n",
            "Iteration 81, loss = 0.00126797\n",
            "Iteration 82, loss = 0.00126166\n",
            "Iteration 83, loss = 0.00125546\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 84, loss = 0.00125154\n",
            "Iteration 85, loss = 0.00125031\n",
            "Iteration 86, loss = 0.00124910\n",
            "Iteration 87, loss = 0.00124789\n",
            "Iteration 88, loss = 0.00124669\n",
            "Iteration 89, loss = 0.00124549\n",
            "Iteration 90, loss = 0.00124430\n",
            "Iteration 91, loss = 0.00124311\n",
            "Iteration 92, loss = 0.00124192\n",
            "Iteration 93, loss = 0.00124074\n",
            "Iteration 94, loss = 0.00123956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.00123880\n",
            "Iteration 96, loss = 0.00123857\n",
            "Iteration 97, loss = 0.00123833\n",
            "Iteration 98, loss = 0.00123810\n",
            "Iteration 99, loss = 0.00123786\n",
            "Iteration 100, loss = 0.00123763\n",
            "Iteration 101, loss = 0.00123739\n",
            "Iteration 102, loss = 0.00123716\n",
            "Iteration 103, loss = 0.00123693\n",
            "Iteration 104, loss = 0.00123669\n",
            "Iteration 105, loss = 0.00123646\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 106, loss = 0.00123631\n",
            "Iteration 107, loss = 0.00123626\n",
            "Iteration 108, loss = 0.00123622\n",
            "Iteration 109, loss = 0.00123617\n",
            "Iteration 110, loss = 0.00123612\n",
            "Iteration 111, loss = 0.00123608\n",
            "Iteration 112, loss = 0.00123603\n",
            "Iteration 113, loss = 0.00123598\n",
            "Iteration 114, loss = 0.00123594\n",
            "Iteration 115, loss = 0.00123589\n",
            "Iteration 116, loss = 0.00123584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 117, loss = 0.00123581\n",
            "Iteration 118, loss = 0.00123580\n",
            "Iteration 119, loss = 0.00123579\n",
            "Iteration 120, loss = 0.00123579\n",
            "Iteration 121, loss = 0.00123578\n",
            "Iteration 122, loss = 0.00123577\n",
            "Iteration 123, loss = 0.00123576\n",
            "Iteration 124, loss = 0.00123575\n",
            "Iteration 125, loss = 0.00123574\n",
            "Iteration 126, loss = 0.00123573\n",
            "Iteration 127, loss = 0.00123572\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.46468946\n",
            "Iteration 2, loss = 0.30564708\n",
            "Iteration 3, loss = 0.28208066\n",
            "Iteration 4, loss = 0.26033130\n",
            "Iteration 5, loss = 0.24025888\n",
            "Iteration 6, loss = 0.22173411\n",
            "Iteration 7, loss = 0.20463767\n",
            "Iteration 8, loss = 0.18885941\n",
            "Iteration 9, loss = 0.17429772\n",
            "Iteration 10, loss = 0.16085878\n",
            "Iteration 11, loss = 0.14845603\n",
            "Iteration 12, loss = 0.13700957\n",
            "Iteration 13, loss = 0.12644567\n",
            "Iteration 14, loss = 0.11669628\n",
            "Iteration 15, loss = 0.10769861\n",
            "Iteration 16, loss = 0.09939468\n",
            "Iteration 17, loss = 0.09173102\n",
            "Iteration 18, loss = 0.08465825\n",
            "Iteration 19, loss = 0.07813081\n",
            "Iteration 20, loss = 0.07210667\n",
            "Iteration 21, loss = 0.06654700\n",
            "Iteration 22, loss = 0.06141600\n",
            "Iteration 23, loss = 0.05668062\n",
            "Iteration 24, loss = 0.05231036\n",
            "Iteration 25, loss = 0.04827705\n",
            "Iteration 26, loss = 0.04455473\n",
            "Iteration 27, loss = 0.04111941\n",
            "Iteration 28, loss = 0.03794896\n",
            "Iteration 29, loss = 0.03502297\n",
            "Iteration 30, loss = 0.03232258\n",
            "Iteration 31, loss = 0.02983040\n",
            "Iteration 32, loss = 0.02753038\n",
            "Iteration 33, loss = 0.02540769\n",
            "Iteration 34, loss = 0.02344868\n",
            "Iteration 35, loss = 0.02164070\n",
            "Iteration 36, loss = 0.01997213\n",
            "Iteration 37, loss = 0.01843221\n",
            "Iteration 38, loss = 0.01701103\n",
            "Iteration 39, loss = 0.01569942\n",
            "Iteration 40, loss = 0.01448894\n",
            "Iteration 41, loss = 0.01337180\n",
            "Iteration 42, loss = 0.01234079\n",
            "Iteration 43, loss = 0.01138927\n",
            "Iteration 44, loss = 0.01051112\n",
            "Iteration 45, loss = 0.00970068\n",
            "Iteration 46, loss = 0.00895272\n",
            "Iteration 47, loss = 0.00826244\n",
            "Iteration 48, loss = 0.00762538\n",
            "Iteration 49, loss = 0.00703743\n",
            "Iteration 50, loss = 0.00649482\n",
            "Iteration 51, loss = 0.00599405\n",
            "Iteration 52, loss = 0.00553189\n",
            "Iteration 53, loss = 0.00510536\n",
            "Iteration 54, loss = 0.00471172\n",
            "Iteration 55, loss = 0.00434843\n",
            "Iteration 56, loss = 0.00401315\n",
            "Iteration 57, loss = 0.00370373\n",
            "Iteration 58, loss = 0.00341818\n",
            "Iteration 59, loss = 0.00315471\n",
            "Iteration 60, loss = 0.00291176\n",
            "Iteration 61, loss = 0.00268811\n",
            "Iteration 62, loss = 0.00248323\n",
            "Iteration 63, loss = 0.00229771\n",
            "Iteration 64, loss = 0.00213369\n",
            "Iteration 65, loss = 0.00199406\n",
            "Iteration 66, loss = 0.00187964\n",
            "Iteration 67, loss = 0.00178650\n",
            "Iteration 68, loss = 0.00170800\n",
            "Iteration 69, loss = 0.00163892\n",
            "Iteration 70, loss = 0.00157656\n",
            "Iteration 71, loss = 0.00151972\n",
            "Iteration 72, loss = 0.00146772\n",
            "Iteration 73, loss = 0.00142009\n",
            "Iteration 74, loss = 0.00137643\n",
            "Iteration 75, loss = 0.00133641\n",
            "Iteration 76, loss = 0.00129969\n",
            "Iteration 77, loss = 0.00126599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.00124576\n",
            "Iteration 79, loss = 0.00123964\n",
            "Iteration 80, loss = 0.00123370\n",
            "Iteration 81, loss = 0.00122785\n",
            "Iteration 82, loss = 0.00122211\n",
            "Iteration 83, loss = 0.00121646\n",
            "Iteration 84, loss = 0.00121091\n",
            "Iteration 85, loss = 0.00120545\n",
            "Iteration 86, loss = 0.00120008\n",
            "Iteration 87, loss = 0.00119480\n",
            "Iteration 88, loss = 0.00118961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.00118633\n",
            "Iteration 90, loss = 0.00118530\n",
            "Iteration 91, loss = 0.00118429\n",
            "Iteration 92, loss = 0.00118328\n",
            "Iteration 93, loss = 0.00118227\n",
            "Iteration 94, loss = 0.00118127\n",
            "Iteration 95, loss = 0.00118027\n",
            "Iteration 96, loss = 0.00117928\n",
            "Iteration 97, loss = 0.00117829\n",
            "Iteration 98, loss = 0.00117730\n",
            "Iteration 99, loss = 0.00117631\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 100, loss = 0.00117568\n",
            "Iteration 101, loss = 0.00117548\n",
            "Iteration 102, loss = 0.00117529\n",
            "Iteration 103, loss = 0.00117509\n",
            "Iteration 104, loss = 0.00117489\n",
            "Iteration 105, loss = 0.00117470\n",
            "Iteration 106, loss = 0.00117450\n",
            "Iteration 107, loss = 0.00117431\n",
            "Iteration 108, loss = 0.00117411\n",
            "Iteration 109, loss = 0.00117392\n",
            "Iteration 110, loss = 0.00117372\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.00117360\n",
            "Iteration 112, loss = 0.00117356\n",
            "Iteration 113, loss = 0.00117352\n",
            "Iteration 114, loss = 0.00117348\n",
            "Iteration 115, loss = 0.00117344\n",
            "Iteration 116, loss = 0.00117340\n",
            "Iteration 117, loss = 0.00117336\n",
            "Iteration 118, loss = 0.00117332\n",
            "Iteration 119, loss = 0.00117328\n",
            "Iteration 120, loss = 0.00117325\n",
            "Iteration 121, loss = 0.00117321\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.00117318\n",
            "Iteration 123, loss = 0.00117317\n",
            "Iteration 124, loss = 0.00117317\n",
            "Iteration 125, loss = 0.00117316\n",
            "Iteration 126, loss = 0.00117315\n",
            "Iteration 127, loss = 0.00117314\n",
            "Iteration 128, loss = 0.00117313\n",
            "Iteration 129, loss = 0.00117313\n",
            "Iteration 130, loss = 0.00117312\n",
            "Iteration 131, loss = 0.00117311\n",
            "Iteration 132, loss = 0.00117310\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.14085074\n",
            "Iteration 2, loss = 0.13002551\n",
            "Iteration 3, loss = 0.12000010\n",
            "Iteration 4, loss = 0.11074769\n",
            "Iteration 5, loss = 0.10220867\n",
            "Iteration 6, loss = 0.09432804\n",
            "Iteration 7, loss = 0.08705504\n",
            "Iteration 8, loss = 0.08034281\n",
            "Iteration 9, loss = 0.07414816\n",
            "Iteration 10, loss = 0.06843125\n",
            "Iteration 11, loss = 0.06315548\n",
            "Iteration 12, loss = 0.05828736\n",
            "Iteration 13, loss = 0.05379633\n",
            "Iteration 14, loss = 0.04965401\n",
            "Iteration 15, loss = 0.04583332\n",
            "Iteration 16, loss = 0.04230870\n",
            "Iteration 17, loss = 0.03905679\n",
            "Iteration 18, loss = 0.03605641\n",
            "Iteration 19, loss = 0.03328815\n",
            "Iteration 20, loss = 0.03073416\n",
            "Iteration 21, loss = 0.02837795\n",
            "Iteration 22, loss = 0.02620433\n",
            "Iteration 23, loss = 0.02419929\n",
            "Iteration 24, loss = 0.02234988\n",
            "Iteration 25, loss = 0.02064417\n",
            "Iteration 26, loss = 0.01907114\n",
            "Iteration 27, loss = 0.01762062\n",
            "Iteration 28, loss = 0.01628325\n",
            "Iteration 29, loss = 0.01505035\n",
            "Iteration 30, loss = 0.01391395\n",
            "Iteration 31, loss = 0.01286668\n",
            "Iteration 32, loss = 0.01190174\n",
            "Iteration 33, loss = 0.01101283\n",
            "Iteration 34, loss = 0.01019416\n",
            "Iteration 35, loss = 0.00944044\n",
            "Iteration 36, loss = 0.00874676\n",
            "Iteration 37, loss = 0.00810852\n",
            "Iteration 38, loss = 0.00752145\n",
            "Iteration 39, loss = 0.00698158\n",
            "Iteration 40, loss = 0.00648524\n",
            "Iteration 41, loss = 0.00602905\n",
            "Iteration 42, loss = 0.00560986\n",
            "Iteration 43, loss = 0.00522476\n",
            "Iteration 44, loss = 0.00487108\n",
            "Iteration 45, loss = 0.00454633\n",
            "Iteration 46, loss = 0.00424818\n",
            "Iteration 47, loss = 0.00397449\n",
            "Iteration 48, loss = 0.00372330\n",
            "Iteration 49, loss = 0.00349279\n",
            "Iteration 50, loss = 0.00328128\n",
            "Iteration 51, loss = 0.00308720\n",
            "Iteration 52, loss = 0.00290912\n",
            "Iteration 53, loss = 0.00274571\n",
            "Iteration 54, loss = 0.00259573\n",
            "Iteration 55, loss = 0.00245807\n",
            "Iteration 56, loss = 0.00233170\n",
            "Iteration 57, loss = 0.00221566\n",
            "Iteration 58, loss = 0.00210907\n",
            "Iteration 59, loss = 0.00201113\n",
            "Iteration 60, loss = 0.00192113\n",
            "Iteration 61, loss = 0.00183837\n",
            "Iteration 62, loss = 0.00176226\n",
            "Iteration 63, loss = 0.00169223\n",
            "Iteration 64, loss = 0.00162778\n",
            "Iteration 65, loss = 0.00156845\n",
            "Iteration 66, loss = 0.00151383\n",
            "Iteration 67, loss = 0.00146352\n",
            "Iteration 68, loss = 0.00141719\n",
            "Iteration 69, loss = 0.00137451\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00134882\n",
            "Iteration 71, loss = 0.00134103\n",
            "Iteration 72, loss = 0.00133346\n",
            "Iteration 73, loss = 0.00132601\n",
            "Iteration 74, loss = 0.00131869\n",
            "Iteration 75, loss = 0.00131148\n",
            "Iteration 76, loss = 0.00130439\n",
            "Iteration 77, loss = 0.00129742\n",
            "Iteration 78, loss = 0.00129056\n",
            "Iteration 79, loss = 0.00128380\n",
            "Iteration 80, loss = 0.00127716\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00127296\n",
            "Iteration 82, loss = 0.00127164\n",
            "Iteration 83, loss = 0.00127034\n",
            "Iteration 84, loss = 0.00126905\n",
            "Iteration 85, loss = 0.00126776\n",
            "Iteration 86, loss = 0.00126648\n",
            "Iteration 87, loss = 0.00126520\n",
            "Iteration 88, loss = 0.00126392\n",
            "Iteration 89, loss = 0.00126265\n",
            "Iteration 90, loss = 0.00126138\n",
            "Iteration 91, loss = 0.00126012\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00125931\n",
            "Iteration 93, loss = 0.00125905\n",
            "Iteration 94, loss = 0.00125880\n",
            "Iteration 95, loss = 0.00125855\n",
            "Iteration 96, loss = 0.00125830\n",
            "Iteration 97, loss = 0.00125805\n",
            "Iteration 98, loss = 0.00125780\n",
            "Iteration 99, loss = 0.00125755\n",
            "Iteration 100, loss = 0.00125730\n",
            "Iteration 101, loss = 0.00125705\n",
            "Iteration 102, loss = 0.00125679\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00125663\n",
            "Iteration 104, loss = 0.00125658\n",
            "Iteration 105, loss = 0.00125653\n",
            "Iteration 106, loss = 0.00125648\n",
            "Iteration 107, loss = 0.00125643\n",
            "Iteration 108, loss = 0.00125638\n",
            "Iteration 109, loss = 0.00125633\n",
            "Iteration 110, loss = 0.00125628\n",
            "Iteration 111, loss = 0.00125623\n",
            "Iteration 112, loss = 0.00125618\n",
            "Iteration 113, loss = 0.00125613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00125610\n",
            "Iteration 115, loss = 0.00125609\n",
            "Iteration 116, loss = 0.00125608\n",
            "Iteration 117, loss = 0.00125607\n",
            "Iteration 118, loss = 0.00125606\n",
            "Iteration 119, loss = 0.00125605\n",
            "Iteration 120, loss = 0.00125604\n",
            "Iteration 121, loss = 0.00125603\n",
            "Iteration 122, loss = 0.00125602\n",
            "Iteration 123, loss = 0.00125601\n",
            "Iteration 124, loss = 0.00125600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12964263\n",
            "Iteration 2, loss = 0.11947574\n",
            "Iteration 3, loss = 0.11026376\n",
            "Iteration 4, loss = 0.10176205\n",
            "Iteration 5, loss = 0.09391586\n",
            "Iteration 6, loss = 0.08667463\n",
            "Iteration 7, loss = 0.07999172\n",
            "Iteration 8, loss = 0.07382409\n",
            "Iteration 9, loss = 0.06813201\n",
            "Iteration 10, loss = 0.06287880\n",
            "Iteration 11, loss = 0.05803063\n",
            "Iteration 12, loss = 0.05355628\n",
            "Iteration 13, loss = 0.04942691\n",
            "Iteration 14, loss = 0.04561594\n",
            "Iteration 15, loss = 0.04209884\n",
            "Iteration 16, loss = 0.03885302\n",
            "Iteration 17, loss = 0.03585778\n",
            "Iteration 18, loss = 0.03309437\n",
            "Iteration 19, loss = 0.03054609\n",
            "Iteration 20, loss = 0.02819806\n",
            "Iteration 21, loss = 0.02603608\n",
            "Iteration 22, loss = 0.02404526\n",
            "Iteration 23, loss = 0.02221089\n",
            "Iteration 24, loss = 0.02051990\n",
            "Iteration 25, loss = 0.01896089\n",
            "Iteration 26, loss = 0.01752364\n",
            "Iteration 27, loss = 0.01619878\n",
            "Iteration 28, loss = 0.01497775\n",
            "Iteration 29, loss = 0.01385262\n",
            "Iteration 30, loss = 0.01281607\n",
            "Iteration 31, loss = 0.01186138\n",
            "Iteration 32, loss = 0.01098232\n",
            "Iteration 33, loss = 0.01017310\n",
            "Iteration 34, loss = 0.00942833\n",
            "Iteration 35, loss = 0.00874306\n",
            "Iteration 36, loss = 0.00811266\n",
            "Iteration 37, loss = 0.00753285\n",
            "Iteration 38, loss = 0.00699968\n",
            "Iteration 39, loss = 0.00650949\n",
            "Iteration 40, loss = 0.00605890\n",
            "Iteration 41, loss = 0.00564478\n",
            "Iteration 42, loss = 0.00526423\n",
            "Iteration 43, loss = 0.00491454\n",
            "Iteration 44, loss = 0.00459321\n",
            "Iteration 45, loss = 0.00429795\n",
            "Iteration 46, loss = 0.00402663\n",
            "Iteration 47, loss = 0.00377733\n",
            "Iteration 48, loss = 0.00354822\n",
            "Iteration 49, loss = 0.00333765\n",
            "Iteration 50, loss = 0.00314410\n",
            "Iteration 51, loss = 0.00296618\n",
            "Iteration 52, loss = 0.00280260\n",
            "Iteration 53, loss = 0.00265217\n",
            "Iteration 54, loss = 0.00251380\n",
            "Iteration 55, loss = 0.00238647\n",
            "Iteration 56, loss = 0.00226927\n",
            "Iteration 57, loss = 0.00216138\n",
            "Iteration 58, loss = 0.00206203\n",
            "Iteration 59, loss = 0.00197053\n",
            "Iteration 60, loss = 0.00188624\n",
            "Iteration 61, loss = 0.00180859\n",
            "Iteration 62, loss = 0.00173705\n",
            "Iteration 63, loss = 0.00167111\n",
            "Iteration 64, loss = 0.00161035\n",
            "Iteration 65, loss = 0.00155434\n",
            "Iteration 66, loss = 0.00150271\n",
            "Iteration 67, loss = 0.00145511\n",
            "Iteration 68, loss = 0.00141122\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.00138479\n",
            "Iteration 70, loss = 0.00137677\n",
            "Iteration 71, loss = 0.00136898\n",
            "Iteration 72, loss = 0.00136132\n",
            "Iteration 73, loss = 0.00135378\n",
            "Iteration 74, loss = 0.00134636\n",
            "Iteration 75, loss = 0.00133905\n",
            "Iteration 76, loss = 0.00133187\n",
            "Iteration 77, loss = 0.00132480\n",
            "Iteration 78, loss = 0.00131784\n",
            "Iteration 79, loss = 0.00131100\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.00130667\n",
            "Iteration 81, loss = 0.00130531\n",
            "Iteration 82, loss = 0.00130397\n",
            "Iteration 83, loss = 0.00130264\n",
            "Iteration 84, loss = 0.00130131\n",
            "Iteration 85, loss = 0.00129999\n",
            "Iteration 86, loss = 0.00129867\n",
            "Iteration 87, loss = 0.00129735\n",
            "Iteration 88, loss = 0.00129604\n",
            "Iteration 89, loss = 0.00129474\n",
            "Iteration 90, loss = 0.00129343\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.00129260\n",
            "Iteration 92, loss = 0.00129234\n",
            "Iteration 93, loss = 0.00129208\n",
            "Iteration 94, loss = 0.00129182\n",
            "Iteration 95, loss = 0.00129156\n",
            "Iteration 96, loss = 0.00129130\n",
            "Iteration 97, loss = 0.00129104\n",
            "Iteration 98, loss = 0.00129078\n",
            "Iteration 99, loss = 0.00129053\n",
            "Iteration 100, loss = 0.00129027\n",
            "Iteration 101, loss = 0.00129001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.00128984\n",
            "Iteration 103, loss = 0.00128979\n",
            "Iteration 104, loss = 0.00128974\n",
            "Iteration 105, loss = 0.00128969\n",
            "Iteration 106, loss = 0.00128964\n",
            "Iteration 107, loss = 0.00128959\n",
            "Iteration 108, loss = 0.00128953\n",
            "Iteration 109, loss = 0.00128948\n",
            "Iteration 110, loss = 0.00128943\n",
            "Iteration 111, loss = 0.00128938\n",
            "Iteration 112, loss = 0.00128933\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 113, loss = 0.00128930\n",
            "Iteration 114, loss = 0.00128929\n",
            "Iteration 115, loss = 0.00128928\n",
            "Iteration 116, loss = 0.00128926\n",
            "Iteration 117, loss = 0.00128925\n",
            "Iteration 118, loss = 0.00128924\n",
            "Iteration 119, loss = 0.00128923\n",
            "Iteration 120, loss = 0.00128922\n",
            "Iteration 121, loss = 0.00128921\n",
            "Iteration 122, loss = 0.00128920\n",
            "Iteration 123, loss = 0.00128919\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.39062559\n",
            "Iteration 2, loss = 0.29385251\n",
            "Iteration 3, loss = 0.27119549\n",
            "Iteration 4, loss = 0.25028541\n",
            "Iteration 5, loss = 0.23098756\n",
            "Iteration 6, loss = 0.21317764\n",
            "Iteration 7, loss = 0.19674093\n",
            "Iteration 8, loss = 0.18157154\n",
            "Iteration 9, loss = 0.16757177\n",
            "Iteration 10, loss = 0.15465142\n",
            "Iteration 11, loss = 0.14272728\n",
            "Iteration 12, loss = 0.13172252\n",
            "Iteration 13, loss = 0.12156628\n",
            "Iteration 14, loss = 0.11219311\n",
            "Iteration 15, loss = 0.10354264\n",
            "Iteration 16, loss = 0.09555916\n",
            "Iteration 17, loss = 0.08819122\n",
            "Iteration 18, loss = 0.08139138\n",
            "Iteration 19, loss = 0.07511584\n",
            "Iteration 20, loss = 0.06932415\n",
            "Iteration 21, loss = 0.06397903\n",
            "Iteration 22, loss = 0.05904603\n",
            "Iteration 23, loss = 0.05449338\n",
            "Iteration 24, loss = 0.05029176\n",
            "Iteration 25, loss = 0.04641410\n",
            "Iteration 26, loss = 0.04283541\n",
            "Iteration 27, loss = 0.03953266\n",
            "Iteration 28, loss = 0.03648456\n",
            "Iteration 29, loss = 0.03367148\n",
            "Iteration 30, loss = 0.03107529\n",
            "Iteration 31, loss = 0.02867928\n",
            "Iteration 32, loss = 0.02646801\n",
            "Iteration 33, loss = 0.02442724\n",
            "Iteration 34, loss = 0.02254382\n",
            "Iteration 35, loss = 0.02080561\n",
            "Iteration 36, loss = 0.01920143\n",
            "Iteration 37, loss = 0.01772094\n",
            "Iteration 38, loss = 0.01635459\n",
            "Iteration 39, loss = 0.01509360\n",
            "Iteration 40, loss = 0.01392983\n",
            "Iteration 41, loss = 0.01285579\n",
            "Iteration 42, loss = 0.01186457\n",
            "Iteration 43, loss = 0.01094977\n",
            "Iteration 44, loss = 0.01010551\n",
            "Iteration 45, loss = 0.00932634\n",
            "Iteration 46, loss = 0.00860725\n",
            "Iteration 47, loss = 0.00794360\n",
            "Iteration 48, loss = 0.00733112\n",
            "Iteration 49, loss = 0.00676587\n",
            "Iteration 50, loss = 0.00624420\n",
            "Iteration 51, loss = 0.00576275\n",
            "Iteration 52, loss = 0.00531842\n",
            "Iteration 53, loss = 0.00490835\n",
            "Iteration 54, loss = 0.00452990\n",
            "Iteration 55, loss = 0.00418064\n",
            "Iteration 56, loss = 0.00385831\n",
            "Iteration 57, loss = 0.00356087\n",
            "Iteration 58, loss = 0.00328651\n",
            "Iteration 59, loss = 0.00303369\n",
            "Iteration 60, loss = 0.00280145\n",
            "Iteration 61, loss = 0.00258972\n",
            "Iteration 62, loss = 0.00239985\n",
            "Iteration 63, loss = 0.00223431\n",
            "Iteration 64, loss = 0.00209465\n",
            "Iteration 65, loss = 0.00197833\n",
            "Iteration 66, loss = 0.00187928\n",
            "Iteration 67, loss = 0.00179215\n",
            "Iteration 68, loss = 0.00171380\n",
            "Iteration 69, loss = 0.00164269\n",
            "Iteration 70, loss = 0.00157792\n",
            "Iteration 71, loss = 0.00151885\n",
            "Iteration 72, loss = 0.00146495\n",
            "Iteration 73, loss = 0.00141573\n",
            "Iteration 74, loss = 0.00137077\n",
            "Iteration 75, loss = 0.00132969\n",
            "Iteration 76, loss = 0.00129212\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00126964\n",
            "Iteration 78, loss = 0.00126284\n",
            "Iteration 79, loss = 0.00125625\n",
            "Iteration 80, loss = 0.00124977\n",
            "Iteration 81, loss = 0.00124341\n",
            "Iteration 82, loss = 0.00123716\n",
            "Iteration 83, loss = 0.00123101\n",
            "Iteration 84, loss = 0.00122498\n",
            "Iteration 85, loss = 0.00121905\n",
            "Iteration 86, loss = 0.00121322\n",
            "Iteration 87, loss = 0.00120749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00120387\n",
            "Iteration 89, loss = 0.00120274\n",
            "Iteration 90, loss = 0.00120162\n",
            "Iteration 91, loss = 0.00120051\n",
            "Iteration 92, loss = 0.00119940\n",
            "Iteration 93, loss = 0.00119830\n",
            "Iteration 94, loss = 0.00119720\n",
            "Iteration 95, loss = 0.00119610\n",
            "Iteration 96, loss = 0.00119501\n",
            "Iteration 97, loss = 0.00119392\n",
            "Iteration 98, loss = 0.00119283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00119214\n",
            "Iteration 100, loss = 0.00119192\n",
            "Iteration 101, loss = 0.00119170\n",
            "Iteration 102, loss = 0.00119149\n",
            "Iteration 103, loss = 0.00119127\n",
            "Iteration 104, loss = 0.00119106\n",
            "Iteration 105, loss = 0.00119084\n",
            "Iteration 106, loss = 0.00119063\n",
            "Iteration 107, loss = 0.00119041\n",
            "Iteration 108, loss = 0.00119020\n",
            "Iteration 109, loss = 0.00118998\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00118985\n",
            "Iteration 111, loss = 0.00118980\n",
            "Iteration 112, loss = 0.00118976\n",
            "Iteration 113, loss = 0.00118972\n",
            "Iteration 114, loss = 0.00118967\n",
            "Iteration 115, loss = 0.00118963\n",
            "Iteration 116, loss = 0.00118959\n",
            "Iteration 117, loss = 0.00118954\n",
            "Iteration 118, loss = 0.00118950\n",
            "Iteration 119, loss = 0.00118946\n",
            "Iteration 120, loss = 0.00118942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00118939\n",
            "Iteration 122, loss = 0.00118938\n",
            "Iteration 123, loss = 0.00118937\n",
            "Iteration 124, loss = 0.00118936\n",
            "Iteration 125, loss = 0.00118935\n",
            "Iteration 126, loss = 0.00118935\n",
            "Iteration 127, loss = 0.00118934\n",
            "Iteration 128, loss = 0.00118933\n",
            "Iteration 129, loss = 0.00118932\n",
            "Iteration 130, loss = 0.00118931\n",
            "Iteration 131, loss = 0.00118930\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13588950\n",
            "Iteration 2, loss = 0.12544468\n",
            "Iteration 3, loss = 0.11577554\n",
            "Iteration 4, loss = 0.10685241\n",
            "Iteration 5, loss = 0.09861776\n",
            "Iteration 6, loss = 0.09101857\n",
            "Iteration 7, loss = 0.08400592\n",
            "Iteration 8, loss = 0.07753466\n",
            "Iteration 9, loss = 0.07156309\n",
            "Iteration 10, loss = 0.06605272\n",
            "Iteration 11, loss = 0.06096802\n",
            "Iteration 12, loss = 0.05627617\n",
            "Iteration 13, loss = 0.05194691\n",
            "Iteration 14, loss = 0.04795230\n",
            "Iteration 15, loss = 0.04426656\n",
            "Iteration 16, loss = 0.04086591\n",
            "Iteration 17, loss = 0.03772840\n",
            "Iteration 18, loss = 0.03483380\n",
            "Iteration 19, loss = 0.03216340\n",
            "Iteration 20, loss = 0.02969999\n",
            "Iteration 21, loss = 0.02742765\n",
            "Iteration 22, loss = 0.02533171\n",
            "Iteration 23, loss = 0.02339864\n",
            "Iteration 24, loss = 0.02161595\n",
            "Iteration 25, loss = 0.01997211\n",
            "Iteration 26, loss = 0.01845647\n",
            "Iteration 27, loss = 0.01705921\n",
            "Iteration 28, loss = 0.01577127\n",
            "Iteration 29, loss = 0.01458426\n",
            "Iteration 30, loss = 0.01349047\n",
            "Iteration 31, loss = 0.01248274\n",
            "Iteration 32, loss = 0.01155451\n",
            "Iteration 33, loss = 0.01069968\n",
            "Iteration 34, loss = 0.00991263\n",
            "Iteration 35, loss = 0.00918819\n",
            "Iteration 36, loss = 0.00852156\n",
            "Iteration 37, loss = 0.00790829\n",
            "Iteration 38, loss = 0.00734422\n",
            "Iteration 39, loss = 0.00682554\n",
            "Iteration 40, loss = 0.00634872\n",
            "Iteration 41, loss = 0.00591046\n",
            "Iteration 42, loss = 0.00550771\n",
            "Iteration 43, loss = 0.00513763\n",
            "Iteration 44, loss = 0.00479761\n",
            "Iteration 45, loss = 0.00448520\n",
            "Iteration 46, loss = 0.00419820\n",
            "Iteration 47, loss = 0.00393452\n",
            "Iteration 48, loss = 0.00369227\n",
            "Iteration 49, loss = 0.00346970\n",
            "Iteration 50, loss = 0.00326519\n",
            "Iteration 51, loss = 0.00307729\n",
            "Iteration 52, loss = 0.00290464\n",
            "Iteration 53, loss = 0.00274597\n",
            "Iteration 54, loss = 0.00260010\n",
            "Iteration 55, loss = 0.00246597\n",
            "Iteration 56, loss = 0.00234267\n",
            "Iteration 57, loss = 0.00222936\n",
            "Iteration 58, loss = 0.00212510\n",
            "Iteration 59, loss = 0.00202914\n",
            "Iteration 60, loss = 0.00194079\n",
            "Iteration 61, loss = 0.00185942\n",
            "Iteration 62, loss = 0.00178447\n",
            "Iteration 63, loss = 0.00171543\n",
            "Iteration 64, loss = 0.00165182\n",
            "Iteration 65, loss = 0.00159321\n",
            "Iteration 66, loss = 0.00153920\n",
            "Iteration 67, loss = 0.00148942\n",
            "Iteration 68, loss = 0.00144353\n",
            "Iteration 69, loss = 0.00140123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00137575\n",
            "Iteration 71, loss = 0.00136802\n",
            "Iteration 72, loss = 0.00136052\n",
            "Iteration 73, loss = 0.00135313\n",
            "Iteration 74, loss = 0.00134587\n",
            "Iteration 75, loss = 0.00133872\n",
            "Iteration 76, loss = 0.00133168\n",
            "Iteration 77, loss = 0.00132476\n",
            "Iteration 78, loss = 0.00131795\n",
            "Iteration 79, loss = 0.00131125\n",
            "Iteration 80, loss = 0.00130466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00130049\n",
            "Iteration 82, loss = 0.00129918\n",
            "Iteration 83, loss = 0.00129789\n",
            "Iteration 84, loss = 0.00129661\n",
            "Iteration 85, loss = 0.00129533\n",
            "Iteration 86, loss = 0.00129405\n",
            "Iteration 87, loss = 0.00129278\n",
            "Iteration 88, loss = 0.00129151\n",
            "Iteration 89, loss = 0.00129025\n",
            "Iteration 90, loss = 0.00128899\n",
            "Iteration 91, loss = 0.00128774\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00128694\n",
            "Iteration 93, loss = 0.00128668\n",
            "Iteration 94, loss = 0.00128643\n",
            "Iteration 95, loss = 0.00128618\n",
            "Iteration 96, loss = 0.00128593\n",
            "Iteration 97, loss = 0.00128568\n",
            "Iteration 98, loss = 0.00128543\n",
            "Iteration 99, loss = 0.00128519\n",
            "Iteration 100, loss = 0.00128494\n",
            "Iteration 101, loss = 0.00128469\n",
            "Iteration 102, loss = 0.00128444\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00128428\n",
            "Iteration 104, loss = 0.00128423\n",
            "Iteration 105, loss = 0.00128418\n",
            "Iteration 106, loss = 0.00128413\n",
            "Iteration 107, loss = 0.00128408\n",
            "Iteration 108, loss = 0.00128403\n",
            "Iteration 109, loss = 0.00128398\n",
            "Iteration 110, loss = 0.00128393\n",
            "Iteration 111, loss = 0.00128388\n",
            "Iteration 112, loss = 0.00128383\n",
            "Iteration 113, loss = 0.00128378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00128375\n",
            "Iteration 115, loss = 0.00128374\n",
            "Iteration 116, loss = 0.00128373\n",
            "Iteration 117, loss = 0.00128372\n",
            "Iteration 118, loss = 0.00128371\n",
            "Iteration 119, loss = 0.00128370\n",
            "Iteration 120, loss = 0.00128369\n",
            "Iteration 121, loss = 0.00128368\n",
            "Iteration 122, loss = 0.00128367\n",
            "Iteration 123, loss = 0.00128366\n",
            "Iteration 124, loss = 0.00128365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.24985812\n",
            "Iteration 3, loss = 0.23059322\n",
            "Iteration 4, loss = 0.21281370\n",
            "Iteration 5, loss = 0.19640505\n",
            "Iteration 6, loss = 0.18126156\n",
            "Iteration 7, loss = 0.16728569\n",
            "Iteration 8, loss = 0.15438740\n",
            "Iteration 9, loss = 0.14248361\n",
            "Iteration 10, loss = 0.13149765\n",
            "Iteration 11, loss = 0.12135874\n",
            "Iteration 12, loss = 0.11200157\n",
            "Iteration 13, loss = 0.10336587\n",
            "Iteration 14, loss = 0.09539602\n",
            "Iteration 15, loss = 0.08804066\n",
            "Iteration 16, loss = 0.08125243\n",
            "Iteration 17, loss = 0.07498760\n",
            "Iteration 18, loss = 0.06920580\n",
            "Iteration 19, loss = 0.06386980\n",
            "Iteration 20, loss = 0.05894522\n",
            "Iteration 21, loss = 0.05440035\n",
            "Iteration 22, loss = 0.05020590\n",
            "Iteration 23, loss = 0.04633486\n",
            "Iteration 24, loss = 0.04276228\n",
            "Iteration 25, loss = 0.03946517\n",
            "Iteration 26, loss = 0.03642227\n",
            "Iteration 27, loss = 0.03361399\n",
            "Iteration 28, loss = 0.03102224\n",
            "Iteration 29, loss = 0.02863032\n",
            "Iteration 30, loss = 0.02642283\n",
            "Iteration 31, loss = 0.02438554\n",
            "Iteration 32, loss = 0.02250533\n",
            "Iteration 33, loss = 0.02077009\n",
            "Iteration 34, loss = 0.01916865\n",
            "Iteration 35, loss = 0.01769068\n",
            "Iteration 36, loss = 0.01632667\n",
            "Iteration 37, loss = 0.01506783\n",
            "Iteration 38, loss = 0.01390605\n",
            "Iteration 39, loss = 0.01283385\n",
            "Iteration 40, loss = 0.01184431\n",
            "Iteration 41, loss = 0.01093108\n",
            "Iteration 42, loss = 0.01008825\n",
            "Iteration 43, loss = 0.00931042\n",
            "Iteration 44, loss = 0.00859255\n",
            "Iteration 45, loss = 0.00793004\n",
            "Iteration 46, loss = 0.00731861\n",
            "Iteration 47, loss = 0.00675432\n",
            "Iteration 48, loss = 0.00623354\n",
            "Iteration 49, loss = 0.00575291\n",
            "Iteration 50, loss = 0.00530935\n",
            "Iteration 51, loss = 0.00490000\n",
            "Iteration 52, loss = 0.00452227\n",
            "Iteration 53, loss = 0.00417386\n",
            "Iteration 54, loss = 0.00385287\n",
            "Iteration 55, loss = 0.00355811\n",
            "Iteration 56, loss = 0.00328957\n",
            "Iteration 57, loss = 0.00304882\n",
            "Iteration 58, loss = 0.00283834\n",
            "Iteration 59, loss = 0.00265859\n",
            "Iteration 60, loss = 0.00250533\n",
            "Iteration 61, loss = 0.00237148\n",
            "Iteration 62, loss = 0.00225137\n",
            "Iteration 63, loss = 0.00214190\n",
            "Iteration 64, loss = 0.00204153\n",
            "Iteration 65, loss = 0.00194932\n",
            "Iteration 66, loss = 0.00186456\n",
            "Iteration 67, loss = 0.00178661\n",
            "Iteration 68, loss = 0.00171491\n",
            "Iteration 69, loss = 0.00164896\n",
            "Iteration 70, loss = 0.00158827\n",
            "Iteration 71, loss = 0.00153243\n",
            "Iteration 72, loss = 0.00148104\n",
            "Iteration 73, loss = 0.00143374\n",
            "Iteration 74, loss = 0.00139019\n",
            "Iteration 75, loss = 0.00135009\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.00132596\n",
            "Iteration 77, loss = 0.00131865\n",
            "Iteration 78, loss = 0.00131154\n",
            "Iteration 79, loss = 0.00130455\n",
            "Iteration 80, loss = 0.00129768\n",
            "Iteration 81, loss = 0.00129091\n",
            "Iteration 82, loss = 0.00128426\n",
            "Iteration 83, loss = 0.00127771\n",
            "Iteration 84, loss = 0.00127127\n",
            "Iteration 85, loss = 0.00126494\n",
            "Iteration 86, loss = 0.00125871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.00125476\n",
            "Iteration 88, loss = 0.00125352\n",
            "Iteration 89, loss = 0.00125231\n",
            "Iteration 90, loss = 0.00125110\n",
            "Iteration 91, loss = 0.00124989\n",
            "Iteration 92, loss = 0.00124868\n",
            "Iteration 93, loss = 0.00124748\n",
            "Iteration 94, loss = 0.00124628\n",
            "Iteration 95, loss = 0.00124509\n",
            "Iteration 96, loss = 0.00124390\n",
            "Iteration 97, loss = 0.00124271\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.00124196\n",
            "Iteration 99, loss = 0.00124172\n",
            "Iteration 100, loss = 0.00124148\n",
            "Iteration 101, loss = 0.00124125\n",
            "Iteration 102, loss = 0.00124101\n",
            "Iteration 103, loss = 0.00124078\n",
            "Iteration 104, loss = 0.00124054\n",
            "Iteration 105, loss = 0.00124030\n",
            "Iteration 106, loss = 0.00124007\n",
            "Iteration 107, loss = 0.00123983\n",
            "Iteration 108, loss = 0.00123960\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.00123945\n",
            "Iteration 110, loss = 0.00123940\n",
            "Iteration 111, loss = 0.00123936\n",
            "Iteration 112, loss = 0.00123931\n",
            "Iteration 113, loss = 0.00123926\n",
            "Iteration 114, loss = 0.00123921\n",
            "Iteration 115, loss = 0.00123917\n",
            "Iteration 116, loss = 0.00123912\n",
            "Iteration 117, loss = 0.00123907\n",
            "Iteration 118, loss = 0.00123903\n",
            "Iteration 119, loss = 0.00123898\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.00123895\n",
            "Iteration 121, loss = 0.00123894\n",
            "Iteration 122, loss = 0.00123893\n",
            "Iteration 123, loss = 0.00123892\n",
            "Iteration 124, loss = 0.00123891\n",
            "Iteration 125, loss = 0.00123890\n",
            "Iteration 126, loss = 0.00123889\n",
            "Iteration 127, loss = 0.00123888\n",
            "Iteration 128, loss = 0.00123888\n",
            "Iteration 129, loss = 0.00123887\n",
            "Iteration 130, loss = 0.00123886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.33643325\n",
            "Iteration 2, loss = 0.24944538\n",
            "Iteration 3, loss = 0.23021230\n",
            "Iteration 4, loss = 0.21246216\n",
            "Iteration 5, loss = 0.19608062\n",
            "Iteration 6, loss = 0.18096214\n",
            "Iteration 7, loss = 0.16700935\n",
            "Iteration 8, loss = 0.15413237\n",
            "Iteration 9, loss = 0.14224825\n",
            "Iteration 10, loss = 0.13128043\n",
            "Iteration 11, loss = 0.12115827\n",
            "Iteration 12, loss = 0.11181656\n",
            "Iteration 13, loss = 0.10319512\n",
            "Iteration 14, loss = 0.09523843\n",
            "Iteration 15, loss = 0.08789523\n",
            "Iteration 16, loss = 0.08111821\n",
            "Iteration 17, loss = 0.07486373\n",
            "Iteration 18, loss = 0.06909148\n",
            "Iteration 19, loss = 0.06376430\n",
            "Iteration 20, loss = 0.05884785\n",
            "Iteration 21, loss = 0.05431049\n",
            "Iteration 22, loss = 0.05012297\n",
            "Iteration 23, loss = 0.04625832\n",
            "Iteration 24, loss = 0.04269165\n",
            "Iteration 25, loss = 0.03939998\n",
            "Iteration 26, loss = 0.03636211\n",
            "Iteration 27, loss = 0.03355846\n",
            "Iteration 28, loss = 0.03097099\n",
            "Iteration 29, loss = 0.02858303\n",
            "Iteration 30, loss = 0.02637918\n",
            "Iteration 31, loss = 0.02434526\n",
            "Iteration 32, loss = 0.02246815\n",
            "Iteration 33, loss = 0.02073578\n",
            "Iteration 34, loss = 0.01913699\n",
            "Iteration 35, loss = 0.01766146\n",
            "Iteration 36, loss = 0.01629970\n",
            "Iteration 37, loss = 0.01504294\n",
            "Iteration 38, loss = 0.01388308\n",
            "Iteration 39, loss = 0.01281265\n",
            "Iteration 40, loss = 0.01182475\n",
            "Iteration 41, loss = 0.01091302\n",
            "Iteration 42, loss = 0.01007159\n",
            "Iteration 43, loss = 0.00929504\n",
            "Iteration 44, loss = 0.00857836\n",
            "Iteration 45, loss = 0.00791694\n",
            "Iteration 46, loss = 0.00730652\n",
            "Iteration 47, loss = 0.00674316\n",
            "Iteration 48, loss = 0.00622324\n",
            "Iteration 49, loss = 0.00574341\n",
            "Iteration 50, loss = 0.00530057\n",
            "Iteration 51, loss = 0.00489188\n",
            "Iteration 52, loss = 0.00451470\n",
            "Iteration 53, loss = 0.00416661\n",
            "Iteration 54, loss = 0.00384540\n",
            "Iteration 55, loss = 0.00354907\n",
            "Iteration 56, loss = 0.00327596\n",
            "Iteration 57, loss = 0.00302492\n",
            "Iteration 58, loss = 0.00279577\n",
            "Iteration 59, loss = 0.00258982\n",
            "Iteration 60, loss = 0.00240984\n",
            "Iteration 61, loss = 0.00225799\n",
            "Iteration 62, loss = 0.00213223\n",
            "Iteration 63, loss = 0.00202608\n",
            "Iteration 64, loss = 0.00193290\n",
            "Iteration 65, loss = 0.00184872\n",
            "Iteration 66, loss = 0.00177177\n",
            "Iteration 67, loss = 0.00170121\n",
            "Iteration 68, loss = 0.00163643\n",
            "Iteration 69, loss = 0.00157695\n",
            "Iteration 70, loss = 0.00152229\n",
            "Iteration 71, loss = 0.00147206\n",
            "Iteration 72, loss = 0.00142589\n",
            "Iteration 73, loss = 0.00138344\n",
            "Iteration 74, loss = 0.00134439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 75, loss = 0.00132091\n",
            "Iteration 76, loss = 0.00131380\n",
            "Iteration 77, loss = 0.00130689\n",
            "Iteration 78, loss = 0.00130009\n",
            "Iteration 79, loss = 0.00129341\n",
            "Iteration 80, loss = 0.00128684\n",
            "Iteration 81, loss = 0.00128037\n",
            "Iteration 82, loss = 0.00127401\n",
            "Iteration 83, loss = 0.00126776\n",
            "Iteration 84, loss = 0.00126161\n",
            "Iteration 85, loss = 0.00125556\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 86, loss = 0.00125173\n",
            "Iteration 87, loss = 0.00125053\n",
            "Iteration 88, loss = 0.00124935\n",
            "Iteration 89, loss = 0.00124817\n",
            "Iteration 90, loss = 0.00124700\n",
            "Iteration 91, loss = 0.00124583\n",
            "Iteration 92, loss = 0.00124466\n",
            "Iteration 93, loss = 0.00124350\n",
            "Iteration 94, loss = 0.00124235\n",
            "Iteration 95, loss = 0.00124119\n",
            "Iteration 96, loss = 0.00124004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.00123931\n",
            "Iteration 98, loss = 0.00123907\n",
            "Iteration 99, loss = 0.00123884\n",
            "Iteration 100, loss = 0.00123862\n",
            "Iteration 101, loss = 0.00123839\n",
            "Iteration 102, loss = 0.00123816\n",
            "Iteration 103, loss = 0.00123793\n",
            "Iteration 104, loss = 0.00123770\n",
            "Iteration 105, loss = 0.00123747\n",
            "Iteration 106, loss = 0.00123725\n",
            "Iteration 107, loss = 0.00123702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 108, loss = 0.00123687\n",
            "Iteration 109, loss = 0.00123683\n",
            "Iteration 110, loss = 0.00123678\n",
            "Iteration 111, loss = 0.00123674\n",
            "Iteration 112, loss = 0.00123669\n",
            "Iteration 113, loss = 0.00123664\n",
            "Iteration 114, loss = 0.00123660\n",
            "Iteration 115, loss = 0.00123655\n",
            "Iteration 116, loss = 0.00123651\n",
            "Iteration 117, loss = 0.00123646\n",
            "Iteration 118, loss = 0.00123642\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 119, loss = 0.00123639\n",
            "Iteration 120, loss = 0.00123638\n",
            "Iteration 121, loss = 0.00123637\n",
            "Iteration 122, loss = 0.00123636\n",
            "Iteration 123, loss = 0.00123635\n",
            "Iteration 124, loss = 0.00123634\n",
            "Iteration 125, loss = 0.00123633\n",
            "Iteration 126, loss = 0.00123632\n",
            "Iteration 127, loss = 0.00123632\n",
            "Iteration 128, loss = 0.00123631\n",
            "Iteration 129, loss = 0.00123630\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.14521375\n",
            "Iteration 2, loss = 0.13403028\n",
            "Iteration 3, loss = 0.12369846\n",
            "Iteration 4, loss = 0.11416366\n",
            "Iteration 5, loss = 0.10536436\n",
            "Iteration 6, loss = 0.09724385\n",
            "Iteration 7, loss = 0.08974980\n",
            "Iteration 8, loss = 0.08283394\n",
            "Iteration 9, loss = 0.07645173\n",
            "Iteration 10, loss = 0.07056205\n",
            "Iteration 11, loss = 0.06512698\n",
            "Iteration 12, loss = 0.06011151\n",
            "Iteration 13, loss = 0.05548332\n",
            "Iteration 14, loss = 0.05121260\n",
            "Iteration 15, loss = 0.04727183\n",
            "Iteration 16, loss = 0.04363562\n",
            "Iteration 17, loss = 0.04028054\n",
            "Iteration 18, loss = 0.03718496\n",
            "Iteration 19, loss = 0.03432893\n",
            "Iteration 20, loss = 0.03169401\n",
            "Iteration 21, loss = 0.02926323\n",
            "Iteration 22, loss = 0.02702089\n",
            "Iteration 23, loss = 0.02495252\n",
            "Iteration 24, loss = 0.02304479\n",
            "Iteration 25, loss = 0.02128535\n",
            "Iteration 26, loss = 0.01966285\n",
            "Iteration 27, loss = 0.01816680\n",
            "Iteration 28, loss = 0.01678752\n",
            "Iteration 29, loss = 0.01551609\n",
            "Iteration 30, loss = 0.01434426\n",
            "Iteration 31, loss = 0.01326442\n",
            "Iteration 32, loss = 0.01226956\n",
            "Iteration 33, loss = 0.01135319\n",
            "Iteration 34, loss = 0.01050931\n",
            "Iteration 35, loss = 0.00973239\n",
            "Iteration 36, loss = 0.00901731\n",
            "Iteration 37, loss = 0.00835939\n",
            "Iteration 38, loss = 0.00775422\n",
            "Iteration 39, loss = 0.00719776\n",
            "Iteration 40, loss = 0.00668623\n",
            "Iteration 41, loss = 0.00621611\n",
            "Iteration 42, loss = 0.00578416\n",
            "Iteration 43, loss = 0.00538735\n",
            "Iteration 44, loss = 0.00502289\n",
            "Iteration 45, loss = 0.00468820\n",
            "Iteration 46, loss = 0.00438090\n",
            "Iteration 47, loss = 0.00409876\n",
            "Iteration 48, loss = 0.00383976\n",
            "Iteration 49, loss = 0.00360201\n",
            "Iteration 50, loss = 0.00338379\n",
            "Iteration 51, loss = 0.00318349\n",
            "Iteration 52, loss = 0.00299964\n",
            "Iteration 53, loss = 0.00283085\n",
            "Iteration 54, loss = 0.00267587\n",
            "Iteration 55, loss = 0.00253351\n",
            "Iteration 56, loss = 0.00240269\n",
            "Iteration 57, loss = 0.00228251\n",
            "Iteration 58, loss = 0.00217210\n",
            "Iteration 59, loss = 0.00207067\n",
            "Iteration 60, loss = 0.00197748\n",
            "Iteration 61, loss = 0.00189183\n",
            "Iteration 62, loss = 0.00181309\n",
            "Iteration 63, loss = 0.00174069\n",
            "Iteration 64, loss = 0.00167409\n",
            "Iteration 65, loss = 0.00161282\n",
            "Iteration 66, loss = 0.00155644\n",
            "Iteration 67, loss = 0.00150456\n",
            "Iteration 68, loss = 0.00145680\n",
            "Iteration 69, loss = 0.00141283\n",
            "Iteration 70, loss = 0.00137233\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 71, loss = 0.00134796\n",
            "Iteration 72, loss = 0.00134057\n",
            "Iteration 73, loss = 0.00133339\n",
            "Iteration 74, loss = 0.00132633\n",
            "Iteration 75, loss = 0.00131939\n",
            "Iteration 76, loss = 0.00131255\n",
            "Iteration 77, loss = 0.00130583\n",
            "Iteration 78, loss = 0.00129922\n",
            "Iteration 79, loss = 0.00129271\n",
            "Iteration 80, loss = 0.00128631\n",
            "Iteration 81, loss = 0.00128001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.00127603\n",
            "Iteration 83, loss = 0.00127478\n",
            "Iteration 84, loss = 0.00127355\n",
            "Iteration 85, loss = 0.00127232\n",
            "Iteration 86, loss = 0.00127110\n",
            "Iteration 87, loss = 0.00126988\n",
            "Iteration 88, loss = 0.00126867\n",
            "Iteration 89, loss = 0.00126746\n",
            "Iteration 90, loss = 0.00126625\n",
            "Iteration 91, loss = 0.00126505\n",
            "Iteration 92, loss = 0.00126385\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 93, loss = 0.00126309\n",
            "Iteration 94, loss = 0.00126285\n",
            "Iteration 95, loss = 0.00126261\n",
            "Iteration 96, loss = 0.00126237\n",
            "Iteration 97, loss = 0.00126213\n",
            "Iteration 98, loss = 0.00126189\n",
            "Iteration 99, loss = 0.00126166\n",
            "Iteration 100, loss = 0.00126142\n",
            "Iteration 101, loss = 0.00126118\n",
            "Iteration 102, loss = 0.00126094\n",
            "Iteration 103, loss = 0.00126071\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 104, loss = 0.00126055\n",
            "Iteration 105, loss = 0.00126051\n",
            "Iteration 106, loss = 0.00126046\n",
            "Iteration 107, loss = 0.00126041\n",
            "Iteration 108, loss = 0.00126036\n",
            "Iteration 109, loss = 0.00126032\n",
            "Iteration 110, loss = 0.00126027\n",
            "Iteration 111, loss = 0.00126022\n",
            "Iteration 112, loss = 0.00126017\n",
            "Iteration 113, loss = 0.00126013\n",
            "Iteration 114, loss = 0.00126008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 115, loss = 0.00126005\n",
            "Iteration 116, loss = 0.00126004\n",
            "Iteration 117, loss = 0.00126003\n",
            "Iteration 118, loss = 0.00126002\n",
            "Iteration 119, loss = 0.00126001\n",
            "Iteration 120, loss = 0.00126000\n",
            "Iteration 121, loss = 0.00125999\n",
            "Iteration 122, loss = 0.00125998\n",
            "Iteration 123, loss = 0.00125997\n",
            "Iteration 124, loss = 0.00125996\n",
            "Iteration 125, loss = 0.00125995\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.27307514\n",
            "Iteration 3, loss = 0.25202013\n",
            "Iteration 4, loss = 0.23258853\n",
            "Iteration 5, loss = 0.21465518\n",
            "Iteration 6, loss = 0.19810454\n",
            "Iteration 7, loss = 0.18283001\n",
            "Iteration 8, loss = 0.16873321\n",
            "Iteration 9, loss = 0.15572331\n",
            "Iteration 10, loss = 0.14371652\n",
            "Iteration 11, loss = 0.13263549\n",
            "Iteration 12, loss = 0.12240885\n",
            "Iteration 13, loss = 0.11297072\n",
            "Iteration 14, loss = 0.10426029\n",
            "Iteration 15, loss = 0.09622148\n",
            "Iteration 16, loss = 0.08880248\n",
            "Iteration 17, loss = 0.08195551\n",
            "Iteration 18, loss = 0.07563646\n",
            "Iteration 19, loss = 0.06980464\n",
            "Iteration 20, loss = 0.06442246\n",
            "Iteration 21, loss = 0.05945528\n",
            "Iteration 22, loss = 0.05487107\n",
            "Iteration 23, loss = 0.05064033\n",
            "Iteration 24, loss = 0.04673579\n",
            "Iteration 25, loss = 0.04313230\n",
            "Iteration 26, loss = 0.03980666\n",
            "Iteration 27, loss = 0.03673743\n",
            "Iteration 28, loss = 0.03390485\n",
            "Iteration 29, loss = 0.03129067\n",
            "Iteration 30, loss = 0.02887806\n",
            "Iteration 31, loss = 0.02665146\n",
            "Iteration 32, loss = 0.02459655\n",
            "Iteration 33, loss = 0.02270007\n",
            "Iteration 34, loss = 0.02094982\n",
            "Iteration 35, loss = 0.01933452\n",
            "Iteration 36, loss = 0.01784376\n",
            "Iteration 37, loss = 0.01646795\n",
            "Iteration 38, loss = 0.01519821\n",
            "Iteration 39, loss = 0.01402638\n",
            "Iteration 40, loss = 0.01294490\n",
            "Iteration 41, loss = 0.01194680\n",
            "Iteration 42, loss = 0.01102566\n",
            "Iteration 43, loss = 0.01017555\n",
            "Iteration 44, loss = 0.00939098\n",
            "Iteration 45, loss = 0.00866690\n",
            "Iteration 46, loss = 0.00799866\n",
            "Iteration 47, loss = 0.00738193\n",
            "Iteration 48, loss = 0.00681276\n",
            "Iteration 49, loss = 0.00628747\n",
            "Iteration 50, loss = 0.00580269\n",
            "Iteration 51, loss = 0.00535529\n",
            "Iteration 52, loss = 0.00494241\n",
            "Iteration 53, loss = 0.00456145\n",
            "Iteration 54, loss = 0.00421013\n",
            "Iteration 55, loss = 0.00388665\n",
            "Iteration 56, loss = 0.00359004\n",
            "Iteration 57, loss = 0.00332064\n",
            "Iteration 58, loss = 0.00308027\n",
            "Iteration 59, loss = 0.00287096\n",
            "Iteration 60, loss = 0.00269162\n",
            "Iteration 61, loss = 0.00253678\n",
            "Iteration 62, loss = 0.00239975\n",
            "Iteration 63, loss = 0.00227595\n",
            "Iteration 64, loss = 0.00216297\n",
            "Iteration 65, loss = 0.00205948\n",
            "Iteration 66, loss = 0.00196458\n",
            "Iteration 67, loss = 0.00187749\n",
            "Iteration 68, loss = 0.00179756\n",
            "Iteration 69, loss = 0.00172417\n",
            "Iteration 70, loss = 0.00165675\n",
            "Iteration 71, loss = 0.00159480\n",
            "Iteration 72, loss = 0.00153786\n",
            "Iteration 73, loss = 0.00148551\n",
            "Iteration 74, loss = 0.00143739\n",
            "Iteration 75, loss = 0.00139316\n",
            "Iteration 76, loss = 0.00135248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00132803\n",
            "Iteration 78, loss = 0.00132061\n",
            "Iteration 79, loss = 0.00131342\n",
            "Iteration 80, loss = 0.00130634\n",
            "Iteration 81, loss = 0.00129938\n",
            "Iteration 82, loss = 0.00129254\n",
            "Iteration 83, loss = 0.00128580\n",
            "Iteration 84, loss = 0.00127918\n",
            "Iteration 85, loss = 0.00127267\n",
            "Iteration 86, loss = 0.00126626\n",
            "Iteration 87, loss = 0.00125996\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00125598\n",
            "Iteration 89, loss = 0.00125473\n",
            "Iteration 90, loss = 0.00125350\n",
            "Iteration 91, loss = 0.00125227\n",
            "Iteration 92, loss = 0.00125105\n",
            "Iteration 93, loss = 0.00124983\n",
            "Iteration 94, loss = 0.00124862\n",
            "Iteration 95, loss = 0.00124741\n",
            "Iteration 96, loss = 0.00124620\n",
            "Iteration 97, loss = 0.00124500\n",
            "Iteration 98, loss = 0.00124380\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00124304\n",
            "Iteration 100, loss = 0.00124280\n",
            "Iteration 101, loss = 0.00124256\n",
            "Iteration 102, loss = 0.00124232\n",
            "Iteration 103, loss = 0.00124208\n",
            "Iteration 104, loss = 0.00124184\n",
            "Iteration 105, loss = 0.00124161\n",
            "Iteration 106, loss = 0.00124137\n",
            "Iteration 107, loss = 0.00124113\n",
            "Iteration 108, loss = 0.00124089\n",
            "Iteration 109, loss = 0.00124066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00124050\n",
            "Iteration 111, loss = 0.00124046\n",
            "Iteration 112, loss = 0.00124041\n",
            "Iteration 113, loss = 0.00124036\n",
            "Iteration 114, loss = 0.00124031\n",
            "Iteration 115, loss = 0.00124027\n",
            "Iteration 116, loss = 0.00124022\n",
            "Iteration 117, loss = 0.00124017\n",
            "Iteration 118, loss = 0.00124013\n",
            "Iteration 119, loss = 0.00124008\n",
            "Iteration 120, loss = 0.00124003\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00124000\n",
            "Iteration 122, loss = 0.00123999\n",
            "Iteration 123, loss = 0.00123998\n",
            "Iteration 124, loss = 0.00123997\n",
            "Iteration 125, loss = 0.00123996\n",
            "Iteration 126, loss = 0.00123995\n",
            "Iteration 127, loss = 0.00123994\n",
            "Iteration 128, loss = 0.00123993\n",
            "Iteration 129, loss = 0.00123992\n",
            "Iteration 130, loss = 0.00123991\n",
            "Iteration 131, loss = 0.00123991\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33847     0]\n",
            " [ 3774     0]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  5.4min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95     33847\n",
            "           1       0.00      0.00      0.00      3774\n",
            "\n",
            "    accuracy                           0.90     37621\n",
            "   macro avg       0.45      0.50      0.47     37621\n",
            "weighted avg       0.81      0.90      0.85     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Iteration 1, loss = 0.84748268\n",
            "Iteration 2, loss = 0.41454529\n",
            "Iteration 3, loss = 0.38595636\n",
            "Iteration 4, loss = 0.36519672\n",
            "Iteration 5, loss = 0.35079046\n",
            "Iteration 6, loss = 0.34024583\n",
            "Iteration 7, loss = 0.33140415\n",
            "Iteration 8, loss = 0.32704613\n",
            "Iteration 9, loss = 0.32247886\n",
            "Iteration 10, loss = 0.31945477\n",
            "Iteration 11, loss = 0.31682871\n",
            "Iteration 12, loss = 0.31529479\n",
            "Iteration 13, loss = 0.31478712\n",
            "Iteration 14, loss = 0.31562441\n",
            "Iteration 15, loss = 0.31434182\n",
            "Iteration 16, loss = 0.31325954\n",
            "Iteration 17, loss = 0.31357244\n",
            "Iteration 18, loss = 0.31267464\n",
            "Iteration 19, loss = 0.31213219\n",
            "Iteration 20, loss = 0.31233281\n",
            "Iteration 21, loss = 0.31085765\n",
            "Iteration 22, loss = 0.31202217\n",
            "Iteration 23, loss = 0.31211559\n",
            "Iteration 24, loss = 0.31070725\n",
            "Iteration 25, loss = 0.31131160\n",
            "Iteration 26, loss = 0.31151021\n",
            "Iteration 27, loss = 0.31147202\n",
            "Iteration 28, loss = 0.31172220\n",
            "Iteration 29, loss = 0.31132988\n",
            "Iteration 30, loss = 0.31066208\n",
            "Iteration 31, loss = 0.31056618\n",
            "Iteration 32, loss = 0.31176888\n",
            "Iteration 33, loss = 0.31053214\n",
            "Iteration 34, loss = 0.31094069\n",
            "Iteration 35, loss = 0.31252955\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.30424451\n",
            "Iteration 37, loss = 0.30233430\n",
            "Iteration 38, loss = 0.30073074\n",
            "Iteration 39, loss = 0.29919805\n",
            "Iteration 40, loss = 0.29789448\n",
            "Iteration 41, loss = 0.29630499\n",
            "Iteration 42, loss = 0.29473355\n",
            "Iteration 43, loss = 0.29391806\n",
            "Iteration 44, loss = 0.29240380\n",
            "Iteration 45, loss = 0.29278472\n",
            "Iteration 46, loss = 0.29180908\n",
            "Iteration 47, loss = 0.29224415\n",
            "Iteration 48, loss = 0.29139437\n",
            "Iteration 49, loss = 0.29239669\n",
            "Iteration 50, loss = 0.29296488\n",
            "Iteration 51, loss = 0.29154291\n",
            "Iteration 52, loss = 0.29433845\n",
            "Iteration 53, loss = 0.29232066\n",
            "Iteration 54, loss = 0.29506632\n",
            "Iteration 55, loss = 0.29445792\n",
            "Iteration 56, loss = 0.29331951\n",
            "Iteration 57, loss = 0.29356748\n",
            "Iteration 58, loss = 0.29458241\n",
            "Iteration 59, loss = 0.29396317\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 60, loss = 0.27808101\n",
            "Iteration 61, loss = 0.27652196\n",
            "Iteration 62, loss = 0.27598045\n",
            "Iteration 63, loss = 0.27570186\n",
            "Iteration 64, loss = 0.27507744\n",
            "Iteration 65, loss = 0.27468123\n",
            "Iteration 66, loss = 0.27435051\n",
            "Iteration 67, loss = 0.27385207\n",
            "Iteration 68, loss = 0.27337027\n",
            "Iteration 69, loss = 0.27303513\n",
            "Iteration 70, loss = 0.27242644\n",
            "Iteration 71, loss = 0.27214852\n",
            "Iteration 72, loss = 0.27193202\n",
            "Iteration 73, loss = 0.27119323\n",
            "Iteration 74, loss = 0.27090223\n",
            "Iteration 75, loss = 0.27052808\n",
            "Iteration 76, loss = 0.27025382\n",
            "Iteration 77, loss = 0.26984005\n",
            "Iteration 78, loss = 0.26937745\n",
            "Iteration 79, loss = 0.26921457\n",
            "Iteration 80, loss = 0.26871436\n",
            "Iteration 81, loss = 0.26824198\n",
            "Iteration 82, loss = 0.26814522\n",
            "Iteration 83, loss = 0.26753690\n",
            "Iteration 84, loss = 0.26740530\n",
            "Iteration 85, loss = 0.26704463\n",
            "Iteration 86, loss = 0.26635309\n",
            "Iteration 87, loss = 0.26627615\n",
            "Iteration 88, loss = 0.26585883\n",
            "Iteration 89, loss = 0.26567229\n",
            "Iteration 90, loss = 0.26530076\n",
            "Iteration 91, loss = 0.26499722\n",
            "Iteration 92, loss = 0.26471447\n",
            "Iteration 93, loss = 0.26424949\n",
            "Iteration 94, loss = 0.26417029\n",
            "Iteration 95, loss = 0.26370898\n",
            "Iteration 96, loss = 0.26365347\n",
            "Iteration 97, loss = 0.26314142\n",
            "Iteration 98, loss = 0.26307884\n",
            "Iteration 99, loss = 0.26264838\n",
            "Iteration 100, loss = 0.26249977\n",
            "Iteration 101, loss = 0.26230357\n",
            "Iteration 102, loss = 0.26181607\n",
            "Iteration 103, loss = 0.26160895\n",
            "Iteration 104, loss = 0.26164754\n",
            "Iteration 105, loss = 0.26146980\n",
            "Iteration 106, loss = 0.26109342\n",
            "Iteration 107, loss = 0.26091785\n",
            "Iteration 108, loss = 0.26079903\n",
            "Iteration 109, loss = 0.26047814\n",
            "Iteration 110, loss = 0.26028157\n",
            "Iteration 111, loss = 0.25989549\n",
            "Iteration 112, loss = 0.25983183\n",
            "Iteration 113, loss = 0.25952422\n",
            "Iteration 114, loss = 0.25928219\n",
            "Iteration 115, loss = 0.25922041\n",
            "Iteration 116, loss = 0.25921145\n",
            "Iteration 117, loss = 0.25896904\n",
            "Iteration 118, loss = 0.25870821\n",
            "Iteration 119, loss = 0.25875586\n",
            "Iteration 120, loss = 0.25849504\n",
            "Iteration 121, loss = 0.25845734\n",
            "Iteration 122, loss = 0.25820095\n",
            "Iteration 123, loss = 0.25802117\n",
            "Iteration 124, loss = 0.25789952\n",
            "Iteration 125, loss = 0.25764246\n",
            "Iteration 126, loss = 0.25735278\n",
            "Iteration 127, loss = 0.25720323\n",
            "Iteration 128, loss = 0.25721126\n",
            "Iteration 129, loss = 0.25712064\n",
            "Iteration 130, loss = 0.25672417\n",
            "Iteration 131, loss = 0.25667844\n",
            "Iteration 132, loss = 0.25667540\n",
            "Iteration 133, loss = 0.25675883\n",
            "Iteration 134, loss = 0.25654644\n",
            "Iteration 135, loss = 0.25606010\n",
            "Iteration 136, loss = 0.25596414\n",
            "Iteration 137, loss = 0.25593021\n",
            "Iteration 138, loss = 0.25575533\n",
            "Iteration 139, loss = 0.25563804\n",
            "Iteration 140, loss = 0.25582338\n",
            "Iteration 141, loss = 0.25534469\n",
            "Iteration 142, loss = 0.25519318\n",
            "Iteration 143, loss = 0.25544084\n",
            "Iteration 144, loss = 0.25543025\n",
            "Iteration 145, loss = 0.25486376\n",
            "Iteration 146, loss = 0.25507094\n",
            "Iteration 147, loss = 0.25484813\n",
            "Iteration 148, loss = 0.25483807\n",
            "Iteration 149, loss = 0.25478985\n",
            "Iteration 150, loss = 0.25458754\n",
            "Iteration 151, loss = 0.25425980\n",
            "Iteration 152, loss = 0.25452369\n",
            "Iteration 153, loss = 0.25409086\n",
            "Iteration 154, loss = 0.25406077\n",
            "Iteration 155, loss = 0.25425089\n",
            "Iteration 156, loss = 0.25410081\n",
            "Iteration 157, loss = 0.25397574\n",
            "Iteration 158, loss = 0.25370835\n",
            "Iteration 159, loss = 0.25391892\n",
            "Iteration 160, loss = 0.25374954\n",
            "Iteration 161, loss = 0.25351027\n",
            "Iteration 162, loss = 0.25352222\n",
            "Iteration 163, loss = 0.25311468\n",
            "Iteration 164, loss = 0.25295444\n",
            "Iteration 165, loss = 0.25339540\n",
            "Iteration 166, loss = 0.25271915\n",
            "Iteration 167, loss = 0.25327351\n",
            "Iteration 168, loss = 0.25272794\n",
            "Iteration 169, loss = 0.25294895\n",
            "Iteration 170, loss = 0.25265362\n",
            "Iteration 171, loss = 0.25295101\n",
            "Iteration 172, loss = 0.25270679\n",
            "Iteration 173, loss = 0.25285631\n",
            "Iteration 174, loss = 0.25260109\n",
            "Iteration 175, loss = 0.25203555\n",
            "Iteration 176, loss = 0.25215114\n",
            "Iteration 177, loss = 0.25238003\n",
            "Iteration 178, loss = 0.25225623\n",
            "Iteration 179, loss = 0.25219738\n",
            "Iteration 180, loss = 0.25184254\n",
            "Iteration 181, loss = 0.25215469\n",
            "Iteration 182, loss = 0.25233561\n",
            "Iteration 183, loss = 0.25189895\n",
            "Iteration 184, loss = 0.25168496\n",
            "Iteration 185, loss = 0.25208220\n",
            "Iteration 186, loss = 0.25146565\n",
            "Iteration 187, loss = 0.25138918\n",
            "Iteration 188, loss = 0.25108832\n",
            "Iteration 189, loss = 0.25156245\n",
            "Iteration 190, loss = 0.25127988\n",
            "Iteration 191, loss = 0.25123650\n",
            "Iteration 192, loss = 0.25150326\n",
            "Iteration 193, loss = 0.25127806\n",
            "Iteration 194, loss = 0.25148952\n",
            "Iteration 195, loss = 0.25127424\n",
            "Iteration 196, loss = 0.25082387\n",
            "Iteration 197, loss = 0.25130728\n",
            "Iteration 198, loss = 0.25090269\n",
            "Iteration 199, loss = 0.25080319\n",
            "Iteration 200, loss = 0.25055385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.13272186\n",
            "Iteration 2, loss = 0.10103985\n",
            "Iteration 3, loss = 0.07692643\n",
            "Iteration 4, loss = 0.05857743\n",
            "Iteration 5, loss = 0.04461726\n",
            "Iteration 6, loss = 0.03399920\n",
            "Iteration 7, loss = 0.02592684\n",
            "Iteration 8, loss = 0.01979423\n",
            "Iteration 9, loss = 0.01514024\n",
            "Iteration 10, loss = 0.01161411\n",
            "Iteration 11, loss = 0.00894846\n",
            "Iteration 12, loss = 0.00693874\n",
            "Iteration 13, loss = 0.00542787\n",
            "Iteration 14, loss = 0.00429467\n",
            "Iteration 15, loss = 0.00344599\n",
            "Iteration 16, loss = 0.00281096\n",
            "Iteration 17, loss = 0.00233570\n",
            "Iteration 18, loss = 0.00197923\n",
            "Iteration 19, loss = 0.00171079\n",
            "Iteration 20, loss = 0.00150823\n",
            "Iteration 21, loss = 0.00135508\n",
            "Iteration 22, loss = 0.00123908\n",
            "Iteration 23, loss = 0.00115111\n",
            "Iteration 24, loss = 0.00108432\n",
            "Iteration 25, loss = 0.00103358\n",
            "Iteration 26, loss = 0.00099501\n",
            "Iteration 27, loss = 0.00096567\n",
            "Iteration 28, loss = 0.00094334\n",
            "Iteration 29, loss = 0.00092634\n",
            "Iteration 30, loss = 0.00091340\n",
            "Iteration 31, loss = 0.00090353\n",
            "Iteration 32, loss = 0.00089601\n",
            "Iteration 33, loss = 0.00089027\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00088740\n",
            "Iteration 35, loss = 0.00088657\n",
            "Iteration 36, loss = 0.00088579\n",
            "Iteration 37, loss = 0.00088505\n",
            "Iteration 38, loss = 0.00088435\n",
            "Iteration 39, loss = 0.00088369\n",
            "Iteration 40, loss = 0.00088306\n",
            "Iteration 41, loss = 0.00088246\n",
            "Iteration 42, loss = 0.00088189\n",
            "Iteration 43, loss = 0.00088135\n",
            "Iteration 44, loss = 0.00088084\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00088054\n",
            "Iteration 46, loss = 0.00088044\n",
            "Iteration 47, loss = 0.00088035\n",
            "Iteration 48, loss = 0.00088025\n",
            "Iteration 49, loss = 0.00088016\n",
            "Iteration 50, loss = 0.00088007\n",
            "Iteration 51, loss = 0.00087998\n",
            "Iteration 52, loss = 0.00087989\n",
            "Iteration 53, loss = 0.00087980\n",
            "Iteration 54, loss = 0.00087971\n",
            "Iteration 55, loss = 0.00087962\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00087957\n",
            "Iteration 57, loss = 0.00087955\n",
            "Iteration 58, loss = 0.00087954\n",
            "Iteration 59, loss = 0.00087952\n",
            "Iteration 60, loss = 0.00087950\n",
            "Iteration 61, loss = 0.00087948\n",
            "Iteration 62, loss = 0.00087947\n",
            "Iteration 63, loss = 0.00087945\n",
            "Iteration 64, loss = 0.00087943\n",
            "Iteration 65, loss = 0.00087942\n",
            "Iteration 66, loss = 0.00087940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00087939\n",
            "Iteration 68, loss = 0.00087939\n",
            "Iteration 69, loss = 0.00087938\n",
            "Iteration 70, loss = 0.00087938\n",
            "Iteration 71, loss = 0.00087938\n",
            "Iteration 72, loss = 0.00087937\n",
            "Iteration 73, loss = 0.00087937\n",
            "Iteration 74, loss = 0.00087937\n",
            "Iteration 75, loss = 0.00087936\n",
            "Iteration 76, loss = 0.00087936\n",
            "Iteration 77, loss = 0.00087936\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00087935\n",
            "Iteration 79, loss = 0.00087935\n",
            "Iteration 80, loss = 0.00087935\n",
            "Iteration 81, loss = 0.00087935\n",
            "Iteration 82, loss = 0.00087935\n",
            "Iteration 83, loss = 0.00087935\n",
            "Iteration 84, loss = 0.00087935\n",
            "Iteration 85, loss = 0.00087935\n",
            "Iteration 86, loss = 0.00087935\n",
            "Iteration 87, loss = 0.00087935\n",
            "Iteration 88, loss = 0.00087935\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.32030779\n",
            "Iteration 2, loss = 0.20387856\n",
            "Iteration 3, loss = 0.15518364\n",
            "Iteration 4, loss = 0.11811915\n",
            "Iteration 5, loss = 0.08990724\n",
            "Iteration 6, loss = 0.06843355\n",
            "Iteration 7, loss = 0.05208869\n",
            "Iteration 8, loss = 0.03964768\n",
            "Iteration 9, loss = 0.03017812\n",
            "Iteration 10, loss = 0.02297029\n",
            "Iteration 11, loss = 0.01748401\n",
            "Iteration 12, loss = 0.01330808\n",
            "Iteration 13, loss = 0.01012954\n",
            "Iteration 14, loss = 0.00771017\n",
            "Iteration 15, loss = 0.00586865\n",
            "Iteration 16, loss = 0.00446698\n",
            "Iteration 17, loss = 0.00340093\n",
            "Iteration 18, loss = 0.00261016\n",
            "Iteration 19, loss = 0.00211014\n",
            "Iteration 20, loss = 0.00180473\n",
            "Iteration 21, loss = 0.00158297\n",
            "Iteration 22, loss = 0.00141749\n",
            "Iteration 23, loss = 0.00129351\n",
            "Iteration 24, loss = 0.00120033\n",
            "Iteration 25, loss = 0.00113012\n",
            "Iteration 26, loss = 0.00107709\n",
            "Iteration 27, loss = 0.00103697\n",
            "Iteration 28, loss = 0.00100656\n",
            "Iteration 29, loss = 0.00098348\n",
            "Iteration 30, loss = 0.00096595\n",
            "Iteration 31, loss = 0.00095263\n",
            "Iteration 32, loss = 0.00094248\n",
            "Iteration 33, loss = 0.00093476\n",
            "Iteration 34, loss = 0.00092887\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00092593\n",
            "Iteration 36, loss = 0.00092509\n",
            "Iteration 37, loss = 0.00092429\n",
            "Iteration 38, loss = 0.00092353\n",
            "Iteration 39, loss = 0.00092282\n",
            "Iteration 40, loss = 0.00092214\n",
            "Iteration 41, loss = 0.00092150\n",
            "Iteration 42, loss = 0.00092089\n",
            "Iteration 43, loss = 0.00092031\n",
            "Iteration 44, loss = 0.00091976\n",
            "Iteration 45, loss = 0.00091924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00091893\n",
            "Iteration 47, loss = 0.00091884\n",
            "Iteration 48, loss = 0.00091874\n",
            "Iteration 49, loss = 0.00091864\n",
            "Iteration 50, loss = 0.00091855\n",
            "Iteration 51, loss = 0.00091846\n",
            "Iteration 52, loss = 0.00091836\n",
            "Iteration 53, loss = 0.00091827\n",
            "Iteration 54, loss = 0.00091818\n",
            "Iteration 55, loss = 0.00091809\n",
            "Iteration 56, loss = 0.00091800\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00091795\n",
            "Iteration 58, loss = 0.00091793\n",
            "Iteration 59, loss = 0.00091792\n",
            "Iteration 60, loss = 0.00091790\n",
            "Iteration 61, loss = 0.00091788\n",
            "Iteration 62, loss = 0.00091786\n",
            "Iteration 63, loss = 0.00091785\n",
            "Iteration 64, loss = 0.00091783\n",
            "Iteration 65, loss = 0.00091781\n",
            "Iteration 66, loss = 0.00091779\n",
            "Iteration 67, loss = 0.00091778\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00091777\n",
            "Iteration 69, loss = 0.00091776\n",
            "Iteration 70, loss = 0.00091776\n",
            "Iteration 71, loss = 0.00091776\n",
            "Iteration 72, loss = 0.00091775\n",
            "Iteration 73, loss = 0.00091775\n",
            "Iteration 74, loss = 0.00091775\n",
            "Iteration 75, loss = 0.00091774\n",
            "Iteration 76, loss = 0.00091774\n",
            "Iteration 77, loss = 0.00091774\n",
            "Iteration 78, loss = 0.00091773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00091773\n",
            "Iteration 80, loss = 0.00091773\n",
            "Iteration 81, loss = 0.00091773\n",
            "Iteration 82, loss = 0.00091773\n",
            "Iteration 83, loss = 0.00091773\n",
            "Iteration 84, loss = 0.00091773\n",
            "Iteration 85, loss = 0.00091773\n",
            "Iteration 86, loss = 0.00091773\n",
            "Iteration 87, loss = 0.00091773\n",
            "Iteration 88, loss = 0.00091772\n",
            "Iteration 89, loss = 0.00091772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13196226\n",
            "Iteration 2, loss = 0.10045395\n",
            "Iteration 3, loss = 0.07647013\n",
            "Iteration 4, loss = 0.05822644\n",
            "Iteration 5, loss = 0.04434673\n",
            "Iteration 6, loss = 0.03378926\n",
            "Iteration 7, loss = 0.02576230\n",
            "Iteration 8, loss = 0.01966361\n",
            "Iteration 9, loss = 0.01503557\n",
            "Iteration 10, loss = 0.01152933\n",
            "Iteration 11, loss = 0.00887920\n",
            "Iteration 12, loss = 0.00688238\n",
            "Iteration 13, loss = 0.00538293\n",
            "Iteration 14, loss = 0.00426006\n",
            "Iteration 15, loss = 0.00342043\n",
            "Iteration 16, loss = 0.00279261\n",
            "Iteration 17, loss = 0.00232266\n",
            "Iteration 18, loss = 0.00197033\n",
            "Iteration 19, loss = 0.00170574\n",
            "Iteration 20, loss = 0.00150652\n",
            "Iteration 21, loss = 0.00135601\n",
            "Iteration 22, loss = 0.00124186\n",
            "Iteration 23, loss = 0.00115545\n",
            "Iteration 24, loss = 0.00108997\n",
            "Iteration 25, loss = 0.00104029\n",
            "Iteration 26, loss = 0.00100257\n",
            "Iteration 27, loss = 0.00097391\n",
            "Iteration 28, loss = 0.00095211\n",
            "Iteration 29, loss = 0.00093553\n",
            "Iteration 30, loss = 0.00092290\n",
            "Iteration 31, loss = 0.00091329\n",
            "Iteration 32, loss = 0.00090595\n",
            "Iteration 33, loss = 0.00090036\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.00089756\n",
            "Iteration 35, loss = 0.00089675\n",
            "Iteration 36, loss = 0.00089600\n",
            "Iteration 37, loss = 0.00089528\n",
            "Iteration 38, loss = 0.00089460\n",
            "Iteration 39, loss = 0.00089395\n",
            "Iteration 40, loss = 0.00089334\n",
            "Iteration 41, loss = 0.00089276\n",
            "Iteration 42, loss = 0.00089221\n",
            "Iteration 43, loss = 0.00089169\n",
            "Iteration 44, loss = 0.00089119\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.00089090\n",
            "Iteration 46, loss = 0.00089080\n",
            "Iteration 47, loss = 0.00089071\n",
            "Iteration 48, loss = 0.00089062\n",
            "Iteration 49, loss = 0.00089053\n",
            "Iteration 50, loss = 0.00089044\n",
            "Iteration 51, loss = 0.00089035\n",
            "Iteration 52, loss = 0.00089027\n",
            "Iteration 53, loss = 0.00089018\n",
            "Iteration 54, loss = 0.00089009\n",
            "Iteration 55, loss = 0.00089001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 56, loss = 0.00088996\n",
            "Iteration 57, loss = 0.00088994\n",
            "Iteration 58, loss = 0.00088992\n",
            "Iteration 59, loss = 0.00088991\n",
            "Iteration 60, loss = 0.00088989\n",
            "Iteration 61, loss = 0.00088987\n",
            "Iteration 62, loss = 0.00088986\n",
            "Iteration 63, loss = 0.00088984\n",
            "Iteration 64, loss = 0.00088982\n",
            "Iteration 65, loss = 0.00088981\n",
            "Iteration 66, loss = 0.00088979\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 67, loss = 0.00088978\n",
            "Iteration 68, loss = 0.00088978\n",
            "Iteration 69, loss = 0.00088978\n",
            "Iteration 70, loss = 0.00088977\n",
            "Iteration 71, loss = 0.00088977\n",
            "Iteration 72, loss = 0.00088977\n",
            "Iteration 73, loss = 0.00088976\n",
            "Iteration 74, loss = 0.00088976\n",
            "Iteration 75, loss = 0.00088976\n",
            "Iteration 76, loss = 0.00088975\n",
            "Iteration 77, loss = 0.00088975\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 78, loss = 0.00088975\n",
            "Iteration 79, loss = 0.00088975\n",
            "Iteration 80, loss = 0.00088975\n",
            "Iteration 81, loss = 0.00088975\n",
            "Iteration 82, loss = 0.00088974\n",
            "Iteration 83, loss = 0.00088974\n",
            "Iteration 84, loss = 0.00088974\n",
            "Iteration 85, loss = 0.00088974\n",
            "Iteration 86, loss = 0.00088974\n",
            "Iteration 87, loss = 0.00088974\n",
            "Iteration 88, loss = 0.00088974\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.19706824\n",
            "Iteration 3, loss = 0.14999992\n",
            "Iteration 4, loss = 0.11417352\n",
            "Iteration 5, loss = 0.08690400\n",
            "Iteration 6, loss = 0.06614761\n",
            "Iteration 7, loss = 0.05034873\n",
            "Iteration 8, loss = 0.03832330\n",
            "Iteration 9, loss = 0.02917006\n",
            "Iteration 10, loss = 0.02220300\n",
            "Iteration 11, loss = 0.01689997\n",
            "Iteration 12, loss = 0.01286354\n",
            "Iteration 13, loss = 0.00979117\n",
            "Iteration 14, loss = 0.00745262\n",
            "Iteration 15, loss = 0.00567262\n",
            "Iteration 16, loss = 0.00431783\n",
            "Iteration 17, loss = 0.00329021\n",
            "Iteration 18, loss = 0.00255399\n",
            "Iteration 19, loss = 0.00209533\n",
            "Iteration 20, loss = 0.00178446\n",
            "Iteration 21, loss = 0.00155698\n",
            "Iteration 22, loss = 0.00138923\n",
            "Iteration 23, loss = 0.00126492\n",
            "Iteration 24, loss = 0.00117237\n",
            "Iteration 25, loss = 0.00110318\n",
            "Iteration 26, loss = 0.00105126\n",
            "Iteration 27, loss = 0.00101216\n",
            "Iteration 28, loss = 0.00098265\n",
            "Iteration 29, loss = 0.00096032\n",
            "Iteration 30, loss = 0.00094340\n",
            "Iteration 31, loss = 0.00093056\n",
            "Iteration 32, loss = 0.00092081\n",
            "Iteration 33, loss = 0.00091339\n",
            "Iteration 34, loss = 0.00090774\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00090492\n",
            "Iteration 36, loss = 0.00090411\n",
            "Iteration 37, loss = 0.00090335\n",
            "Iteration 38, loss = 0.00090263\n",
            "Iteration 39, loss = 0.00090194\n",
            "Iteration 40, loss = 0.00090129\n",
            "Iteration 41, loss = 0.00090068\n",
            "Iteration 42, loss = 0.00090009\n",
            "Iteration 43, loss = 0.00089954\n",
            "Iteration 44, loss = 0.00089902\n",
            "Iteration 45, loss = 0.00089852\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00089823\n",
            "Iteration 47, loss = 0.00089813\n",
            "Iteration 48, loss = 0.00089804\n",
            "Iteration 49, loss = 0.00089795\n",
            "Iteration 50, loss = 0.00089786\n",
            "Iteration 51, loss = 0.00089777\n",
            "Iteration 52, loss = 0.00089768\n",
            "Iteration 53, loss = 0.00089760\n",
            "Iteration 54, loss = 0.00089751\n",
            "Iteration 55, loss = 0.00089742\n",
            "Iteration 56, loss = 0.00089734\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00089729\n",
            "Iteration 58, loss = 0.00089727\n",
            "Iteration 59, loss = 0.00089725\n",
            "Iteration 60, loss = 0.00089724\n",
            "Iteration 61, loss = 0.00089722\n",
            "Iteration 62, loss = 0.00089720\n",
            "Iteration 63, loss = 0.00089719\n",
            "Iteration 64, loss = 0.00089717\n",
            "Iteration 65, loss = 0.00089716\n",
            "Iteration 66, loss = 0.00089714\n",
            "Iteration 67, loss = 0.00089712\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00089711\n",
            "Iteration 69, loss = 0.00089711\n",
            "Iteration 70, loss = 0.00089711\n",
            "Iteration 71, loss = 0.00089710\n",
            "Iteration 72, loss = 0.00089710\n",
            "Iteration 73, loss = 0.00089710\n",
            "Iteration 74, loss = 0.00089709\n",
            "Iteration 75, loss = 0.00089709\n",
            "Iteration 76, loss = 0.00089709\n",
            "Iteration 77, loss = 0.00089708\n",
            "Iteration 78, loss = 0.00089708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00089708\n",
            "Iteration 80, loss = 0.00089708\n",
            "Iteration 81, loss = 0.00089708\n",
            "Iteration 82, loss = 0.00089708\n",
            "Iteration 83, loss = 0.00089708\n",
            "Iteration 84, loss = 0.00089707\n",
            "Iteration 85, loss = 0.00089707\n",
            "Iteration 86, loss = 0.00089707\n",
            "Iteration 87, loss = 0.00089707\n",
            "Iteration 88, loss = 0.00089707\n",
            "Iteration 89, loss = 0.00089707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.31401148\n",
            "Iteration 2, loss = 0.23304152\n",
            "Iteration 3, loss = 0.17738124\n",
            "Iteration 4, loss = 0.13501501\n",
            "Iteration 5, loss = 0.10276765\n",
            "Iteration 6, loss = 0.07822234\n",
            "Iteration 7, loss = 0.05953950\n",
            "Iteration 8, loss = 0.04531892\n",
            "Iteration 9, loss = 0.03449482\n",
            "Iteration 10, loss = 0.02625598\n",
            "Iteration 11, loss = 0.01998493\n",
            "Iteration 12, loss = 0.01521168\n",
            "Iteration 13, loss = 0.01157848\n",
            "Iteration 14, loss = 0.00881304\n",
            "Iteration 15, loss = 0.00670811\n",
            "Iteration 16, loss = 0.00510593\n",
            "Iteration 17, loss = 0.00388642\n",
            "Iteration 18, loss = 0.00295887\n",
            "Iteration 19, loss = 0.00226998\n",
            "Iteration 20, loss = 0.00183891\n",
            "Iteration 21, loss = 0.00159041\n",
            "Iteration 22, loss = 0.00141422\n",
            "Iteration 23, loss = 0.00128382\n",
            "Iteration 24, loss = 0.00118674\n",
            "Iteration 25, loss = 0.00111404\n",
            "Iteration 26, loss = 0.00105882\n",
            "Iteration 27, loss = 0.00101719\n",
            "Iteration 28, loss = 0.00098604\n",
            "Iteration 29, loss = 0.00096264\n",
            "Iteration 30, loss = 0.00094504\n",
            "Iteration 31, loss = 0.00093177\n",
            "Iteration 32, loss = 0.00092174\n",
            "Iteration 33, loss = 0.00091413\n",
            "Iteration 34, loss = 0.00090837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00090549\n",
            "Iteration 36, loss = 0.00090467\n",
            "Iteration 37, loss = 0.00090389\n",
            "Iteration 38, loss = 0.00090315\n",
            "Iteration 39, loss = 0.00090245\n",
            "Iteration 40, loss = 0.00090179\n",
            "Iteration 41, loss = 0.00090117\n",
            "Iteration 42, loss = 0.00090057\n",
            "Iteration 43, loss = 0.00090001\n",
            "Iteration 44, loss = 0.00089947\n",
            "Iteration 45, loss = 0.00089897\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00089867\n",
            "Iteration 47, loss = 0.00089858\n",
            "Iteration 48, loss = 0.00089848\n",
            "Iteration 49, loss = 0.00089839\n",
            "Iteration 50, loss = 0.00089830\n",
            "Iteration 51, loss = 0.00089821\n",
            "Iteration 52, loss = 0.00089812\n",
            "Iteration 53, loss = 0.00089803\n",
            "Iteration 54, loss = 0.00089794\n",
            "Iteration 55, loss = 0.00089785\n",
            "Iteration 56, loss = 0.00089777\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00089771\n",
            "Iteration 58, loss = 0.00089770\n",
            "Iteration 59, loss = 0.00089768\n",
            "Iteration 60, loss = 0.00089766\n",
            "Iteration 61, loss = 0.00089765\n",
            "Iteration 62, loss = 0.00089763\n",
            "Iteration 63, loss = 0.00089761\n",
            "Iteration 64, loss = 0.00089760\n",
            "Iteration 65, loss = 0.00089758\n",
            "Iteration 66, loss = 0.00089756\n",
            "Iteration 67, loss = 0.00089755\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00089753\n",
            "Iteration 69, loss = 0.00089753\n",
            "Iteration 70, loss = 0.00089753\n",
            "Iteration 71, loss = 0.00089752\n",
            "Iteration 72, loss = 0.00089752\n",
            "Iteration 73, loss = 0.00089752\n",
            "Iteration 74, loss = 0.00089751\n",
            "Iteration 75, loss = 0.00089751\n",
            "Iteration 76, loss = 0.00089751\n",
            "Iteration 77, loss = 0.00089750\n",
            "Iteration 78, loss = 0.00089750\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00089750\n",
            "Iteration 80, loss = 0.00089750\n",
            "Iteration 81, loss = 0.00089750\n",
            "Iteration 82, loss = 0.00089750\n",
            "Iteration 83, loss = 0.00089750\n",
            "Iteration 84, loss = 0.00089750\n",
            "Iteration 85, loss = 0.00089750\n",
            "Iteration 86, loss = 0.00089749\n",
            "Iteration 87, loss = 0.00089749\n",
            "Iteration 88, loss = 0.00089749\n",
            "Iteration 89, loss = 0.00089749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.30796398\n",
            "Iteration 2, loss = 0.21044771\n",
            "Iteration 3, loss = 0.16018379\n",
            "Iteration 4, loss = 0.12192505\n",
            "Iteration 5, loss = 0.09280413\n",
            "Iteration 6, loss = 0.07063854\n",
            "Iteration 7, loss = 0.05376703\n",
            "Iteration 8, loss = 0.04092517\n",
            "Iteration 9, loss = 0.03115049\n",
            "Iteration 10, loss = 0.02371042\n",
            "Iteration 11, loss = 0.01804736\n",
            "Iteration 12, loss = 0.01373688\n",
            "Iteration 13, loss = 0.01045592\n",
            "Iteration 14, loss = 0.00795860\n",
            "Iteration 15, loss = 0.00605775\n",
            "Iteration 16, loss = 0.00461090\n",
            "Iteration 17, loss = 0.00350994\n",
            "Iteration 18, loss = 0.00268208\n",
            "Iteration 19, loss = 0.00212961\n",
            "Iteration 20, loss = 0.00180251\n",
            "Iteration 21, loss = 0.00157271\n",
            "Iteration 22, loss = 0.00140300\n",
            "Iteration 23, loss = 0.00127713\n",
            "Iteration 24, loss = 0.00118336\n",
            "Iteration 25, loss = 0.00111320\n",
            "Iteration 26, loss = 0.00106051\n",
            "Iteration 27, loss = 0.00102081\n",
            "Iteration 28, loss = 0.00099083\n",
            "Iteration 29, loss = 0.00096813\n",
            "Iteration 30, loss = 0.00095091\n",
            "Iteration 31, loss = 0.00093783\n",
            "Iteration 32, loss = 0.00092786\n",
            "Iteration 33, loss = 0.00092027\n",
            "Iteration 34, loss = 0.00091451\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00091164\n",
            "Iteration 36, loss = 0.00091082\n",
            "Iteration 37, loss = 0.00091004\n",
            "Iteration 38, loss = 0.00090931\n",
            "Iteration 39, loss = 0.00090861\n",
            "Iteration 40, loss = 0.00090795\n",
            "Iteration 41, loss = 0.00090732\n",
            "Iteration 42, loss = 0.00090673\n",
            "Iteration 43, loss = 0.00090617\n",
            "Iteration 44, loss = 0.00090563\n",
            "Iteration 45, loss = 0.00090513\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00090483\n",
            "Iteration 47, loss = 0.00090474\n",
            "Iteration 48, loss = 0.00090464\n",
            "Iteration 49, loss = 0.00090455\n",
            "Iteration 50, loss = 0.00090446\n",
            "Iteration 51, loss = 0.00090437\n",
            "Iteration 52, loss = 0.00090428\n",
            "Iteration 53, loss = 0.00090419\n",
            "Iteration 54, loss = 0.00090410\n",
            "Iteration 55, loss = 0.00090402\n",
            "Iteration 56, loss = 0.00090393\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00090388\n",
            "Iteration 58, loss = 0.00090386\n",
            "Iteration 59, loss = 0.00090384\n",
            "Iteration 60, loss = 0.00090383\n",
            "Iteration 61, loss = 0.00090381\n",
            "Iteration 62, loss = 0.00090379\n",
            "Iteration 63, loss = 0.00090378\n",
            "Iteration 64, loss = 0.00090376\n",
            "Iteration 65, loss = 0.00090374\n",
            "Iteration 66, loss = 0.00090373\n",
            "Iteration 67, loss = 0.00090371\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00090370\n",
            "Iteration 69, loss = 0.00090370\n",
            "Iteration 70, loss = 0.00090369\n",
            "Iteration 71, loss = 0.00090369\n",
            "Iteration 72, loss = 0.00090369\n",
            "Iteration 73, loss = 0.00090368\n",
            "Iteration 74, loss = 0.00090368\n",
            "Iteration 75, loss = 0.00090368\n",
            "Iteration 76, loss = 0.00090367\n",
            "Iteration 77, loss = 0.00090367\n",
            "Iteration 78, loss = 0.00090367\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00090366\n",
            "Iteration 80, loss = 0.00090366\n",
            "Iteration 81, loss = 0.00090366\n",
            "Iteration 82, loss = 0.00090366\n",
            "Iteration 83, loss = 0.00090366\n",
            "Iteration 84, loss = 0.00090366\n",
            "Iteration 85, loss = 0.00090366\n",
            "Iteration 86, loss = 0.00090366\n",
            "Iteration 87, loss = 0.00090366\n",
            "Iteration 88, loss = 0.00090366\n",
            "Iteration 89, loss = 0.00090366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23072797\n",
            "Iteration 3, loss = 0.17562026\n",
            "Iteration 4, loss = 0.13367463\n",
            "Iteration 5, loss = 0.10174741\n",
            "Iteration 6, loss = 0.07744578\n",
            "Iteration 7, loss = 0.05894841\n",
            "Iteration 8, loss = 0.04486901\n",
            "Iteration 9, loss = 0.03415237\n",
            "Iteration 10, loss = 0.02599532\n",
            "Iteration 11, loss = 0.01978653\n",
            "Iteration 12, loss = 0.01506066\n",
            "Iteration 13, loss = 0.01146353\n",
            "Iteration 14, loss = 0.00872555\n",
            "Iteration 15, loss = 0.00664151\n",
            "Iteration 16, loss = 0.00505524\n",
            "Iteration 17, loss = 0.00384784\n",
            "Iteration 18, loss = 0.00292947\n",
            "Iteration 19, loss = 0.00224673\n",
            "Iteration 20, loss = 0.00181599\n",
            "Iteration 21, loss = 0.00156515\n",
            "Iteration 22, loss = 0.00138873\n",
            "Iteration 23, loss = 0.00125969\n",
            "Iteration 24, loss = 0.00116463\n",
            "Iteration 25, loss = 0.00109407\n",
            "Iteration 26, loss = 0.00104152\n",
            "Iteration 27, loss = 0.00100233\n",
            "Iteration 28, loss = 0.00097297\n",
            "Iteration 29, loss = 0.00095088\n",
            "Iteration 30, loss = 0.00093422\n",
            "Iteration 31, loss = 0.00092163\n",
            "Iteration 32, loss = 0.00091210\n",
            "Iteration 33, loss = 0.00090486\n",
            "Iteration 34, loss = 0.00089936\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00089662\n",
            "Iteration 36, loss = 0.00089584\n",
            "Iteration 37, loss = 0.00089509\n",
            "Iteration 38, loss = 0.00089439\n",
            "Iteration 39, loss = 0.00089372\n",
            "Iteration 40, loss = 0.00089309\n",
            "Iteration 41, loss = 0.00089249\n",
            "Iteration 42, loss = 0.00089193\n",
            "Iteration 43, loss = 0.00089139\n",
            "Iteration 44, loss = 0.00089088\n",
            "Iteration 45, loss = 0.00089040\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00089011\n",
            "Iteration 47, loss = 0.00089002\n",
            "Iteration 48, loss = 0.00088993\n",
            "Iteration 49, loss = 0.00088985\n",
            "Iteration 50, loss = 0.00088976\n",
            "Iteration 51, loss = 0.00088967\n",
            "Iteration 52, loss = 0.00088959\n",
            "Iteration 53, loss = 0.00088950\n",
            "Iteration 54, loss = 0.00088942\n",
            "Iteration 55, loss = 0.00088933\n",
            "Iteration 56, loss = 0.00088925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00088920\n",
            "Iteration 58, loss = 0.00088919\n",
            "Iteration 59, loss = 0.00088917\n",
            "Iteration 60, loss = 0.00088915\n",
            "Iteration 61, loss = 0.00088914\n",
            "Iteration 62, loss = 0.00088912\n",
            "Iteration 63, loss = 0.00088911\n",
            "Iteration 64, loss = 0.00088909\n",
            "Iteration 65, loss = 0.00088907\n",
            "Iteration 66, loss = 0.00088906\n",
            "Iteration 67, loss = 0.00088904\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00088903\n",
            "Iteration 69, loss = 0.00088903\n",
            "Iteration 70, loss = 0.00088903\n",
            "Iteration 71, loss = 0.00088902\n",
            "Iteration 72, loss = 0.00088902\n",
            "Iteration 73, loss = 0.00088902\n",
            "Iteration 74, loss = 0.00088901\n",
            "Iteration 75, loss = 0.00088901\n",
            "Iteration 76, loss = 0.00088901\n",
            "Iteration 77, loss = 0.00088900\n",
            "Iteration 78, loss = 0.00088900\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00088900\n",
            "Iteration 80, loss = 0.00088900\n",
            "Iteration 81, loss = 0.00088900\n",
            "Iteration 82, loss = 0.00088900\n",
            "Iteration 83, loss = 0.00088900\n",
            "Iteration 84, loss = 0.00088899\n",
            "Iteration 85, loss = 0.00088899\n",
            "Iteration 86, loss = 0.00088899\n",
            "Iteration 87, loss = 0.00088899\n",
            "Iteration 88, loss = 0.00088899\n",
            "Iteration 89, loss = 0.00088899\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.21577761\n",
            "Iteration 3, loss = 0.16424069\n",
            "Iteration 4, loss = 0.12501299\n",
            "Iteration 5, loss = 0.09515454\n",
            "Iteration 6, loss = 0.07242756\n",
            "Iteration 7, loss = 0.05512876\n",
            "Iteration 8, loss = 0.04196166\n",
            "Iteration 9, loss = 0.03193942\n",
            "Iteration 10, loss = 0.02431092\n",
            "Iteration 11, loss = 0.01850443\n",
            "Iteration 12, loss = 0.01408478\n",
            "Iteration 13, loss = 0.01072073\n",
            "Iteration 14, loss = 0.00816016\n",
            "Iteration 15, loss = 0.00621117\n",
            "Iteration 16, loss = 0.00472768\n",
            "Iteration 17, loss = 0.00359889\n",
            "Iteration 18, loss = 0.00275141\n",
            "Iteration 19, loss = 0.00219082\n",
            "Iteration 20, loss = 0.00185853\n",
            "Iteration 21, loss = 0.00162152\n",
            "Iteration 22, loss = 0.00144453\n",
            "Iteration 23, loss = 0.00131189\n",
            "Iteration 24, loss = 0.00121218\n",
            "Iteration 25, loss = 0.00113702\n",
            "Iteration 26, loss = 0.00108022\n",
            "Iteration 27, loss = 0.00103724\n",
            "Iteration 28, loss = 0.00100465\n",
            "Iteration 29, loss = 0.00097992\n",
            "Iteration 30, loss = 0.00096114\n",
            "Iteration 31, loss = 0.00094685\n",
            "Iteration 32, loss = 0.00093596\n",
            "Iteration 33, loss = 0.00092767\n",
            "Iteration 34, loss = 0.00092137\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00091822\n",
            "Iteration 36, loss = 0.00091732\n",
            "Iteration 37, loss = 0.00091647\n",
            "Iteration 38, loss = 0.00091566\n",
            "Iteration 39, loss = 0.00091489\n",
            "Iteration 40, loss = 0.00091417\n",
            "Iteration 41, loss = 0.00091348\n",
            "Iteration 42, loss = 0.00091283\n",
            "Iteration 43, loss = 0.00091222\n",
            "Iteration 44, loss = 0.00091163\n",
            "Iteration 45, loss = 0.00091108\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00091075\n",
            "Iteration 47, loss = 0.00091065\n",
            "Iteration 48, loss = 0.00091054\n",
            "Iteration 49, loss = 0.00091044\n",
            "Iteration 50, loss = 0.00091034\n",
            "Iteration 51, loss = 0.00091024\n",
            "Iteration 52, loss = 0.00091014\n",
            "Iteration 53, loss = 0.00091005\n",
            "Iteration 54, loss = 0.00090995\n",
            "Iteration 55, loss = 0.00090985\n",
            "Iteration 56, loss = 0.00090976\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00090970\n",
            "Iteration 58, loss = 0.00090968\n",
            "Iteration 59, loss = 0.00090967\n",
            "Iteration 60, loss = 0.00090965\n",
            "Iteration 61, loss = 0.00090963\n",
            "Iteration 62, loss = 0.00090961\n",
            "Iteration 63, loss = 0.00090959\n",
            "Iteration 64, loss = 0.00090957\n",
            "Iteration 65, loss = 0.00090956\n",
            "Iteration 66, loss = 0.00090954\n",
            "Iteration 67, loss = 0.00090952\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00090951\n",
            "Iteration 69, loss = 0.00090950\n",
            "Iteration 70, loss = 0.00090950\n",
            "Iteration 71, loss = 0.00090950\n",
            "Iteration 72, loss = 0.00090949\n",
            "Iteration 73, loss = 0.00090949\n",
            "Iteration 74, loss = 0.00090949\n",
            "Iteration 75, loss = 0.00090948\n",
            "Iteration 76, loss = 0.00090948\n",
            "Iteration 77, loss = 0.00090947\n",
            "Iteration 78, loss = 0.00090947\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00090947\n",
            "Iteration 80, loss = 0.00090947\n",
            "Iteration 81, loss = 0.00090947\n",
            "Iteration 82, loss = 0.00090947\n",
            "Iteration 83, loss = 0.00090947\n",
            "Iteration 84, loss = 0.00090946\n",
            "Iteration 85, loss = 0.00090946\n",
            "Iteration 86, loss = 0.00090946\n",
            "Iteration 87, loss = 0.00090946\n",
            "Iteration 88, loss = 0.00090946\n",
            "Iteration 89, loss = 0.00090946\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.30848607\n",
            "Iteration 2, loss = 0.22026029\n",
            "Iteration 3, loss = 0.16765271\n",
            "Iteration 4, loss = 0.12761007\n",
            "Iteration 5, loss = 0.09713133\n",
            "Iteration 6, loss = 0.07393221\n",
            "Iteration 7, loss = 0.05627404\n",
            "Iteration 8, loss = 0.04283339\n",
            "Iteration 9, loss = 0.03260295\n",
            "Iteration 10, loss = 0.02481597\n",
            "Iteration 11, loss = 0.01888885\n",
            "Iteration 12, loss = 0.01437739\n",
            "Iteration 13, loss = 0.01094345\n",
            "Iteration 14, loss = 0.00832969\n",
            "Iteration 15, loss = 0.00634020\n",
            "Iteration 16, loss = 0.00482589\n",
            "Iteration 17, loss = 0.00367336\n",
            "Iteration 18, loss = 0.00280059\n",
            "Iteration 19, loss = 0.00219116\n",
            "Iteration 20, loss = 0.00183563\n",
            "Iteration 21, loss = 0.00159881\n",
            "Iteration 22, loss = 0.00142532\n",
            "Iteration 23, loss = 0.00129725\n",
            "Iteration 24, loss = 0.00120220\n",
            "Iteration 25, loss = 0.00113130\n",
            "Iteration 26, loss = 0.00107819\n",
            "Iteration 27, loss = 0.00103825\n",
            "Iteration 28, loss = 0.00100812\n",
            "Iteration 29, loss = 0.00098536\n",
            "Iteration 30, loss = 0.00096812\n",
            "Iteration 31, loss = 0.00095505\n",
            "Iteration 32, loss = 0.00094511\n",
            "Iteration 33, loss = 0.00093755\n",
            "Iteration 34, loss = 0.00093180\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00092893\n",
            "Iteration 36, loss = 0.00092811\n",
            "Iteration 37, loss = 0.00092733\n",
            "Iteration 38, loss = 0.00092660\n",
            "Iteration 39, loss = 0.00092590\n",
            "Iteration 40, loss = 0.00092524\n",
            "Iteration 41, loss = 0.00092461\n",
            "Iteration 42, loss = 0.00092402\n",
            "Iteration 43, loss = 0.00092345\n",
            "Iteration 44, loss = 0.00092292\n",
            "Iteration 45, loss = 0.00092241\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00092211\n",
            "Iteration 47, loss = 0.00092202\n",
            "Iteration 48, loss = 0.00092193\n",
            "Iteration 49, loss = 0.00092183\n",
            "Iteration 50, loss = 0.00092174\n",
            "Iteration 51, loss = 0.00092165\n",
            "Iteration 52, loss = 0.00092156\n",
            "Iteration 53, loss = 0.00092147\n",
            "Iteration 54, loss = 0.00092138\n",
            "Iteration 55, loss = 0.00092130\n",
            "Iteration 56, loss = 0.00092121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00092116\n",
            "Iteration 58, loss = 0.00092114\n",
            "Iteration 59, loss = 0.00092112\n",
            "Iteration 60, loss = 0.00092111\n",
            "Iteration 61, loss = 0.00092109\n",
            "Iteration 62, loss = 0.00092107\n",
            "Iteration 63, loss = 0.00092106\n",
            "Iteration 64, loss = 0.00092104\n",
            "Iteration 65, loss = 0.00092102\n",
            "Iteration 66, loss = 0.00092101\n",
            "Iteration 67, loss = 0.00092099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00092098\n",
            "Iteration 69, loss = 0.00092098\n",
            "Iteration 70, loss = 0.00092097\n",
            "Iteration 71, loss = 0.00092097\n",
            "Iteration 72, loss = 0.00092097\n",
            "Iteration 73, loss = 0.00092096\n",
            "Iteration 74, loss = 0.00092096\n",
            "Iteration 75, loss = 0.00092096\n",
            "Iteration 76, loss = 0.00092095\n",
            "Iteration 77, loss = 0.00092095\n",
            "Iteration 78, loss = 0.00092095\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00092094\n",
            "Iteration 80, loss = 0.00092094\n",
            "Iteration 81, loss = 0.00092094\n",
            "Iteration 82, loss = 0.00092094\n",
            "Iteration 83, loss = 0.00092094\n",
            "Iteration 84, loss = 0.00092094\n",
            "Iteration 85, loss = 0.00092094\n",
            "Iteration 86, loss = 0.00092094\n",
            "Iteration 87, loss = 0.00092094\n",
            "Iteration 88, loss = 0.00092094\n",
            "Iteration 89, loss = 0.00092094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.22792728\n",
            "Iteration 3, loss = 0.17348850\n",
            "Iteration 4, loss = 0.13205202\n",
            "Iteration 5, loss = 0.10051235\n",
            "Iteration 6, loss = 0.07650570\n",
            "Iteration 7, loss = 0.05823287\n",
            "Iteration 8, loss = 0.04432437\n",
            "Iteration 9, loss = 0.03373781\n",
            "Iteration 10, loss = 0.02567978\n",
            "Iteration 11, loss = 0.01954635\n",
            "Iteration 12, loss = 0.01487785\n",
            "Iteration 13, loss = 0.01132438\n",
            "Iteration 14, loss = 0.00861963\n",
            "Iteration 15, loss = 0.00656090\n",
            "Iteration 16, loss = 0.00499387\n",
            "Iteration 17, loss = 0.00380114\n",
            "Iteration 18, loss = 0.00289452\n",
            "Iteration 19, loss = 0.00223056\n",
            "Iteration 20, loss = 0.00183170\n",
            "Iteration 21, loss = 0.00159365\n",
            "Iteration 22, loss = 0.00142121\n",
            "Iteration 23, loss = 0.00129302\n",
            "Iteration 24, loss = 0.00119727\n",
            "Iteration 25, loss = 0.00112547\n",
            "Iteration 26, loss = 0.00107144\n",
            "Iteration 27, loss = 0.00103068\n",
            "Iteration 28, loss = 0.00099987\n",
            "Iteration 29, loss = 0.00097653\n",
            "Iteration 30, loss = 0.00095882\n",
            "Iteration 31, loss = 0.00094538\n",
            "Iteration 32, loss = 0.00093515\n",
            "Iteration 33, loss = 0.00092737\n",
            "Iteration 34, loss = 0.00092144\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.00091848\n",
            "Iteration 36, loss = 0.00091763\n",
            "Iteration 37, loss = 0.00091683\n",
            "Iteration 38, loss = 0.00091607\n",
            "Iteration 39, loss = 0.00091535\n",
            "Iteration 40, loss = 0.00091467\n",
            "Iteration 41, loss = 0.00091402\n",
            "Iteration 42, loss = 0.00091341\n",
            "Iteration 43, loss = 0.00091283\n",
            "Iteration 44, loss = 0.00091228\n",
            "Iteration 45, loss = 0.00091176\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.00091145\n",
            "Iteration 47, loss = 0.00091135\n",
            "Iteration 48, loss = 0.00091125\n",
            "Iteration 49, loss = 0.00091116\n",
            "Iteration 50, loss = 0.00091106\n",
            "Iteration 51, loss = 0.00091097\n",
            "Iteration 52, loss = 0.00091088\n",
            "Iteration 53, loss = 0.00091078\n",
            "Iteration 54, loss = 0.00091069\n",
            "Iteration 55, loss = 0.00091060\n",
            "Iteration 56, loss = 0.00091051\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.00091046\n",
            "Iteration 58, loss = 0.00091044\n",
            "Iteration 59, loss = 0.00091043\n",
            "Iteration 60, loss = 0.00091041\n",
            "Iteration 61, loss = 0.00091039\n",
            "Iteration 62, loss = 0.00091037\n",
            "Iteration 63, loss = 0.00091036\n",
            "Iteration 64, loss = 0.00091034\n",
            "Iteration 65, loss = 0.00091032\n",
            "Iteration 66, loss = 0.00091030\n",
            "Iteration 67, loss = 0.00091029\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.00091028\n",
            "Iteration 69, loss = 0.00091027\n",
            "Iteration 70, loss = 0.00091027\n",
            "Iteration 71, loss = 0.00091027\n",
            "Iteration 72, loss = 0.00091026\n",
            "Iteration 73, loss = 0.00091026\n",
            "Iteration 74, loss = 0.00091026\n",
            "Iteration 75, loss = 0.00091025\n",
            "Iteration 76, loss = 0.00091025\n",
            "Iteration 77, loss = 0.00091024\n",
            "Iteration 78, loss = 0.00091024\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.00091024\n",
            "Iteration 80, loss = 0.00091024\n",
            "Iteration 81, loss = 0.00091024\n",
            "Iteration 82, loss = 0.00091024\n",
            "Iteration 83, loss = 0.00091024\n",
            "Iteration 84, loss = 0.00091024\n",
            "Iteration 85, loss = 0.00091024\n",
            "Iteration 86, loss = 0.00091023\n",
            "Iteration 87, loss = 0.00091023\n",
            "Iteration 88, loss = 0.00091023\n",
            "Iteration 89, loss = 0.00091023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 12.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.19406552\n",
            "Iteration 2, loss = 0.16229025\n",
            "Iteration 3, loss = 0.14977713\n",
            "Iteration 4, loss = 0.13822881\n",
            "Iteration 5, loss = 0.12757090\n",
            "Iteration 6, loss = 0.11773476\n",
            "Iteration 7, loss = 0.10865701\n",
            "Iteration 8, loss = 0.10027919\n",
            "Iteration 9, loss = 0.09254733\n",
            "Iteration 10, loss = 0.08541162\n",
            "Iteration 11, loss = 0.07882610\n",
            "Iteration 12, loss = 0.07274834\n",
            "Iteration 13, loss = 0.06713920\n",
            "Iteration 14, loss = 0.06196254\n",
            "Iteration 15, loss = 0.05718502\n",
            "Iteration 16, loss = 0.05277586\n",
            "Iteration 17, loss = 0.04870667\n",
            "Iteration 18, loss = 0.04495122\n",
            "Iteration 19, loss = 0.04148533\n",
            "Iteration 20, loss = 0.03828667\n",
            "Iteration 21, loss = 0.03533464\n",
            "Iteration 22, loss = 0.03261022\n",
            "Iteration 23, loss = 0.03009586\n",
            "Iteration 24, loss = 0.02777537\n",
            "Iteration 25, loss = 0.02563380\n",
            "Iteration 26, loss = 0.02365734\n",
            "Iteration 27, loss = 0.02183328\n",
            "Iteration 28, loss = 0.02014986\n",
            "Iteration 29, loss = 0.01859624\n",
            "Iteration 30, loss = 0.01716241\n",
            "Iteration 31, loss = 0.01583913\n",
            "Iteration 32, loss = 0.01461788\n",
            "Iteration 33, loss = 0.01349079\n",
            "Iteration 34, loss = 0.01245061\n",
            "Iteration 35, loss = 0.01149062\n",
            "Iteration 36, loss = 0.01060466\n",
            "Iteration 37, loss = 0.00978700\n",
            "Iteration 38, loss = 0.00903239\n",
            "Iteration 39, loss = 0.00833596\n",
            "Iteration 40, loss = 0.00769323\n",
            "Iteration 41, loss = 0.00710006\n",
            "Iteration 42, loss = 0.00655262\n",
            "Iteration 43, loss = 0.00604741\n",
            "Iteration 44, loss = 0.00558118\n",
            "Iteration 45, loss = 0.00515104\n",
            "Iteration 46, loss = 0.00475446\n",
            "Iteration 47, loss = 0.00438962\n",
            "Iteration 48, loss = 0.00405579\n",
            "Iteration 49, loss = 0.00375403\n",
            "Iteration 50, loss = 0.00348693\n",
            "Iteration 51, loss = 0.00325608\n",
            "Iteration 52, loss = 0.00305807\n",
            "Iteration 53, loss = 0.00288496\n",
            "Iteration 54, loss = 0.00272947\n",
            "Iteration 55, loss = 0.00258745\n",
            "Iteration 56, loss = 0.00245686\n",
            "Iteration 57, loss = 0.00233656\n",
            "Iteration 58, loss = 0.00222567\n",
            "Iteration 59, loss = 0.00212343\n",
            "Iteration 60, loss = 0.00202915\n",
            "Iteration 61, loss = 0.00194222\n",
            "Iteration 62, loss = 0.00186206\n",
            "Iteration 63, loss = 0.00178812\n",
            "Iteration 64, loss = 0.00171993\n",
            "Iteration 65, loss = 0.00165704\n",
            "Iteration 66, loss = 0.00159903\n",
            "Iteration 67, loss = 0.00154551\n",
            "Iteration 68, loss = 0.00149615\n",
            "Iteration 69, loss = 0.00145061\n",
            "Iteration 70, loss = 0.00140860\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 71, loss = 0.00138329\n",
            "Iteration 72, loss = 0.00137560\n",
            "Iteration 73, loss = 0.00136814\n",
            "Iteration 74, loss = 0.00136080\n",
            "Iteration 75, loss = 0.00135357\n",
            "Iteration 76, loss = 0.00134646\n",
            "Iteration 77, loss = 0.00133946\n",
            "Iteration 78, loss = 0.00133257\n",
            "Iteration 79, loss = 0.00132580\n",
            "Iteration 80, loss = 0.00131913\n",
            "Iteration 81, loss = 0.00131257\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.00130841\n",
            "Iteration 83, loss = 0.00130711\n",
            "Iteration 84, loss = 0.00130583\n",
            "Iteration 85, loss = 0.00130455\n",
            "Iteration 86, loss = 0.00130328\n",
            "Iteration 87, loss = 0.00130201\n",
            "Iteration 88, loss = 0.00130074\n",
            "Iteration 89, loss = 0.00129948\n",
            "Iteration 90, loss = 0.00129822\n",
            "Iteration 91, loss = 0.00129697\n",
            "Iteration 92, loss = 0.00129572\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 93, loss = 0.00129492\n",
            "Iteration 94, loss = 0.00129467\n",
            "Iteration 95, loss = 0.00129442\n",
            "Iteration 96, loss = 0.00129417\n",
            "Iteration 97, loss = 0.00129392\n",
            "Iteration 98, loss = 0.00129367\n",
            "Iteration 99, loss = 0.00129343\n",
            "Iteration 100, loss = 0.00129318\n",
            "Iteration 101, loss = 0.00129293\n",
            "Iteration 102, loss = 0.00129268\n",
            "Iteration 103, loss = 0.00129244\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 104, loss = 0.00129228\n",
            "Iteration 105, loss = 0.00129223\n",
            "Iteration 106, loss = 0.00129218\n",
            "Iteration 107, loss = 0.00129213\n",
            "Iteration 108, loss = 0.00129208\n",
            "Iteration 109, loss = 0.00129203\n",
            "Iteration 110, loss = 0.00129198\n",
            "Iteration 111, loss = 0.00129193\n",
            "Iteration 112, loss = 0.00129188\n",
            "Iteration 113, loss = 0.00129183\n",
            "Iteration 114, loss = 0.00129178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 115, loss = 0.00129175\n",
            "Iteration 116, loss = 0.00129174\n",
            "Iteration 117, loss = 0.00129173\n",
            "Iteration 118, loss = 0.00129172\n",
            "Iteration 119, loss = 0.00129171\n",
            "Iteration 120, loss = 0.00129170\n",
            "Iteration 121, loss = 0.00129169\n",
            "Iteration 122, loss = 0.00129168\n",
            "Iteration 123, loss = 0.00129167\n",
            "Iteration 124, loss = 0.00129166\n",
            "Iteration 125, loss = 0.00129165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13945590\n",
            "Iteration 2, loss = 0.12873787\n",
            "Iteration 3, loss = 0.11881175\n",
            "Iteration 4, loss = 0.10965096\n",
            "Iteration 5, loss = 0.10119651\n",
            "Iteration 6, loss = 0.09339391\n",
            "Iteration 7, loss = 0.08619293\n",
            "Iteration 8, loss = 0.07954717\n",
            "Iteration 9, loss = 0.07341381\n",
            "Iteration 10, loss = 0.06775337\n",
            "Iteration 11, loss = 0.06252937\n",
            "Iteration 12, loss = 0.05770822\n",
            "Iteration 13, loss = 0.05325897\n",
            "Iteration 14, loss = 0.04915327\n",
            "Iteration 15, loss = 0.04536538\n",
            "Iteration 16, loss = 0.04187201\n",
            "Iteration 17, loss = 0.03865147\n",
            "Iteration 18, loss = 0.03568251\n",
            "Iteration 19, loss = 0.03294466\n",
            "Iteration 20, loss = 0.03041930\n",
            "Iteration 21, loss = 0.02808977\n",
            "Iteration 22, loss = 0.02594095\n",
            "Iteration 23, loss = 0.02395891\n",
            "Iteration 24, loss = 0.02213086\n",
            "Iteration 25, loss = 0.02044498\n",
            "Iteration 26, loss = 0.01889040\n",
            "Iteration 27, loss = 0.01745705\n",
            "Iteration 28, loss = 0.01613566\n",
            "Iteration 29, loss = 0.01491767\n",
            "Iteration 30, loss = 0.01379518\n",
            "Iteration 31, loss = 0.01276090\n",
            "Iteration 32, loss = 0.01180808\n",
            "Iteration 33, loss = 0.01093050\n",
            "Iteration 34, loss = 0.01012241\n",
            "Iteration 35, loss = 0.00937854\n",
            "Iteration 36, loss = 0.00869400\n",
            "Iteration 37, loss = 0.00806420\n",
            "Iteration 38, loss = 0.00748495\n",
            "Iteration 39, loss = 0.00695234\n",
            "Iteration 40, loss = 0.00646274\n",
            "Iteration 41, loss = 0.00601275\n",
            "Iteration 42, loss = 0.00559929\n",
            "Iteration 43, loss = 0.00521946\n",
            "Iteration 44, loss = 0.00487059\n",
            "Iteration 45, loss = 0.00455020\n",
            "Iteration 46, loss = 0.00425598\n",
            "Iteration 47, loss = 0.00398582\n",
            "Iteration 48, loss = 0.00373776\n",
            "Iteration 49, loss = 0.00350998\n",
            "Iteration 50, loss = 0.00330083\n",
            "Iteration 51, loss = 0.00310876\n",
            "Iteration 52, loss = 0.00293232\n",
            "Iteration 53, loss = 0.00277021\n",
            "Iteration 54, loss = 0.00262124\n",
            "Iteration 55, loss = 0.00248432\n",
            "Iteration 56, loss = 0.00235843\n",
            "Iteration 57, loss = 0.00224267\n",
            "Iteration 58, loss = 0.00213619\n",
            "Iteration 59, loss = 0.00203822\n",
            "Iteration 60, loss = 0.00194805\n",
            "Iteration 61, loss = 0.00186504\n",
            "Iteration 62, loss = 0.00178860\n",
            "Iteration 63, loss = 0.00171817\n",
            "Iteration 64, loss = 0.00165327\n",
            "Iteration 65, loss = 0.00159346\n",
            "Iteration 66, loss = 0.00153836\n",
            "Iteration 67, loss = 0.00148762\n",
            "Iteration 68, loss = 0.00144088\n",
            "Iteration 69, loss = 0.00139783\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00137192\n",
            "Iteration 71, loss = 0.00136407\n",
            "Iteration 72, loss = 0.00135644\n",
            "Iteration 73, loss = 0.00134893\n",
            "Iteration 74, loss = 0.00134154\n",
            "Iteration 75, loss = 0.00133428\n",
            "Iteration 76, loss = 0.00132713\n",
            "Iteration 77, loss = 0.00132010\n",
            "Iteration 78, loss = 0.00131318\n",
            "Iteration 79, loss = 0.00130637\n",
            "Iteration 80, loss = 0.00129968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00129544\n",
            "Iteration 82, loss = 0.00129411\n",
            "Iteration 83, loss = 0.00129280\n",
            "Iteration 84, loss = 0.00129150\n",
            "Iteration 85, loss = 0.00129020\n",
            "Iteration 86, loss = 0.00128890\n",
            "Iteration 87, loss = 0.00128761\n",
            "Iteration 88, loss = 0.00128633\n",
            "Iteration 89, loss = 0.00128504\n",
            "Iteration 90, loss = 0.00128377\n",
            "Iteration 91, loss = 0.00128249\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00128168\n",
            "Iteration 93, loss = 0.00128142\n",
            "Iteration 94, loss = 0.00128117\n",
            "Iteration 95, loss = 0.00128091\n",
            "Iteration 96, loss = 0.00128066\n",
            "Iteration 97, loss = 0.00128041\n",
            "Iteration 98, loss = 0.00128015\n",
            "Iteration 99, loss = 0.00127990\n",
            "Iteration 100, loss = 0.00127965\n",
            "Iteration 101, loss = 0.00127940\n",
            "Iteration 102, loss = 0.00127914\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00127898\n",
            "Iteration 104, loss = 0.00127893\n",
            "Iteration 105, loss = 0.00127888\n",
            "Iteration 106, loss = 0.00127883\n",
            "Iteration 107, loss = 0.00127878\n",
            "Iteration 108, loss = 0.00127873\n",
            "Iteration 109, loss = 0.00127868\n",
            "Iteration 110, loss = 0.00127863\n",
            "Iteration 111, loss = 0.00127858\n",
            "Iteration 112, loss = 0.00127853\n",
            "Iteration 113, loss = 0.00127848\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00127845\n",
            "Iteration 115, loss = 0.00127844\n",
            "Iteration 116, loss = 0.00127843\n",
            "Iteration 117, loss = 0.00127842\n",
            "Iteration 118, loss = 0.00127841\n",
            "Iteration 119, loss = 0.00127840\n",
            "Iteration 120, loss = 0.00127839\n",
            "Iteration 121, loss = 0.00127838\n",
            "Iteration 122, loss = 0.00127837\n",
            "Iteration 123, loss = 0.00127836\n",
            "Iteration 124, loss = 0.00127835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.25593075\n",
            "Iteration 2, loss = 0.20350207\n",
            "Iteration 3, loss = 0.18781138\n",
            "Iteration 4, loss = 0.17333049\n",
            "Iteration 5, loss = 0.15996613\n",
            "Iteration 6, loss = 0.14763220\n",
            "Iteration 7, loss = 0.13624926\n",
            "Iteration 8, loss = 0.12574399\n",
            "Iteration 9, loss = 0.11604870\n",
            "Iteration 10, loss = 0.10710096\n",
            "Iteration 11, loss = 0.09884311\n",
            "Iteration 12, loss = 0.09122198\n",
            "Iteration 13, loss = 0.08418846\n",
            "Iteration 14, loss = 0.07769724\n",
            "Iteration 15, loss = 0.07170653\n",
            "Iteration 16, loss = 0.06617771\n",
            "Iteration 17, loss = 0.06107519\n",
            "Iteration 18, loss = 0.05636609\n",
            "Iteration 19, loss = 0.05202007\n",
            "Iteration 20, loss = 0.04800915\n",
            "Iteration 21, loss = 0.04430748\n",
            "Iteration 22, loss = 0.04089123\n",
            "Iteration 23, loss = 0.03773838\n",
            "Iteration 24, loss = 0.03482862\n",
            "Iteration 25, loss = 0.03214322\n",
            "Iteration 26, loss = 0.02966487\n",
            "Iteration 27, loss = 0.02737761\n",
            "Iteration 28, loss = 0.02526670\n",
            "Iteration 29, loss = 0.02331855\n",
            "Iteration 30, loss = 0.02152061\n",
            "Iteration 31, loss = 0.01986130\n",
            "Iteration 32, loss = 0.01832993\n",
            "Iteration 33, loss = 0.01691663\n",
            "Iteration 34, loss = 0.01561230\n",
            "Iteration 35, loss = 0.01440854\n",
            "Iteration 36, loss = 0.01329759\n",
            "Iteration 37, loss = 0.01227230\n",
            "Iteration 38, loss = 0.01132607\n",
            "Iteration 39, loss = 0.01045279\n",
            "Iteration 40, loss = 0.00964684\n",
            "Iteration 41, loss = 0.00890304\n",
            "Iteration 42, loss = 0.00821659\n",
            "Iteration 43, loss = 0.00758306\n",
            "Iteration 44, loss = 0.00699838\n",
            "Iteration 45, loss = 0.00645878\n",
            "Iteration 46, loss = 0.00596079\n",
            "Iteration 47, loss = 0.00550119\n",
            "Iteration 48, loss = 0.00507703\n",
            "Iteration 49, loss = 0.00468558\n",
            "Iteration 50, loss = 0.00432430\n",
            "Iteration 51, loss = 0.00399089\n",
            "Iteration 52, loss = 0.00368321\n",
            "Iteration 53, loss = 0.00339932\n",
            "Iteration 54, loss = 0.00313756\n",
            "Iteration 55, loss = 0.00289664\n",
            "Iteration 56, loss = 0.00267602\n",
            "Iteration 57, loss = 0.00247634\n",
            "Iteration 58, loss = 0.00229979\n",
            "Iteration 59, loss = 0.00214913\n",
            "Iteration 60, loss = 0.00202460\n",
            "Iteration 61, loss = 0.00192169\n",
            "Iteration 62, loss = 0.00183363\n",
            "Iteration 63, loss = 0.00175530\n",
            "Iteration 64, loss = 0.00168411\n",
            "Iteration 65, loss = 0.00161886\n",
            "Iteration 66, loss = 0.00155894\n",
            "Iteration 67, loss = 0.00150386\n",
            "Iteration 68, loss = 0.00145325\n",
            "Iteration 69, loss = 0.00140674\n",
            "Iteration 70, loss = 0.00136400\n",
            "Iteration 71, loss = 0.00132472\n",
            "Iteration 72, loss = 0.00128861\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 73, loss = 0.00126690\n",
            "Iteration 74, loss = 0.00126033\n",
            "Iteration 75, loss = 0.00125394\n",
            "Iteration 76, loss = 0.00124766\n",
            "Iteration 77, loss = 0.00124149\n",
            "Iteration 78, loss = 0.00123542\n",
            "Iteration 79, loss = 0.00122944\n",
            "Iteration 80, loss = 0.00122357\n",
            "Iteration 81, loss = 0.00121779\n",
            "Iteration 82, loss = 0.00121211\n",
            "Iteration 83, loss = 0.00120652\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 84, loss = 0.00120299\n",
            "Iteration 85, loss = 0.00120188\n",
            "Iteration 86, loss = 0.00120079\n",
            "Iteration 87, loss = 0.00119970\n",
            "Iteration 88, loss = 0.00119862\n",
            "Iteration 89, loss = 0.00119754\n",
            "Iteration 90, loss = 0.00119646\n",
            "Iteration 91, loss = 0.00119539\n",
            "Iteration 92, loss = 0.00119432\n",
            "Iteration 93, loss = 0.00119326\n",
            "Iteration 94, loss = 0.00119219\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.00119152\n",
            "Iteration 96, loss = 0.00119130\n",
            "Iteration 97, loss = 0.00119109\n",
            "Iteration 98, loss = 0.00119088\n",
            "Iteration 99, loss = 0.00119067\n",
            "Iteration 100, loss = 0.00119046\n",
            "Iteration 101, loss = 0.00119025\n",
            "Iteration 102, loss = 0.00119003\n",
            "Iteration 103, loss = 0.00118982\n",
            "Iteration 104, loss = 0.00118961\n",
            "Iteration 105, loss = 0.00118940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 106, loss = 0.00118927\n",
            "Iteration 107, loss = 0.00118923\n",
            "Iteration 108, loss = 0.00118918\n",
            "Iteration 109, loss = 0.00118914\n",
            "Iteration 110, loss = 0.00118910\n",
            "Iteration 111, loss = 0.00118906\n",
            "Iteration 112, loss = 0.00118902\n",
            "Iteration 113, loss = 0.00118897\n",
            "Iteration 114, loss = 0.00118893\n",
            "Iteration 115, loss = 0.00118889\n",
            "Iteration 116, loss = 0.00118885\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 117, loss = 0.00118882\n",
            "Iteration 118, loss = 0.00118881\n",
            "Iteration 119, loss = 0.00118880\n",
            "Iteration 120, loss = 0.00118880\n",
            "Iteration 121, loss = 0.00118879\n",
            "Iteration 122, loss = 0.00118878\n",
            "Iteration 123, loss = 0.00118877\n",
            "Iteration 124, loss = 0.00118876\n",
            "Iteration 125, loss = 0.00118875\n",
            "Iteration 126, loss = 0.00118875\n",
            "Iteration 127, loss = 0.00118874\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13778173\n",
            "Iteration 2, loss = 0.12719237\n",
            "Iteration 3, loss = 0.11738541\n",
            "Iteration 4, loss = 0.10833460\n",
            "Iteration 5, loss = 0.09998164\n",
            "Iteration 6, loss = 0.09227272\n",
            "Iteration 7, loss = 0.08515818\n",
            "Iteration 8, loss = 0.07859220\n",
            "Iteration 9, loss = 0.07253248\n",
            "Iteration 10, loss = 0.06693998\n",
            "Iteration 11, loss = 0.06177868\n",
            "Iteration 12, loss = 0.05701534\n",
            "Iteration 13, loss = 0.05261928\n",
            "Iteration 14, loss = 0.04856220\n",
            "Iteration 15, loss = 0.04481805\n",
            "Iteration 16, loss = 0.04136294\n",
            "Iteration 17, loss = 0.03817521\n",
            "Iteration 18, loss = 0.03523551\n",
            "Iteration 19, loss = 0.03252638\n",
            "Iteration 20, loss = 0.03003086\n",
            "Iteration 21, loss = 0.02773153\n",
            "Iteration 22, loss = 0.02561194\n",
            "Iteration 23, loss = 0.02365753\n",
            "Iteration 24, loss = 0.02185533\n",
            "Iteration 25, loss = 0.02019361\n",
            "Iteration 26, loss = 0.01866157\n",
            "Iteration 27, loss = 0.01724927\n",
            "Iteration 28, loss = 0.01594752\n",
            "Iteration 29, loss = 0.01474788\n",
            "Iteration 30, loss = 0.01364252\n",
            "Iteration 31, loss = 0.01262421\n",
            "Iteration 32, loss = 0.01168629\n",
            "Iteration 33, loss = 0.01082260\n",
            "Iteration 34, loss = 0.01002745\n",
            "Iteration 35, loss = 0.00929557\n",
            "Iteration 36, loss = 0.00862210\n",
            "Iteration 37, loss = 0.00800251\n",
            "Iteration 38, loss = 0.00743262\n",
            "Iteration 39, loss = 0.00690858\n",
            "Iteration 40, loss = 0.00642678\n",
            "Iteration 41, loss = 0.00598391\n",
            "Iteration 42, loss = 0.00557693\n",
            "Iteration 43, loss = 0.00520301\n",
            "Iteration 44, loss = 0.00485948\n",
            "Iteration 45, loss = 0.00454391\n",
            "Iteration 46, loss = 0.00425409\n",
            "Iteration 47, loss = 0.00398792\n",
            "Iteration 48, loss = 0.00374346\n",
            "Iteration 49, loss = 0.00351889\n",
            "Iteration 50, loss = 0.00331257\n",
            "Iteration 51, loss = 0.00312296\n",
            "Iteration 52, loss = 0.00294863\n",
            "Iteration 53, loss = 0.00278827\n",
            "Iteration 54, loss = 0.00264074\n",
            "Iteration 55, loss = 0.00250496\n",
            "Iteration 56, loss = 0.00237997\n",
            "Iteration 57, loss = 0.00226489\n",
            "Iteration 58, loss = 0.00215892\n",
            "Iteration 59, loss = 0.00206135\n",
            "Iteration 60, loss = 0.00197150\n",
            "Iteration 61, loss = 0.00188875\n",
            "Iteration 62, loss = 0.00181250\n",
            "Iteration 63, loss = 0.00174224\n",
            "Iteration 64, loss = 0.00167747\n",
            "Iteration 65, loss = 0.00161775\n",
            "Iteration 66, loss = 0.00156268\n",
            "Iteration 67, loss = 0.00151188\n",
            "Iteration 68, loss = 0.00146502\n",
            "Iteration 69, loss = 0.00142178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00139572\n",
            "Iteration 71, loss = 0.00138781\n",
            "Iteration 72, loss = 0.00138012\n",
            "Iteration 73, loss = 0.00137256\n",
            "Iteration 74, loss = 0.00136512\n",
            "Iteration 75, loss = 0.00135779\n",
            "Iteration 76, loss = 0.00135059\n",
            "Iteration 77, loss = 0.00134350\n",
            "Iteration 78, loss = 0.00133652\n",
            "Iteration 79, loss = 0.00132966\n",
            "Iteration 80, loss = 0.00132290\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00131863\n",
            "Iteration 82, loss = 0.00131729\n",
            "Iteration 83, loss = 0.00131597\n",
            "Iteration 84, loss = 0.00131465\n",
            "Iteration 85, loss = 0.00131334\n",
            "Iteration 86, loss = 0.00131204\n",
            "Iteration 87, loss = 0.00131073\n",
            "Iteration 88, loss = 0.00130944\n",
            "Iteration 89, loss = 0.00130814\n",
            "Iteration 90, loss = 0.00130685\n",
            "Iteration 91, loss = 0.00130557\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00130475\n",
            "Iteration 93, loss = 0.00130449\n",
            "Iteration 94, loss = 0.00130423\n",
            "Iteration 95, loss = 0.00130398\n",
            "Iteration 96, loss = 0.00130372\n",
            "Iteration 97, loss = 0.00130347\n",
            "Iteration 98, loss = 0.00130321\n",
            "Iteration 99, loss = 0.00130296\n",
            "Iteration 100, loss = 0.00130270\n",
            "Iteration 101, loss = 0.00130245\n",
            "Iteration 102, loss = 0.00130219\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00130203\n",
            "Iteration 104, loss = 0.00130198\n",
            "Iteration 105, loss = 0.00130193\n",
            "Iteration 106, loss = 0.00130187\n",
            "Iteration 107, loss = 0.00130182\n",
            "Iteration 108, loss = 0.00130177\n",
            "Iteration 109, loss = 0.00130172\n",
            "Iteration 110, loss = 0.00130167\n",
            "Iteration 111, loss = 0.00130162\n",
            "Iteration 112, loss = 0.00130157\n",
            "Iteration 113, loss = 0.00130152\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00130149\n",
            "Iteration 115, loss = 0.00130148\n",
            "Iteration 116, loss = 0.00130147\n",
            "Iteration 117, loss = 0.00130146\n",
            "Iteration 118, loss = 0.00130145\n",
            "Iteration 119, loss = 0.00130144\n",
            "Iteration 120, loss = 0.00130143\n",
            "Iteration 121, loss = 0.00130142\n",
            "Iteration 122, loss = 0.00130141\n",
            "Iteration 123, loss = 0.00130140\n",
            "Iteration 124, loss = 0.00130138\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.35360704\n",
            "Iteration 2, loss = 0.32000355\n",
            "Iteration 3, loss = 0.29533020\n",
            "Iteration 4, loss = 0.27255925\n",
            "Iteration 5, loss = 0.25154402\n",
            "Iteration 6, loss = 0.23214913\n",
            "Iteration 7, loss = 0.21424965\n",
            "Iteration 8, loss = 0.19773028\n",
            "Iteration 9, loss = 0.18248461\n",
            "Iteration 10, loss = 0.16841444\n",
            "Iteration 11, loss = 0.15542912\n",
            "Iteration 12, loss = 0.14344501\n",
            "Iteration 13, loss = 0.13238492\n",
            "Iteration 14, loss = 0.12217760\n",
            "Iteration 15, loss = 0.11275729\n",
            "Iteration 16, loss = 0.10406333\n",
            "Iteration 17, loss = 0.09603969\n",
            "Iteration 18, loss = 0.08863471\n",
            "Iteration 19, loss = 0.08180068\n",
            "Iteration 20, loss = 0.07549357\n",
            "Iteration 21, loss = 0.06967276\n",
            "Iteration 22, loss = 0.06430076\n",
            "Iteration 23, loss = 0.05934295\n",
            "Iteration 24, loss = 0.05476741\n",
            "Iteration 25, loss = 0.05054466\n",
            "Iteration 26, loss = 0.04664750\n",
            "Iteration 27, loss = 0.04305082\n",
            "Iteration 28, loss = 0.03973146\n",
            "Iteration 29, loss = 0.03666803\n",
            "Iteration 30, loss = 0.03384080\n",
            "Iteration 31, loss = 0.03123156\n",
            "Iteration 32, loss = 0.02882350\n",
            "Iteration 33, loss = 0.02660111\n",
            "Iteration 34, loss = 0.02455008\n",
            "Iteration 35, loss = 0.02265718\n",
            "Iteration 36, loss = 0.02091024\n",
            "Iteration 37, loss = 0.01929799\n",
            "Iteration 38, loss = 0.01781005\n",
            "Iteration 39, loss = 0.01643683\n",
            "Iteration 40, loss = 0.01516950\n",
            "Iteration 41, loss = 0.01399988\n",
            "Iteration 42, loss = 0.01292044\n",
            "Iteration 43, loss = 0.01192423\n",
            "Iteration 44, loss = 0.01100483\n",
            "Iteration 45, loss = 0.01015632\n",
            "Iteration 46, loss = 0.00937324\n",
            "Iteration 47, loss = 0.00865053\n",
            "Iteration 48, loss = 0.00798354\n",
            "Iteration 49, loss = 0.00736799\n",
            "Iteration 50, loss = 0.00679989\n",
            "Iteration 51, loss = 0.00627560\n",
            "Iteration 52, loss = 0.00579173\n",
            "Iteration 53, loss = 0.00534516\n",
            "Iteration 54, loss = 0.00493303\n",
            "Iteration 55, loss = 0.00455268\n",
            "Iteration 56, loss = 0.00420165\n",
            "Iteration 57, loss = 0.00387769\n",
            "Iteration 58, loss = 0.00357871\n",
            "Iteration 59, loss = 0.00330280\n",
            "Iteration 60, loss = 0.00304819\n",
            "Iteration 61, loss = 0.00281334\n",
            "Iteration 62, loss = 0.00259699\n",
            "Iteration 63, loss = 0.00239836\n",
            "Iteration 64, loss = 0.00221760\n",
            "Iteration 65, loss = 0.00205628\n",
            "Iteration 66, loss = 0.00191731\n",
            "Iteration 67, loss = 0.00180300\n",
            "Iteration 68, loss = 0.00171168\n",
            "Iteration 69, loss = 0.00163740\n",
            "Iteration 70, loss = 0.00157377\n",
            "Iteration 71, loss = 0.00151663\n",
            "Iteration 72, loss = 0.00146398\n",
            "Iteration 73, loss = 0.00141573\n",
            "Iteration 74, loss = 0.00137153\n",
            "Iteration 75, loss = 0.00133102\n",
            "Iteration 76, loss = 0.00129385\n",
            "Iteration 77, loss = 0.00125974\n",
            "Iteration 78, loss = 0.00122843\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 79, loss = 0.00120964\n",
            "Iteration 80, loss = 0.00120396\n",
            "Iteration 81, loss = 0.00119844\n",
            "Iteration 82, loss = 0.00119301\n",
            "Iteration 83, loss = 0.00118768\n",
            "Iteration 84, loss = 0.00118243\n",
            "Iteration 85, loss = 0.00117728\n",
            "Iteration 86, loss = 0.00117221\n",
            "Iteration 87, loss = 0.00116723\n",
            "Iteration 88, loss = 0.00116233\n",
            "Iteration 89, loss = 0.00115752\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.00115447\n",
            "Iteration 91, loss = 0.00115352\n",
            "Iteration 92, loss = 0.00115258\n",
            "Iteration 93, loss = 0.00115164\n",
            "Iteration 94, loss = 0.00115071\n",
            "Iteration 95, loss = 0.00114978\n",
            "Iteration 96, loss = 0.00114885\n",
            "Iteration 97, loss = 0.00114793\n",
            "Iteration 98, loss = 0.00114701\n",
            "Iteration 99, loss = 0.00114609\n",
            "Iteration 100, loss = 0.00114518\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.00114459\n",
            "Iteration 102, loss = 0.00114441\n",
            "Iteration 103, loss = 0.00114423\n",
            "Iteration 104, loss = 0.00114404\n",
            "Iteration 105, loss = 0.00114386\n",
            "Iteration 106, loss = 0.00114368\n",
            "Iteration 107, loss = 0.00114350\n",
            "Iteration 108, loss = 0.00114332\n",
            "Iteration 109, loss = 0.00114314\n",
            "Iteration 110, loss = 0.00114296\n",
            "Iteration 111, loss = 0.00114277\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 112, loss = 0.00114266\n",
            "Iteration 113, loss = 0.00114262\n",
            "Iteration 114, loss = 0.00114259\n",
            "Iteration 115, loss = 0.00114255\n",
            "Iteration 116, loss = 0.00114251\n",
            "Iteration 117, loss = 0.00114248\n",
            "Iteration 118, loss = 0.00114244\n",
            "Iteration 119, loss = 0.00114241\n",
            "Iteration 120, loss = 0.00114237\n",
            "Iteration 121, loss = 0.00114233\n",
            "Iteration 122, loss = 0.00114230\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 123, loss = 0.00114227\n",
            "Iteration 124, loss = 0.00114227\n",
            "Iteration 125, loss = 0.00114226\n",
            "Iteration 126, loss = 0.00114225\n",
            "Iteration 127, loss = 0.00114224\n",
            "Iteration 128, loss = 0.00114224\n",
            "Iteration 129, loss = 0.00114223\n",
            "Iteration 130, loss = 0.00114222\n",
            "Iteration 131, loss = 0.00114222\n",
            "Iteration 132, loss = 0.00114221\n",
            "Iteration 133, loss = 0.00114220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.29366539\n",
            "Iteration 3, loss = 0.27102280\n",
            "Iteration 4, loss = 0.25012603\n",
            "Iteration 5, loss = 0.23084048\n",
            "Iteration 6, loss = 0.21304190\n",
            "Iteration 7, loss = 0.19661565\n",
            "Iteration 8, loss = 0.18145593\n",
            "Iteration 9, loss = 0.16746506\n",
            "Iteration 10, loss = 0.15455295\n",
            "Iteration 11, loss = 0.14263639\n",
            "Iteration 12, loss = 0.13163865\n",
            "Iteration 13, loss = 0.12148887\n",
            "Iteration 14, loss = 0.11212167\n",
            "Iteration 15, loss = 0.10347671\n",
            "Iteration 16, loss = 0.09549831\n",
            "Iteration 17, loss = 0.08813507\n",
            "Iteration 18, loss = 0.08133956\n",
            "Iteration 19, loss = 0.07506800\n",
            "Iteration 20, loss = 0.06928001\n",
            "Iteration 21, loss = 0.06393829\n",
            "Iteration 22, loss = 0.05900843\n",
            "Iteration 23, loss = 0.05445868\n",
            "Iteration 24, loss = 0.05025973\n",
            "Iteration 25, loss = 0.04638454\n",
            "Iteration 26, loss = 0.04280814\n",
            "Iteration 27, loss = 0.03950749\n",
            "Iteration 28, loss = 0.03646133\n",
            "Iteration 29, loss = 0.03365003\n",
            "Iteration 30, loss = 0.03105550\n",
            "Iteration 31, loss = 0.02866102\n",
            "Iteration 32, loss = 0.02645116\n",
            "Iteration 33, loss = 0.02441169\n",
            "Iteration 34, loss = 0.02252946\n",
            "Iteration 35, loss = 0.02079237\n",
            "Iteration 36, loss = 0.01918920\n",
            "Iteration 37, loss = 0.01770965\n",
            "Iteration 38, loss = 0.01634418\n",
            "Iteration 39, loss = 0.01508399\n",
            "Iteration 40, loss = 0.01392096\n",
            "Iteration 41, loss = 0.01284761\n",
            "Iteration 42, loss = 0.01185701\n",
            "Iteration 43, loss = 0.01094280\n",
            "Iteration 44, loss = 0.01009907\n",
            "Iteration 45, loss = 0.00932040\n",
            "Iteration 46, loss = 0.00860177\n",
            "Iteration 47, loss = 0.00793854\n",
            "Iteration 48, loss = 0.00732645\n",
            "Iteration 49, loss = 0.00676156\n",
            "Iteration 50, loss = 0.00624022\n",
            "Iteration 51, loss = 0.00575908\n",
            "Iteration 52, loss = 0.00531503\n",
            "Iteration 53, loss = 0.00490523\n",
            "Iteration 54, loss = 0.00452702\n",
            "Iteration 55, loss = 0.00417797\n",
            "Iteration 56, loss = 0.00385584\n",
            "Iteration 57, loss = 0.00355858\n",
            "Iteration 58, loss = 0.00328434\n",
            "Iteration 59, loss = 0.00303155\n",
            "Iteration 60, loss = 0.00279915\n",
            "Iteration 61, loss = 0.00258700\n",
            "Iteration 62, loss = 0.00239647\n",
            "Iteration 63, loss = 0.00223057\n",
            "Iteration 64, loss = 0.00209199\n",
            "Iteration 65, loss = 0.00197913\n",
            "Iteration 66, loss = 0.00188547\n",
            "Iteration 67, loss = 0.00180415\n",
            "Iteration 68, loss = 0.00173111\n",
            "Iteration 69, loss = 0.00166454\n",
            "Iteration 70, loss = 0.00160356\n",
            "Iteration 71, loss = 0.00154763\n",
            "Iteration 72, loss = 0.00149629\n",
            "Iteration 73, loss = 0.00144917\n",
            "Iteration 74, loss = 0.00140591\n",
            "Iteration 75, loss = 0.00136618\n",
            "Iteration 76, loss = 0.00132968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00130776\n",
            "Iteration 78, loss = 0.00130112\n",
            "Iteration 79, loss = 0.00129467\n",
            "Iteration 80, loss = 0.00128834\n",
            "Iteration 81, loss = 0.00128210\n",
            "Iteration 82, loss = 0.00127598\n",
            "Iteration 83, loss = 0.00126995\n",
            "Iteration 84, loss = 0.00126402\n",
            "Iteration 85, loss = 0.00125820\n",
            "Iteration 86, loss = 0.00125247\n",
            "Iteration 87, loss = 0.00124683\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00124327\n",
            "Iteration 89, loss = 0.00124215\n",
            "Iteration 90, loss = 0.00124105\n",
            "Iteration 91, loss = 0.00123995\n",
            "Iteration 92, loss = 0.00123886\n",
            "Iteration 93, loss = 0.00123777\n",
            "Iteration 94, loss = 0.00123669\n",
            "Iteration 95, loss = 0.00123561\n",
            "Iteration 96, loss = 0.00123453\n",
            "Iteration 97, loss = 0.00123345\n",
            "Iteration 98, loss = 0.00123238\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00123170\n",
            "Iteration 100, loss = 0.00123148\n",
            "Iteration 101, loss = 0.00123127\n",
            "Iteration 102, loss = 0.00123106\n",
            "Iteration 103, loss = 0.00123084\n",
            "Iteration 104, loss = 0.00123063\n",
            "Iteration 105, loss = 0.00123042\n",
            "Iteration 106, loss = 0.00123021\n",
            "Iteration 107, loss = 0.00122999\n",
            "Iteration 108, loss = 0.00122978\n",
            "Iteration 109, loss = 0.00122957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00122944\n",
            "Iteration 111, loss = 0.00122939\n",
            "Iteration 112, loss = 0.00122935\n",
            "Iteration 113, loss = 0.00122931\n",
            "Iteration 114, loss = 0.00122927\n",
            "Iteration 115, loss = 0.00122922\n",
            "Iteration 116, loss = 0.00122918\n",
            "Iteration 117, loss = 0.00122914\n",
            "Iteration 118, loss = 0.00122910\n",
            "Iteration 119, loss = 0.00122905\n",
            "Iteration 120, loss = 0.00122901\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00122898\n",
            "Iteration 122, loss = 0.00122898\n",
            "Iteration 123, loss = 0.00122897\n",
            "Iteration 124, loss = 0.00122896\n",
            "Iteration 125, loss = 0.00122895\n",
            "Iteration 126, loss = 0.00122894\n",
            "Iteration 127, loss = 0.00122893\n",
            "Iteration 128, loss = 0.00122892\n",
            "Iteration 129, loss = 0.00122892\n",
            "Iteration 130, loss = 0.00122891\n",
            "Iteration 131, loss = 0.00122890\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13176210\n",
            "Iteration 2, loss = 0.12161947\n",
            "Iteration 3, loss = 0.11224484\n",
            "Iteration 4, loss = 0.10359349\n",
            "Iteration 5, loss = 0.09560958\n",
            "Iteration 6, loss = 0.08824164\n",
            "Iteration 7, loss = 0.08144218\n",
            "Iteration 8, loss = 0.07516740\n",
            "Iteration 9, loss = 0.06937685\n",
            "Iteration 10, loss = 0.06403321\n",
            "Iteration 11, loss = 0.05910206\n",
            "Iteration 12, loss = 0.05455160\n",
            "Iteration 13, loss = 0.05035253\n",
            "Iteration 14, loss = 0.04647777\n",
            "Iteration 15, loss = 0.04290236\n",
            "Iteration 16, loss = 0.03960324\n",
            "Iteration 17, loss = 0.03655915\n",
            "Iteration 18, loss = 0.03375048\n",
            "Iteration 19, loss = 0.03115911\n",
            "Iteration 20, loss = 0.02876834\n",
            "Iteration 21, loss = 0.02656275\n",
            "Iteration 22, loss = 0.02452812\n",
            "Iteration 23, loss = 0.02265131\n",
            "Iteration 24, loss = 0.02092023\n",
            "Iteration 25, loss = 0.01932369\n",
            "Iteration 26, loss = 0.01785138\n",
            "Iteration 27, loss = 0.01649378\n",
            "Iteration 28, loss = 0.01524213\n",
            "Iteration 29, loss = 0.01408831\n",
            "Iteration 30, loss = 0.01302486\n",
            "Iteration 31, loss = 0.01204487\n",
            "Iteration 32, loss = 0.01114198\n",
            "Iteration 33, loss = 0.01031032\n",
            "Iteration 34, loss = 0.00954446\n",
            "Iteration 35, loss = 0.00883938\n",
            "Iteration 36, loss = 0.00819046\n",
            "Iteration 37, loss = 0.00759341\n",
            "Iteration 38, loss = 0.00704429\n",
            "Iteration 39, loss = 0.00653946\n",
            "Iteration 40, loss = 0.00607552\n",
            "Iteration 41, loss = 0.00564930\n",
            "Iteration 42, loss = 0.00525785\n",
            "Iteration 43, loss = 0.00489849\n",
            "Iteration 44, loss = 0.00456870\n",
            "Iteration 45, loss = 0.00426615\n",
            "Iteration 46, loss = 0.00398869\n",
            "Iteration 47, loss = 0.00373432\n",
            "Iteration 48, loss = 0.00350112\n",
            "Iteration 49, loss = 0.00328737\n",
            "Iteration 50, loss = 0.00309145\n",
            "Iteration 51, loss = 0.00291187\n",
            "Iteration 52, loss = 0.00274728\n",
            "Iteration 53, loss = 0.00259642\n",
            "Iteration 54, loss = 0.00245813\n",
            "Iteration 55, loss = 0.00233134\n",
            "Iteration 56, loss = 0.00221508\n",
            "Iteration 57, loss = 0.00210844\n",
            "Iteration 58, loss = 0.00201061\n",
            "Iteration 59, loss = 0.00192085\n",
            "Iteration 60, loss = 0.00183846\n",
            "Iteration 61, loss = 0.00176283\n",
            "Iteration 62, loss = 0.00169338\n",
            "Iteration 63, loss = 0.00162958\n",
            "Iteration 64, loss = 0.00157096\n",
            "Iteration 65, loss = 0.00151709\n",
            "Iteration 66, loss = 0.00146757\n",
            "Iteration 67, loss = 0.00142204\n",
            "Iteration 68, loss = 0.00138016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.00135498\n",
            "Iteration 70, loss = 0.00134734\n",
            "Iteration 71, loss = 0.00133993\n",
            "Iteration 72, loss = 0.00133264\n",
            "Iteration 73, loss = 0.00132547\n",
            "Iteration 74, loss = 0.00131842\n",
            "Iteration 75, loss = 0.00131148\n",
            "Iteration 76, loss = 0.00130466\n",
            "Iteration 77, loss = 0.00129795\n",
            "Iteration 78, loss = 0.00129135\n",
            "Iteration 79, loss = 0.00128486\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.00128075\n",
            "Iteration 81, loss = 0.00127946\n",
            "Iteration 82, loss = 0.00127819\n",
            "Iteration 83, loss = 0.00127693\n",
            "Iteration 84, loss = 0.00127567\n",
            "Iteration 85, loss = 0.00127441\n",
            "Iteration 86, loss = 0.00127316\n",
            "Iteration 87, loss = 0.00127192\n",
            "Iteration 88, loss = 0.00127067\n",
            "Iteration 89, loss = 0.00126943\n",
            "Iteration 90, loss = 0.00126820\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.00126741\n",
            "Iteration 92, loss = 0.00126716\n",
            "Iteration 93, loss = 0.00126692\n",
            "Iteration 94, loss = 0.00126667\n",
            "Iteration 95, loss = 0.00126643\n",
            "Iteration 96, loss = 0.00126618\n",
            "Iteration 97, loss = 0.00126594\n",
            "Iteration 98, loss = 0.00126569\n",
            "Iteration 99, loss = 0.00126545\n",
            "Iteration 100, loss = 0.00126520\n",
            "Iteration 101, loss = 0.00126496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.00126480\n",
            "Iteration 103, loss = 0.00126475\n",
            "Iteration 104, loss = 0.00126470\n",
            "Iteration 105, loss = 0.00126465\n",
            "Iteration 106, loss = 0.00126460\n",
            "Iteration 107, loss = 0.00126455\n",
            "Iteration 108, loss = 0.00126451\n",
            "Iteration 109, loss = 0.00126446\n",
            "Iteration 110, loss = 0.00126441\n",
            "Iteration 111, loss = 0.00126436\n",
            "Iteration 112, loss = 0.00126431\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 113, loss = 0.00126428\n",
            "Iteration 114, loss = 0.00126427\n",
            "Iteration 115, loss = 0.00126426\n",
            "Iteration 116, loss = 0.00126425\n",
            "Iteration 117, loss = 0.00126424\n",
            "Iteration 118, loss = 0.00126423\n",
            "Iteration 119, loss = 0.00126422\n",
            "Iteration 120, loss = 0.00126421\n",
            "Iteration 121, loss = 0.00126420\n",
            "Iteration 122, loss = 0.00126419\n",
            "Iteration 123, loss = 0.00126418\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.30698932\n",
            "Iteration 3, loss = 0.28331941\n",
            "Iteration 4, loss = 0.26147453\n",
            "Iteration 5, loss = 0.24131397\n",
            "Iteration 6, loss = 0.22270785\n",
            "Iteration 7, loss = 0.20553633\n",
            "Iteration 8, loss = 0.18968879\n",
            "Iteration 9, loss = 0.17506314\n",
            "Iteration 10, loss = 0.16156519\n",
            "Iteration 11, loss = 0.14910797\n",
            "Iteration 12, loss = 0.13761124\n",
            "Iteration 13, loss = 0.12700095\n",
            "Iteration 14, loss = 0.11720875\n",
            "Iteration 15, loss = 0.10817156\n",
            "Iteration 16, loss = 0.09983117\n",
            "Iteration 17, loss = 0.09213385\n",
            "Iteration 18, loss = 0.08503002\n",
            "Iteration 19, loss = 0.07847392\n",
            "Iteration 20, loss = 0.07242332\n",
            "Iteration 21, loss = 0.06683924\n",
            "Iteration 22, loss = 0.06168571\n",
            "Iteration 23, loss = 0.05692953\n",
            "Iteration 24, loss = 0.05254008\n",
            "Iteration 25, loss = 0.04848906\n",
            "Iteration 26, loss = 0.04475039\n",
            "Iteration 27, loss = 0.04129998\n",
            "Iteration 28, loss = 0.03811562\n",
            "Iteration 29, loss = 0.03517677\n",
            "Iteration 30, loss = 0.03246453\n",
            "Iteration 31, loss = 0.02996140\n",
            "Iteration 32, loss = 0.02765128\n",
            "Iteration 33, loss = 0.02551927\n",
            "Iteration 34, loss = 0.02355165\n",
            "Iteration 35, loss = 0.02173574\n",
            "Iteration 36, loss = 0.02005984\n",
            "Iteration 37, loss = 0.01851316\n",
            "Iteration 38, loss = 0.01708573\n",
            "Iteration 39, loss = 0.01576836\n",
            "Iteration 40, loss = 0.01455257\n",
            "Iteration 41, loss = 0.01343052\n",
            "Iteration 42, loss = 0.01239498\n",
            "Iteration 43, loss = 0.01143929\n",
            "Iteration 44, loss = 0.01055728\n",
            "Iteration 45, loss = 0.00974328\n",
            "Iteration 46, loss = 0.00899204\n",
            "Iteration 47, loss = 0.00829872\n",
            "Iteration 48, loss = 0.00765886\n",
            "Iteration 49, loss = 0.00706834\n",
            "Iteration 50, loss = 0.00652335\n",
            "Iteration 51, loss = 0.00602037\n",
            "Iteration 52, loss = 0.00555618\n",
            "Iteration 53, loss = 0.00512778\n",
            "Iteration 54, loss = 0.00473241\n",
            "Iteration 55, loss = 0.00436753\n",
            "Iteration 56, loss = 0.00403078\n",
            "Iteration 57, loss = 0.00372002\n",
            "Iteration 58, loss = 0.00343330\n",
            "Iteration 59, loss = 0.00316891\n",
            "Iteration 60, loss = 0.00292560\n",
            "Iteration 61, loss = 0.00270287\n",
            "Iteration 62, loss = 0.00250150\n",
            "Iteration 63, loss = 0.00232386\n",
            "Iteration 64, loss = 0.00217252\n",
            "Iteration 65, loss = 0.00204689\n",
            "Iteration 66, loss = 0.00194173\n",
            "Iteration 67, loss = 0.00185056\n",
            "Iteration 68, loss = 0.00176916\n",
            "Iteration 69, loss = 0.00169544\n",
            "Iteration 70, loss = 0.00162834\n",
            "Iteration 71, loss = 0.00156717\n",
            "Iteration 72, loss = 0.00151136\n",
            "Iteration 73, loss = 0.00146041\n",
            "Iteration 74, loss = 0.00141388\n",
            "Iteration 75, loss = 0.00137136\n",
            "Iteration 76, loss = 0.00133250\n",
            "Iteration 77, loss = 0.00129697\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.00127570\n",
            "Iteration 79, loss = 0.00126927\n",
            "Iteration 80, loss = 0.00126304\n",
            "Iteration 81, loss = 0.00125691\n",
            "Iteration 82, loss = 0.00125090\n",
            "Iteration 83, loss = 0.00124498\n",
            "Iteration 84, loss = 0.00123917\n",
            "Iteration 85, loss = 0.00123347\n",
            "Iteration 86, loss = 0.00122786\n",
            "Iteration 87, loss = 0.00122235\n",
            "Iteration 88, loss = 0.00121693\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.00121351\n",
            "Iteration 90, loss = 0.00121243\n",
            "Iteration 91, loss = 0.00121138\n",
            "Iteration 92, loss = 0.00121033\n",
            "Iteration 93, loss = 0.00120928\n",
            "Iteration 94, loss = 0.00120823\n",
            "Iteration 95, loss = 0.00120719\n",
            "Iteration 96, loss = 0.00120615\n",
            "Iteration 97, loss = 0.00120512\n",
            "Iteration 98, loss = 0.00120409\n",
            "Iteration 99, loss = 0.00120306\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 100, loss = 0.00120241\n",
            "Iteration 101, loss = 0.00120220\n",
            "Iteration 102, loss = 0.00120200\n",
            "Iteration 103, loss = 0.00120179\n",
            "Iteration 104, loss = 0.00120159\n",
            "Iteration 105, loss = 0.00120139\n",
            "Iteration 106, loss = 0.00120118\n",
            "Iteration 107, loss = 0.00120098\n",
            "Iteration 108, loss = 0.00120078\n",
            "Iteration 109, loss = 0.00120057\n",
            "Iteration 110, loss = 0.00120037\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.00120024\n",
            "Iteration 112, loss = 0.00120020\n",
            "Iteration 113, loss = 0.00120016\n",
            "Iteration 114, loss = 0.00120012\n",
            "Iteration 115, loss = 0.00120008\n",
            "Iteration 116, loss = 0.00120004\n",
            "Iteration 117, loss = 0.00119999\n",
            "Iteration 118, loss = 0.00119995\n",
            "Iteration 119, loss = 0.00119991\n",
            "Iteration 120, loss = 0.00119987\n",
            "Iteration 121, loss = 0.00119983\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.00119981\n",
            "Iteration 123, loss = 0.00119980\n",
            "Iteration 124, loss = 0.00119979\n",
            "Iteration 125, loss = 0.00119978\n",
            "Iteration 126, loss = 0.00119977\n",
            "Iteration 127, loss = 0.00119977\n",
            "Iteration 128, loss = 0.00119976\n",
            "Iteration 129, loss = 0.00119975\n",
            "Iteration 130, loss = 0.00119974\n",
            "Iteration 131, loss = 0.00119973\n",
            "Iteration 132, loss = 0.00119973\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13692983\n",
            "Iteration 2, loss = 0.12640594\n",
            "Iteration 3, loss = 0.11665962\n",
            "Iteration 4, loss = 0.10766477\n",
            "Iteration 5, loss = 0.09936345\n",
            "Iteration 6, loss = 0.09170220\n",
            "Iteration 7, loss = 0.08463165\n",
            "Iteration 8, loss = 0.07810627\n",
            "Iteration 9, loss = 0.07208401\n",
            "Iteration 10, loss = 0.06652609\n",
            "Iteration 11, loss = 0.06139671\n",
            "Iteration 12, loss = 0.05666281\n",
            "Iteration 13, loss = 0.05229392\n",
            "Iteration 14, loss = 0.04826189\n",
            "Iteration 15, loss = 0.04454074\n",
            "Iteration 16, loss = 0.04110654\n",
            "Iteration 17, loss = 0.03793722\n",
            "Iteration 18, loss = 0.03501256\n",
            "Iteration 19, loss = 0.03231418\n",
            "Iteration 20, loss = 0.02982569\n",
            "Iteration 21, loss = 0.02753251\n",
            "Iteration 22, loss = 0.02542090\n",
            "Iteration 23, loss = 0.02347655\n",
            "Iteration 24, loss = 0.02168516\n",
            "Iteration 25, loss = 0.02003388\n",
            "Iteration 26, loss = 0.01851154\n",
            "Iteration 27, loss = 0.01710812\n",
            "Iteration 28, loss = 0.01581449\n",
            "Iteration 29, loss = 0.01462226\n",
            "Iteration 30, loss = 0.01352367\n",
            "Iteration 31, loss = 0.01251155\n",
            "Iteration 32, loss = 0.01157929\n",
            "Iteration 33, loss = 0.01072079\n",
            "Iteration 34, loss = 0.00993040\n",
            "Iteration 35, loss = 0.00920290\n",
            "Iteration 36, loss = 0.00853349\n",
            "Iteration 37, loss = 0.00791768\n",
            "Iteration 38, loss = 0.00735132\n",
            "Iteration 39, loss = 0.00683058\n",
            "Iteration 40, loss = 0.00635189\n",
            "Iteration 41, loss = 0.00591197\n",
            "Iteration 42, loss = 0.00550778\n",
            "Iteration 43, loss = 0.00513654\n",
            "Iteration 44, loss = 0.00479562\n",
            "Iteration 45, loss = 0.00448259\n",
            "Iteration 46, loss = 0.00419516\n",
            "Iteration 47, loss = 0.00393123\n",
            "Iteration 48, loss = 0.00368884\n",
            "Iteration 49, loss = 0.00346623\n",
            "Iteration 50, loss = 0.00326175\n",
            "Iteration 51, loss = 0.00307388\n",
            "Iteration 52, loss = 0.00290124\n",
            "Iteration 53, loss = 0.00274255\n",
            "Iteration 54, loss = 0.00259666\n",
            "Iteration 55, loss = 0.00246251\n",
            "Iteration 56, loss = 0.00233915\n",
            "Iteration 57, loss = 0.00222567\n",
            "Iteration 58, loss = 0.00212128\n",
            "Iteration 59, loss = 0.00202522\n",
            "Iteration 60, loss = 0.00193682\n",
            "Iteration 61, loss = 0.00185545\n",
            "Iteration 62, loss = 0.00178053\n",
            "Iteration 63, loss = 0.00171155\n",
            "Iteration 64, loss = 0.00164803\n",
            "Iteration 65, loss = 0.00158952\n",
            "Iteration 66, loss = 0.00153562\n",
            "Iteration 67, loss = 0.00148597\n",
            "Iteration 68, loss = 0.00144021\n",
            "Iteration 69, loss = 0.00139805\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.00137266\n",
            "Iteration 71, loss = 0.00136496\n",
            "Iteration 72, loss = 0.00135748\n",
            "Iteration 73, loss = 0.00135012\n",
            "Iteration 74, loss = 0.00134288\n",
            "Iteration 75, loss = 0.00133576\n",
            "Iteration 76, loss = 0.00132875\n",
            "Iteration 77, loss = 0.00132186\n",
            "Iteration 78, loss = 0.00131507\n",
            "Iteration 79, loss = 0.00130840\n",
            "Iteration 80, loss = 0.00130183\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.00129767\n",
            "Iteration 82, loss = 0.00129637\n",
            "Iteration 83, loss = 0.00129509\n",
            "Iteration 84, loss = 0.00129381\n",
            "Iteration 85, loss = 0.00129254\n",
            "Iteration 86, loss = 0.00129127\n",
            "Iteration 87, loss = 0.00129000\n",
            "Iteration 88, loss = 0.00128874\n",
            "Iteration 89, loss = 0.00128748\n",
            "Iteration 90, loss = 0.00128623\n",
            "Iteration 91, loss = 0.00128498\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.00128418\n",
            "Iteration 93, loss = 0.00128393\n",
            "Iteration 94, loss = 0.00128368\n",
            "Iteration 95, loss = 0.00128343\n",
            "Iteration 96, loss = 0.00128318\n",
            "Iteration 97, loss = 0.00128293\n",
            "Iteration 98, loss = 0.00128268\n",
            "Iteration 99, loss = 0.00128244\n",
            "Iteration 100, loss = 0.00128219\n",
            "Iteration 101, loss = 0.00128194\n",
            "Iteration 102, loss = 0.00128169\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 103, loss = 0.00128153\n",
            "Iteration 104, loss = 0.00128148\n",
            "Iteration 105, loss = 0.00128143\n",
            "Iteration 106, loss = 0.00128139\n",
            "Iteration 107, loss = 0.00128134\n",
            "Iteration 108, loss = 0.00128129\n",
            "Iteration 109, loss = 0.00128124\n",
            "Iteration 110, loss = 0.00128119\n",
            "Iteration 111, loss = 0.00128114\n",
            "Iteration 112, loss = 0.00128109\n",
            "Iteration 113, loss = 0.00128104\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.00128101\n",
            "Iteration 115, loss = 0.00128100\n",
            "Iteration 116, loss = 0.00128099\n",
            "Iteration 117, loss = 0.00128098\n",
            "Iteration 118, loss = 0.00128097\n",
            "Iteration 119, loss = 0.00128096\n",
            "Iteration 120, loss = 0.00128095\n",
            "Iteration 121, loss = 0.00128094\n",
            "Iteration 122, loss = 0.00128093\n",
            "Iteration 123, loss = 0.00128092\n",
            "Iteration 124, loss = 0.00128091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.26470550\n",
            "Iteration 3, loss = 0.24429581\n",
            "Iteration 4, loss = 0.22545979\n",
            "Iteration 5, loss = 0.20807608\n",
            "Iteration 6, loss = 0.19203271\n",
            "Iteration 7, loss = 0.17722635\n",
            "Iteration 8, loss = 0.16356160\n",
            "Iteration 9, loss = 0.15095045\n",
            "Iteration 10, loss = 0.13931166\n",
            "Iteration 11, loss = 0.12857027\n",
            "Iteration 12, loss = 0.11865707\n",
            "Iteration 13, loss = 0.10950821\n",
            "Iteration 14, loss = 0.10106476\n",
            "Iteration 15, loss = 0.09327232\n",
            "Iteration 16, loss = 0.08608071\n",
            "Iteration 17, loss = 0.07944360\n",
            "Iteration 18, loss = 0.07331823\n",
            "Iteration 19, loss = 0.06766515\n",
            "Iteration 20, loss = 0.06244794\n",
            "Iteration 21, loss = 0.05763299\n",
            "Iteration 22, loss = 0.05318930\n",
            "Iteration 23, loss = 0.04908822\n",
            "Iteration 24, loss = 0.04530336\n",
            "Iteration 25, loss = 0.04181032\n",
            "Iteration 26, loss = 0.03858660\n",
            "Iteration 27, loss = 0.03561144\n",
            "Iteration 28, loss = 0.03286568\n",
            "Iteration 29, loss = 0.03033163\n",
            "Iteration 30, loss = 0.02799296\n",
            "Iteration 31, loss = 0.02583461\n",
            "Iteration 32, loss = 0.02384267\n",
            "Iteration 33, loss = 0.02200432\n",
            "Iteration 34, loss = 0.02030771\n",
            "Iteration 35, loss = 0.01874192\n",
            "Iteration 36, loss = 0.01729686\n",
            "Iteration 37, loss = 0.01596321\n",
            "Iteration 38, loss = 0.01473239\n",
            "Iteration 39, loss = 0.01359648\n",
            "Iteration 40, loss = 0.01254814\n",
            "Iteration 41, loss = 0.01158064\n",
            "Iteration 42, loss = 0.01068773\n",
            "Iteration 43, loss = 0.00986367\n",
            "Iteration 44, loss = 0.00910315\n",
            "Iteration 45, loss = 0.00840127\n",
            "Iteration 46, loss = 0.00775350\n",
            "Iteration 47, loss = 0.00715568\n",
            "Iteration 48, loss = 0.00660395\n",
            "Iteration 49, loss = 0.00609477\n",
            "Iteration 50, loss = 0.00562484\n",
            "Iteration 51, loss = 0.00519115\n",
            "Iteration 52, loss = 0.00479090\n",
            "Iteration 53, loss = 0.00442156\n",
            "Iteration 54, loss = 0.00408080\n",
            "Iteration 55, loss = 0.00376666\n",
            "Iteration 56, loss = 0.00347769\n",
            "Iteration 57, loss = 0.00321332\n",
            "Iteration 58, loss = 0.00297430\n",
            "Iteration 59, loss = 0.00276252\n",
            "Iteration 60, loss = 0.00257907\n",
            "Iteration 61, loss = 0.00242156\n",
            "Iteration 62, loss = 0.00228440\n",
            "Iteration 63, loss = 0.00216226\n",
            "Iteration 64, loss = 0.00205190\n",
            "Iteration 65, loss = 0.00195154\n",
            "Iteration 66, loss = 0.00186008\n",
            "Iteration 67, loss = 0.00177667\n",
            "Iteration 68, loss = 0.00170057\n",
            "Iteration 69, loss = 0.00163112\n",
            "Iteration 70, loss = 0.00156772\n",
            "Iteration 71, loss = 0.00150981\n",
            "Iteration 72, loss = 0.00145691\n",
            "Iteration 73, loss = 0.00140857\n",
            "Iteration 74, loss = 0.00136436\n",
            "Iteration 75, loss = 0.00132392\n",
            "Iteration 76, loss = 0.00128692\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.00126475\n",
            "Iteration 78, loss = 0.00125804\n",
            "Iteration 79, loss = 0.00125154\n",
            "Iteration 80, loss = 0.00124515\n",
            "Iteration 81, loss = 0.00123887\n",
            "Iteration 82, loss = 0.00123271\n",
            "Iteration 83, loss = 0.00122664\n",
            "Iteration 84, loss = 0.00122069\n",
            "Iteration 85, loss = 0.00121483\n",
            "Iteration 86, loss = 0.00120908\n",
            "Iteration 87, loss = 0.00120342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.00119985\n",
            "Iteration 89, loss = 0.00119873\n",
            "Iteration 90, loss = 0.00119762\n",
            "Iteration 91, loss = 0.00119652\n",
            "Iteration 92, loss = 0.00119543\n",
            "Iteration 93, loss = 0.00119434\n",
            "Iteration 94, loss = 0.00119325\n",
            "Iteration 95, loss = 0.00119217\n",
            "Iteration 96, loss = 0.00119109\n",
            "Iteration 97, loss = 0.00119001\n",
            "Iteration 98, loss = 0.00118894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.00118825\n",
            "Iteration 100, loss = 0.00118804\n",
            "Iteration 101, loss = 0.00118782\n",
            "Iteration 102, loss = 0.00118761\n",
            "Iteration 103, loss = 0.00118740\n",
            "Iteration 104, loss = 0.00118719\n",
            "Iteration 105, loss = 0.00118697\n",
            "Iteration 106, loss = 0.00118676\n",
            "Iteration 107, loss = 0.00118655\n",
            "Iteration 108, loss = 0.00118634\n",
            "Iteration 109, loss = 0.00118612\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.00118599\n",
            "Iteration 111, loss = 0.00118594\n",
            "Iteration 112, loss = 0.00118590\n",
            "Iteration 113, loss = 0.00118586\n",
            "Iteration 114, loss = 0.00118582\n",
            "Iteration 115, loss = 0.00118577\n",
            "Iteration 116, loss = 0.00118573\n",
            "Iteration 117, loss = 0.00118569\n",
            "Iteration 118, loss = 0.00118565\n",
            "Iteration 119, loss = 0.00118561\n",
            "Iteration 120, loss = 0.00118556\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.00118554\n",
            "Iteration 122, loss = 0.00118553\n",
            "Iteration 123, loss = 0.00118552\n",
            "Iteration 124, loss = 0.00118551\n",
            "Iteration 125, loss = 0.00118550\n",
            "Iteration 126, loss = 0.00118549\n",
            "Iteration 127, loss = 0.00118548\n",
            "Iteration 128, loss = 0.00118548\n",
            "Iteration 129, loss = 0.00118547\n",
            "Iteration 130, loss = 0.00118546\n",
            "Iteration 131, loss = 0.00118545\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33847     0]\n",
            " [ 3774     0]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  5.4min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95     33847\n",
            "           1       0.00      0.00      0.00      3774\n",
            "\n",
            "    accuracy                           0.90     37621\n",
            "   macro avg       0.45      0.50      0.47     37621\n",
            "weighted avg       0.81      0.90      0.85     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.42441039\n",
            "Iteration 3, loss = 0.39300004\n",
            "Iteration 4, loss = 0.37058842\n",
            "Iteration 5, loss = 0.35399869\n",
            "Iteration 6, loss = 0.34278176\n",
            "Iteration 7, loss = 0.33383233\n",
            "Iteration 8, loss = 0.32753492\n",
            "Iteration 9, loss = 0.32305722\n",
            "Iteration 10, loss = 0.31907923\n",
            "Iteration 11, loss = 0.31782830\n",
            "Iteration 12, loss = 0.31596749\n",
            "Iteration 13, loss = 0.31405618\n",
            "Iteration 14, loss = 0.31414408\n",
            "Iteration 15, loss = 0.31148500\n",
            "Iteration 16, loss = 0.31255907\n",
            "Iteration 17, loss = 0.31227197\n",
            "Iteration 18, loss = 0.31207987\n",
            "Iteration 19, loss = 0.31180801\n",
            "Iteration 20, loss = 0.31168960\n",
            "Iteration 21, loss = 0.31287150\n",
            "Iteration 22, loss = 0.31261124\n",
            "Iteration 23, loss = 0.31233602\n",
            "Iteration 24, loss = 0.31171260\n",
            "Iteration 25, loss = 0.31112928\n",
            "Iteration 26, loss = 0.31082099\n",
            "Iteration 27, loss = 0.31183298\n",
            "Iteration 28, loss = 0.31060038\n",
            "Iteration 29, loss = 0.31050814\n",
            "Iteration 30, loss = 0.31177810\n",
            "Iteration 31, loss = 0.31060830\n",
            "Iteration 32, loss = 0.30977306\n",
            "Iteration 33, loss = 0.31104007\n",
            "Iteration 34, loss = 0.31089050\n",
            "Iteration 35, loss = 0.30989601\n",
            "Iteration 36, loss = 0.31124164\n",
            "Iteration 37, loss = 0.31032742\n",
            "Iteration 38, loss = 0.31007490\n",
            "Iteration 39, loss = 0.30982031\n",
            "Iteration 40, loss = 0.31065553\n",
            "Iteration 41, loss = 0.31060760\n",
            "Iteration 42, loss = 0.31094914\n",
            "Iteration 43, loss = 0.31059990\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.30127571\n",
            "Iteration 45, loss = 0.29881335\n",
            "Iteration 46, loss = 0.29686329\n",
            "Iteration 47, loss = 0.29487985\n",
            "Iteration 48, loss = 0.29366438\n",
            "Iteration 49, loss = 0.29215959\n",
            "Iteration 50, loss = 0.29033477\n",
            "Iteration 51, loss = 0.28893557\n",
            "Iteration 52, loss = 0.28733010\n",
            "Iteration 53, loss = 0.28682432\n",
            "Iteration 54, loss = 0.28710384\n",
            "Iteration 55, loss = 0.28614768\n",
            "Iteration 56, loss = 0.28648715\n",
            "Iteration 57, loss = 0.28693323\n",
            "Iteration 58, loss = 0.28948993\n",
            "Iteration 59, loss = 0.28812598\n",
            "Iteration 60, loss = 0.28912435\n",
            "Iteration 61, loss = 0.29204100\n",
            "Iteration 62, loss = 0.28586031\n",
            "Iteration 63, loss = 0.29135363\n",
            "Iteration 64, loss = 0.29070203\n",
            "Iteration 65, loss = 0.28957580\n",
            "Iteration 66, loss = 0.28939333\n",
            "Iteration 67, loss = 0.29239602\n",
            "Iteration 68, loss = 0.29437972\n",
            "Iteration 69, loss = 0.29569343\n",
            "Iteration 70, loss = 0.29356881\n",
            "Iteration 71, loss = 0.29090571\n",
            "Iteration 72, loss = 0.29342788\n",
            "Iteration 73, loss = 0.28988081\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.26585642\n",
            "Iteration 75, loss = 0.26493920\n",
            "Iteration 76, loss = 0.26446127\n",
            "Iteration 77, loss = 0.26369691\n",
            "Iteration 78, loss = 0.26297580\n",
            "Iteration 79, loss = 0.26244378\n",
            "Iteration 80, loss = 0.26153136\n",
            "Iteration 81, loss = 0.26110545\n",
            "Iteration 82, loss = 0.26061429\n",
            "Iteration 83, loss = 0.25999378\n",
            "Iteration 84, loss = 0.25950437\n",
            "Iteration 85, loss = 0.25892151\n",
            "Iteration 86, loss = 0.25831864\n",
            "Iteration 87, loss = 0.25757575\n",
            "Iteration 88, loss = 0.25712426\n",
            "Iteration 89, loss = 0.25647580\n",
            "Iteration 90, loss = 0.25605163\n",
            "Iteration 91, loss = 0.25573934\n",
            "Iteration 92, loss = 0.25481014\n",
            "Iteration 93, loss = 0.25466278\n",
            "Iteration 94, loss = 0.25399433\n",
            "Iteration 95, loss = 0.25346648\n",
            "Iteration 96, loss = 0.25314466\n",
            "Iteration 97, loss = 0.25267187\n",
            "Iteration 98, loss = 0.25201113\n",
            "Iteration 99, loss = 0.25148895\n",
            "Iteration 100, loss = 0.25107441\n",
            "Iteration 101, loss = 0.25058737\n",
            "Iteration 102, loss = 0.25035983\n",
            "Iteration 103, loss = 0.25001003\n",
            "Iteration 104, loss = 0.24961729\n",
            "Iteration 105, loss = 0.24889582\n",
            "Iteration 106, loss = 0.24857880\n",
            "Iteration 107, loss = 0.24846853\n",
            "Iteration 108, loss = 0.24791291\n",
            "Iteration 109, loss = 0.24747630\n",
            "Iteration 110, loss = 0.24711438\n",
            "Iteration 111, loss = 0.24669607\n",
            "Iteration 112, loss = 0.24655197\n",
            "Iteration 113, loss = 0.24617368\n",
            "Iteration 114, loss = 0.24565723\n",
            "Iteration 115, loss = 0.24544295\n",
            "Iteration 116, loss = 0.24526205\n",
            "Iteration 117, loss = 0.24492247\n",
            "Iteration 118, loss = 0.24432336\n",
            "Iteration 119, loss = 0.24436557\n",
            "Iteration 120, loss = 0.24424198\n",
            "Iteration 121, loss = 0.24361311\n",
            "Iteration 122, loss = 0.24328500\n",
            "Iteration 123, loss = 0.24296174\n",
            "Iteration 124, loss = 0.24280274\n",
            "Iteration 125, loss = 0.24291215\n",
            "Iteration 126, loss = 0.24249407\n",
            "Iteration 127, loss = 0.24191130\n",
            "Iteration 128, loss = 0.24173452\n",
            "Iteration 129, loss = 0.24142441\n",
            "Iteration 130, loss = 0.24126222\n",
            "Iteration 131, loss = 0.24099542\n",
            "Iteration 132, loss = 0.24100804\n",
            "Iteration 133, loss = 0.24083096\n",
            "Iteration 134, loss = 0.23998761\n",
            "Iteration 135, loss = 0.23971458\n",
            "Iteration 136, loss = 0.23973956\n",
            "Iteration 137, loss = 0.23975057\n",
            "Iteration 138, loss = 0.23973423\n",
            "Iteration 139, loss = 0.23901481\n",
            "Iteration 140, loss = 0.23892339\n",
            "Iteration 141, loss = 0.23863178\n",
            "Iteration 142, loss = 0.23836592\n",
            "Iteration 143, loss = 0.23833107\n",
            "Iteration 144, loss = 0.23806418\n",
            "Iteration 145, loss = 0.23826205\n",
            "Iteration 146, loss = 0.23799876\n",
            "Iteration 147, loss = 0.23773439\n",
            "Iteration 148, loss = 0.23767751\n",
            "Iteration 149, loss = 0.23737357\n",
            "Iteration 150, loss = 0.23739055\n",
            "Iteration 151, loss = 0.23707345\n",
            "Iteration 152, loss = 0.23715831\n",
            "Iteration 153, loss = 0.23702647\n",
            "Iteration 154, loss = 0.23635797\n",
            "Iteration 155, loss = 0.23651497\n",
            "Iteration 156, loss = 0.23638384\n",
            "Iteration 157, loss = 0.23626539\n",
            "Iteration 158, loss = 0.23604302\n",
            "Iteration 159, loss = 0.23601059\n",
            "Iteration 160, loss = 0.23585154\n",
            "Iteration 161, loss = 0.23604236\n",
            "Iteration 162, loss = 0.23568626\n",
            "Iteration 163, loss = 0.23580458\n",
            "Iteration 164, loss = 0.23563054\n",
            "Iteration 165, loss = 0.23539650\n",
            "Iteration 166, loss = 0.23563490\n",
            "Iteration 167, loss = 0.23505718\n",
            "Iteration 168, loss = 0.23537221\n",
            "Iteration 169, loss = 0.23533736\n",
            "Iteration 170, loss = 0.23509642\n",
            "Iteration 171, loss = 0.23500903\n",
            "Iteration 172, loss = 0.23466364\n",
            "Iteration 173, loss = 0.23463375\n",
            "Iteration 174, loss = 0.23479586\n",
            "Iteration 175, loss = 0.23494678\n",
            "Iteration 176, loss = 0.23455573\n",
            "Iteration 177, loss = 0.23424524\n",
            "Iteration 178, loss = 0.23400313\n",
            "Iteration 179, loss = 0.23393301\n",
            "Iteration 180, loss = 0.23418602\n",
            "Iteration 181, loss = 0.23391107\n",
            "Iteration 182, loss = 0.23372707\n",
            "Iteration 183, loss = 0.23402622\n",
            "Iteration 184, loss = 0.23404425\n",
            "Iteration 185, loss = 0.23366457\n",
            "Iteration 186, loss = 0.23423971\n",
            "Iteration 187, loss = 0.23337267\n",
            "Iteration 188, loss = 0.23418063\n",
            "Iteration 189, loss = 0.23360735\n",
            "Iteration 190, loss = 0.23325786\n",
            "Iteration 191, loss = 0.23335981\n",
            "Iteration 192, loss = 0.23358582\n",
            "Iteration 193, loss = 0.23310036\n",
            "Iteration 194, loss = 0.23356953\n",
            "Iteration 195, loss = 0.23317566\n",
            "Iteration 196, loss = 0.23368230\n",
            "Iteration 197, loss = 0.23293076\n",
            "Iteration 198, loss = 0.23319396\n",
            "Iteration 199, loss = 0.23304537\n",
            "Iteration 200, loss = 0.23302473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.24655688\n",
            "Iteration 3, loss = 0.20853281\n",
            "Iteration 4, loss = 0.17966522\n",
            "Iteration 5, loss = 0.15854395\n",
            "Iteration 6, loss = 0.14214686\n",
            "Iteration 7, loss = 0.12991679\n",
            "Iteration 8, loss = 0.12050271\n",
            "Iteration 9, loss = 0.11313924\n",
            "Iteration 10, loss = 0.10771776\n",
            "Iteration 11, loss = 0.10378813\n",
            "Iteration 12, loss = 0.10062128\n",
            "Iteration 13, loss = 0.09821410\n",
            "Iteration 14, loss = 0.09645977\n",
            "Iteration 15, loss = 0.09508849\n",
            "Iteration 16, loss = 0.09391978\n",
            "Iteration 17, loss = 0.09293560\n",
            "Iteration 18, loss = 0.09244669\n",
            "Iteration 19, loss = 0.09231952\n",
            "Iteration 20, loss = 0.09163985\n",
            "Iteration 21, loss = 0.09129820\n",
            "Iteration 22, loss = 0.09124306\n",
            "Iteration 23, loss = 0.09106396\n",
            "Iteration 24, loss = 0.09125124\n",
            "Iteration 25, loss = 0.09074522\n",
            "Iteration 26, loss = 0.09049111\n",
            "Iteration 27, loss = 0.09072832\n",
            "Iteration 28, loss = 0.09070434\n",
            "Iteration 29, loss = 0.09075917\n",
            "Iteration 30, loss = 0.09050171\n",
            "Iteration 31, loss = 0.09032322\n",
            "Iteration 32, loss = 0.09050352\n",
            "Iteration 33, loss = 0.09029412\n",
            "Iteration 34, loss = 0.09057192\n",
            "Iteration 35, loss = 0.09037794\n",
            "Iteration 36, loss = 0.09049538\n",
            "Iteration 37, loss = 0.09041924\n",
            "Iteration 38, loss = 0.09036015\n",
            "Iteration 39, loss = 0.09029320\n",
            "Iteration 40, loss = 0.09001690\n",
            "Iteration 41, loss = 0.09042997\n",
            "Iteration 42, loss = 0.09038130\n",
            "Iteration 43, loss = 0.09027605\n",
            "Iteration 44, loss = 0.09054230\n",
            "Iteration 45, loss = 0.09018905\n",
            "Iteration 46, loss = 0.09032327\n",
            "Iteration 47, loss = 0.09028218\n",
            "Iteration 48, loss = 0.09041762\n",
            "Iteration 49, loss = 0.09008323\n",
            "Iteration 50, loss = 0.09022531\n",
            "Iteration 51, loss = 0.09033943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.08918179\n",
            "Iteration 53, loss = 0.08913561\n",
            "Iteration 54, loss = 0.08919870\n",
            "Iteration 55, loss = 0.08920192\n",
            "Iteration 56, loss = 0.08904889\n",
            "Iteration 57, loss = 0.08909817\n",
            "Iteration 58, loss = 0.08902349\n",
            "Iteration 59, loss = 0.08899157\n",
            "Iteration 60, loss = 0.08904547\n",
            "Iteration 61, loss = 0.08874632\n",
            "Iteration 62, loss = 0.08889361\n",
            "Iteration 63, loss = 0.08883672\n",
            "Iteration 64, loss = 0.08878693\n",
            "Iteration 65, loss = 0.08888509\n",
            "Iteration 66, loss = 0.08897935\n",
            "Iteration 67, loss = 0.08889534\n",
            "Iteration 68, loss = 0.08872114\n",
            "Iteration 69, loss = 0.08874200\n",
            "Iteration 70, loss = 0.08874312\n",
            "Iteration 71, loss = 0.08899186\n",
            "Iteration 72, loss = 0.08867680\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.08831028\n",
            "Iteration 74, loss = 0.08826365\n",
            "Iteration 75, loss = 0.08824917\n",
            "Iteration 76, loss = 0.08822124\n",
            "Iteration 77, loss = 0.08823383\n",
            "Iteration 78, loss = 0.08819951\n",
            "Iteration 79, loss = 0.08822144\n",
            "Iteration 80, loss = 0.08817786\n",
            "Iteration 81, loss = 0.08820719\n",
            "Iteration 82, loss = 0.08819373\n",
            "Iteration 83, loss = 0.08826144\n",
            "Iteration 84, loss = 0.08818891\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 85, loss = 0.08806631\n",
            "Iteration 86, loss = 0.08806313\n",
            "Iteration 87, loss = 0.08806634\n",
            "Iteration 88, loss = 0.08805493\n",
            "Iteration 89, loss = 0.08807384\n",
            "Iteration 90, loss = 0.08806871\n",
            "Iteration 91, loss = 0.08807240\n",
            "Iteration 92, loss = 0.08804626\n",
            "Iteration 93, loss = 0.08807540\n",
            "Iteration 94, loss = 0.08805642\n",
            "Iteration 95, loss = 0.08805416\n",
            "Iteration 96, loss = 0.08806016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 97, loss = 0.08802742\n",
            "Iteration 98, loss = 0.08802526\n",
            "Iteration 99, loss = 0.08802330\n",
            "Iteration 100, loss = 0.08802079\n",
            "Iteration 101, loss = 0.08802305\n",
            "Iteration 102, loss = 0.08802180\n",
            "Iteration 103, loss = 0.08802141\n",
            "Iteration 104, loss = 0.08802355\n",
            "Iteration 105, loss = 0.08802047\n",
            "Iteration 106, loss = 0.08801912\n",
            "Iteration 107, loss = 0.08802231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 108, loss = 0.08801519\n",
            "Iteration 109, loss = 0.08801330\n",
            "Iteration 110, loss = 0.08801390\n",
            "Iteration 111, loss = 0.08801425\n",
            "Iteration 112, loss = 0.08801361\n",
            "Iteration 113, loss = 0.08801405\n",
            "Iteration 114, loss = 0.08801306\n",
            "Iteration 115, loss = 0.08801299\n",
            "Iteration 116, loss = 0.08801441\n",
            "Iteration 117, loss = 0.08801350\n",
            "Iteration 118, loss = 0.08801352\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.24580874\n",
            "Iteration 3, loss = 0.20872217\n",
            "Iteration 4, loss = 0.18043752\n",
            "Iteration 5, loss = 0.15891566\n",
            "Iteration 6, loss = 0.14235879\n",
            "Iteration 7, loss = 0.13018004\n",
            "Iteration 8, loss = 0.12036361\n",
            "Iteration 9, loss = 0.11332238\n",
            "Iteration 10, loss = 0.10802982\n",
            "Iteration 11, loss = 0.10414603\n",
            "Iteration 12, loss = 0.10074534\n",
            "Iteration 13, loss = 0.09824155\n",
            "Iteration 14, loss = 0.09653232\n",
            "Iteration 15, loss = 0.09503178\n",
            "Iteration 16, loss = 0.09396522\n",
            "Iteration 17, loss = 0.09302210\n",
            "Iteration 18, loss = 0.09252822\n",
            "Iteration 19, loss = 0.09203411\n",
            "Iteration 20, loss = 0.09162402\n",
            "Iteration 21, loss = 0.09155866\n",
            "Iteration 22, loss = 0.09109188\n",
            "Iteration 23, loss = 0.09104729\n",
            "Iteration 24, loss = 0.09133688\n",
            "Iteration 25, loss = 0.09070197\n",
            "Iteration 26, loss = 0.09072475\n",
            "Iteration 27, loss = 0.09088350\n",
            "Iteration 28, loss = 0.09039440\n",
            "Iteration 29, loss = 0.09044998\n",
            "Iteration 30, loss = 0.09061465\n",
            "Iteration 31, loss = 0.09045808\n",
            "Iteration 32, loss = 0.09077189\n",
            "Iteration 33, loss = 0.09059565\n",
            "Iteration 34, loss = 0.09032728\n",
            "Iteration 35, loss = 0.09081866\n",
            "Iteration 36, loss = 0.09049697\n",
            "Iteration 37, loss = 0.09059689\n",
            "Iteration 38, loss = 0.09052423\n",
            "Iteration 39, loss = 0.09075305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 40, loss = 0.08947841\n",
            "Iteration 41, loss = 0.08940713\n",
            "Iteration 42, loss = 0.08937665\n",
            "Iteration 43, loss = 0.08950267\n",
            "Iteration 44, loss = 0.08931946\n",
            "Iteration 45, loss = 0.08930872\n",
            "Iteration 46, loss = 0.08944379\n",
            "Iteration 47, loss = 0.08928663\n",
            "Iteration 48, loss = 0.08923690\n",
            "Iteration 49, loss = 0.08934378\n",
            "Iteration 50, loss = 0.08921428\n",
            "Iteration 51, loss = 0.08929756\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.08887116\n",
            "Iteration 53, loss = 0.08878953\n",
            "Iteration 54, loss = 0.08877174\n",
            "Iteration 55, loss = 0.08878004\n",
            "Iteration 56, loss = 0.08878864\n",
            "Iteration 57, loss = 0.08877180\n",
            "Iteration 58, loss = 0.08875351\n",
            "Iteration 59, loss = 0.08876423\n",
            "Iteration 60, loss = 0.08880141\n",
            "Iteration 61, loss = 0.08871758\n",
            "Iteration 62, loss = 0.08874869\n",
            "Iteration 63, loss = 0.08876709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 64, loss = 0.08861257\n",
            "Iteration 65, loss = 0.08860791\n",
            "Iteration 66, loss = 0.08861372\n",
            "Iteration 67, loss = 0.08858887\n",
            "Iteration 68, loss = 0.08861140\n",
            "Iteration 69, loss = 0.08861157\n",
            "Iteration 70, loss = 0.08859991\n",
            "Iteration 71, loss = 0.08858261\n",
            "Iteration 72, loss = 0.08860269\n",
            "Iteration 73, loss = 0.08861378\n",
            "Iteration 74, loss = 0.08860387\n",
            "Iteration 75, loss = 0.08861941\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 76, loss = 0.08857809\n",
            "Iteration 77, loss = 0.08857410\n",
            "Iteration 78, loss = 0.08857795\n",
            "Iteration 79, loss = 0.08857425\n",
            "Iteration 80, loss = 0.08857363\n",
            "Iteration 81, loss = 0.08857597\n",
            "Iteration 82, loss = 0.08857911\n",
            "Iteration 83, loss = 0.08857697\n",
            "Iteration 84, loss = 0.08857532\n",
            "Iteration 85, loss = 0.08857341\n",
            "Iteration 86, loss = 0.08857439\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 87, loss = 0.08856953\n",
            "Iteration 88, loss = 0.08856873\n",
            "Iteration 89, loss = 0.08856892\n",
            "Iteration 90, loss = 0.08856906\n",
            "Iteration 91, loss = 0.08856860\n",
            "Iteration 92, loss = 0.08856834\n",
            "Iteration 93, loss = 0.08856908\n",
            "Iteration 94, loss = 0.08856798\n",
            "Iteration 95, loss = 0.08856905\n",
            "Iteration 96, loss = 0.08856856\n",
            "Iteration 97, loss = 0.08856782\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.22458507\n",
            "Iteration 3, loss = 0.19217087\n",
            "Iteration 4, loss = 0.16749103\n",
            "Iteration 5, loss = 0.14902249\n",
            "Iteration 6, loss = 0.13461617\n",
            "Iteration 7, loss = 0.12438192\n",
            "Iteration 8, loss = 0.11599334\n",
            "Iteration 9, loss = 0.10999869\n",
            "Iteration 10, loss = 0.10515974\n",
            "Iteration 11, loss = 0.10177074\n",
            "Iteration 12, loss = 0.09902700\n",
            "Iteration 13, loss = 0.09693672\n",
            "Iteration 14, loss = 0.09519543\n",
            "Iteration 15, loss = 0.09400156\n",
            "Iteration 16, loss = 0.09305213\n",
            "Iteration 17, loss = 0.09253026\n",
            "Iteration 18, loss = 0.09203839\n",
            "Iteration 19, loss = 0.09159309\n",
            "Iteration 20, loss = 0.09148229\n",
            "Iteration 21, loss = 0.09119878\n",
            "Iteration 22, loss = 0.09066873\n",
            "Iteration 23, loss = 0.09065681\n",
            "Iteration 24, loss = 0.09012730\n",
            "Iteration 25, loss = 0.09033471\n",
            "Iteration 26, loss = 0.09022755\n",
            "Iteration 27, loss = 0.09017737\n",
            "Iteration 28, loss = 0.09028293\n",
            "Iteration 29, loss = 0.09017033\n",
            "Iteration 30, loss = 0.09016613\n",
            "Iteration 31, loss = 0.09023591\n",
            "Iteration 32, loss = 0.09016647\n",
            "Iteration 33, loss = 0.09031236\n",
            "Iteration 34, loss = 0.09004777\n",
            "Iteration 35, loss = 0.09036355\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.08907117\n",
            "Iteration 37, loss = 0.08903625\n",
            "Iteration 38, loss = 0.08912444\n",
            "Iteration 39, loss = 0.08892880\n",
            "Iteration 40, loss = 0.08898000\n",
            "Iteration 41, loss = 0.08895681\n",
            "Iteration 42, loss = 0.08889074\n",
            "Iteration 43, loss = 0.08893394\n",
            "Iteration 44, loss = 0.08905447\n",
            "Iteration 45, loss = 0.08888069\n",
            "Iteration 46, loss = 0.08889215\n",
            "Iteration 47, loss = 0.08882225\n",
            "Iteration 48, loss = 0.08889425\n",
            "Iteration 49, loss = 0.08887297\n",
            "Iteration 50, loss = 0.08881956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 51, loss = 0.08850200\n",
            "Iteration 52, loss = 0.08843172\n",
            "Iteration 53, loss = 0.08852414\n",
            "Iteration 54, loss = 0.08838227\n",
            "Iteration 55, loss = 0.08846823\n",
            "Iteration 56, loss = 0.08842182\n",
            "Iteration 57, loss = 0.08842558\n",
            "Iteration 58, loss = 0.08847688\n",
            "Iteration 59, loss = 0.08841138\n",
            "Iteration 60, loss = 0.08837974\n",
            "Iteration 61, loss = 0.08839084\n",
            "Iteration 62, loss = 0.08839391\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 63, loss = 0.08827314\n",
            "Iteration 64, loss = 0.08827471\n",
            "Iteration 65, loss = 0.08826335\n",
            "Iteration 66, loss = 0.08828340\n",
            "Iteration 67, loss = 0.08826765\n",
            "Iteration 68, loss = 0.08825886\n",
            "Iteration 69, loss = 0.08827377\n",
            "Iteration 70, loss = 0.08827216\n",
            "Iteration 71, loss = 0.08826646\n",
            "Iteration 72, loss = 0.08827301\n",
            "Iteration 73, loss = 0.08824660\n",
            "Iteration 74, loss = 0.08824123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 75, loss = 0.08827814\n",
            "Iteration 76, loss = 0.08823997\n",
            "Iteration 77, loss = 0.08823601\n",
            "Iteration 78, loss = 0.08823804\n",
            "Iteration 79, loss = 0.08823433\n",
            "Iteration 80, loss = 0.08823758\n",
            "Iteration 81, loss = 0.08823550\n",
            "Iteration 82, loss = 0.08823456\n",
            "Iteration 83, loss = 0.08823793\n",
            "Iteration 84, loss = 0.08823491\n",
            "Iteration 85, loss = 0.08823413\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 86, loss = 0.08823309\n",
            "Iteration 87, loss = 0.08822903\n",
            "Iteration 88, loss = 0.08822976\n",
            "Iteration 89, loss = 0.08823017\n",
            "Iteration 90, loss = 0.08823032\n",
            "Iteration 91, loss = 0.08822980\n",
            "Iteration 92, loss = 0.08822913\n",
            "Iteration 93, loss = 0.08822980\n",
            "Iteration 94, loss = 0.08822924\n",
            "Iteration 95, loss = 0.08822949\n",
            "Iteration 96, loss = 0.08822872\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.57733098\n",
            "Iteration 2, loss = 0.18138137\n",
            "Iteration 3, loss = 0.15850622\n",
            "Iteration 4, loss = 0.14182860\n",
            "Iteration 5, loss = 0.12960875\n",
            "Iteration 6, loss = 0.11995306\n",
            "Iteration 7, loss = 0.11286302\n",
            "Iteration 8, loss = 0.10746705\n",
            "Iteration 9, loss = 0.10342992\n",
            "Iteration 10, loss = 0.10019796\n",
            "Iteration 11, loss = 0.09794644\n",
            "Iteration 12, loss = 0.09605692\n",
            "Iteration 13, loss = 0.09491361\n",
            "Iteration 14, loss = 0.09364426\n",
            "Iteration 15, loss = 0.09287601\n",
            "Iteration 16, loss = 0.09235073\n",
            "Iteration 17, loss = 0.09249377\n",
            "Iteration 18, loss = 0.09163388\n",
            "Iteration 19, loss = 0.09142619\n",
            "Iteration 20, loss = 0.09123754\n",
            "Iteration 21, loss = 0.09098857\n",
            "Iteration 22, loss = 0.09104221\n",
            "Iteration 23, loss = 0.09051399\n",
            "Iteration 24, loss = 0.09080850\n",
            "Iteration 25, loss = 0.09054138\n",
            "Iteration 26, loss = 0.09061321\n",
            "Iteration 27, loss = 0.09037582\n",
            "Iteration 28, loss = 0.09052634\n",
            "Iteration 29, loss = 0.09023025\n",
            "Iteration 30, loss = 0.09055703\n",
            "Iteration 31, loss = 0.09068870\n",
            "Iteration 32, loss = 0.09029729\n",
            "Iteration 33, loss = 0.09032601\n",
            "Iteration 34, loss = 0.09061242\n",
            "Iteration 35, loss = 0.09037333\n",
            "Iteration 36, loss = 0.09040885\n",
            "Iteration 37, loss = 0.09046777\n",
            "Iteration 38, loss = 0.09030439\n",
            "Iteration 39, loss = 0.09045869\n",
            "Iteration 40, loss = 0.09035331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 41, loss = 0.08931153\n",
            "Iteration 42, loss = 0.08925370\n",
            "Iteration 43, loss = 0.08919016\n",
            "Iteration 44, loss = 0.08915827\n",
            "Iteration 45, loss = 0.08916726\n",
            "Iteration 46, loss = 0.08922238\n",
            "Iteration 47, loss = 0.08904218\n",
            "Iteration 48, loss = 0.08902191\n",
            "Iteration 49, loss = 0.08905052\n",
            "Iteration 50, loss = 0.08905956\n",
            "Iteration 51, loss = 0.08915038\n",
            "Iteration 52, loss = 0.08904656\n",
            "Iteration 53, loss = 0.08899044\n",
            "Iteration 54, loss = 0.08911015\n",
            "Iteration 55, loss = 0.08903069\n",
            "Iteration 56, loss = 0.08893465\n",
            "Iteration 57, loss = 0.08900692\n",
            "Iteration 58, loss = 0.08899378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.08851918\n",
            "Iteration 60, loss = 0.08842847\n",
            "Iteration 61, loss = 0.08846473\n",
            "Iteration 62, loss = 0.08846556\n",
            "Iteration 63, loss = 0.08846552\n",
            "Iteration 64, loss = 0.08839405\n",
            "Iteration 65, loss = 0.08840164\n",
            "Iteration 66, loss = 0.08835554\n",
            "Iteration 67, loss = 0.08844187\n",
            "Iteration 68, loss = 0.08842922\n",
            "Iteration 69, loss = 0.08838686\n",
            "Iteration 70, loss = 0.08837812\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.08828851\n",
            "Iteration 72, loss = 0.08825792\n",
            "Iteration 73, loss = 0.08827134\n",
            "Iteration 74, loss = 0.08826175\n",
            "Iteration 75, loss = 0.08827992\n",
            "Iteration 76, loss = 0.08825807\n",
            "Iteration 77, loss = 0.08825361\n",
            "Iteration 78, loss = 0.08826035\n",
            "Iteration 79, loss = 0.08825606\n",
            "Iteration 80, loss = 0.08825149\n",
            "Iteration 81, loss = 0.08825756\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 82, loss = 0.08823744\n",
            "Iteration 83, loss = 0.08822254\n",
            "Iteration 84, loss = 0.08822294\n",
            "Iteration 85, loss = 0.08822110\n",
            "Iteration 86, loss = 0.08822357\n",
            "Iteration 87, loss = 0.08821700\n",
            "Iteration 88, loss = 0.08822444\n",
            "Iteration 89, loss = 0.08822082\n",
            "Iteration 90, loss = 0.08821994\n",
            "Iteration 91, loss = 0.08821868\n",
            "Iteration 92, loss = 0.08821449\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 93, loss = 0.08821185\n",
            "Iteration 94, loss = 0.08821125\n",
            "Iteration 95, loss = 0.08821222\n",
            "Iteration 96, loss = 0.08821148\n",
            "Iteration 97, loss = 0.08821174\n",
            "Iteration 98, loss = 0.08821147\n",
            "Iteration 99, loss = 0.08821141\n",
            "Iteration 100, loss = 0.08821112\n",
            "Iteration 101, loss = 0.08821202\n",
            "Iteration 102, loss = 0.08821052\n",
            "Iteration 103, loss = 0.08821100\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23044391\n",
            "Iteration 3, loss = 0.19646185\n",
            "Iteration 4, loss = 0.17096550\n",
            "Iteration 5, loss = 0.15175095\n",
            "Iteration 6, loss = 0.13733941\n",
            "Iteration 7, loss = 0.12591798\n",
            "Iteration 8, loss = 0.11791661\n",
            "Iteration 9, loss = 0.11130339\n",
            "Iteration 10, loss = 0.10647522\n",
            "Iteration 11, loss = 0.10231513\n",
            "Iteration 12, loss = 0.09952526\n",
            "Iteration 13, loss = 0.09762579\n",
            "Iteration 14, loss = 0.09600320\n",
            "Iteration 15, loss = 0.09460855\n",
            "Iteration 16, loss = 0.09380581\n",
            "Iteration 17, loss = 0.09330061\n",
            "Iteration 18, loss = 0.09219169\n",
            "Iteration 19, loss = 0.09211815\n",
            "Iteration 20, loss = 0.09164590\n",
            "Iteration 21, loss = 0.09155884\n",
            "Iteration 22, loss = 0.09121922\n",
            "Iteration 23, loss = 0.09092386\n",
            "Iteration 24, loss = 0.09096386\n",
            "Iteration 25, loss = 0.09083259\n",
            "Iteration 26, loss = 0.09057791\n",
            "Iteration 27, loss = 0.09067887\n",
            "Iteration 28, loss = 0.09076893\n",
            "Iteration 29, loss = 0.09061765\n",
            "Iteration 30, loss = 0.09070100\n",
            "Iteration 31, loss = 0.09073886\n",
            "Iteration 32, loss = 0.09088577\n",
            "Iteration 33, loss = 0.09042162\n",
            "Iteration 34, loss = 0.09031121\n",
            "Iteration 35, loss = 0.09030675\n",
            "Iteration 36, loss = 0.09052160\n",
            "Iteration 37, loss = 0.09037087\n",
            "Iteration 38, loss = 0.09037083\n",
            "Iteration 39, loss = 0.09040125\n",
            "Iteration 40, loss = 0.09052302\n",
            "Iteration 41, loss = 0.09051685\n",
            "Iteration 42, loss = 0.09057313\n",
            "Iteration 43, loss = 0.09068650\n",
            "Iteration 44, loss = 0.09063038\n",
            "Iteration 45, loss = 0.09060735\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 46, loss = 0.08955737\n",
            "Iteration 47, loss = 0.08948473\n",
            "Iteration 48, loss = 0.08928276\n",
            "Iteration 49, loss = 0.08947157\n",
            "Iteration 50, loss = 0.08931490\n",
            "Iteration 51, loss = 0.08935915\n",
            "Iteration 52, loss = 0.08944151\n",
            "Iteration 53, loss = 0.08926647\n",
            "Iteration 54, loss = 0.08937960\n",
            "Iteration 55, loss = 0.08936103\n",
            "Iteration 56, loss = 0.08933693\n",
            "Iteration 57, loss = 0.08928148\n",
            "Iteration 58, loss = 0.08922877\n",
            "Iteration 59, loss = 0.08936741\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 60, loss = 0.08888787\n",
            "Iteration 61, loss = 0.08887248\n",
            "Iteration 62, loss = 0.08878146\n",
            "Iteration 63, loss = 0.08883486\n",
            "Iteration 64, loss = 0.08881727\n",
            "Iteration 65, loss = 0.08883269\n",
            "Iteration 66, loss = 0.08881329\n",
            "Iteration 67, loss = 0.08886993\n",
            "Iteration 68, loss = 0.08880981\n",
            "Iteration 69, loss = 0.08884469\n",
            "Iteration 70, loss = 0.08880176\n",
            "Iteration 71, loss = 0.08886619\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 72, loss = 0.08873384\n",
            "Iteration 73, loss = 0.08870141\n",
            "Iteration 74, loss = 0.08869676\n",
            "Iteration 75, loss = 0.08868482\n",
            "Iteration 76, loss = 0.08869813\n",
            "Iteration 77, loss = 0.08869590\n",
            "Iteration 78, loss = 0.08869218\n",
            "Iteration 79, loss = 0.08868951\n",
            "Iteration 80, loss = 0.08868656\n",
            "Iteration 81, loss = 0.08870018\n",
            "Iteration 82, loss = 0.08869198\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 83, loss = 0.08867436\n",
            "Iteration 84, loss = 0.08866201\n",
            "Iteration 85, loss = 0.08866028\n",
            "Iteration 86, loss = 0.08865822\n",
            "Iteration 87, loss = 0.08866090\n",
            "Iteration 88, loss = 0.08865865\n",
            "Iteration 89, loss = 0.08866069\n",
            "Iteration 90, loss = 0.08866020\n",
            "Iteration 91, loss = 0.08865977\n",
            "Iteration 92, loss = 0.08865686\n",
            "Iteration 93, loss = 0.08865719\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 94, loss = 0.08866065\n",
            "Iteration 95, loss = 0.08865398\n",
            "Iteration 96, loss = 0.08865326\n",
            "Iteration 97, loss = 0.08865357\n",
            "Iteration 98, loss = 0.08865320\n",
            "Iteration 99, loss = 0.08865303\n",
            "Iteration 100, loss = 0.08865351\n",
            "Iteration 101, loss = 0.08865358\n",
            "Iteration 102, loss = 0.08865338\n",
            "Iteration 103, loss = 0.08865257\n",
            "Iteration 104, loss = 0.08865366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.24448245\n",
            "Iteration 3, loss = 0.20719690\n",
            "Iteration 4, loss = 0.17909911\n",
            "Iteration 5, loss = 0.15802217\n",
            "Iteration 6, loss = 0.14166842\n",
            "Iteration 7, loss = 0.12961724\n",
            "Iteration 8, loss = 0.12021349\n",
            "Iteration 9, loss = 0.11347766\n",
            "Iteration 10, loss = 0.10820826\n",
            "Iteration 11, loss = 0.10367054\n",
            "Iteration 12, loss = 0.10041233\n",
            "Iteration 13, loss = 0.09824312\n",
            "Iteration 14, loss = 0.09633859\n",
            "Iteration 15, loss = 0.09471689\n",
            "Iteration 16, loss = 0.09387591\n",
            "Iteration 17, loss = 0.09295902\n",
            "Iteration 18, loss = 0.09250902\n",
            "Iteration 19, loss = 0.09206519\n",
            "Iteration 20, loss = 0.09172747\n",
            "Iteration 21, loss = 0.09140661\n",
            "Iteration 22, loss = 0.09149002\n",
            "Iteration 23, loss = 0.09135135\n",
            "Iteration 24, loss = 0.09084845\n",
            "Iteration 25, loss = 0.09051333\n",
            "Iteration 26, loss = 0.09069217\n",
            "Iteration 27, loss = 0.09056854\n",
            "Iteration 28, loss = 0.09109256\n",
            "Iteration 29, loss = 0.09082586\n",
            "Iteration 30, loss = 0.09066400\n",
            "Iteration 31, loss = 0.09039067\n",
            "Iteration 32, loss = 0.09038897\n",
            "Iteration 33, loss = 0.09054650\n",
            "Iteration 34, loss = 0.09044933\n",
            "Iteration 35, loss = 0.09063444\n",
            "Iteration 36, loss = 0.09069553\n",
            "Iteration 37, loss = 0.09044343\n",
            "Iteration 38, loss = 0.09037011\n",
            "Iteration 39, loss = 0.09047232\n",
            "Iteration 40, loss = 0.09066891\n",
            "Iteration 41, loss = 0.09061646\n",
            "Iteration 42, loss = 0.09045953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.08940423\n",
            "Iteration 44, loss = 0.08950852\n",
            "Iteration 45, loss = 0.08955439\n",
            "Iteration 46, loss = 0.08940598\n",
            "Iteration 47, loss = 0.08929073\n",
            "Iteration 48, loss = 0.08930225\n",
            "Iteration 49, loss = 0.08939980\n",
            "Iteration 50, loss = 0.08931761\n",
            "Iteration 51, loss = 0.08926618\n",
            "Iteration 52, loss = 0.08930321\n",
            "Iteration 53, loss = 0.08926286\n",
            "Iteration 54, loss = 0.08929692\n",
            "Iteration 55, loss = 0.08928893\n",
            "Iteration 56, loss = 0.08925246\n",
            "Iteration 57, loss = 0.08920235\n",
            "Iteration 58, loss = 0.08915953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.08898783\n",
            "Iteration 60, loss = 0.08879135\n",
            "Iteration 61, loss = 0.08879532\n",
            "Iteration 62, loss = 0.08878474\n",
            "Iteration 63, loss = 0.08880072\n",
            "Iteration 64, loss = 0.08878250\n",
            "Iteration 65, loss = 0.08879704\n",
            "Iteration 66, loss = 0.08879648\n",
            "Iteration 67, loss = 0.08875632\n",
            "Iteration 68, loss = 0.08879055\n",
            "Iteration 69, loss = 0.08878395\n",
            "Iteration 70, loss = 0.08879293\n",
            "Iteration 71, loss = 0.08872040\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 72, loss = 0.08864465\n",
            "Iteration 73, loss = 0.08863085\n",
            "Iteration 74, loss = 0.08863281\n",
            "Iteration 75, loss = 0.08863725\n",
            "Iteration 76, loss = 0.08865438\n",
            "Iteration 77, loss = 0.08862972\n",
            "Iteration 78, loss = 0.08864716\n",
            "Iteration 79, loss = 0.08864849\n",
            "Iteration 80, loss = 0.08863625\n",
            "Iteration 81, loss = 0.08864338\n",
            "Iteration 82, loss = 0.08865127\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 83, loss = 0.08861178\n",
            "Iteration 84, loss = 0.08861148\n",
            "Iteration 85, loss = 0.08860532\n",
            "Iteration 86, loss = 0.08861297\n",
            "Iteration 87, loss = 0.08860996\n",
            "Iteration 88, loss = 0.08860779\n",
            "Iteration 89, loss = 0.08860713\n",
            "Iteration 90, loss = 0.08860845\n",
            "Iteration 91, loss = 0.08860851\n",
            "Iteration 92, loss = 0.08861137\n",
            "Iteration 93, loss = 0.08860873\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 94, loss = 0.08860259\n",
            "Iteration 95, loss = 0.08860173\n",
            "Iteration 96, loss = 0.08860297\n",
            "Iteration 97, loss = 0.08860218\n",
            "Iteration 98, loss = 0.08860256\n",
            "Iteration 99, loss = 0.08860245\n",
            "Iteration 100, loss = 0.08860251\n",
            "Iteration 101, loss = 0.08860218\n",
            "Iteration 102, loss = 0.08860241\n",
            "Iteration 103, loss = 0.08860317\n",
            "Iteration 104, loss = 0.08860165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.41917653\n",
            "Iteration 2, loss = 0.17444235\n",
            "Iteration 3, loss = 0.15353768\n",
            "Iteration 4, loss = 0.13827881\n",
            "Iteration 5, loss = 0.12692802\n",
            "Iteration 6, loss = 0.11804080\n",
            "Iteration 7, loss = 0.11145660\n",
            "Iteration 8, loss = 0.10607649\n",
            "Iteration 9, loss = 0.10277755\n",
            "Iteration 10, loss = 0.09958619\n",
            "Iteration 11, loss = 0.09742514\n",
            "Iteration 12, loss = 0.09575601\n",
            "Iteration 13, loss = 0.09419122\n",
            "Iteration 14, loss = 0.09369294\n",
            "Iteration 15, loss = 0.09290078\n",
            "Iteration 16, loss = 0.09285755\n",
            "Iteration 17, loss = 0.09223211\n",
            "Iteration 18, loss = 0.09175621\n",
            "Iteration 19, loss = 0.09167521\n",
            "Iteration 20, loss = 0.09147854\n",
            "Iteration 21, loss = 0.09126099\n",
            "Iteration 22, loss = 0.09127637\n",
            "Iteration 23, loss = 0.09128343\n",
            "Iteration 24, loss = 0.09069140\n",
            "Iteration 25, loss = 0.09062608\n",
            "Iteration 26, loss = 0.09060178\n",
            "Iteration 27, loss = 0.09046891\n",
            "Iteration 28, loss = 0.09038043\n",
            "Iteration 29, loss = 0.09055346\n",
            "Iteration 30, loss = 0.09025069\n",
            "Iteration 31, loss = 0.09041252\n",
            "Iteration 32, loss = 0.09023349\n",
            "Iteration 33, loss = 0.09047006\n",
            "Iteration 34, loss = 0.09036827\n",
            "Iteration 35, loss = 0.09028238\n",
            "Iteration 36, loss = 0.09050515\n",
            "Iteration 37, loss = 0.09027649\n",
            "Iteration 38, loss = 0.09034778\n",
            "Iteration 39, loss = 0.09057777\n",
            "Iteration 40, loss = 0.09041058\n",
            "Iteration 41, loss = 0.09005002\n",
            "Iteration 42, loss = 0.09035823\n",
            "Iteration 43, loss = 0.09018432\n",
            "Iteration 44, loss = 0.09010530\n",
            "Iteration 45, loss = 0.09011238\n",
            "Iteration 46, loss = 0.09024804\n",
            "Iteration 47, loss = 0.09017048\n",
            "Iteration 48, loss = 0.09002930\n",
            "Iteration 49, loss = 0.09018281\n",
            "Iteration 50, loss = 0.09023044\n",
            "Iteration 51, loss = 0.09002019\n",
            "Iteration 52, loss = 0.09032584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.08903874\n",
            "Iteration 54, loss = 0.08889367\n",
            "Iteration 55, loss = 0.08900103\n",
            "Iteration 56, loss = 0.08894421\n",
            "Iteration 57, loss = 0.08896742\n",
            "Iteration 58, loss = 0.08892773\n",
            "Iteration 59, loss = 0.08893475\n",
            "Iteration 60, loss = 0.08888089\n",
            "Iteration 61, loss = 0.08878242\n",
            "Iteration 62, loss = 0.08885170\n",
            "Iteration 63, loss = 0.08877682\n",
            "Iteration 64, loss = 0.08884701\n",
            "Iteration 65, loss = 0.08863653\n",
            "Iteration 66, loss = 0.08874936\n",
            "Iteration 67, loss = 0.08882361\n",
            "Iteration 68, loss = 0.08870569\n",
            "Iteration 69, loss = 0.08868870\n",
            "Iteration 70, loss = 0.08853592\n",
            "Iteration 71, loss = 0.08874896\n",
            "Iteration 72, loss = 0.08860384\n",
            "Iteration 73, loss = 0.08888209\n",
            "Iteration 74, loss = 0.08877350\n",
            "Iteration 75, loss = 0.08866402\n",
            "Iteration 76, loss = 0.08884774\n",
            "Iteration 77, loss = 0.08862130\n",
            "Iteration 78, loss = 0.08852986\n",
            "Iteration 79, loss = 0.08852459\n",
            "Iteration 80, loss = 0.08870961\n",
            "Iteration 81, loss = 0.08861761\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.08822159\n",
            "Iteration 83, loss = 0.08813165\n",
            "Iteration 84, loss = 0.08810406\n",
            "Iteration 85, loss = 0.08811720\n",
            "Iteration 86, loss = 0.08804593\n",
            "Iteration 87, loss = 0.08811982\n",
            "Iteration 88, loss = 0.08811385\n",
            "Iteration 89, loss = 0.08806936\n",
            "Iteration 90, loss = 0.08810103\n",
            "Iteration 91, loss = 0.08810522\n",
            "Iteration 92, loss = 0.08809576\n",
            "Iteration 93, loss = 0.08808869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.08796229\n",
            "Iteration 95, loss = 0.08793637\n",
            "Iteration 96, loss = 0.08789687\n",
            "Iteration 97, loss = 0.08793936\n",
            "Iteration 98, loss = 0.08792688\n",
            "Iteration 99, loss = 0.08794882\n",
            "Iteration 100, loss = 0.08793570\n",
            "Iteration 101, loss = 0.08792193\n",
            "Iteration 102, loss = 0.08791877\n",
            "Iteration 103, loss = 0.08792701\n",
            "Iteration 104, loss = 0.08792932\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 105, loss = 0.08789208\n",
            "Iteration 106, loss = 0.08788785\n",
            "Iteration 107, loss = 0.08788883\n",
            "Iteration 108, loss = 0.08789051\n",
            "Iteration 109, loss = 0.08788958\n",
            "Iteration 110, loss = 0.08788870\n",
            "Iteration 111, loss = 0.08788893\n",
            "Iteration 112, loss = 0.08788475\n",
            "Iteration 113, loss = 0.08788762\n",
            "Iteration 114, loss = 0.08789500\n",
            "Iteration 115, loss = 0.08788685\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 116, loss = 0.08788513\n",
            "Iteration 117, loss = 0.08787970\n",
            "Iteration 118, loss = 0.08787964\n",
            "Iteration 119, loss = 0.08787972\n",
            "Iteration 120, loss = 0.08787936\n",
            "Iteration 121, loss = 0.08787971\n",
            "Iteration 122, loss = 0.08788047\n",
            "Iteration 123, loss = 0.08787893\n",
            "Iteration 124, loss = 0.08787959\n",
            "Iteration 125, loss = 0.08787985\n",
            "Iteration 126, loss = 0.08787997\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23512340\n",
            "Iteration 3, loss = 0.19931926\n",
            "Iteration 4, loss = 0.17323754\n",
            "Iteration 5, loss = 0.15322864\n",
            "Iteration 6, loss = 0.13828231\n",
            "Iteration 7, loss = 0.12687287\n",
            "Iteration 8, loss = 0.11800660\n",
            "Iteration 9, loss = 0.11120242\n",
            "Iteration 10, loss = 0.10611462\n",
            "Iteration 11, loss = 0.10245708\n",
            "Iteration 12, loss = 0.09971124\n",
            "Iteration 13, loss = 0.09726026\n",
            "Iteration 14, loss = 0.09544993\n",
            "Iteration 15, loss = 0.09474491\n",
            "Iteration 16, loss = 0.09339258\n",
            "Iteration 17, loss = 0.09239296\n",
            "Iteration 18, loss = 0.09205979\n",
            "Iteration 19, loss = 0.09167699\n",
            "Iteration 20, loss = 0.09114906\n",
            "Iteration 21, loss = 0.09101647\n",
            "Iteration 22, loss = 0.09085060\n",
            "Iteration 23, loss = 0.09038575\n",
            "Iteration 24, loss = 0.09031666\n",
            "Iteration 25, loss = 0.09012384\n",
            "Iteration 26, loss = 0.09026639\n",
            "Iteration 27, loss = 0.09022633\n",
            "Iteration 28, loss = 0.09010302\n",
            "Iteration 29, loss = 0.09012495\n",
            "Iteration 30, loss = 0.08987110\n",
            "Iteration 31, loss = 0.08997364\n",
            "Iteration 32, loss = 0.09019049\n",
            "Iteration 33, loss = 0.08978279\n",
            "Iteration 34, loss = 0.08998743\n",
            "Iteration 35, loss = 0.09007391\n",
            "Iteration 36, loss = 0.08994289\n",
            "Iteration 37, loss = 0.08997709\n",
            "Iteration 38, loss = 0.08986386\n",
            "Iteration 39, loss = 0.08986725\n",
            "Iteration 40, loss = 0.09007600\n",
            "Iteration 41, loss = 0.09007779\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 42, loss = 0.08897549\n",
            "Iteration 43, loss = 0.08886760\n",
            "Iteration 44, loss = 0.08900475\n",
            "Iteration 45, loss = 0.08884535\n",
            "Iteration 46, loss = 0.08880006\n",
            "Iteration 47, loss = 0.08890584\n",
            "Iteration 48, loss = 0.08888132\n",
            "Iteration 49, loss = 0.08882444\n",
            "Iteration 50, loss = 0.08871552\n",
            "Iteration 51, loss = 0.08865985\n",
            "Iteration 52, loss = 0.08866906\n",
            "Iteration 53, loss = 0.08863706\n",
            "Iteration 54, loss = 0.08879391\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 55, loss = 0.08820527\n",
            "Iteration 56, loss = 0.08822394\n",
            "Iteration 57, loss = 0.08820450\n",
            "Iteration 58, loss = 0.08827737\n",
            "Iteration 59, loss = 0.08821515\n",
            "Iteration 60, loss = 0.08816577\n",
            "Iteration 61, loss = 0.08818191\n",
            "Iteration 62, loss = 0.08818110\n",
            "Iteration 63, loss = 0.08822746\n",
            "Iteration 64, loss = 0.08818932\n",
            "Iteration 65, loss = 0.08822546\n",
            "Iteration 66, loss = 0.08818801\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 67, loss = 0.08804624\n",
            "Iteration 68, loss = 0.08804867\n",
            "Iteration 69, loss = 0.08804573\n",
            "Iteration 70, loss = 0.08803542\n",
            "Iteration 71, loss = 0.08804454\n",
            "Iteration 72, loss = 0.08804058\n",
            "Iteration 73, loss = 0.08803204\n",
            "Iteration 74, loss = 0.08802458\n",
            "Iteration 75, loss = 0.08804417\n",
            "Iteration 76, loss = 0.08801944\n",
            "Iteration 77, loss = 0.08804256\n",
            "Iteration 78, loss = 0.08803444\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 79, loss = 0.08799704\n",
            "Iteration 80, loss = 0.08799925\n",
            "Iteration 81, loss = 0.08800026\n",
            "Iteration 82, loss = 0.08799774\n",
            "Iteration 83, loss = 0.08800199\n",
            "Iteration 84, loss = 0.08799949\n",
            "Iteration 85, loss = 0.08800021\n",
            "Iteration 86, loss = 0.08799881\n",
            "Iteration 87, loss = 0.08799998\n",
            "Iteration 88, loss = 0.08799960\n",
            "Iteration 89, loss = 0.08799879\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 90, loss = 0.08799469\n",
            "Iteration 91, loss = 0.08799277\n",
            "Iteration 92, loss = 0.08799225\n",
            "Iteration 93, loss = 0.08799303\n",
            "Iteration 94, loss = 0.08799187\n",
            "Iteration 95, loss = 0.08799236\n",
            "Iteration 96, loss = 0.08799312\n",
            "Iteration 97, loss = 0.08799216\n",
            "Iteration 98, loss = 0.08799186\n",
            "Iteration 99, loss = 0.08799236\n",
            "Iteration 100, loss = 0.08799211\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.21162422\n",
            "Iteration 3, loss = 0.18215366\n",
            "Iteration 4, loss = 0.15983737\n",
            "Iteration 5, loss = 0.14331522\n",
            "Iteration 6, loss = 0.13062232\n",
            "Iteration 7, loss = 0.12123859\n",
            "Iteration 8, loss = 0.11364439\n",
            "Iteration 9, loss = 0.10833822\n",
            "Iteration 10, loss = 0.10393103\n",
            "Iteration 11, loss = 0.10068651\n",
            "Iteration 12, loss = 0.09871879\n",
            "Iteration 13, loss = 0.09645309\n",
            "Iteration 14, loss = 0.09546969\n",
            "Iteration 15, loss = 0.09392136\n",
            "Iteration 16, loss = 0.09334011\n",
            "Iteration 17, loss = 0.09255060\n",
            "Iteration 18, loss = 0.09203461\n",
            "Iteration 19, loss = 0.09181084\n",
            "Iteration 20, loss = 0.09133012\n",
            "Iteration 21, loss = 0.09144444\n",
            "Iteration 22, loss = 0.09087384\n",
            "Iteration 23, loss = 0.09153498\n",
            "Iteration 24, loss = 0.09079796\n",
            "Iteration 25, loss = 0.09081216\n",
            "Iteration 26, loss = 0.09089943\n",
            "Iteration 27, loss = 0.09052734\n",
            "Iteration 28, loss = 0.09069674\n",
            "Iteration 29, loss = 0.09053825\n",
            "Iteration 30, loss = 0.09048475\n",
            "Iteration 31, loss = 0.09068609\n",
            "Iteration 32, loss = 0.09034370\n",
            "Iteration 33, loss = 0.09057793\n",
            "Iteration 34, loss = 0.09052305\n",
            "Iteration 35, loss = 0.09062999\n",
            "Iteration 36, loss = 0.09062493\n",
            "Iteration 37, loss = 0.09026855\n",
            "Iteration 38, loss = 0.09063424\n",
            "Iteration 39, loss = 0.09062623\n",
            "Iteration 40, loss = 0.09065130\n",
            "Iteration 41, loss = 0.09013253\n",
            "Iteration 42, loss = 0.09027903\n",
            "Iteration 43, loss = 0.09062989\n",
            "Iteration 44, loss = 0.09030931\n",
            "Iteration 45, loss = 0.09031618\n",
            "Iteration 46, loss = 0.09049721\n",
            "Iteration 47, loss = 0.09027047\n",
            "Iteration 48, loss = 0.09040397\n",
            "Iteration 49, loss = 0.09036892\n",
            "Iteration 50, loss = 0.09003359\n",
            "Iteration 51, loss = 0.09039643\n",
            "Iteration 52, loss = 0.09048142\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.08925723\n",
            "Iteration 54, loss = 0.08905026\n",
            "Iteration 55, loss = 0.08908009\n",
            "Iteration 56, loss = 0.08915227\n",
            "Iteration 57, loss = 0.08902623\n",
            "Iteration 58, loss = 0.08901882\n",
            "Iteration 59, loss = 0.08892813\n",
            "Iteration 60, loss = 0.08902731\n",
            "Iteration 61, loss = 0.08901340\n",
            "Iteration 62, loss = 0.08906825\n",
            "Iteration 63, loss = 0.08889030\n",
            "Iteration 64, loss = 0.08909067\n",
            "Iteration 65, loss = 0.08894023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.08850060\n",
            "Iteration 67, loss = 0.08844303\n",
            "Iteration 68, loss = 0.08842014\n",
            "Iteration 69, loss = 0.08841704\n",
            "Iteration 70, loss = 0.08841758\n",
            "Iteration 71, loss = 0.08837188\n",
            "Iteration 72, loss = 0.08837985\n",
            "Iteration 73, loss = 0.08836776\n",
            "Iteration 74, loss = 0.08836307\n",
            "Iteration 75, loss = 0.08840312\n",
            "Iteration 76, loss = 0.08836072\n",
            "Iteration 77, loss = 0.08832735\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 78, loss = 0.08823189\n",
            "Iteration 79, loss = 0.08820831\n",
            "Iteration 80, loss = 0.08821859\n",
            "Iteration 81, loss = 0.08822053\n",
            "Iteration 82, loss = 0.08820370\n",
            "Iteration 83, loss = 0.08821740\n",
            "Iteration 84, loss = 0.08821659\n",
            "Iteration 85, loss = 0.08820141\n",
            "Iteration 86, loss = 0.08821247\n",
            "Iteration 87, loss = 0.08822214\n",
            "Iteration 88, loss = 0.08821055\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 89, loss = 0.08818721\n",
            "Iteration 90, loss = 0.08817780\n",
            "Iteration 91, loss = 0.08817668\n",
            "Iteration 92, loss = 0.08817219\n",
            "Iteration 93, loss = 0.08817446\n",
            "Iteration 94, loss = 0.08817939\n",
            "Iteration 95, loss = 0.08817537\n",
            "Iteration 96, loss = 0.08817347\n",
            "Iteration 97, loss = 0.08817562\n",
            "Iteration 98, loss = 0.08817717\n",
            "Iteration 99, loss = 0.08817422\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 100, loss = 0.08816783\n",
            "Iteration 101, loss = 0.08816724\n",
            "Iteration 102, loss = 0.08816741\n",
            "Iteration 103, loss = 0.08816741\n",
            "Iteration 104, loss = 0.08816700\n",
            "Iteration 105, loss = 0.08816745\n",
            "Iteration 106, loss = 0.08816696\n",
            "Iteration 107, loss = 0.08816639\n",
            "Iteration 108, loss = 0.08816687\n",
            "Iteration 109, loss = 0.08816722\n",
            "Iteration 110, loss = 0.08816617\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.87926614\n",
            "Iteration 2, loss = 0.22616566\n",
            "Iteration 3, loss = 0.19230496\n",
            "Iteration 4, loss = 0.16753120\n",
            "Iteration 5, loss = 0.14897851\n",
            "Iteration 6, loss = 0.13457921\n",
            "Iteration 7, loss = 0.12357323\n",
            "Iteration 8, loss = 0.11576438\n",
            "Iteration 9, loss = 0.10975543\n",
            "Iteration 10, loss = 0.10477104\n",
            "Iteration 11, loss = 0.10125567\n",
            "Iteration 12, loss = 0.09898274\n",
            "Iteration 13, loss = 0.09653116\n",
            "Iteration 14, loss = 0.09507664\n",
            "Iteration 15, loss = 0.09400574\n",
            "Iteration 16, loss = 0.09318963\n",
            "Iteration 17, loss = 0.09226565\n",
            "Iteration 18, loss = 0.09166089\n",
            "Iteration 19, loss = 0.09137847\n",
            "Iteration 20, loss = 0.09120115\n",
            "Iteration 21, loss = 0.09095483\n",
            "Iteration 22, loss = 0.09064063\n",
            "Iteration 23, loss = 0.09056822\n",
            "Iteration 24, loss = 0.09054927\n",
            "Iteration 25, loss = 0.09027048\n",
            "Iteration 26, loss = 0.09043088\n",
            "Iteration 27, loss = 0.09019207\n",
            "Iteration 28, loss = 0.09019395\n",
            "Iteration 29, loss = 0.09017460\n",
            "Iteration 30, loss = 0.09034786\n",
            "Iteration 31, loss = 0.09013890\n",
            "Iteration 32, loss = 0.09014887\n",
            "Iteration 33, loss = 0.09034731\n",
            "Iteration 34, loss = 0.09038133\n",
            "Iteration 35, loss = 0.09024270\n",
            "Iteration 36, loss = 0.09019154\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.08896767\n",
            "Iteration 38, loss = 0.08887639\n",
            "Iteration 39, loss = 0.08888428\n",
            "Iteration 40, loss = 0.08884005\n",
            "Iteration 41, loss = 0.08875220\n",
            "Iteration 42, loss = 0.08885185\n",
            "Iteration 43, loss = 0.08890254\n",
            "Iteration 44, loss = 0.08880353\n",
            "Iteration 45, loss = 0.08875761\n",
            "Iteration 46, loss = 0.08867307\n",
            "Iteration 47, loss = 0.08873693\n",
            "Iteration 48, loss = 0.08880887\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.08830582\n",
            "Iteration 50, loss = 0.08828171\n",
            "Iteration 51, loss = 0.08827202\n",
            "Iteration 52, loss = 0.08825907\n",
            "Iteration 53, loss = 0.08827605\n",
            "Iteration 54, loss = 0.08827231\n",
            "Iteration 55, loss = 0.08832206\n",
            "Iteration 56, loss = 0.08827874\n",
            "Iteration 57, loss = 0.08831468\n",
            "Iteration 58, loss = 0.08827378\n",
            "Iteration 59, loss = 0.08819056\n",
            "Iteration 60, loss = 0.08827947\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 61, loss = 0.08814839\n",
            "Iteration 62, loss = 0.08811283\n",
            "Iteration 63, loss = 0.08811040\n",
            "Iteration 64, loss = 0.08810534\n",
            "Iteration 65, loss = 0.08808669\n",
            "Iteration 66, loss = 0.08811319\n",
            "Iteration 67, loss = 0.08811367\n",
            "Iteration 68, loss = 0.08811319\n",
            "Iteration 69, loss = 0.08809405\n",
            "Iteration 70, loss = 0.08810239\n",
            "Iteration 71, loss = 0.08809295\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 72, loss = 0.08806186\n",
            "Iteration 73, loss = 0.08806278\n",
            "Iteration 74, loss = 0.08805987\n",
            "Iteration 75, loss = 0.08805883\n",
            "Iteration 76, loss = 0.08806081\n",
            "Iteration 77, loss = 0.08806134\n",
            "Iteration 78, loss = 0.08806163\n",
            "Iteration 79, loss = 0.08805478\n",
            "Iteration 80, loss = 0.08806672\n",
            "Iteration 81, loss = 0.08805918\n",
            "Iteration 82, loss = 0.08805493\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 83, loss = 0.08806327\n",
            "Iteration 84, loss = 0.08805218\n",
            "Iteration 85, loss = 0.08805284\n",
            "Iteration 86, loss = 0.08805184\n",
            "Iteration 87, loss = 0.08805216\n",
            "Iteration 88, loss = 0.08805238\n",
            "Iteration 89, loss = 0.08805202\n",
            "Iteration 90, loss = 0.08805202\n",
            "Iteration 91, loss = 0.08805159\n",
            "Iteration 92, loss = 0.08805236\n",
            "Iteration 93, loss = 0.08805179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 14.9min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.77381368\n",
            "Iteration 2, loss = 0.19847875\n",
            "Iteration 3, loss = 0.18846664\n",
            "Iteration 4, loss = 0.17923423\n",
            "Iteration 5, loss = 0.17099709\n",
            "Iteration 6, loss = 0.16478635\n",
            "Iteration 7, loss = 0.15861794\n",
            "Iteration 8, loss = 0.15212853\n",
            "Iteration 9, loss = 0.14690005\n",
            "Iteration 10, loss = 0.14178403\n",
            "Iteration 11, loss = 0.13774269\n",
            "Iteration 12, loss = 0.13323321\n",
            "Iteration 13, loss = 0.12987980\n",
            "Iteration 14, loss = 0.12632986\n",
            "Iteration 15, loss = 0.12307612\n",
            "Iteration 16, loss = 0.12062705\n",
            "Iteration 17, loss = 0.11776524\n",
            "Iteration 18, loss = 0.11569350\n",
            "Iteration 19, loss = 0.11305181\n",
            "Iteration 20, loss = 0.11140714\n",
            "Iteration 21, loss = 0.10890911\n",
            "Iteration 22, loss = 0.10755584\n",
            "Iteration 23, loss = 0.10674314\n",
            "Iteration 24, loss = 0.10477884\n",
            "Iteration 25, loss = 0.10303347\n",
            "Iteration 26, loss = 0.10249681\n",
            "Iteration 27, loss = 0.10055067\n",
            "Iteration 28, loss = 0.09978902\n",
            "Iteration 29, loss = 0.09860357\n",
            "Iteration 30, loss = 0.09740627\n",
            "Iteration 31, loss = 0.09722297\n",
            "Iteration 32, loss = 0.09634885\n",
            "Iteration 33, loss = 0.09517041\n",
            "Iteration 34, loss = 0.09471782\n",
            "Iteration 35, loss = 0.09441006\n",
            "Iteration 36, loss = 0.09351917\n",
            "Iteration 37, loss = 0.09308926\n",
            "Iteration 38, loss = 0.09223271\n",
            "Iteration 39, loss = 0.09229668\n",
            "Iteration 40, loss = 0.09151763\n",
            "Iteration 41, loss = 0.09084111\n",
            "Iteration 42, loss = 0.09149944\n",
            "Iteration 43, loss = 0.09078324\n",
            "Iteration 44, loss = 0.09037354\n",
            "Iteration 45, loss = 0.09026483\n",
            "Iteration 46, loss = 0.09042621\n",
            "Iteration 47, loss = 0.08964727\n",
            "Iteration 48, loss = 0.08882903\n",
            "Iteration 49, loss = 0.08918978\n",
            "Iteration 50, loss = 0.08884578\n",
            "Iteration 51, loss = 0.08891578\n",
            "Iteration 52, loss = 0.08882407\n",
            "Iteration 53, loss = 0.08827756\n",
            "Iteration 54, loss = 0.08827790\n",
            "Iteration 55, loss = 0.08779597\n",
            "Iteration 56, loss = 0.08851695\n",
            "Iteration 57, loss = 0.08803482\n",
            "Iteration 58, loss = 0.08796254\n",
            "Iteration 59, loss = 0.08820789\n",
            "Iteration 60, loss = 0.08779159\n",
            "Iteration 61, loss = 0.08777572\n",
            "Iteration 62, loss = 0.08791783\n",
            "Iteration 63, loss = 0.08745386\n",
            "Iteration 64, loss = 0.08775610\n",
            "Iteration 65, loss = 0.08722072\n",
            "Iteration 66, loss = 0.08716952\n",
            "Iteration 67, loss = 0.08743673\n",
            "Iteration 68, loss = 0.08750771\n",
            "Iteration 69, loss = 0.08716032\n",
            "Iteration 70, loss = 0.08721264\n",
            "Iteration 71, loss = 0.08723527\n",
            "Iteration 72, loss = 0.08694348\n",
            "Iteration 73, loss = 0.08679650\n",
            "Iteration 74, loss = 0.08742445\n",
            "Iteration 75, loss = 0.08717631\n",
            "Iteration 76, loss = 0.08709505\n",
            "Iteration 77, loss = 0.08659139\n",
            "Iteration 78, loss = 0.08716075\n",
            "Iteration 79, loss = 0.08722495\n",
            "Iteration 80, loss = 0.08705998\n",
            "Iteration 81, loss = 0.08717359\n",
            "Iteration 82, loss = 0.08751788\n",
            "Iteration 83, loss = 0.08704054\n",
            "Iteration 84, loss = 0.08660200\n",
            "Iteration 85, loss = 0.08710478\n",
            "Iteration 86, loss = 0.08677975\n",
            "Iteration 87, loss = 0.08751585\n",
            "Iteration 88, loss = 0.08682055\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 89, loss = 0.08568210\n",
            "Iteration 90, loss = 0.08530803\n",
            "Iteration 91, loss = 0.08549950\n",
            "Iteration 92, loss = 0.08506525\n",
            "Iteration 93, loss = 0.08587099\n",
            "Iteration 94, loss = 0.08552132\n",
            "Iteration 95, loss = 0.08529109\n",
            "Iteration 96, loss = 0.08518755\n",
            "Iteration 97, loss = 0.08549084\n",
            "Iteration 98, loss = 0.08547714\n",
            "Iteration 99, loss = 0.08549461\n",
            "Iteration 100, loss = 0.08557420\n",
            "Iteration 101, loss = 0.08532553\n",
            "Iteration 102, loss = 0.08551497\n",
            "Iteration 103, loss = 0.08510590\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 104, loss = 0.08508081\n",
            "Iteration 105, loss = 0.08472750\n",
            "Iteration 106, loss = 0.08469864\n",
            "Iteration 107, loss = 0.08473804\n",
            "Iteration 108, loss = 0.08459597\n",
            "Iteration 109, loss = 0.08469659\n",
            "Iteration 110, loss = 0.08474699\n",
            "Iteration 111, loss = 0.08470825\n",
            "Iteration 112, loss = 0.08471818\n",
            "Iteration 113, loss = 0.08465862\n",
            "Iteration 114, loss = 0.08464822\n",
            "Iteration 115, loss = 0.08465358\n",
            "Iteration 116, loss = 0.08480542\n",
            "Iteration 117, loss = 0.08474678\n",
            "Iteration 118, loss = 0.08469859\n",
            "Iteration 119, loss = 0.08467602\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 120, loss = 0.08457834\n",
            "Iteration 121, loss = 0.08457087\n",
            "Iteration 122, loss = 0.08457147\n",
            "Iteration 123, loss = 0.08455460\n",
            "Iteration 124, loss = 0.08451923\n",
            "Iteration 125, loss = 0.08455837\n",
            "Iteration 126, loss = 0.08454785\n",
            "Iteration 127, loss = 0.08456797\n",
            "Iteration 128, loss = 0.08456950\n",
            "Iteration 129, loss = 0.08456959\n",
            "Iteration 130, loss = 0.08456084\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 131, loss = 0.08453286\n",
            "Iteration 132, loss = 0.08452893\n",
            "Iteration 133, loss = 0.08451657\n",
            "Iteration 134, loss = 0.08451474\n",
            "Iteration 135, loss = 0.08452005\n",
            "Iteration 136, loss = 0.08451473\n",
            "Iteration 137, loss = 0.08451353\n",
            "Iteration 138, loss = 0.08451491\n",
            "Iteration 139, loss = 0.08451140\n",
            "Iteration 140, loss = 0.08451371\n",
            "Iteration 141, loss = 0.08451487\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 142, loss = 0.08450533\n",
            "Iteration 143, loss = 0.08450681\n",
            "Iteration 144, loss = 0.08450561\n",
            "Iteration 145, loss = 0.08450839\n",
            "Iteration 146, loss = 0.08450626\n",
            "Iteration 147, loss = 0.08450624\n",
            "Iteration 148, loss = 0.08450663\n",
            "Iteration 149, loss = 0.08450560\n",
            "Iteration 150, loss = 0.08450664\n",
            "Iteration 151, loss = 0.08450555\n",
            "Iteration 152, loss = 0.08450672\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.86028369\n",
            "Iteration 2, loss = 0.27726967\n",
            "Iteration 3, loss = 0.26245179\n",
            "Iteration 4, loss = 0.24874060\n",
            "Iteration 5, loss = 0.23610694\n",
            "Iteration 6, loss = 0.22480006\n",
            "Iteration 7, loss = 0.21402917\n",
            "Iteration 8, loss = 0.20428208\n",
            "Iteration 9, loss = 0.19575918\n",
            "Iteration 10, loss = 0.18715640\n",
            "Iteration 11, loss = 0.17955316\n",
            "Iteration 12, loss = 0.17210716\n",
            "Iteration 13, loss = 0.16586902\n",
            "Iteration 14, loss = 0.15967246\n",
            "Iteration 15, loss = 0.15446985\n",
            "Iteration 16, loss = 0.14916718\n",
            "Iteration 17, loss = 0.14424579\n",
            "Iteration 18, loss = 0.13983141\n",
            "Iteration 19, loss = 0.13609703\n",
            "Iteration 20, loss = 0.13226920\n",
            "Iteration 21, loss = 0.12857096\n",
            "Iteration 22, loss = 0.12537551\n",
            "Iteration 23, loss = 0.12290051\n",
            "Iteration 24, loss = 0.11987548\n",
            "Iteration 25, loss = 0.11681175\n",
            "Iteration 26, loss = 0.11471808\n",
            "Iteration 27, loss = 0.11251120\n",
            "Iteration 28, loss = 0.11121618\n",
            "Iteration 29, loss = 0.10954500\n",
            "Iteration 30, loss = 0.10759055\n",
            "Iteration 31, loss = 0.10591443\n",
            "Iteration 32, loss = 0.10427850\n",
            "Iteration 33, loss = 0.10288021\n",
            "Iteration 34, loss = 0.10135926\n",
            "Iteration 35, loss = 0.10117006\n",
            "Iteration 36, loss = 0.09973638\n",
            "Iteration 37, loss = 0.09851206\n",
            "Iteration 38, loss = 0.09805475\n",
            "Iteration 39, loss = 0.09745331\n",
            "Iteration 40, loss = 0.09705437\n",
            "Iteration 41, loss = 0.09577091\n",
            "Iteration 42, loss = 0.09483201\n",
            "Iteration 43, loss = 0.09454268\n",
            "Iteration 44, loss = 0.09394431\n",
            "Iteration 45, loss = 0.09370130\n",
            "Iteration 46, loss = 0.09303641\n",
            "Iteration 47, loss = 0.09300001\n",
            "Iteration 48, loss = 0.09255169\n",
            "Iteration 49, loss = 0.09145631\n",
            "Iteration 50, loss = 0.09112488\n",
            "Iteration 51, loss = 0.09080486\n",
            "Iteration 52, loss = 0.09093993\n",
            "Iteration 53, loss = 0.09095381\n",
            "Iteration 54, loss = 0.08993678\n",
            "Iteration 55, loss = 0.08984776\n",
            "Iteration 56, loss = 0.08946179\n",
            "Iteration 57, loss = 0.08944560\n",
            "Iteration 58, loss = 0.08926795\n",
            "Iteration 59, loss = 0.08889691\n",
            "Iteration 60, loss = 0.08915223\n",
            "Iteration 61, loss = 0.08889034\n",
            "Iteration 62, loss = 0.08855330\n",
            "Iteration 63, loss = 0.08853327\n",
            "Iteration 64, loss = 0.08823598\n",
            "Iteration 65, loss = 0.08860817\n",
            "Iteration 66, loss = 0.08848333\n",
            "Iteration 67, loss = 0.08776378\n",
            "Iteration 68, loss = 0.08794645\n",
            "Iteration 69, loss = 0.08839649\n",
            "Iteration 70, loss = 0.08798724\n",
            "Iteration 71, loss = 0.08812343\n",
            "Iteration 72, loss = 0.08765527\n",
            "Iteration 73, loss = 0.08730093\n",
            "Iteration 74, loss = 0.08758233\n",
            "Iteration 75, loss = 0.08745984\n",
            "Iteration 76, loss = 0.08775006\n",
            "Iteration 77, loss = 0.08725600\n",
            "Iteration 78, loss = 0.08737162\n",
            "Iteration 79, loss = 0.08744521\n",
            "Iteration 80, loss = 0.08702563\n",
            "Iteration 81, loss = 0.08760475\n",
            "Iteration 82, loss = 0.08736659\n",
            "Iteration 83, loss = 0.08780453\n",
            "Iteration 84, loss = 0.08705687\n",
            "Iteration 85, loss = 0.08767214\n",
            "Iteration 86, loss = 0.08705200\n",
            "Iteration 87, loss = 0.08747566\n",
            "Iteration 88, loss = 0.08745073\n",
            "Iteration 89, loss = 0.08757415\n",
            "Iteration 90, loss = 0.08733456\n",
            "Iteration 91, loss = 0.08735928\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 92, loss = 0.08623464\n",
            "Iteration 93, loss = 0.08564721\n",
            "Iteration 94, loss = 0.08557993\n",
            "Iteration 95, loss = 0.08607618\n",
            "Iteration 96, loss = 0.08563674\n",
            "Iteration 97, loss = 0.08576460\n",
            "Iteration 98, loss = 0.08551868\n",
            "Iteration 99, loss = 0.08559439\n",
            "Iteration 100, loss = 0.08562982\n",
            "Iteration 101, loss = 0.08564110\n",
            "Iteration 102, loss = 0.08579617\n",
            "Iteration 103, loss = 0.08566624\n",
            "Iteration 104, loss = 0.08575452\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 105, loss = 0.08531668\n",
            "Iteration 106, loss = 0.08513995\n",
            "Iteration 107, loss = 0.08513776\n",
            "Iteration 108, loss = 0.08514642\n",
            "Iteration 109, loss = 0.08520050\n",
            "Iteration 110, loss = 0.08516404\n",
            "Iteration 111, loss = 0.08521700\n",
            "Iteration 112, loss = 0.08505716\n",
            "Iteration 113, loss = 0.08507699\n",
            "Iteration 114, loss = 0.08500691\n",
            "Iteration 115, loss = 0.08510953\n",
            "Iteration 116, loss = 0.08506912\n",
            "Iteration 117, loss = 0.08512424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 118, loss = 0.08498763\n",
            "Iteration 119, loss = 0.08499331\n",
            "Iteration 120, loss = 0.08497773\n",
            "Iteration 121, loss = 0.08496846\n",
            "Iteration 122, loss = 0.08498125\n",
            "Iteration 123, loss = 0.08496212\n",
            "Iteration 124, loss = 0.08499637\n",
            "Iteration 125, loss = 0.08495992\n",
            "Iteration 126, loss = 0.08496616\n",
            "Iteration 127, loss = 0.08497668\n",
            "Iteration 128, loss = 0.08496901\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 129, loss = 0.08495093\n",
            "Iteration 130, loss = 0.08493625\n",
            "Iteration 131, loss = 0.08493744\n",
            "Iteration 132, loss = 0.08493856\n",
            "Iteration 133, loss = 0.08493872\n",
            "Iteration 134, loss = 0.08493970\n",
            "Iteration 135, loss = 0.08493974\n",
            "Iteration 136, loss = 0.08493651\n",
            "Iteration 137, loss = 0.08493583\n",
            "Iteration 138, loss = 0.08493230\n",
            "Iteration 139, loss = 0.08493483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 140, loss = 0.08492866\n",
            "Iteration 141, loss = 0.08492940\n",
            "Iteration 142, loss = 0.08492949\n",
            "Iteration 143, loss = 0.08492922\n",
            "Iteration 144, loss = 0.08492913\n",
            "Iteration 145, loss = 0.08492913\n",
            "Iteration 146, loss = 0.08492840\n",
            "Iteration 147, loss = 0.08492908\n",
            "Iteration 148, loss = 0.08492867\n",
            "Iteration 149, loss = 0.08492849\n",
            "Iteration 150, loss = 0.08492767\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.27262186\n",
            "Iteration 3, loss = 0.25788600\n",
            "Iteration 4, loss = 0.24433377\n",
            "Iteration 5, loss = 0.23199015\n",
            "Iteration 6, loss = 0.22105645\n",
            "Iteration 7, loss = 0.21050916\n",
            "Iteration 8, loss = 0.20109889\n",
            "Iteration 9, loss = 0.19229671\n",
            "Iteration 10, loss = 0.18396774\n",
            "Iteration 11, loss = 0.17638744\n",
            "Iteration 12, loss = 0.16928917\n",
            "Iteration 13, loss = 0.16326713\n",
            "Iteration 14, loss = 0.15707705\n",
            "Iteration 15, loss = 0.15181139\n",
            "Iteration 16, loss = 0.14655205\n",
            "Iteration 17, loss = 0.14212993\n",
            "Iteration 18, loss = 0.13825546\n",
            "Iteration 19, loss = 0.13409663\n",
            "Iteration 20, loss = 0.13018382\n",
            "Iteration 21, loss = 0.12681806\n",
            "Iteration 22, loss = 0.12353908\n",
            "Iteration 23, loss = 0.12098752\n",
            "Iteration 24, loss = 0.11833059\n",
            "Iteration 25, loss = 0.11564220\n",
            "Iteration 26, loss = 0.11360240\n",
            "Iteration 27, loss = 0.11183433\n",
            "Iteration 28, loss = 0.10934585\n",
            "Iteration 29, loss = 0.10753719\n",
            "Iteration 30, loss = 0.10612233\n",
            "Iteration 31, loss = 0.10521041\n",
            "Iteration 32, loss = 0.10351084\n",
            "Iteration 33, loss = 0.10185405\n",
            "Iteration 34, loss = 0.10084457\n",
            "Iteration 35, loss = 0.09966694\n",
            "Iteration 36, loss = 0.09852561\n",
            "Iteration 37, loss = 0.09792641\n",
            "Iteration 38, loss = 0.09669629\n",
            "Iteration 39, loss = 0.09628431\n",
            "Iteration 40, loss = 0.09554599\n",
            "Iteration 41, loss = 0.09440077\n",
            "Iteration 42, loss = 0.09403671\n",
            "Iteration 43, loss = 0.09401814\n",
            "Iteration 44, loss = 0.09295918\n",
            "Iteration 45, loss = 0.09255842\n",
            "Iteration 46, loss = 0.09223526\n",
            "Iteration 47, loss = 0.09237026\n",
            "Iteration 48, loss = 0.09063216\n",
            "Iteration 49, loss = 0.09111921\n",
            "Iteration 50, loss = 0.09040763\n",
            "Iteration 51, loss = 0.09060172\n",
            "Iteration 52, loss = 0.08966983\n",
            "Iteration 53, loss = 0.08986699\n",
            "Iteration 54, loss = 0.08939659\n",
            "Iteration 55, loss = 0.08921670\n",
            "Iteration 56, loss = 0.08943315\n",
            "Iteration 57, loss = 0.08901044\n",
            "Iteration 58, loss = 0.08874564\n",
            "Iteration 59, loss = 0.08880305\n",
            "Iteration 60, loss = 0.08801679\n",
            "Iteration 61, loss = 0.08822391\n",
            "Iteration 62, loss = 0.08779751\n",
            "Iteration 63, loss = 0.08828733\n",
            "Iteration 64, loss = 0.08844489\n",
            "Iteration 65, loss = 0.08790648\n",
            "Iteration 66, loss = 0.08745348\n",
            "Iteration 67, loss = 0.08800354\n",
            "Iteration 68, loss = 0.08724200\n",
            "Iteration 69, loss = 0.08750108\n",
            "Iteration 70, loss = 0.08796316\n",
            "Iteration 71, loss = 0.08730999\n",
            "Iteration 72, loss = 0.08776495\n",
            "Iteration 73, loss = 0.08697517\n",
            "Iteration 74, loss = 0.08734777\n",
            "Iteration 75, loss = 0.08693581\n",
            "Iteration 76, loss = 0.08677407\n",
            "Iteration 77, loss = 0.08731252\n",
            "Iteration 78, loss = 0.08694147\n",
            "Iteration 79, loss = 0.08674209\n",
            "Iteration 80, loss = 0.08740268\n",
            "Iteration 81, loss = 0.08723971\n",
            "Iteration 82, loss = 0.08654125\n",
            "Iteration 83, loss = 0.08650147\n",
            "Iteration 84, loss = 0.08639908\n",
            "Iteration 85, loss = 0.08650524\n",
            "Iteration 86, loss = 0.08670992\n",
            "Iteration 87, loss = 0.08653879\n",
            "Iteration 88, loss = 0.08659559\n",
            "Iteration 89, loss = 0.08654885\n",
            "Iteration 90, loss = 0.08662436\n",
            "Iteration 91, loss = 0.08680174\n",
            "Iteration 92, loss = 0.08676650\n",
            "Iteration 93, loss = 0.08638838\n",
            "Iteration 94, loss = 0.08686053\n",
            "Iteration 95, loss = 0.08632317\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 96, loss = 0.08507499\n",
            "Iteration 97, loss = 0.08525454\n",
            "Iteration 98, loss = 0.08499804\n",
            "Iteration 99, loss = 0.08532747\n",
            "Iteration 100, loss = 0.08509129\n",
            "Iteration 101, loss = 0.08515055\n",
            "Iteration 102, loss = 0.08513569\n",
            "Iteration 103, loss = 0.08517714\n",
            "Iteration 104, loss = 0.08504862\n",
            "Iteration 105, loss = 0.08533043\n",
            "Iteration 106, loss = 0.08484784\n",
            "Iteration 107, loss = 0.08482785\n",
            "Iteration 108, loss = 0.08484983\n",
            "Iteration 109, loss = 0.08514912\n",
            "Iteration 110, loss = 0.08501069\n",
            "Iteration 111, loss = 0.08501004\n",
            "Iteration 112, loss = 0.08503910\n",
            "Iteration 113, loss = 0.08507763\n",
            "Iteration 114, loss = 0.08495416\n",
            "Iteration 115, loss = 0.08503020\n",
            "Iteration 116, loss = 0.08483423\n",
            "Iteration 117, loss = 0.08477672\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 118, loss = 0.08454893\n",
            "Iteration 119, loss = 0.08443395\n",
            "Iteration 120, loss = 0.08448852\n",
            "Iteration 121, loss = 0.08433334\n",
            "Iteration 122, loss = 0.08447519\n",
            "Iteration 123, loss = 0.08439452\n",
            "Iteration 124, loss = 0.08435513\n",
            "Iteration 125, loss = 0.08438324\n",
            "Iteration 126, loss = 0.08434170\n",
            "Iteration 127, loss = 0.08440014\n",
            "Iteration 128, loss = 0.08434972\n",
            "Iteration 129, loss = 0.08439474\n",
            "Iteration 130, loss = 0.08435706\n",
            "Iteration 131, loss = 0.08437616\n",
            "Iteration 132, loss = 0.08430670\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 133, loss = 0.08421942\n",
            "Iteration 134, loss = 0.08424607\n",
            "Iteration 135, loss = 0.08422530\n",
            "Iteration 136, loss = 0.08421743\n",
            "Iteration 137, loss = 0.08422401\n",
            "Iteration 138, loss = 0.08423623\n",
            "Iteration 139, loss = 0.08421451\n",
            "Iteration 140, loss = 0.08419583\n",
            "Iteration 141, loss = 0.08419665\n",
            "Iteration 142, loss = 0.08420613\n",
            "Iteration 143, loss = 0.08420805\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 144, loss = 0.08418775\n",
            "Iteration 145, loss = 0.08418020\n",
            "Iteration 146, loss = 0.08417268\n",
            "Iteration 147, loss = 0.08417399\n",
            "Iteration 148, loss = 0.08417854\n",
            "Iteration 149, loss = 0.08417589\n",
            "Iteration 150, loss = 0.08417628\n",
            "Iteration 151, loss = 0.08417473\n",
            "Iteration 152, loss = 0.08417753\n",
            "Iteration 153, loss = 0.08417556\n",
            "Iteration 154, loss = 0.08417170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 155, loss = 0.08416913\n",
            "Iteration 156, loss = 0.08416926\n",
            "Iteration 157, loss = 0.08416793\n",
            "Iteration 158, loss = 0.08416728\n",
            "Iteration 159, loss = 0.08416717\n",
            "Iteration 160, loss = 0.08416744\n",
            "Iteration 161, loss = 0.08416728\n",
            "Iteration 162, loss = 0.08416786\n",
            "Iteration 163, loss = 0.08416726\n",
            "Iteration 164, loss = 0.08416839\n",
            "Iteration 165, loss = 0.08416638\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.23313800\n",
            "Iteration 2, loss = 0.19880587\n",
            "Iteration 3, loss = 0.18930648\n",
            "Iteration 4, loss = 0.18129378\n",
            "Iteration 5, loss = 0.17354108\n",
            "Iteration 6, loss = 0.16602206\n",
            "Iteration 7, loss = 0.15954312\n",
            "Iteration 8, loss = 0.15353037\n",
            "Iteration 9, loss = 0.14837580\n",
            "Iteration 10, loss = 0.14380967\n",
            "Iteration 11, loss = 0.13919607\n",
            "Iteration 12, loss = 0.13517631\n",
            "Iteration 13, loss = 0.13104525\n",
            "Iteration 14, loss = 0.12772242\n",
            "Iteration 15, loss = 0.12472199\n",
            "Iteration 16, loss = 0.12146493\n",
            "Iteration 17, loss = 0.11973156\n",
            "Iteration 18, loss = 0.11661142\n",
            "Iteration 19, loss = 0.11418788\n",
            "Iteration 20, loss = 0.11225039\n",
            "Iteration 21, loss = 0.11015590\n",
            "Iteration 22, loss = 0.10841901\n",
            "Iteration 23, loss = 0.10671308\n",
            "Iteration 24, loss = 0.10533477\n",
            "Iteration 25, loss = 0.10403853\n",
            "Iteration 26, loss = 0.10272335\n",
            "Iteration 27, loss = 0.10129745\n",
            "Iteration 28, loss = 0.09998013\n",
            "Iteration 29, loss = 0.09928640\n",
            "Iteration 30, loss = 0.09835751\n",
            "Iteration 31, loss = 0.09782416\n",
            "Iteration 32, loss = 0.09650144\n",
            "Iteration 33, loss = 0.09590011\n",
            "Iteration 34, loss = 0.09543270\n",
            "Iteration 35, loss = 0.09430574\n",
            "Iteration 36, loss = 0.09432082\n",
            "Iteration 37, loss = 0.09333845\n",
            "Iteration 38, loss = 0.09321550\n",
            "Iteration 39, loss = 0.09236414\n",
            "Iteration 40, loss = 0.09189749\n",
            "Iteration 41, loss = 0.09150624\n",
            "Iteration 42, loss = 0.09156508\n",
            "Iteration 43, loss = 0.09099760\n",
            "Iteration 44, loss = 0.09037665\n",
            "Iteration 45, loss = 0.09076027\n",
            "Iteration 46, loss = 0.09045904\n",
            "Iteration 47, loss = 0.09025331\n",
            "Iteration 48, loss = 0.08996207\n",
            "Iteration 49, loss = 0.08916505\n",
            "Iteration 50, loss = 0.08951301\n",
            "Iteration 51, loss = 0.08927768\n",
            "Iteration 52, loss = 0.08884297\n",
            "Iteration 53, loss = 0.08852101\n",
            "Iteration 54, loss = 0.08820437\n",
            "Iteration 55, loss = 0.08854594\n",
            "Iteration 56, loss = 0.08813169\n",
            "Iteration 57, loss = 0.08820904\n",
            "Iteration 58, loss = 0.08798241\n",
            "Iteration 59, loss = 0.08801330\n",
            "Iteration 60, loss = 0.08801093\n",
            "Iteration 61, loss = 0.08842767\n",
            "Iteration 62, loss = 0.08823491\n",
            "Iteration 63, loss = 0.08854731\n",
            "Iteration 64, loss = 0.08820076\n",
            "Iteration 65, loss = 0.08811827\n",
            "Iteration 66, loss = 0.08717478\n",
            "Iteration 67, loss = 0.08753137\n",
            "Iteration 68, loss = 0.08728841\n",
            "Iteration 69, loss = 0.08718944\n",
            "Iteration 70, loss = 0.08733152\n",
            "Iteration 71, loss = 0.08790010\n",
            "Iteration 72, loss = 0.08769948\n",
            "Iteration 73, loss = 0.08717373\n",
            "Iteration 74, loss = 0.08690401\n",
            "Iteration 75, loss = 0.08696706\n",
            "Iteration 76, loss = 0.08693468\n",
            "Iteration 77, loss = 0.08724207\n",
            "Iteration 78, loss = 0.08763308\n",
            "Iteration 79, loss = 0.08747084\n",
            "Iteration 80, loss = 0.08705161\n",
            "Iteration 81, loss = 0.08682058\n",
            "Iteration 82, loss = 0.08723037\n",
            "Iteration 83, loss = 0.08647553\n",
            "Iteration 84, loss = 0.08773196\n",
            "Iteration 85, loss = 0.08635752\n",
            "Iteration 86, loss = 0.08703419\n",
            "Iteration 87, loss = 0.08676944\n",
            "Iteration 88, loss = 0.08776118\n",
            "Iteration 89, loss = 0.08715465\n",
            "Iteration 90, loss = 0.08732983\n",
            "Iteration 91, loss = 0.08694631\n",
            "Iteration 92, loss = 0.08683221\n",
            "Iteration 93, loss = 0.08701693\n",
            "Iteration 94, loss = 0.08655335\n",
            "Iteration 95, loss = 0.08713338\n",
            "Iteration 96, loss = 0.08710571\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 97, loss = 0.08633984\n",
            "Iteration 98, loss = 0.08540255\n",
            "Iteration 99, loss = 0.08520846\n",
            "Iteration 100, loss = 0.08540586\n",
            "Iteration 101, loss = 0.08525968\n",
            "Iteration 102, loss = 0.08549225\n",
            "Iteration 103, loss = 0.08528358\n",
            "Iteration 104, loss = 0.08526312\n",
            "Iteration 105, loss = 0.08540322\n",
            "Iteration 106, loss = 0.08523341\n",
            "Iteration 107, loss = 0.08501995\n",
            "Iteration 108, loss = 0.08518435\n",
            "Iteration 109, loss = 0.08518967\n",
            "Iteration 110, loss = 0.08553484\n",
            "Iteration 111, loss = 0.08514477\n",
            "Iteration 112, loss = 0.08510164\n",
            "Iteration 113, loss = 0.08513026\n",
            "Iteration 114, loss = 0.08499284\n",
            "Iteration 115, loss = 0.08545846\n",
            "Iteration 116, loss = 0.08487923\n",
            "Iteration 117, loss = 0.08519628\n",
            "Iteration 118, loss = 0.08506548\n",
            "Iteration 119, loss = 0.08557650\n",
            "Iteration 120, loss = 0.08508194\n",
            "Iteration 121, loss = 0.08500320\n",
            "Iteration 122, loss = 0.08514892\n",
            "Iteration 123, loss = 0.08507112\n",
            "Iteration 124, loss = 0.08511146\n",
            "Iteration 125, loss = 0.08524242\n",
            "Iteration 126, loss = 0.08496228\n",
            "Iteration 127, loss = 0.08508307\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 128, loss = 0.08466094\n",
            "Iteration 129, loss = 0.08455902\n",
            "Iteration 130, loss = 0.08456613\n",
            "Iteration 131, loss = 0.08451152\n",
            "Iteration 132, loss = 0.08445337\n",
            "Iteration 133, loss = 0.08456648\n",
            "Iteration 134, loss = 0.08450295\n",
            "Iteration 135, loss = 0.08448406\n",
            "Iteration 136, loss = 0.08451593\n",
            "Iteration 137, loss = 0.08449167\n",
            "Iteration 138, loss = 0.08456201\n",
            "Iteration 139, loss = 0.08449714\n",
            "Iteration 140, loss = 0.08450446\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 141, loss = 0.08448392\n",
            "Iteration 142, loss = 0.08440833\n",
            "Iteration 143, loss = 0.08439288\n",
            "Iteration 144, loss = 0.08437154\n",
            "Iteration 145, loss = 0.08436213\n",
            "Iteration 146, loss = 0.08436997\n",
            "Iteration 147, loss = 0.08435986\n",
            "Iteration 148, loss = 0.08436028\n",
            "Iteration 149, loss = 0.08436411\n",
            "Iteration 150, loss = 0.08437502\n",
            "Iteration 151, loss = 0.08436984\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 152, loss = 0.08433424\n",
            "Iteration 153, loss = 0.08432658\n",
            "Iteration 154, loss = 0.08432988\n",
            "Iteration 155, loss = 0.08432903\n",
            "Iteration 156, loss = 0.08433552\n",
            "Iteration 157, loss = 0.08432953\n",
            "Iteration 158, loss = 0.08432916\n",
            "Iteration 159, loss = 0.08432674\n",
            "Iteration 160, loss = 0.08432790\n",
            "Iteration 161, loss = 0.08432994\n",
            "Iteration 162, loss = 0.08433174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 163, loss = 0.08432190\n",
            "Iteration 164, loss = 0.08432058\n",
            "Iteration 165, loss = 0.08431985\n",
            "Iteration 166, loss = 0.08432229\n",
            "Iteration 167, loss = 0.08432226\n",
            "Iteration 168, loss = 0.08432252\n",
            "Iteration 169, loss = 0.08432126\n",
            "Iteration 170, loss = 0.08432098\n",
            "Iteration 171, loss = 0.08432151\n",
            "Iteration 172, loss = 0.08432105\n",
            "Iteration 173, loss = 0.08432161\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.34875811\n",
            "Iteration 2, loss = 0.20652249\n",
            "Iteration 3, loss = 0.19633341\n",
            "Iteration 4, loss = 0.18746913\n",
            "Iteration 5, loss = 0.17919423\n",
            "Iteration 6, loss = 0.17149055\n",
            "Iteration 7, loss = 0.16498334\n",
            "Iteration 8, loss = 0.15876534\n",
            "Iteration 9, loss = 0.15348417\n",
            "Iteration 10, loss = 0.14771174\n",
            "Iteration 11, loss = 0.14344461\n",
            "Iteration 12, loss = 0.13914818\n",
            "Iteration 13, loss = 0.13499126\n",
            "Iteration 14, loss = 0.13093814\n",
            "Iteration 15, loss = 0.12797690\n",
            "Iteration 16, loss = 0.12444322\n",
            "Iteration 17, loss = 0.12151312\n",
            "Iteration 18, loss = 0.11929721\n",
            "Iteration 19, loss = 0.11671419\n",
            "Iteration 20, loss = 0.11395317\n",
            "Iteration 21, loss = 0.11213744\n",
            "Iteration 22, loss = 0.11069267\n",
            "Iteration 23, loss = 0.10850705\n",
            "Iteration 24, loss = 0.10685904\n",
            "Iteration 25, loss = 0.10514587\n",
            "Iteration 26, loss = 0.10366396\n",
            "Iteration 27, loss = 0.10215940\n",
            "Iteration 28, loss = 0.10110834\n",
            "Iteration 29, loss = 0.10049654\n",
            "Iteration 30, loss = 0.09972925\n",
            "Iteration 31, loss = 0.09802379\n",
            "Iteration 32, loss = 0.09747626\n",
            "Iteration 33, loss = 0.09672844\n",
            "Iteration 34, loss = 0.09625056\n",
            "Iteration 35, loss = 0.09541852\n",
            "Iteration 36, loss = 0.09520054\n",
            "Iteration 37, loss = 0.09433178\n",
            "Iteration 38, loss = 0.09395212\n",
            "Iteration 39, loss = 0.09306683\n",
            "Iteration 40, loss = 0.09284003\n",
            "Iteration 41, loss = 0.09180384\n",
            "Iteration 42, loss = 0.09184802\n",
            "Iteration 43, loss = 0.09178965\n",
            "Iteration 44, loss = 0.09078095\n",
            "Iteration 45, loss = 0.09070343\n",
            "Iteration 46, loss = 0.09078853\n",
            "Iteration 47, loss = 0.09041406\n",
            "Iteration 48, loss = 0.08999870\n",
            "Iteration 49, loss = 0.08962222\n",
            "Iteration 50, loss = 0.09005095\n",
            "Iteration 51, loss = 0.08953997\n",
            "Iteration 52, loss = 0.08939391\n",
            "Iteration 53, loss = 0.08973706\n",
            "Iteration 54, loss = 0.08883429\n",
            "Iteration 55, loss = 0.08938516\n",
            "Iteration 56, loss = 0.08874861\n",
            "Iteration 57, loss = 0.08825093\n",
            "Iteration 58, loss = 0.08819929\n",
            "Iteration 59, loss = 0.08918712\n",
            "Iteration 60, loss = 0.08823189\n",
            "Iteration 61, loss = 0.08819211\n",
            "Iteration 62, loss = 0.08843329\n",
            "Iteration 63, loss = 0.08832803\n",
            "Iteration 64, loss = 0.08802378\n",
            "Iteration 65, loss = 0.08799650\n",
            "Iteration 66, loss = 0.08819214\n",
            "Iteration 67, loss = 0.08772271\n",
            "Iteration 68, loss = 0.08789348\n",
            "Iteration 69, loss = 0.08773928\n",
            "Iteration 70, loss = 0.08769937\n",
            "Iteration 71, loss = 0.08753239\n",
            "Iteration 72, loss = 0.08676978\n",
            "Iteration 73, loss = 0.08743919\n",
            "Iteration 74, loss = 0.08736375\n",
            "Iteration 75, loss = 0.08708923\n",
            "Iteration 76, loss = 0.08805206\n",
            "Iteration 77, loss = 0.08725826\n",
            "Iteration 78, loss = 0.08769039\n",
            "Iteration 79, loss = 0.08776782\n",
            "Iteration 80, loss = 0.08765721\n",
            "Iteration 81, loss = 0.08742075\n",
            "Iteration 82, loss = 0.08738894\n",
            "Iteration 83, loss = 0.08732355\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 84, loss = 0.08567616\n",
            "Iteration 85, loss = 0.08556800\n",
            "Iteration 86, loss = 0.08576864\n",
            "Iteration 87, loss = 0.08551662\n",
            "Iteration 88, loss = 0.08557685\n",
            "Iteration 89, loss = 0.08567803\n",
            "Iteration 90, loss = 0.08562144\n",
            "Iteration 91, loss = 0.08581583\n",
            "Iteration 92, loss = 0.08550485\n",
            "Iteration 93, loss = 0.08545045\n",
            "Iteration 94, loss = 0.08579807\n",
            "Iteration 95, loss = 0.08550051\n",
            "Iteration 96, loss = 0.08568688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 97, loss = 0.08499837\n",
            "Iteration 98, loss = 0.08503026\n",
            "Iteration 99, loss = 0.08503128\n",
            "Iteration 100, loss = 0.08496454\n",
            "Iteration 101, loss = 0.08499441\n",
            "Iteration 102, loss = 0.08497584\n",
            "Iteration 103, loss = 0.08496729\n",
            "Iteration 104, loss = 0.08501961\n",
            "Iteration 105, loss = 0.08492424\n",
            "Iteration 106, loss = 0.08495866\n",
            "Iteration 107, loss = 0.08490731\n",
            "Iteration 108, loss = 0.08493596\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 109, loss = 0.08482130\n",
            "Iteration 110, loss = 0.08481263\n",
            "Iteration 111, loss = 0.08482692\n",
            "Iteration 112, loss = 0.08479201\n",
            "Iteration 113, loss = 0.08482121\n",
            "Iteration 114, loss = 0.08482204\n",
            "Iteration 115, loss = 0.08483660\n",
            "Iteration 116, loss = 0.08481073\n",
            "Iteration 117, loss = 0.08482520\n",
            "Iteration 118, loss = 0.08482396\n",
            "Iteration 119, loss = 0.08481822\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.08478980\n",
            "Iteration 121, loss = 0.08477946\n",
            "Iteration 122, loss = 0.08478426\n",
            "Iteration 123, loss = 0.08477592\n",
            "Iteration 124, loss = 0.08478777\n",
            "Iteration 125, loss = 0.08477974\n",
            "Iteration 126, loss = 0.08478075\n",
            "Iteration 127, loss = 0.08477863\n",
            "Iteration 128, loss = 0.08477978\n",
            "Iteration 129, loss = 0.08478072\n",
            "Iteration 130, loss = 0.08477654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.08477141\n",
            "Iteration 132, loss = 0.08477177\n",
            "Iteration 133, loss = 0.08477174\n",
            "Iteration 134, loss = 0.08477122\n",
            "Iteration 135, loss = 0.08477089\n",
            "Iteration 136, loss = 0.08477092\n",
            "Iteration 137, loss = 0.08477104\n",
            "Iteration 138, loss = 0.08477042\n",
            "Iteration 139, loss = 0.08477184\n",
            "Iteration 140, loss = 0.08477093\n",
            "Iteration 141, loss = 0.08477035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.28883197\n",
            "Iteration 3, loss = 0.27205154\n",
            "Iteration 4, loss = 0.25750170\n",
            "Iteration 5, loss = 0.24366148\n",
            "Iteration 6, loss = 0.23181201\n",
            "Iteration 7, loss = 0.21962593\n",
            "Iteration 8, loss = 0.20982968\n",
            "Iteration 9, loss = 0.20015098\n",
            "Iteration 10, loss = 0.19132175\n",
            "Iteration 11, loss = 0.18337586\n",
            "Iteration 12, loss = 0.17561636\n",
            "Iteration 13, loss = 0.16878691\n",
            "Iteration 14, loss = 0.16252390\n",
            "Iteration 15, loss = 0.15700784\n",
            "Iteration 16, loss = 0.15105720\n",
            "Iteration 17, loss = 0.14656352\n",
            "Iteration 18, loss = 0.14184310\n",
            "Iteration 19, loss = 0.13790430\n",
            "Iteration 20, loss = 0.13368017\n",
            "Iteration 21, loss = 0.12965363\n",
            "Iteration 22, loss = 0.12669333\n",
            "Iteration 23, loss = 0.12389092\n",
            "Iteration 24, loss = 0.12096724\n",
            "Iteration 25, loss = 0.11796532\n",
            "Iteration 26, loss = 0.11624158\n",
            "Iteration 27, loss = 0.11413566\n",
            "Iteration 28, loss = 0.11170217\n",
            "Iteration 29, loss = 0.10979792\n",
            "Iteration 30, loss = 0.10787213\n",
            "Iteration 31, loss = 0.10605876\n",
            "Iteration 32, loss = 0.10506742\n",
            "Iteration 33, loss = 0.10364533\n",
            "Iteration 34, loss = 0.10196052\n",
            "Iteration 35, loss = 0.10190500\n",
            "Iteration 36, loss = 0.09991408\n",
            "Iteration 37, loss = 0.09913293\n",
            "Iteration 38, loss = 0.09800952\n",
            "Iteration 39, loss = 0.09777989\n",
            "Iteration 40, loss = 0.09677647\n",
            "Iteration 41, loss = 0.09623425\n",
            "Iteration 42, loss = 0.09501843\n",
            "Iteration 43, loss = 0.09504343\n",
            "Iteration 44, loss = 0.09427432\n",
            "Iteration 45, loss = 0.09331442\n",
            "Iteration 46, loss = 0.09255301\n",
            "Iteration 47, loss = 0.09286036\n",
            "Iteration 48, loss = 0.09227565\n",
            "Iteration 49, loss = 0.09185650\n",
            "Iteration 50, loss = 0.09205062\n",
            "Iteration 51, loss = 0.09125134\n",
            "Iteration 52, loss = 0.09092376\n",
            "Iteration 53, loss = 0.09124177\n",
            "Iteration 54, loss = 0.09039392\n",
            "Iteration 55, loss = 0.08982356\n",
            "Iteration 56, loss = 0.08955942\n",
            "Iteration 57, loss = 0.08997778\n",
            "Iteration 58, loss = 0.08991688\n",
            "Iteration 59, loss = 0.08912003\n",
            "Iteration 60, loss = 0.08904253\n",
            "Iteration 61, loss = 0.08875602\n",
            "Iteration 62, loss = 0.08897148\n",
            "Iteration 63, loss = 0.08833914\n",
            "Iteration 64, loss = 0.08859280\n",
            "Iteration 65, loss = 0.08838163\n",
            "Iteration 66, loss = 0.08860667\n",
            "Iteration 67, loss = 0.08835132\n",
            "Iteration 68, loss = 0.08858285\n",
            "Iteration 69, loss = 0.08833368\n",
            "Iteration 70, loss = 0.08860925\n",
            "Iteration 71, loss = 0.08743795\n",
            "Iteration 72, loss = 0.08812197\n",
            "Iteration 73, loss = 0.08828813\n",
            "Iteration 74, loss = 0.08799960\n",
            "Iteration 75, loss = 0.08782546\n",
            "Iteration 76, loss = 0.08787041\n",
            "Iteration 77, loss = 0.08818469\n",
            "Iteration 78, loss = 0.08793649\n",
            "Iteration 79, loss = 0.08841718\n",
            "Iteration 80, loss = 0.08705230\n",
            "Iteration 81, loss = 0.08834111\n",
            "Iteration 82, loss = 0.08781068\n",
            "Iteration 83, loss = 0.08751421\n",
            "Iteration 84, loss = 0.08761600\n",
            "Iteration 85, loss = 0.08718848\n",
            "Iteration 86, loss = 0.08724545\n",
            "Iteration 87, loss = 0.08741738\n",
            "Iteration 88, loss = 0.08696376\n",
            "Iteration 89, loss = 0.08750200\n",
            "Iteration 90, loss = 0.08739422\n",
            "Iteration 91, loss = 0.08731173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 92, loss = 0.08594341\n",
            "Iteration 93, loss = 0.08598515\n",
            "Iteration 94, loss = 0.08617044\n",
            "Iteration 95, loss = 0.08585864\n",
            "Iteration 96, loss = 0.08572962\n",
            "Iteration 97, loss = 0.08595425\n",
            "Iteration 98, loss = 0.08593095\n",
            "Iteration 99, loss = 0.08570760\n",
            "Iteration 100, loss = 0.08582433\n",
            "Iteration 101, loss = 0.08589261\n",
            "Iteration 102, loss = 0.08580456\n",
            "Iteration 103, loss = 0.08580906\n",
            "Iteration 104, loss = 0.08575495\n",
            "Iteration 105, loss = 0.08591309\n",
            "Iteration 106, loss = 0.08571495\n",
            "Iteration 107, loss = 0.08554272\n",
            "Iteration 108, loss = 0.08566065\n",
            "Iteration 109, loss = 0.08571298\n",
            "Iteration 110, loss = 0.08555946\n",
            "Iteration 111, loss = 0.08558605\n",
            "Iteration 112, loss = 0.08588592\n",
            "Iteration 113, loss = 0.08556703\n",
            "Iteration 114, loss = 0.08539515\n",
            "Iteration 115, loss = 0.08567836\n",
            "Iteration 116, loss = 0.08560671\n",
            "Iteration 117, loss = 0.08608697\n",
            "Iteration 118, loss = 0.08567419\n",
            "Iteration 119, loss = 0.08570076\n",
            "Iteration 120, loss = 0.08539644\n",
            "Iteration 121, loss = 0.08539192\n",
            "Iteration 122, loss = 0.08562731\n",
            "Iteration 123, loss = 0.08543009\n",
            "Iteration 124, loss = 0.08554333\n",
            "Iteration 125, loss = 0.08564253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 126, loss = 0.08523158\n",
            "Iteration 127, loss = 0.08493681\n",
            "Iteration 128, loss = 0.08495846\n",
            "Iteration 129, loss = 0.08489526\n",
            "Iteration 130, loss = 0.08497413\n",
            "Iteration 131, loss = 0.08498727\n",
            "Iteration 132, loss = 0.08492980\n",
            "Iteration 133, loss = 0.08506869\n",
            "Iteration 134, loss = 0.08490167\n",
            "Iteration 135, loss = 0.08488899\n",
            "Iteration 136, loss = 0.08495662\n",
            "Iteration 137, loss = 0.08493435\n",
            "Iteration 138, loss = 0.08487766\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 139, loss = 0.08477550\n",
            "Iteration 140, loss = 0.08480837\n",
            "Iteration 141, loss = 0.08480020\n",
            "Iteration 142, loss = 0.08480724\n",
            "Iteration 143, loss = 0.08477575\n",
            "Iteration 144, loss = 0.08479103\n",
            "Iteration 145, loss = 0.08477774\n",
            "Iteration 146, loss = 0.08478611\n",
            "Iteration 147, loss = 0.08477262\n",
            "Iteration 148, loss = 0.08477025\n",
            "Iteration 149, loss = 0.08474698\n",
            "Iteration 150, loss = 0.08477885\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 151, loss = 0.08474954\n",
            "Iteration 152, loss = 0.08474838\n",
            "Iteration 153, loss = 0.08475099\n",
            "Iteration 154, loss = 0.08474458\n",
            "Iteration 155, loss = 0.08475473\n",
            "Iteration 156, loss = 0.08474518\n",
            "Iteration 157, loss = 0.08475054\n",
            "Iteration 158, loss = 0.08474355\n",
            "Iteration 159, loss = 0.08475127\n",
            "Iteration 160, loss = 0.08474745\n",
            "Iteration 161, loss = 0.08474632\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 162, loss = 0.08473829\n",
            "Iteration 163, loss = 0.08474080\n",
            "Iteration 164, loss = 0.08473994\n",
            "Iteration 165, loss = 0.08473933\n",
            "Iteration 166, loss = 0.08473963\n",
            "Iteration 167, loss = 0.08473888\n",
            "Iteration 168, loss = 0.08473934\n",
            "Iteration 169, loss = 0.08473911\n",
            "Iteration 170, loss = 0.08473871\n",
            "Iteration 171, loss = 0.08473995\n",
            "Iteration 172, loss = 0.08473847\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.78684888\n",
            "Iteration 2, loss = 0.19757099\n",
            "Iteration 3, loss = 0.18696356\n",
            "Iteration 4, loss = 0.17705006\n",
            "Iteration 5, loss = 0.16997884\n",
            "Iteration 6, loss = 0.16365552\n",
            "Iteration 7, loss = 0.15704489\n",
            "Iteration 8, loss = 0.15085656\n",
            "Iteration 9, loss = 0.14631499\n",
            "Iteration 10, loss = 0.14107353\n",
            "Iteration 11, loss = 0.13635175\n",
            "Iteration 12, loss = 0.13282817\n",
            "Iteration 13, loss = 0.12995963\n",
            "Iteration 14, loss = 0.12574829\n",
            "Iteration 15, loss = 0.12263285\n",
            "Iteration 16, loss = 0.12006778\n",
            "Iteration 17, loss = 0.11724322\n",
            "Iteration 18, loss = 0.11438515\n",
            "Iteration 19, loss = 0.11283667\n",
            "Iteration 20, loss = 0.11066448\n",
            "Iteration 21, loss = 0.10837180\n",
            "Iteration 22, loss = 0.10698053\n",
            "Iteration 23, loss = 0.10581100\n",
            "Iteration 24, loss = 0.10417755\n",
            "Iteration 25, loss = 0.10326070\n",
            "Iteration 26, loss = 0.10176975\n",
            "Iteration 27, loss = 0.10010049\n",
            "Iteration 28, loss = 0.09989284\n",
            "Iteration 29, loss = 0.09864187\n",
            "Iteration 30, loss = 0.09738666\n",
            "Iteration 31, loss = 0.09671906\n",
            "Iteration 32, loss = 0.09547238\n",
            "Iteration 33, loss = 0.09529746\n",
            "Iteration 34, loss = 0.09515713\n",
            "Iteration 35, loss = 0.09370685\n",
            "Iteration 36, loss = 0.09335847\n",
            "Iteration 37, loss = 0.09265896\n",
            "Iteration 38, loss = 0.09250759\n",
            "Iteration 39, loss = 0.09197333\n",
            "Iteration 40, loss = 0.09193873\n",
            "Iteration 41, loss = 0.09145303\n",
            "Iteration 42, loss = 0.09106918\n",
            "Iteration 43, loss = 0.09058818\n",
            "Iteration 44, loss = 0.08998262\n",
            "Iteration 45, loss = 0.09028608\n",
            "Iteration 46, loss = 0.08978077\n",
            "Iteration 47, loss = 0.08982996\n",
            "Iteration 48, loss = 0.08929274\n",
            "Iteration 49, loss = 0.08886426\n",
            "Iteration 50, loss = 0.08888478\n",
            "Iteration 51, loss = 0.08872260\n",
            "Iteration 52, loss = 0.08846014\n",
            "Iteration 53, loss = 0.08846191\n",
            "Iteration 54, loss = 0.08854558\n",
            "Iteration 55, loss = 0.08776580\n",
            "Iteration 56, loss = 0.08886465\n",
            "Iteration 57, loss = 0.08829847\n",
            "Iteration 58, loss = 0.08760474\n",
            "Iteration 59, loss = 0.08759279\n",
            "Iteration 60, loss = 0.08705493\n",
            "Iteration 61, loss = 0.08781973\n",
            "Iteration 62, loss = 0.08759719\n",
            "Iteration 63, loss = 0.08757962\n",
            "Iteration 64, loss = 0.08779193\n",
            "Iteration 65, loss = 0.08748286\n",
            "Iteration 66, loss = 0.08750260\n",
            "Iteration 67, loss = 0.08717500\n",
            "Iteration 68, loss = 0.08719384\n",
            "Iteration 69, loss = 0.08696770\n",
            "Iteration 70, loss = 0.08726762\n",
            "Iteration 71, loss = 0.08714507\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 72, loss = 0.08562643\n",
            "Iteration 73, loss = 0.08578718\n",
            "Iteration 74, loss = 0.08568074\n",
            "Iteration 75, loss = 0.08547919\n",
            "Iteration 76, loss = 0.08538344\n",
            "Iteration 77, loss = 0.08581078\n",
            "Iteration 78, loss = 0.08559078\n",
            "Iteration 79, loss = 0.08570428\n",
            "Iteration 80, loss = 0.08538770\n",
            "Iteration 81, loss = 0.08533931\n",
            "Iteration 82, loss = 0.08569250\n",
            "Iteration 83, loss = 0.08549338\n",
            "Iteration 84, loss = 0.08564476\n",
            "Iteration 85, loss = 0.08545867\n",
            "Iteration 86, loss = 0.08546816\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.08490366\n",
            "Iteration 88, loss = 0.08495484\n",
            "Iteration 89, loss = 0.08484948\n",
            "Iteration 90, loss = 0.08487372\n",
            "Iteration 91, loss = 0.08486492\n",
            "Iteration 92, loss = 0.08485849\n",
            "Iteration 93, loss = 0.08491345\n",
            "Iteration 94, loss = 0.08486426\n",
            "Iteration 95, loss = 0.08489212\n",
            "Iteration 96, loss = 0.08488751\n",
            "Iteration 97, loss = 0.08484503\n",
            "Iteration 98, loss = 0.08493124\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.08472093\n",
            "Iteration 100, loss = 0.08472389\n",
            "Iteration 101, loss = 0.08474832\n",
            "Iteration 102, loss = 0.08474365\n",
            "Iteration 103, loss = 0.08472825\n",
            "Iteration 104, loss = 0.08475684\n",
            "Iteration 105, loss = 0.08472083\n",
            "Iteration 106, loss = 0.08475745\n",
            "Iteration 107, loss = 0.08477405\n",
            "Iteration 108, loss = 0.08475440\n",
            "Iteration 109, loss = 0.08473239\n",
            "Iteration 110, loss = 0.08472472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.08469380\n",
            "Iteration 112, loss = 0.08469499\n",
            "Iteration 113, loss = 0.08469483\n",
            "Iteration 114, loss = 0.08469020\n",
            "Iteration 115, loss = 0.08469259\n",
            "Iteration 116, loss = 0.08469331\n",
            "Iteration 117, loss = 0.08469286\n",
            "Iteration 118, loss = 0.08469007\n",
            "Iteration 119, loss = 0.08469128\n",
            "Iteration 120, loss = 0.08468678\n",
            "Iteration 121, loss = 0.08468796\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.08468147\n",
            "Iteration 123, loss = 0.08468187\n",
            "Iteration 124, loss = 0.08468167\n",
            "Iteration 125, loss = 0.08468090\n",
            "Iteration 126, loss = 0.08468166\n",
            "Iteration 127, loss = 0.08468107\n",
            "Iteration 128, loss = 0.08468135\n",
            "Iteration 129, loss = 0.08468260\n",
            "Iteration 130, loss = 0.08468141\n",
            "Iteration 131, loss = 0.08468135\n",
            "Iteration 132, loss = 0.08468207\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.28332001\n",
            "Iteration 3, loss = 0.26674241\n",
            "Iteration 4, loss = 0.25149542\n",
            "Iteration 5, loss = 0.23776378\n",
            "Iteration 6, loss = 0.22603139\n",
            "Iteration 7, loss = 0.21483627\n",
            "Iteration 8, loss = 0.20472662\n",
            "Iteration 9, loss = 0.19523299\n",
            "Iteration 10, loss = 0.18745593\n",
            "Iteration 11, loss = 0.17961684\n",
            "Iteration 12, loss = 0.17176402\n",
            "Iteration 13, loss = 0.16510125\n",
            "Iteration 14, loss = 0.15960924\n",
            "Iteration 15, loss = 0.15356639\n",
            "Iteration 16, loss = 0.14869649\n",
            "Iteration 17, loss = 0.14434069\n",
            "Iteration 18, loss = 0.13948712\n",
            "Iteration 19, loss = 0.13573527\n",
            "Iteration 20, loss = 0.13189515\n",
            "Iteration 21, loss = 0.12824510\n",
            "Iteration 22, loss = 0.12521174\n",
            "Iteration 23, loss = 0.12251959\n",
            "Iteration 24, loss = 0.11994521\n",
            "Iteration 25, loss = 0.11671262\n",
            "Iteration 26, loss = 0.11488569\n",
            "Iteration 27, loss = 0.11279590\n",
            "Iteration 28, loss = 0.11112071\n",
            "Iteration 29, loss = 0.10906708\n",
            "Iteration 30, loss = 0.10726173\n",
            "Iteration 31, loss = 0.10605511\n",
            "Iteration 32, loss = 0.10454117\n",
            "Iteration 33, loss = 0.10269982\n",
            "Iteration 34, loss = 0.10170281\n",
            "Iteration 35, loss = 0.10040855\n",
            "Iteration 36, loss = 0.09956194\n",
            "Iteration 37, loss = 0.09862734\n",
            "Iteration 38, loss = 0.09773257\n",
            "Iteration 39, loss = 0.09669905\n",
            "Iteration 40, loss = 0.09620932\n",
            "Iteration 41, loss = 0.09533733\n",
            "Iteration 42, loss = 0.09519904\n",
            "Iteration 43, loss = 0.09463537\n",
            "Iteration 44, loss = 0.09339044\n",
            "Iteration 45, loss = 0.09357850\n",
            "Iteration 46, loss = 0.09334197\n",
            "Iteration 47, loss = 0.09283501\n",
            "Iteration 48, loss = 0.09246917\n",
            "Iteration 49, loss = 0.09209960\n",
            "Iteration 50, loss = 0.09161021\n",
            "Iteration 51, loss = 0.09057787\n",
            "Iteration 52, loss = 0.09127365\n",
            "Iteration 53, loss = 0.09070369\n",
            "Iteration 54, loss = 0.09000570\n",
            "Iteration 55, loss = 0.09039472\n",
            "Iteration 56, loss = 0.09015869\n",
            "Iteration 57, loss = 0.08921843\n",
            "Iteration 58, loss = 0.08981772\n",
            "Iteration 59, loss = 0.08936759\n",
            "Iteration 60, loss = 0.08929413\n",
            "Iteration 61, loss = 0.09021163\n",
            "Iteration 62, loss = 0.08887418\n",
            "Iteration 63, loss = 0.08911464\n",
            "Iteration 64, loss = 0.08889341\n",
            "Iteration 65, loss = 0.08974517\n",
            "Iteration 66, loss = 0.08818167\n",
            "Iteration 67, loss = 0.08851933\n",
            "Iteration 68, loss = 0.08869154\n",
            "Iteration 69, loss = 0.08806796\n",
            "Iteration 70, loss = 0.08813787\n",
            "Iteration 71, loss = 0.08797771\n",
            "Iteration 72, loss = 0.08848226\n",
            "Iteration 73, loss = 0.08789239\n",
            "Iteration 74, loss = 0.08817576\n",
            "Iteration 75, loss = 0.08797636\n",
            "Iteration 76, loss = 0.08793273\n",
            "Iteration 77, loss = 0.08799022\n",
            "Iteration 78, loss = 0.08765826\n",
            "Iteration 79, loss = 0.08787277\n",
            "Iteration 80, loss = 0.08744732\n",
            "Iteration 81, loss = 0.08782294\n",
            "Iteration 82, loss = 0.08752014\n",
            "Iteration 83, loss = 0.08747222\n",
            "Iteration 84, loss = 0.08780468\n",
            "Iteration 85, loss = 0.08733680\n",
            "Iteration 86, loss = 0.08828956\n",
            "Iteration 87, loss = 0.08691394\n",
            "Iteration 88, loss = 0.08764883\n",
            "Iteration 89, loss = 0.08710652\n",
            "Iteration 90, loss = 0.08719212\n",
            "Iteration 91, loss = 0.08755062\n",
            "Iteration 92, loss = 0.08752382\n",
            "Iteration 93, loss = 0.08698647\n",
            "Iteration 94, loss = 0.08753386\n",
            "Iteration 95, loss = 0.08720824\n",
            "Iteration 96, loss = 0.08718977\n",
            "Iteration 97, loss = 0.08757888\n",
            "Iteration 98, loss = 0.08743251\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 99, loss = 0.08588286\n",
            "Iteration 100, loss = 0.08572742\n",
            "Iteration 101, loss = 0.08581637\n",
            "Iteration 102, loss = 0.08624759\n",
            "Iteration 103, loss = 0.08573605\n",
            "Iteration 104, loss = 0.08573531\n",
            "Iteration 105, loss = 0.08584818\n",
            "Iteration 106, loss = 0.08589084\n",
            "Iteration 107, loss = 0.08550090\n",
            "Iteration 108, loss = 0.08609797\n",
            "Iteration 109, loss = 0.08563539\n",
            "Iteration 110, loss = 0.08551759\n",
            "Iteration 111, loss = 0.08563595\n",
            "Iteration 112, loss = 0.08583114\n",
            "Iteration 113, loss = 0.08576429\n",
            "Iteration 114, loss = 0.08558215\n",
            "Iteration 115, loss = 0.08551569\n",
            "Iteration 116, loss = 0.08561036\n",
            "Iteration 117, loss = 0.08566644\n",
            "Iteration 118, loss = 0.08560714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 119, loss = 0.08530420\n",
            "Iteration 120, loss = 0.08506286\n",
            "Iteration 121, loss = 0.08503675\n",
            "Iteration 122, loss = 0.08505885\n",
            "Iteration 123, loss = 0.08504847\n",
            "Iteration 124, loss = 0.08494264\n",
            "Iteration 125, loss = 0.08503445\n",
            "Iteration 126, loss = 0.08499599\n",
            "Iteration 127, loss = 0.08498604\n",
            "Iteration 128, loss = 0.08499496\n",
            "Iteration 129, loss = 0.08499526\n",
            "Iteration 130, loss = 0.08505488\n",
            "Iteration 131, loss = 0.08506159\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 132, loss = 0.08486304\n",
            "Iteration 133, loss = 0.08491767\n",
            "Iteration 134, loss = 0.08490722\n",
            "Iteration 135, loss = 0.08487728\n",
            "Iteration 136, loss = 0.08487285\n",
            "Iteration 137, loss = 0.08492834\n",
            "Iteration 138, loss = 0.08490122\n",
            "Iteration 139, loss = 0.08488799\n",
            "Iteration 140, loss = 0.08489439\n",
            "Iteration 141, loss = 0.08492431\n",
            "Iteration 142, loss = 0.08482928\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 143, loss = 0.08488773\n",
            "Iteration 144, loss = 0.08485304\n",
            "Iteration 145, loss = 0.08484732\n",
            "Iteration 146, loss = 0.08484665\n",
            "Iteration 147, loss = 0.08484279\n",
            "Iteration 148, loss = 0.08484699\n",
            "Iteration 149, loss = 0.08484598\n",
            "Iteration 150, loss = 0.08484367\n",
            "Iteration 151, loss = 0.08484731\n",
            "Iteration 152, loss = 0.08484452\n",
            "Iteration 153, loss = 0.08483930\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 154, loss = 0.08483436\n",
            "Iteration 155, loss = 0.08483481\n",
            "Iteration 156, loss = 0.08483532\n",
            "Iteration 157, loss = 0.08483424\n",
            "Iteration 158, loss = 0.08483475\n",
            "Iteration 159, loss = 0.08483422\n",
            "Iteration 160, loss = 0.08483470\n",
            "Iteration 161, loss = 0.08483418\n",
            "Iteration 162, loss = 0.08483471\n",
            "Iteration 163, loss = 0.08483480\n",
            "Iteration 164, loss = 0.08483398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.28765585\n",
            "Iteration 3, loss = 0.27198022\n",
            "Iteration 4, loss = 0.25735523\n",
            "Iteration 5, loss = 0.24474866\n",
            "Iteration 6, loss = 0.23232265\n",
            "Iteration 7, loss = 0.22011891\n",
            "Iteration 8, loss = 0.20935584\n",
            "Iteration 9, loss = 0.19935375\n",
            "Iteration 10, loss = 0.19077104\n",
            "Iteration 11, loss = 0.18285690\n",
            "Iteration 12, loss = 0.17537709\n",
            "Iteration 13, loss = 0.16839170\n",
            "Iteration 14, loss = 0.16214558\n",
            "Iteration 15, loss = 0.15588400\n",
            "Iteration 16, loss = 0.15063056\n",
            "Iteration 17, loss = 0.14557016\n",
            "Iteration 18, loss = 0.14115865\n",
            "Iteration 19, loss = 0.13663564\n",
            "Iteration 20, loss = 0.13238472\n",
            "Iteration 21, loss = 0.12940422\n",
            "Iteration 22, loss = 0.12618010\n",
            "Iteration 23, loss = 0.12308932\n",
            "Iteration 24, loss = 0.11990854\n",
            "Iteration 25, loss = 0.11750235\n",
            "Iteration 26, loss = 0.11492345\n",
            "Iteration 27, loss = 0.11272157\n",
            "Iteration 28, loss = 0.11046533\n",
            "Iteration 29, loss = 0.10898538\n",
            "Iteration 30, loss = 0.10716480\n",
            "Iteration 31, loss = 0.10590389\n",
            "Iteration 32, loss = 0.10424830\n",
            "Iteration 33, loss = 0.10324050\n",
            "Iteration 34, loss = 0.10156855\n",
            "Iteration 35, loss = 0.10081772\n",
            "Iteration 36, loss = 0.09935804\n",
            "Iteration 37, loss = 0.09834476\n",
            "Iteration 38, loss = 0.09806891\n",
            "Iteration 39, loss = 0.09652913\n",
            "Iteration 40, loss = 0.09593885\n",
            "Iteration 41, loss = 0.09502494\n",
            "Iteration 42, loss = 0.09433890\n",
            "Iteration 43, loss = 0.09347758\n",
            "Iteration 44, loss = 0.09340006\n",
            "Iteration 45, loss = 0.09289150\n",
            "Iteration 46, loss = 0.09235115\n",
            "Iteration 47, loss = 0.09193390\n",
            "Iteration 48, loss = 0.09100139\n",
            "Iteration 49, loss = 0.09158771\n",
            "Iteration 50, loss = 0.09079930\n",
            "Iteration 51, loss = 0.09098767\n",
            "Iteration 52, loss = 0.09026231\n",
            "Iteration 53, loss = 0.09046859\n",
            "Iteration 54, loss = 0.08958307\n",
            "Iteration 55, loss = 0.08970319\n",
            "Iteration 56, loss = 0.08982972\n",
            "Iteration 57, loss = 0.08880079\n",
            "Iteration 58, loss = 0.08882132\n",
            "Iteration 59, loss = 0.08890150\n",
            "Iteration 60, loss = 0.08794668\n",
            "Iteration 61, loss = 0.08890180\n",
            "Iteration 62, loss = 0.08833886\n",
            "Iteration 63, loss = 0.08794730\n",
            "Iteration 64, loss = 0.08814499\n",
            "Iteration 65, loss = 0.08763606\n",
            "Iteration 66, loss = 0.08894806\n",
            "Iteration 67, loss = 0.08789459\n",
            "Iteration 68, loss = 0.08729525\n",
            "Iteration 69, loss = 0.08803122\n",
            "Iteration 70, loss = 0.08719357\n",
            "Iteration 71, loss = 0.08711139\n",
            "Iteration 72, loss = 0.08738201\n",
            "Iteration 73, loss = 0.08731619\n",
            "Iteration 74, loss = 0.08725681\n",
            "Iteration 75, loss = 0.08702594\n",
            "Iteration 76, loss = 0.08714093\n",
            "Iteration 77, loss = 0.08769037\n",
            "Iteration 78, loss = 0.08782463\n",
            "Iteration 79, loss = 0.08717453\n",
            "Iteration 80, loss = 0.08668430\n",
            "Iteration 81, loss = 0.08728412\n",
            "Iteration 82, loss = 0.08685057\n",
            "Iteration 83, loss = 0.08716055\n",
            "Iteration 84, loss = 0.08726657\n",
            "Iteration 85, loss = 0.08694161\n",
            "Iteration 86, loss = 0.08685710\n",
            "Iteration 87, loss = 0.08691696\n",
            "Iteration 88, loss = 0.08680040\n",
            "Iteration 89, loss = 0.08655878\n",
            "Iteration 90, loss = 0.08649390\n",
            "Iteration 91, loss = 0.08658317\n",
            "Iteration 92, loss = 0.08665850\n",
            "Iteration 93, loss = 0.08708253\n",
            "Iteration 94, loss = 0.08675014\n",
            "Iteration 95, loss = 0.08706299\n",
            "Iteration 96, loss = 0.08638191\n",
            "Iteration 97, loss = 0.08641414\n",
            "Iteration 98, loss = 0.08708328\n",
            "Iteration 99, loss = 0.08656236\n",
            "Iteration 100, loss = 0.08634209\n",
            "Iteration 101, loss = 0.08697423\n",
            "Iteration 102, loss = 0.08675790\n",
            "Iteration 103, loss = 0.08631711\n",
            "Iteration 104, loss = 0.08680260\n",
            "Iteration 105, loss = 0.08645108\n",
            "Iteration 106, loss = 0.08609200\n",
            "Iteration 107, loss = 0.08665554\n",
            "Iteration 108, loss = 0.08697252\n",
            "Iteration 109, loss = 0.08651485\n",
            "Iteration 110, loss = 0.08638420\n",
            "Iteration 111, loss = 0.08632586\n",
            "Iteration 112, loss = 0.08674772\n",
            "Iteration 113, loss = 0.08674272\n",
            "Iteration 114, loss = 0.08624009\n",
            "Iteration 115, loss = 0.08588309\n",
            "Iteration 116, loss = 0.08647587\n",
            "Iteration 117, loss = 0.08625367\n",
            "Iteration 118, loss = 0.08626412\n",
            "Iteration 119, loss = 0.08681683\n",
            "Iteration 120, loss = 0.08653893\n",
            "Iteration 121, loss = 0.08699632\n",
            "Iteration 122, loss = 0.08653342\n",
            "Iteration 123, loss = 0.08602889\n",
            "Iteration 124, loss = 0.08617165\n",
            "Iteration 125, loss = 0.08743712\n",
            "Iteration 126, loss = 0.08626352\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 127, loss = 0.08531247\n",
            "Iteration 128, loss = 0.08490701\n",
            "Iteration 129, loss = 0.08517147\n",
            "Iteration 130, loss = 0.08502013\n",
            "Iteration 131, loss = 0.08523134\n",
            "Iteration 132, loss = 0.08515816\n",
            "Iteration 133, loss = 0.08512999\n",
            "Iteration 134, loss = 0.08517304\n",
            "Iteration 135, loss = 0.08512178\n",
            "Iteration 136, loss = 0.08508768\n",
            "Iteration 137, loss = 0.08509887\n",
            "Iteration 138, loss = 0.08503893\n",
            "Iteration 139, loss = 0.08491822\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 140, loss = 0.08460302\n",
            "Iteration 141, loss = 0.08445288\n",
            "Iteration 142, loss = 0.08443224\n",
            "Iteration 143, loss = 0.08449224\n",
            "Iteration 144, loss = 0.08435786\n",
            "Iteration 145, loss = 0.08445388\n",
            "Iteration 146, loss = 0.08448459\n",
            "Iteration 147, loss = 0.08439606\n",
            "Iteration 148, loss = 0.08440710\n",
            "Iteration 149, loss = 0.08440089\n",
            "Iteration 150, loss = 0.08437642\n",
            "Iteration 151, loss = 0.08439318\n",
            "Iteration 152, loss = 0.08451319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 153, loss = 0.08428783\n",
            "Iteration 154, loss = 0.08427670\n",
            "Iteration 155, loss = 0.08428228\n",
            "Iteration 156, loss = 0.08427706\n",
            "Iteration 157, loss = 0.08429325\n",
            "Iteration 158, loss = 0.08427945\n",
            "Iteration 159, loss = 0.08427069\n",
            "Iteration 160, loss = 0.08427440\n",
            "Iteration 161, loss = 0.08428841\n",
            "Iteration 162, loss = 0.08427361\n",
            "Iteration 163, loss = 0.08426926\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 164, loss = 0.08425578\n",
            "Iteration 165, loss = 0.08424583\n",
            "Iteration 166, loss = 0.08425163\n",
            "Iteration 167, loss = 0.08424769\n",
            "Iteration 168, loss = 0.08424750\n",
            "Iteration 169, loss = 0.08424763\n",
            "Iteration 170, loss = 0.08424421\n",
            "Iteration 171, loss = 0.08424993\n",
            "Iteration 172, loss = 0.08424692\n",
            "Iteration 173, loss = 0.08424284\n",
            "Iteration 174, loss = 0.08424395\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 175, loss = 0.08423866\n",
            "Iteration 176, loss = 0.08423809\n",
            "Iteration 177, loss = 0.08423865\n",
            "Iteration 178, loss = 0.08423818\n",
            "Iteration 179, loss = 0.08423874\n",
            "Iteration 180, loss = 0.08423910\n",
            "Iteration 181, loss = 0.08423813\n",
            "Iteration 182, loss = 0.08423901\n",
            "Iteration 183, loss = 0.08423851\n",
            "Iteration 184, loss = 0.08423881\n",
            "Iteration 185, loss = 0.08423860\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 2.38745174\n",
            "Iteration 2, loss = 0.26866952\n",
            "Iteration 3, loss = 0.25367518\n",
            "Iteration 4, loss = 0.24068572\n",
            "Iteration 5, loss = 0.22833560\n",
            "Iteration 6, loss = 0.21751178\n",
            "Iteration 7, loss = 0.20721432\n",
            "Iteration 8, loss = 0.19817540\n",
            "Iteration 9, loss = 0.18940074\n",
            "Iteration 10, loss = 0.18068123\n",
            "Iteration 11, loss = 0.17308036\n",
            "Iteration 12, loss = 0.16672037\n",
            "Iteration 13, loss = 0.15963865\n",
            "Iteration 14, loss = 0.15410886\n",
            "Iteration 15, loss = 0.14928668\n",
            "Iteration 16, loss = 0.14438046\n",
            "Iteration 17, loss = 0.13996820\n",
            "Iteration 18, loss = 0.13617954\n",
            "Iteration 19, loss = 0.13150639\n",
            "Iteration 20, loss = 0.12837320\n",
            "Iteration 21, loss = 0.12546882\n",
            "Iteration 22, loss = 0.12296889\n",
            "Iteration 23, loss = 0.11959025\n",
            "Iteration 24, loss = 0.11721013\n",
            "Iteration 25, loss = 0.11462690\n",
            "Iteration 26, loss = 0.11272434\n",
            "Iteration 27, loss = 0.11070849\n",
            "Iteration 28, loss = 0.10887625\n",
            "Iteration 29, loss = 0.10685794\n",
            "Iteration 30, loss = 0.10586835\n",
            "Iteration 31, loss = 0.10394943\n",
            "Iteration 32, loss = 0.10314203\n",
            "Iteration 33, loss = 0.10168127\n",
            "Iteration 34, loss = 0.10025398\n",
            "Iteration 35, loss = 0.09934321\n",
            "Iteration 36, loss = 0.09841149\n",
            "Iteration 37, loss = 0.09695027\n",
            "Iteration 38, loss = 0.09664751\n",
            "Iteration 39, loss = 0.09562539\n",
            "Iteration 40, loss = 0.09560963\n",
            "Iteration 41, loss = 0.09433653\n",
            "Iteration 42, loss = 0.09377527\n",
            "Iteration 43, loss = 0.09336530\n",
            "Iteration 44, loss = 0.09294417\n",
            "Iteration 45, loss = 0.09265705\n",
            "Iteration 46, loss = 0.09224257\n",
            "Iteration 47, loss = 0.09172908\n",
            "Iteration 48, loss = 0.09170795\n",
            "Iteration 49, loss = 0.09091101\n",
            "Iteration 50, loss = 0.09121610\n",
            "Iteration 51, loss = 0.08993052\n",
            "Iteration 52, loss = 0.09054886\n",
            "Iteration 53, loss = 0.08944856\n",
            "Iteration 54, loss = 0.08931038\n",
            "Iteration 55, loss = 0.08936821\n",
            "Iteration 56, loss = 0.08943667\n",
            "Iteration 57, loss = 0.08886192\n",
            "Iteration 58, loss = 0.08856446\n",
            "Iteration 59, loss = 0.08867061\n",
            "Iteration 60, loss = 0.08840011\n",
            "Iteration 61, loss = 0.08819028\n",
            "Iteration 62, loss = 0.08827833\n",
            "Iteration 63, loss = 0.08870680\n",
            "Iteration 64, loss = 0.08777582\n",
            "Iteration 65, loss = 0.08772811\n",
            "Iteration 66, loss = 0.08764075\n",
            "Iteration 67, loss = 0.08763021\n",
            "Iteration 68, loss = 0.08783888\n",
            "Iteration 69, loss = 0.08778339\n",
            "Iteration 70, loss = 0.08765057\n",
            "Iteration 71, loss = 0.08818355\n",
            "Iteration 72, loss = 0.08780118\n",
            "Iteration 73, loss = 0.08721547\n",
            "Iteration 74, loss = 0.08767990\n",
            "Iteration 75, loss = 0.08744912\n",
            "Iteration 76, loss = 0.08726681\n",
            "Iteration 77, loss = 0.08720428\n",
            "Iteration 78, loss = 0.08715631\n",
            "Iteration 79, loss = 0.08728881\n",
            "Iteration 80, loss = 0.08694304\n",
            "Iteration 81, loss = 0.08736882\n",
            "Iteration 82, loss = 0.08704248\n",
            "Iteration 83, loss = 0.08734720\n",
            "Iteration 84, loss = 0.08674019\n",
            "Iteration 85, loss = 0.08751409\n",
            "Iteration 86, loss = 0.08691558\n",
            "Iteration 87, loss = 0.08738969\n",
            "Iteration 88, loss = 0.08704928\n",
            "Iteration 89, loss = 0.08675999\n",
            "Iteration 90, loss = 0.08712833\n",
            "Iteration 91, loss = 0.08693842\n",
            "Iteration 92, loss = 0.08723590\n",
            "Iteration 93, loss = 0.08719884\n",
            "Iteration 94, loss = 0.08709708\n",
            "Iteration 95, loss = 0.08665192\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 96, loss = 0.08590672\n",
            "Iteration 97, loss = 0.08564970\n",
            "Iteration 98, loss = 0.08557920\n",
            "Iteration 99, loss = 0.08534384\n",
            "Iteration 100, loss = 0.08576481\n",
            "Iteration 101, loss = 0.08544717\n",
            "Iteration 102, loss = 0.08540243\n",
            "Iteration 103, loss = 0.08543801\n",
            "Iteration 104, loss = 0.08546572\n",
            "Iteration 105, loss = 0.08540212\n",
            "Iteration 106, loss = 0.08537361\n",
            "Iteration 107, loss = 0.08565907\n",
            "Iteration 108, loss = 0.08529605\n",
            "Iteration 109, loss = 0.08562465\n",
            "Iteration 110, loss = 0.08511647\n",
            "Iteration 111, loss = 0.08520965\n",
            "Iteration 112, loss = 0.08523890\n",
            "Iteration 113, loss = 0.08534044\n",
            "Iteration 114, loss = 0.08497876\n",
            "Iteration 115, loss = 0.08528524\n",
            "Iteration 116, loss = 0.08511710\n",
            "Iteration 117, loss = 0.08518429\n",
            "Iteration 118, loss = 0.08531091\n",
            "Iteration 119, loss = 0.08539022\n",
            "Iteration 120, loss = 0.08499085\n",
            "Iteration 121, loss = 0.08525340\n",
            "Iteration 122, loss = 0.08518398\n",
            "Iteration 123, loss = 0.08534242\n",
            "Iteration 124, loss = 0.08520641\n",
            "Iteration 125, loss = 0.08532262\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 126, loss = 0.08478152\n",
            "Iteration 127, loss = 0.08461612\n",
            "Iteration 128, loss = 0.08470610\n",
            "Iteration 129, loss = 0.08462902\n",
            "Iteration 130, loss = 0.08458142\n",
            "Iteration 131, loss = 0.08462788\n",
            "Iteration 132, loss = 0.08462208\n",
            "Iteration 133, loss = 0.08458642\n",
            "Iteration 134, loss = 0.08462471\n",
            "Iteration 135, loss = 0.08457948\n",
            "Iteration 136, loss = 0.08462073\n",
            "Iteration 137, loss = 0.08465623\n",
            "Iteration 138, loss = 0.08457515\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 139, loss = 0.08453826\n",
            "Iteration 140, loss = 0.08447432\n",
            "Iteration 141, loss = 0.08447051\n",
            "Iteration 142, loss = 0.08447905\n",
            "Iteration 143, loss = 0.08448928\n",
            "Iteration 144, loss = 0.08448043\n",
            "Iteration 145, loss = 0.08448248\n",
            "Iteration 146, loss = 0.08446466\n",
            "Iteration 147, loss = 0.08447409\n",
            "Iteration 148, loss = 0.08448216\n",
            "Iteration 149, loss = 0.08448012\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 150, loss = 0.08446784\n",
            "Iteration 151, loss = 0.08444524\n",
            "Iteration 152, loss = 0.08444630\n",
            "Iteration 153, loss = 0.08444138\n",
            "Iteration 154, loss = 0.08444312\n",
            "Iteration 155, loss = 0.08444475\n",
            "Iteration 156, loss = 0.08444185\n",
            "Iteration 157, loss = 0.08444434\n",
            "Iteration 158, loss = 0.08443257\n",
            "Iteration 159, loss = 0.08444443\n",
            "Iteration 160, loss = 0.08444254\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 161, loss = 0.08443582\n",
            "Iteration 162, loss = 0.08443497\n",
            "Iteration 163, loss = 0.08443489\n",
            "Iteration 164, loss = 0.08443564\n",
            "Iteration 165, loss = 0.08443412\n",
            "Iteration 166, loss = 0.08443416\n",
            "Iteration 167, loss = 0.08443466\n",
            "Iteration 168, loss = 0.08443471\n",
            "Iteration 169, loss = 0.08443472\n",
            "Iteration 170, loss = 0.08443389\n",
            "Iteration 171, loss = 0.08443363\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33835    12]\n",
            " [ 2967   807]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96     33847\n",
            "           1       0.99      0.21      0.35      3774\n",
            "\n",
            "    accuracy                           0.92     37621\n",
            "   macro avg       0.95      0.61      0.65     37621\n",
            "weighted avg       0.93      0.92      0.90     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 97.801% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 97.821% (0.000%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.41234066\n",
            "Iteration 3, loss = 0.38284250\n",
            "Iteration 4, loss = 0.36298290\n",
            "Iteration 5, loss = 0.34845221\n",
            "Iteration 6, loss = 0.33728415\n",
            "Iteration 7, loss = 0.32990245\n",
            "Iteration 8, loss = 0.32495498\n",
            "Iteration 9, loss = 0.32019696\n",
            "Iteration 10, loss = 0.31740220\n",
            "Iteration 11, loss = 0.31434150\n",
            "Iteration 12, loss = 0.31351626\n",
            "Iteration 13, loss = 0.31304754\n",
            "Iteration 14, loss = 0.31095632\n",
            "Iteration 15, loss = 0.31114729\n",
            "Iteration 16, loss = 0.31089227\n",
            "Iteration 17, loss = 0.31044362\n",
            "Iteration 18, loss = 0.31039474\n",
            "Iteration 19, loss = 0.31016662\n",
            "Iteration 20, loss = 0.30972286\n",
            "Iteration 21, loss = 0.30971352\n",
            "Iteration 22, loss = 0.30942109\n",
            "Iteration 23, loss = 0.30946999\n",
            "Iteration 24, loss = 0.30919630\n",
            "Iteration 25, loss = 0.30995336\n",
            "Iteration 26, loss = 0.30909481\n",
            "Iteration 27, loss = 0.30988181\n",
            "Iteration 28, loss = 0.30872915\n",
            "Iteration 29, loss = 0.30905540\n",
            "Iteration 30, loss = 0.31019605\n",
            "Iteration 31, loss = 0.30851847\n",
            "Iteration 32, loss = 0.30872666\n",
            "Iteration 33, loss = 0.30894131\n",
            "Iteration 34, loss = 0.30939463\n",
            "Iteration 35, loss = 0.30849386\n",
            "Iteration 36, loss = 0.30953799\n",
            "Iteration 37, loss = 0.30879953\n",
            "Iteration 38, loss = 0.30968038\n",
            "Iteration 39, loss = 0.30869672\n",
            "Iteration 40, loss = 0.30935358\n",
            "Iteration 41, loss = 0.30915120\n",
            "Iteration 42, loss = 0.30823358\n",
            "Iteration 43, loss = 0.30973148\n",
            "Iteration 44, loss = 0.30822584\n",
            "Iteration 45, loss = 0.30872297\n",
            "Iteration 46, loss = 0.30810631\n",
            "Iteration 47, loss = 0.30873967\n",
            "Iteration 48, loss = 0.30876277\n",
            "Iteration 49, loss = 0.30790565\n",
            "Iteration 50, loss = 0.30846144\n",
            "Iteration 51, loss = 0.30767716\n",
            "Iteration 52, loss = 0.30935646\n",
            "Iteration 53, loss = 0.30792590\n",
            "Iteration 54, loss = 0.30831722\n",
            "Iteration 55, loss = 0.30810005\n",
            "Iteration 56, loss = 0.30743803\n",
            "Iteration 57, loss = 0.30797621\n",
            "Iteration 58, loss = 0.30797032\n",
            "Iteration 59, loss = 0.30913832\n",
            "Iteration 60, loss = 0.30807469\n",
            "Iteration 61, loss = 0.30787131\n",
            "Iteration 62, loss = 0.30826461\n",
            "Iteration 63, loss = 0.30799705\n",
            "Iteration 64, loss = 0.30767239\n",
            "Iteration 65, loss = 0.30740192\n",
            "Iteration 66, loss = 0.30812357\n",
            "Iteration 67, loss = 0.30757351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.29766427\n",
            "Iteration 69, loss = 0.29526547\n",
            "Iteration 70, loss = 0.29325251\n",
            "Iteration 71, loss = 0.29149703\n",
            "Iteration 72, loss = 0.28973552\n",
            "Iteration 73, loss = 0.28794611\n",
            "Iteration 74, loss = 0.28650482\n",
            "Iteration 75, loss = 0.28553521\n",
            "Iteration 76, loss = 0.28337778\n",
            "Iteration 77, loss = 0.28276283\n",
            "Iteration 78, loss = 0.28319051\n",
            "Iteration 79, loss = 0.28294022\n",
            "Iteration 80, loss = 0.28933522\n",
            "Iteration 81, loss = 0.28574022\n",
            "Iteration 82, loss = 0.28525481\n",
            "Iteration 83, loss = 0.28709438\n",
            "Iteration 84, loss = 0.28521793\n",
            "Iteration 85, loss = 0.28880180\n",
            "Iteration 86, loss = 0.28599244\n",
            "Iteration 87, loss = 0.28455231\n",
            "Iteration 88, loss = 0.28748837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.26620354\n",
            "Iteration 90, loss = 0.26478354\n",
            "Iteration 91, loss = 0.26403392\n",
            "Iteration 92, loss = 0.26337914\n",
            "Iteration 93, loss = 0.26291423\n",
            "Iteration 94, loss = 0.26236256\n",
            "Iteration 95, loss = 0.26164582\n",
            "Iteration 96, loss = 0.26113878\n",
            "Iteration 97, loss = 0.26033034\n",
            "Iteration 98, loss = 0.26001356\n",
            "Iteration 99, loss = 0.25933910\n",
            "Iteration 100, loss = 0.25879207\n",
            "Iteration 101, loss = 0.25839946\n",
            "Iteration 102, loss = 0.25792691\n",
            "Iteration 103, loss = 0.25728857\n",
            "Iteration 104, loss = 0.25670906\n",
            "Iteration 105, loss = 0.25624209\n",
            "Iteration 106, loss = 0.25554310\n",
            "Iteration 107, loss = 0.25516437\n",
            "Iteration 108, loss = 0.25453392\n",
            "Iteration 109, loss = 0.25408382\n",
            "Iteration 110, loss = 0.25363317\n",
            "Iteration 111, loss = 0.25306959\n",
            "Iteration 112, loss = 0.25263435\n",
            "Iteration 113, loss = 0.25203908\n",
            "Iteration 114, loss = 0.25174243\n",
            "Iteration 115, loss = 0.25125841\n",
            "Iteration 116, loss = 0.25088273\n",
            "Iteration 117, loss = 0.25054869\n",
            "Iteration 118, loss = 0.25022226\n",
            "Iteration 119, loss = 0.24926666\n",
            "Iteration 120, loss = 0.24906005\n",
            "Iteration 121, loss = 0.24858187\n",
            "Iteration 122, loss = 0.24832635\n",
            "Iteration 123, loss = 0.24762165\n",
            "Iteration 124, loss = 0.24745708\n",
            "Iteration 125, loss = 0.24661467\n",
            "Iteration 126, loss = 0.24642519\n",
            "Iteration 127, loss = 0.24641086\n",
            "Iteration 128, loss = 0.24606774\n",
            "Iteration 129, loss = 0.24532105\n",
            "Iteration 130, loss = 0.24523621\n",
            "Iteration 131, loss = 0.24454445\n",
            "Iteration 132, loss = 0.24424334\n",
            "Iteration 133, loss = 0.24414530\n",
            "Iteration 134, loss = 0.24347929\n",
            "Iteration 135, loss = 0.24340531\n",
            "Iteration 136, loss = 0.24294191\n",
            "Iteration 137, loss = 0.24298049\n",
            "Iteration 138, loss = 0.24245976\n",
            "Iteration 139, loss = 0.24198177\n",
            "Iteration 140, loss = 0.24155922\n",
            "Iteration 141, loss = 0.24152800\n",
            "Iteration 142, loss = 0.24109368\n",
            "Iteration 143, loss = 0.24049178\n",
            "Iteration 144, loss = 0.24057660\n",
            "Iteration 145, loss = 0.24034510\n",
            "Iteration 146, loss = 0.23989668\n",
            "Iteration 147, loss = 0.23952552\n",
            "Iteration 148, loss = 0.23930918\n",
            "Iteration 149, loss = 0.23937573\n",
            "Iteration 150, loss = 0.23864345\n",
            "Iteration 151, loss = 0.23866735\n",
            "Iteration 152, loss = 0.23840499\n",
            "Iteration 153, loss = 0.23814174\n",
            "Iteration 154, loss = 0.23776497\n",
            "Iteration 155, loss = 0.23754016\n",
            "Iteration 156, loss = 0.23726267\n",
            "Iteration 157, loss = 0.23714834\n",
            "Iteration 158, loss = 0.23702080\n",
            "Iteration 159, loss = 0.23669583\n",
            "Iteration 160, loss = 0.23640291\n",
            "Iteration 161, loss = 0.23614290\n",
            "Iteration 162, loss = 0.23581291\n",
            "Iteration 163, loss = 0.23573016\n",
            "Iteration 164, loss = 0.23517305\n",
            "Iteration 165, loss = 0.23552641\n",
            "Iteration 166, loss = 0.23528334\n",
            "Iteration 167, loss = 0.23479307\n",
            "Iteration 168, loss = 0.23496255\n",
            "Iteration 169, loss = 0.23453646\n",
            "Iteration 170, loss = 0.23457104\n",
            "Iteration 171, loss = 0.23453974\n",
            "Iteration 172, loss = 0.23420449\n",
            "Iteration 173, loss = 0.23393906\n",
            "Iteration 174, loss = 0.23372227\n",
            "Iteration 175, loss = 0.23404921\n",
            "Iteration 176, loss = 0.23344001\n",
            "Iteration 177, loss = 0.23350729\n",
            "Iteration 178, loss = 0.23380646\n",
            "Iteration 179, loss = 0.23356580\n",
            "Iteration 180, loss = 0.23287585\n",
            "Iteration 181, loss = 0.23264597\n",
            "Iteration 182, loss = 0.23294593\n",
            "Iteration 183, loss = 0.23284438\n",
            "Iteration 184, loss = 0.23279269\n",
            "Iteration 185, loss = 0.23257945\n",
            "Iteration 186, loss = 0.23242199\n",
            "Iteration 187, loss = 0.23195311\n",
            "Iteration 188, loss = 0.23204351\n",
            "Iteration 189, loss = 0.23230091\n",
            "Iteration 190, loss = 0.23199502\n",
            "Iteration 191, loss = 0.23173657\n",
            "Iteration 192, loss = 0.23186191\n",
            "Iteration 193, loss = 0.23169796\n",
            "Iteration 194, loss = 0.23148001\n",
            "Iteration 195, loss = 0.23136018\n",
            "Iteration 196, loss = 0.23122086\n",
            "Iteration 197, loss = 0.23108736\n",
            "Iteration 198, loss = 0.23114919\n",
            "Iteration 199, loss = 0.23143613\n",
            "Iteration 200, loss = 0.23124071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.40523715\n",
            "Iteration 2, loss = 0.21284358\n",
            "Iteration 3, loss = 0.19222674\n",
            "Iteration 4, loss = 0.17795173\n",
            "Iteration 5, loss = 0.16654027\n",
            "Iteration 6, loss = 0.15796347\n",
            "Iteration 7, loss = 0.15294351\n",
            "Iteration 8, loss = 0.14744747\n",
            "Iteration 9, loss = 0.14443087\n",
            "Iteration 10, loss = 0.14130743\n",
            "Iteration 11, loss = 0.13960874\n",
            "Iteration 12, loss = 0.13775486\n",
            "Iteration 13, loss = 0.13688950\n",
            "Iteration 14, loss = 0.13604202\n",
            "Iteration 15, loss = 0.13514408\n",
            "Iteration 16, loss = 0.13508887\n",
            "Iteration 17, loss = 0.13425698\n",
            "Iteration 18, loss = 0.13384097\n",
            "Iteration 19, loss = 0.13368900\n",
            "Iteration 20, loss = 0.13335774\n",
            "Iteration 21, loss = 0.13338887\n",
            "Iteration 22, loss = 0.13312854\n",
            "Iteration 23, loss = 0.13324857\n",
            "Iteration 24, loss = 0.13275583\n",
            "Iteration 25, loss = 0.13299085\n",
            "Iteration 26, loss = 0.13212790\n",
            "Iteration 27, loss = 0.13338574\n",
            "Iteration 28, loss = 0.13273055\n",
            "Iteration 29, loss = 0.13329236\n",
            "Iteration 30, loss = 0.13320602\n",
            "Iteration 31, loss = 0.13286873\n",
            "Iteration 32, loss = 0.13339135\n",
            "Iteration 33, loss = 0.13277870\n",
            "Iteration 34, loss = 0.13323821\n",
            "Iteration 35, loss = 0.13291804\n",
            "Iteration 36, loss = 0.13279445\n",
            "Iteration 37, loss = 0.13261828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.12975832\n",
            "Iteration 39, loss = 0.12957204\n",
            "Iteration 40, loss = 0.12943540\n",
            "Iteration 41, loss = 0.12941264\n",
            "Iteration 42, loss = 0.12908305\n",
            "Iteration 43, loss = 0.12919325\n",
            "Iteration 44, loss = 0.12925091\n",
            "Iteration 45, loss = 0.12896710\n",
            "Iteration 46, loss = 0.12900696\n",
            "Iteration 47, loss = 0.12893879\n",
            "Iteration 48, loss = 0.12898622\n",
            "Iteration 49, loss = 0.12886039\n",
            "Iteration 50, loss = 0.12898730\n",
            "Iteration 51, loss = 0.12854973\n",
            "Iteration 52, loss = 0.12882332\n",
            "Iteration 53, loss = 0.12848236\n",
            "Iteration 54, loss = 0.12855438\n",
            "Iteration 55, loss = 0.12882933\n",
            "Iteration 56, loss = 0.12841081\n",
            "Iteration 57, loss = 0.12850831\n",
            "Iteration 58, loss = 0.12836125\n",
            "Iteration 59, loss = 0.12835253\n",
            "Iteration 60, loss = 0.12837771\n",
            "Iteration 61, loss = 0.12825320\n",
            "Iteration 62, loss = 0.12815438\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 63, loss = 0.12728249\n",
            "Iteration 64, loss = 0.12723733\n",
            "Iteration 65, loss = 0.12723992\n",
            "Iteration 66, loss = 0.12712776\n",
            "Iteration 67, loss = 0.12706421\n",
            "Iteration 68, loss = 0.12717148\n",
            "Iteration 69, loss = 0.12708052\n",
            "Iteration 70, loss = 0.12716220\n",
            "Iteration 71, loss = 0.12701701\n",
            "Iteration 72, loss = 0.12719627\n",
            "Iteration 73, loss = 0.12717811\n",
            "Iteration 74, loss = 0.12702857\n",
            "Iteration 75, loss = 0.12712584\n",
            "Iteration 76, loss = 0.12713338\n",
            "Iteration 77, loss = 0.12698850\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 78, loss = 0.12678855\n",
            "Iteration 79, loss = 0.12668859\n",
            "Iteration 80, loss = 0.12671251\n",
            "Iteration 81, loss = 0.12665738\n",
            "Iteration 82, loss = 0.12669029\n",
            "Iteration 83, loss = 0.12664294\n",
            "Iteration 84, loss = 0.12664029\n",
            "Iteration 85, loss = 0.12665514\n",
            "Iteration 86, loss = 0.12667442\n",
            "Iteration 87, loss = 0.12665628\n",
            "Iteration 88, loss = 0.12666928\n",
            "Iteration 89, loss = 0.12664282\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 90, loss = 0.12658631\n",
            "Iteration 91, loss = 0.12657327\n",
            "Iteration 92, loss = 0.12656415\n",
            "Iteration 93, loss = 0.12656246\n",
            "Iteration 94, loss = 0.12655852\n",
            "Iteration 95, loss = 0.12656894\n",
            "Iteration 96, loss = 0.12655454\n",
            "Iteration 97, loss = 0.12657075\n",
            "Iteration 98, loss = 0.12656066\n",
            "Iteration 99, loss = 0.12655659\n",
            "Iteration 100, loss = 0.12655005\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 101, loss = 0.12655182\n",
            "Iteration 102, loss = 0.12653685\n",
            "Iteration 103, loss = 0.12653499\n",
            "Iteration 104, loss = 0.12653481\n",
            "Iteration 105, loss = 0.12653731\n",
            "Iteration 106, loss = 0.12653560\n",
            "Iteration 107, loss = 0.12653489\n",
            "Iteration 108, loss = 0.12653544\n",
            "Iteration 109, loss = 0.12653513\n",
            "Iteration 110, loss = 0.12653461\n",
            "Iteration 111, loss = 0.12652921\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.46315199\n",
            "Iteration 2, loss = 0.21087781\n",
            "Iteration 3, loss = 0.19103757\n",
            "Iteration 4, loss = 0.17643554\n",
            "Iteration 5, loss = 0.16545923\n",
            "Iteration 6, loss = 0.15819754\n",
            "Iteration 7, loss = 0.15203276\n",
            "Iteration 8, loss = 0.14754082\n",
            "Iteration 9, loss = 0.14380797\n",
            "Iteration 10, loss = 0.14126183\n",
            "Iteration 11, loss = 0.13954963\n",
            "Iteration 12, loss = 0.13746814\n",
            "Iteration 13, loss = 0.13576419\n",
            "Iteration 14, loss = 0.13518597\n",
            "Iteration 15, loss = 0.13491132\n",
            "Iteration 16, loss = 0.13466601\n",
            "Iteration 17, loss = 0.13388902\n",
            "Iteration 18, loss = 0.13345140\n",
            "Iteration 19, loss = 0.13330746\n",
            "Iteration 20, loss = 0.13286513\n",
            "Iteration 21, loss = 0.13292894\n",
            "Iteration 22, loss = 0.13270939\n",
            "Iteration 23, loss = 0.13197825\n",
            "Iteration 24, loss = 0.13274323\n",
            "Iteration 25, loss = 0.13244237\n",
            "Iteration 26, loss = 0.13278385\n",
            "Iteration 27, loss = 0.13271943\n",
            "Iteration 28, loss = 0.13290296\n",
            "Iteration 29, loss = 0.13250653\n",
            "Iteration 30, loss = 0.13207469\n",
            "Iteration 31, loss = 0.13221638\n",
            "Iteration 32, loss = 0.13254801\n",
            "Iteration 33, loss = 0.13258144\n",
            "Iteration 34, loss = 0.13231477\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.12934354\n",
            "Iteration 36, loss = 0.12937346\n",
            "Iteration 37, loss = 0.12929432\n",
            "Iteration 38, loss = 0.12891881\n",
            "Iteration 39, loss = 0.12889058\n",
            "Iteration 40, loss = 0.12895891\n",
            "Iteration 41, loss = 0.12883782\n",
            "Iteration 42, loss = 0.12870404\n",
            "Iteration 43, loss = 0.12869313\n",
            "Iteration 44, loss = 0.12856101\n",
            "Iteration 45, loss = 0.12872796\n",
            "Iteration 46, loss = 0.12863187\n",
            "Iteration 47, loss = 0.12866013\n",
            "Iteration 48, loss = 0.12858040\n",
            "Iteration 49, loss = 0.12845095\n",
            "Iteration 50, loss = 0.12853204\n",
            "Iteration 51, loss = 0.12874504\n",
            "Iteration 52, loss = 0.12820536\n",
            "Iteration 53, loss = 0.12849441\n",
            "Iteration 54, loss = 0.12809854\n",
            "Iteration 55, loss = 0.12808635\n",
            "Iteration 56, loss = 0.12822099\n",
            "Iteration 57, loss = 0.12803405\n",
            "Iteration 58, loss = 0.12806341\n",
            "Iteration 59, loss = 0.12813003\n",
            "Iteration 60, loss = 0.12814016\n",
            "Iteration 61, loss = 0.12787040\n",
            "Iteration 62, loss = 0.12781766\n",
            "Iteration 63, loss = 0.12798472\n",
            "Iteration 64, loss = 0.12800376\n",
            "Iteration 65, loss = 0.12796415\n",
            "Iteration 66, loss = 0.12781297\n",
            "Iteration 67, loss = 0.12774847\n",
            "Iteration 68, loss = 0.12771592\n",
            "Iteration 69, loss = 0.12757674\n",
            "Iteration 70, loss = 0.12768627\n",
            "Iteration 71, loss = 0.12800641\n",
            "Iteration 72, loss = 0.12747682\n",
            "Iteration 73, loss = 0.12747126\n",
            "Iteration 74, loss = 0.12734039\n",
            "Iteration 75, loss = 0.12765513\n",
            "Iteration 76, loss = 0.12746951\n",
            "Iteration 77, loss = 0.12747476\n",
            "Iteration 78, loss = 0.12721157\n",
            "Iteration 79, loss = 0.12735838\n",
            "Iteration 80, loss = 0.12727314\n",
            "Iteration 81, loss = 0.12719425\n",
            "Iteration 82, loss = 0.12725653\n",
            "Iteration 83, loss = 0.12730277\n",
            "Iteration 84, loss = 0.12745760\n",
            "Iteration 85, loss = 0.12743529\n",
            "Iteration 86, loss = 0.12704877\n",
            "Iteration 87, loss = 0.12751768\n",
            "Iteration 88, loss = 0.12744688\n",
            "Iteration 89, loss = 0.12708988\n",
            "Iteration 90, loss = 0.12733234\n",
            "Iteration 91, loss = 0.12731802\n",
            "Iteration 92, loss = 0.12714432\n",
            "Iteration 93, loss = 0.12703892\n",
            "Iteration 94, loss = 0.12691092\n",
            "Iteration 95, loss = 0.12697959\n",
            "Iteration 96, loss = 0.12723855\n",
            "Iteration 97, loss = 0.12682668\n",
            "Iteration 98, loss = 0.12730353\n",
            "Iteration 99, loss = 0.12694135\n",
            "Iteration 100, loss = 0.12706605\n",
            "Iteration 101, loss = 0.12705218\n",
            "Iteration 102, loss = 0.12714786\n",
            "Iteration 103, loss = 0.12715118\n",
            "Iteration 104, loss = 0.12714918\n",
            "Iteration 105, loss = 0.12682561\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 106, loss = 0.12586872\n",
            "Iteration 107, loss = 0.12547684\n",
            "Iteration 108, loss = 0.12547111\n",
            "Iteration 109, loss = 0.12552885\n",
            "Iteration 110, loss = 0.12548078\n",
            "Iteration 111, loss = 0.12551342\n",
            "Iteration 112, loss = 0.12556877\n",
            "Iteration 113, loss = 0.12551727\n",
            "Iteration 114, loss = 0.12552463\n",
            "Iteration 115, loss = 0.12541896\n",
            "Iteration 116, loss = 0.12547801\n",
            "Iteration 117, loss = 0.12543671\n",
            "Iteration 118, loss = 0.12542131\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 119, loss = 0.12502677\n",
            "Iteration 120, loss = 0.12504374\n",
            "Iteration 121, loss = 0.12500539\n",
            "Iteration 122, loss = 0.12503916\n",
            "Iteration 123, loss = 0.12496657\n",
            "Iteration 124, loss = 0.12497152\n",
            "Iteration 125, loss = 0.12500791\n",
            "Iteration 126, loss = 0.12500982\n",
            "Iteration 127, loss = 0.12495176\n",
            "Iteration 128, loss = 0.12500778\n",
            "Iteration 129, loss = 0.12497399\n",
            "Iteration 130, loss = 0.12503059\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 131, loss = 0.12487161\n",
            "Iteration 132, loss = 0.12487889\n",
            "Iteration 133, loss = 0.12486562\n",
            "Iteration 134, loss = 0.12487871\n",
            "Iteration 135, loss = 0.12487867\n",
            "Iteration 136, loss = 0.12488020\n",
            "Iteration 137, loss = 0.12487253\n",
            "Iteration 138, loss = 0.12486373\n",
            "Iteration 139, loss = 0.12487198\n",
            "Iteration 140, loss = 0.12486348\n",
            "Iteration 141, loss = 0.12487164\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 142, loss = 0.12484672\n",
            "Iteration 143, loss = 0.12484485\n",
            "Iteration 144, loss = 0.12483779\n",
            "Iteration 145, loss = 0.12484530\n",
            "Iteration 146, loss = 0.12484382\n",
            "Iteration 147, loss = 0.12484192\n",
            "Iteration 148, loss = 0.12484045\n",
            "Iteration 149, loss = 0.12484389\n",
            "Iteration 150, loss = 0.12484075\n",
            "Iteration 151, loss = 0.12484158\n",
            "Iteration 152, loss = 0.12483889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.26144530\n",
            "Iteration 3, loss = 0.22960616\n",
            "Iteration 4, loss = 0.20599557\n",
            "Iteration 5, loss = 0.18841160\n",
            "Iteration 6, loss = 0.17526711\n",
            "Iteration 7, loss = 0.16463513\n",
            "Iteration 8, loss = 0.15680352\n",
            "Iteration 9, loss = 0.15147323\n",
            "Iteration 10, loss = 0.14703398\n",
            "Iteration 11, loss = 0.14365116\n",
            "Iteration 12, loss = 0.14037629\n",
            "Iteration 13, loss = 0.13918642\n",
            "Iteration 14, loss = 0.13852342\n",
            "Iteration 15, loss = 0.13609339\n",
            "Iteration 16, loss = 0.13556406\n",
            "Iteration 17, loss = 0.13504161\n",
            "Iteration 18, loss = 0.13434447\n",
            "Iteration 19, loss = 0.13394180\n",
            "Iteration 20, loss = 0.13327970\n",
            "Iteration 21, loss = 0.13353053\n",
            "Iteration 22, loss = 0.13316112\n",
            "Iteration 23, loss = 0.13317181\n",
            "Iteration 24, loss = 0.13312645\n",
            "Iteration 25, loss = 0.13314149\n",
            "Iteration 26, loss = 0.13308124\n",
            "Iteration 27, loss = 0.13291492\n",
            "Iteration 28, loss = 0.13291359\n",
            "Iteration 29, loss = 0.13341506\n",
            "Iteration 30, loss = 0.13325583\n",
            "Iteration 31, loss = 0.13262542\n",
            "Iteration 32, loss = 0.13220734\n",
            "Iteration 33, loss = 0.13314199\n",
            "Iteration 34, loss = 0.13283884\n",
            "Iteration 35, loss = 0.13306787\n",
            "Iteration 36, loss = 0.13317574\n",
            "Iteration 37, loss = 0.13294812\n",
            "Iteration 38, loss = 0.13388264\n",
            "Iteration 39, loss = 0.13270407\n",
            "Iteration 40, loss = 0.13267861\n",
            "Iteration 41, loss = 0.13309620\n",
            "Iteration 42, loss = 0.13291423\n",
            "Iteration 43, loss = 0.13246397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.13015465\n",
            "Iteration 45, loss = 0.12973311\n",
            "Iteration 46, loss = 0.12971740\n",
            "Iteration 47, loss = 0.12954087\n",
            "Iteration 48, loss = 0.12948459\n",
            "Iteration 49, loss = 0.12938808\n",
            "Iteration 50, loss = 0.12943092\n",
            "Iteration 51, loss = 0.12934099\n",
            "Iteration 52, loss = 0.12927886\n",
            "Iteration 53, loss = 0.12913494\n",
            "Iteration 54, loss = 0.12931030\n",
            "Iteration 55, loss = 0.12923127\n",
            "Iteration 56, loss = 0.12933315\n",
            "Iteration 57, loss = 0.12906922\n",
            "Iteration 58, loss = 0.12921897\n",
            "Iteration 59, loss = 0.12906284\n",
            "Iteration 60, loss = 0.12904569\n",
            "Iteration 61, loss = 0.12903570\n",
            "Iteration 62, loss = 0.12893246\n",
            "Iteration 63, loss = 0.12875332\n",
            "Iteration 64, loss = 0.12888440\n",
            "Iteration 65, loss = 0.12889003\n",
            "Iteration 66, loss = 0.12896054\n",
            "Iteration 67, loss = 0.12875640\n",
            "Iteration 68, loss = 0.12863211\n",
            "Iteration 69, loss = 0.12870295\n",
            "Iteration 70, loss = 0.12879980\n",
            "Iteration 71, loss = 0.12874307\n",
            "Iteration 72, loss = 0.12855064\n",
            "Iteration 73, loss = 0.12889853\n",
            "Iteration 74, loss = 0.12843947\n",
            "Iteration 75, loss = 0.12839454\n",
            "Iteration 76, loss = 0.12846730\n",
            "Iteration 77, loss = 0.12831503\n",
            "Iteration 78, loss = 0.12833891\n",
            "Iteration 79, loss = 0.12827205\n",
            "Iteration 80, loss = 0.12854935\n",
            "Iteration 81, loss = 0.12833886\n",
            "Iteration 82, loss = 0.12862143\n",
            "Iteration 83, loss = 0.12823722\n",
            "Iteration 84, loss = 0.12821750\n",
            "Iteration 85, loss = 0.12853308\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 86, loss = 0.12721686\n",
            "Iteration 87, loss = 0.12715929\n",
            "Iteration 88, loss = 0.12719871\n",
            "Iteration 89, loss = 0.12714708\n",
            "Iteration 90, loss = 0.12708749\n",
            "Iteration 91, loss = 0.12703104\n",
            "Iteration 92, loss = 0.12705206\n",
            "Iteration 93, loss = 0.12711478\n",
            "Iteration 94, loss = 0.12713660\n",
            "Iteration 95, loss = 0.12714447\n",
            "Iteration 96, loss = 0.12711069\n",
            "Iteration 97, loss = 0.12699538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.12672777\n",
            "Iteration 99, loss = 0.12666570\n",
            "Iteration 100, loss = 0.12668407\n",
            "Iteration 101, loss = 0.12667292\n",
            "Iteration 102, loss = 0.12666460\n",
            "Iteration 103, loss = 0.12669744\n",
            "Iteration 104, loss = 0.12669499\n",
            "Iteration 105, loss = 0.12666579\n",
            "Iteration 106, loss = 0.12669268\n",
            "Iteration 107, loss = 0.12664642\n",
            "Iteration 108, loss = 0.12665737\n",
            "Iteration 109, loss = 0.12663416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.12656147\n",
            "Iteration 111, loss = 0.12655807\n",
            "Iteration 112, loss = 0.12655301\n",
            "Iteration 113, loss = 0.12654129\n",
            "Iteration 114, loss = 0.12655698\n",
            "Iteration 115, loss = 0.12654882\n",
            "Iteration 116, loss = 0.12654261\n",
            "Iteration 117, loss = 0.12654596\n",
            "Iteration 118, loss = 0.12655837\n",
            "Iteration 119, loss = 0.12654340\n",
            "Iteration 120, loss = 0.12655183\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.12652768\n",
            "Iteration 122, loss = 0.12652651\n",
            "Iteration 123, loss = 0.12652699\n",
            "Iteration 124, loss = 0.12652729\n",
            "Iteration 125, loss = 0.12652769\n",
            "Iteration 126, loss = 0.12652838\n",
            "Iteration 127, loss = 0.12652623\n",
            "Iteration 128, loss = 0.12652973\n",
            "Iteration 129, loss = 0.12652525\n",
            "Iteration 130, loss = 0.12652525\n",
            "Iteration 131, loss = 0.12652753\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.25718553\n",
            "Iteration 3, loss = 0.22698564\n",
            "Iteration 4, loss = 0.20393430\n",
            "Iteration 5, loss = 0.18698071\n",
            "Iteration 6, loss = 0.17393266\n",
            "Iteration 7, loss = 0.16434780\n",
            "Iteration 8, loss = 0.15636550\n",
            "Iteration 9, loss = 0.15111419\n",
            "Iteration 10, loss = 0.14660538\n",
            "Iteration 11, loss = 0.14347438\n",
            "Iteration 12, loss = 0.14065357\n",
            "Iteration 13, loss = 0.13870888\n",
            "Iteration 14, loss = 0.13739558\n",
            "Iteration 15, loss = 0.13620710\n",
            "Iteration 16, loss = 0.13550436\n",
            "Iteration 17, loss = 0.13459518\n",
            "Iteration 18, loss = 0.13396454\n",
            "Iteration 19, loss = 0.13364101\n",
            "Iteration 20, loss = 0.13325781\n",
            "Iteration 21, loss = 0.13367485\n",
            "Iteration 22, loss = 0.13293001\n",
            "Iteration 23, loss = 0.13306691\n",
            "Iteration 24, loss = 0.13308262\n",
            "Iteration 25, loss = 0.13260495\n",
            "Iteration 26, loss = 0.13362974\n",
            "Iteration 27, loss = 0.13249630\n",
            "Iteration 28, loss = 0.13283811\n",
            "Iteration 29, loss = 0.13232882\n",
            "Iteration 30, loss = 0.13285433\n",
            "Iteration 31, loss = 0.13249492\n",
            "Iteration 32, loss = 0.13288717\n",
            "Iteration 33, loss = 0.13221963\n",
            "Iteration 34, loss = 0.13258398\n",
            "Iteration 35, loss = 0.13265640\n",
            "Iteration 36, loss = 0.13259040\n",
            "Iteration 37, loss = 0.13213675\n",
            "Iteration 38, loss = 0.13247756\n",
            "Iteration 39, loss = 0.13235421\n",
            "Iteration 40, loss = 0.13284083\n",
            "Iteration 41, loss = 0.13192151\n",
            "Iteration 42, loss = 0.13222754\n",
            "Iteration 43, loss = 0.13252541\n",
            "Iteration 44, loss = 0.13223825\n",
            "Iteration 45, loss = 0.13248339\n",
            "Iteration 46, loss = 0.13208493\n",
            "Iteration 47, loss = 0.13267047\n",
            "Iteration 48, loss = 0.13283163\n",
            "Iteration 49, loss = 0.13281631\n",
            "Iteration 50, loss = 0.13216063\n",
            "Iteration 51, loss = 0.13252549\n",
            "Iteration 52, loss = 0.13266466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.12997790\n",
            "Iteration 54, loss = 0.12945803\n",
            "Iteration 55, loss = 0.12952803\n",
            "Iteration 56, loss = 0.12928586\n",
            "Iteration 57, loss = 0.12932382\n",
            "Iteration 58, loss = 0.12931093\n",
            "Iteration 59, loss = 0.12904216\n",
            "Iteration 60, loss = 0.12926539\n",
            "Iteration 61, loss = 0.12929464\n",
            "Iteration 62, loss = 0.12920879\n",
            "Iteration 63, loss = 0.12883097\n",
            "Iteration 64, loss = 0.12907546\n",
            "Iteration 65, loss = 0.12870914\n",
            "Iteration 66, loss = 0.12870165\n",
            "Iteration 67, loss = 0.12882257\n",
            "Iteration 68, loss = 0.12893228\n",
            "Iteration 69, loss = 0.12898069\n",
            "Iteration 70, loss = 0.12890024\n",
            "Iteration 71, loss = 0.12891118\n",
            "Iteration 72, loss = 0.12890807\n",
            "Iteration 73, loss = 0.12864136\n",
            "Iteration 74, loss = 0.12878759\n",
            "Iteration 75, loss = 0.12854635\n",
            "Iteration 76, loss = 0.12867480\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 77, loss = 0.12774620\n",
            "Iteration 78, loss = 0.12768340\n",
            "Iteration 79, loss = 0.12767893\n",
            "Iteration 80, loss = 0.12768932\n",
            "Iteration 81, loss = 0.12764416\n",
            "Iteration 82, loss = 0.12766006\n",
            "Iteration 83, loss = 0.12764278\n",
            "Iteration 84, loss = 0.12760802\n",
            "Iteration 85, loss = 0.12764151\n",
            "Iteration 86, loss = 0.12762459\n",
            "Iteration 87, loss = 0.12770158\n",
            "Iteration 88, loss = 0.12760057\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 89, loss = 0.12737545\n",
            "Iteration 90, loss = 0.12728514\n",
            "Iteration 91, loss = 0.12728405\n",
            "Iteration 92, loss = 0.12722819\n",
            "Iteration 93, loss = 0.12728945\n",
            "Iteration 94, loss = 0.12725762\n",
            "Iteration 95, loss = 0.12723393\n",
            "Iteration 96, loss = 0.12723045\n",
            "Iteration 97, loss = 0.12722728\n",
            "Iteration 98, loss = 0.12725089\n",
            "Iteration 99, loss = 0.12723434\n",
            "Iteration 100, loss = 0.12725160\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 101, loss = 0.12718852\n",
            "Iteration 102, loss = 0.12717503\n",
            "Iteration 103, loss = 0.12717154\n",
            "Iteration 104, loss = 0.12716973\n",
            "Iteration 105, loss = 0.12715860\n",
            "Iteration 106, loss = 0.12716750\n",
            "Iteration 107, loss = 0.12716270\n",
            "Iteration 108, loss = 0.12716167\n",
            "Iteration 109, loss = 0.12716512\n",
            "Iteration 110, loss = 0.12716569\n",
            "Iteration 111, loss = 0.12716506\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 112, loss = 0.12714511\n",
            "Iteration 113, loss = 0.12714277\n",
            "Iteration 114, loss = 0.12714552\n",
            "Iteration 115, loss = 0.12714379\n",
            "Iteration 116, loss = 0.12714460\n",
            "Iteration 117, loss = 0.12714510\n",
            "Iteration 118, loss = 0.12714242\n",
            "Iteration 119, loss = 0.12714323\n",
            "Iteration 120, loss = 0.12714363\n",
            "Iteration 121, loss = 0.12714338\n",
            "Iteration 122, loss = 0.12714217\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.26260849\n",
            "Iteration 3, loss = 0.23061394\n",
            "Iteration 4, loss = 0.20711661\n",
            "Iteration 5, loss = 0.18928150\n",
            "Iteration 6, loss = 0.17592320\n",
            "Iteration 7, loss = 0.16518956\n",
            "Iteration 8, loss = 0.15726524\n",
            "Iteration 9, loss = 0.15097605\n",
            "Iteration 10, loss = 0.14686021\n",
            "Iteration 11, loss = 0.14347975\n",
            "Iteration 12, loss = 0.14038778\n",
            "Iteration 13, loss = 0.13870727\n",
            "Iteration 14, loss = 0.13719106\n",
            "Iteration 15, loss = 0.13589108\n",
            "Iteration 16, loss = 0.13522804\n",
            "Iteration 17, loss = 0.13464204\n",
            "Iteration 18, loss = 0.13407538\n",
            "Iteration 19, loss = 0.13355872\n",
            "Iteration 20, loss = 0.13349673\n",
            "Iteration 21, loss = 0.13301451\n",
            "Iteration 22, loss = 0.13354014\n",
            "Iteration 23, loss = 0.13289111\n",
            "Iteration 24, loss = 0.13237274\n",
            "Iteration 25, loss = 0.13255210\n",
            "Iteration 26, loss = 0.13254696\n",
            "Iteration 27, loss = 0.13232499\n",
            "Iteration 28, loss = 0.13238540\n",
            "Iteration 29, loss = 0.13296645\n",
            "Iteration 30, loss = 0.13258895\n",
            "Iteration 31, loss = 0.13211726\n",
            "Iteration 32, loss = 0.13244438\n",
            "Iteration 33, loss = 0.13229762\n",
            "Iteration 34, loss = 0.13256563\n",
            "Iteration 35, loss = 0.13255456\n",
            "Iteration 36, loss = 0.13243017\n",
            "Iteration 37, loss = 0.13194184\n",
            "Iteration 38, loss = 0.13266693\n",
            "Iteration 39, loss = 0.13228844\n",
            "Iteration 40, loss = 0.13213568\n",
            "Iteration 41, loss = 0.13256136\n",
            "Iteration 42, loss = 0.13228755\n",
            "Iteration 43, loss = 0.13235903\n",
            "Iteration 44, loss = 0.13234580\n",
            "Iteration 45, loss = 0.13231218\n",
            "Iteration 46, loss = 0.13245077\n",
            "Iteration 47, loss = 0.13266997\n",
            "Iteration 48, loss = 0.13270303\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.12951672\n",
            "Iteration 50, loss = 0.12926628\n",
            "Iteration 51, loss = 0.12909420\n",
            "Iteration 52, loss = 0.12904278\n",
            "Iteration 53, loss = 0.12900233\n",
            "Iteration 54, loss = 0.12878782\n",
            "Iteration 55, loss = 0.12865161\n",
            "Iteration 56, loss = 0.12886397\n",
            "Iteration 57, loss = 0.12870653\n",
            "Iteration 58, loss = 0.12898528\n",
            "Iteration 59, loss = 0.12854768\n",
            "Iteration 60, loss = 0.12860500\n",
            "Iteration 61, loss = 0.12842715\n",
            "Iteration 62, loss = 0.12871285\n",
            "Iteration 63, loss = 0.12856543\n",
            "Iteration 64, loss = 0.12852764\n",
            "Iteration 65, loss = 0.12841415\n",
            "Iteration 66, loss = 0.12820423\n",
            "Iteration 67, loss = 0.12827553\n",
            "Iteration 68, loss = 0.12847011\n",
            "Iteration 69, loss = 0.12826960\n",
            "Iteration 70, loss = 0.12822681\n",
            "Iteration 71, loss = 0.12802235\n",
            "Iteration 72, loss = 0.12811082\n",
            "Iteration 73, loss = 0.12796587\n",
            "Iteration 74, loss = 0.12805576\n",
            "Iteration 75, loss = 0.12813148\n",
            "Iteration 76, loss = 0.12799637\n",
            "Iteration 77, loss = 0.12797590\n",
            "Iteration 78, loss = 0.12753890\n",
            "Iteration 79, loss = 0.12786836\n",
            "Iteration 80, loss = 0.12800738\n",
            "Iteration 81, loss = 0.12788536\n",
            "Iteration 82, loss = 0.12794369\n",
            "Iteration 83, loss = 0.12786105\n",
            "Iteration 84, loss = 0.12765752\n",
            "Iteration 85, loss = 0.12786443\n",
            "Iteration 86, loss = 0.12767145\n",
            "Iteration 87, loss = 0.12752851\n",
            "Iteration 88, loss = 0.12785468\n",
            "Iteration 89, loss = 0.12788700\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.12657283\n",
            "Iteration 91, loss = 0.12634003\n",
            "Iteration 92, loss = 0.12650826\n",
            "Iteration 93, loss = 0.12645038\n",
            "Iteration 94, loss = 0.12655559\n",
            "Iteration 95, loss = 0.12644866\n",
            "Iteration 96, loss = 0.12649717\n",
            "Iteration 97, loss = 0.12645219\n",
            "Iteration 98, loss = 0.12643267\n",
            "Iteration 99, loss = 0.12644926\n",
            "Iteration 100, loss = 0.12645702\n",
            "Iteration 101, loss = 0.12655162\n",
            "Iteration 102, loss = 0.12637363\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 103, loss = 0.12605557\n",
            "Iteration 104, loss = 0.12603494\n",
            "Iteration 105, loss = 0.12606525\n",
            "Iteration 106, loss = 0.12602258\n",
            "Iteration 107, loss = 0.12601593\n",
            "Iteration 108, loss = 0.12603690\n",
            "Iteration 109, loss = 0.12602948\n",
            "Iteration 110, loss = 0.12597355\n",
            "Iteration 111, loss = 0.12600417\n",
            "Iteration 112, loss = 0.12601408\n",
            "Iteration 113, loss = 0.12603944\n",
            "Iteration 114, loss = 0.12598013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 115, loss = 0.12592469\n",
            "Iteration 116, loss = 0.12590940\n",
            "Iteration 117, loss = 0.12590876\n",
            "Iteration 118, loss = 0.12590519\n",
            "Iteration 119, loss = 0.12590260\n",
            "Iteration 120, loss = 0.12591371\n",
            "Iteration 121, loss = 0.12587218\n",
            "Iteration 122, loss = 0.12591676\n",
            "Iteration 123, loss = 0.12592020\n",
            "Iteration 124, loss = 0.12588591\n",
            "Iteration 125, loss = 0.12592294\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 126, loss = 0.12588359\n",
            "Iteration 127, loss = 0.12588239\n",
            "Iteration 128, loss = 0.12588310\n",
            "Iteration 129, loss = 0.12588349\n",
            "Iteration 130, loss = 0.12587897\n",
            "Iteration 131, loss = 0.12588424\n",
            "Iteration 132, loss = 0.12588118\n",
            "Iteration 133, loss = 0.12588563\n",
            "Iteration 134, loss = 0.12588339\n",
            "Iteration 135, loss = 0.12587981\n",
            "Iteration 136, loss = 0.12588174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.71720258\n",
            "Iteration 2, loss = 0.23607360\n",
            "Iteration 3, loss = 0.21023993\n",
            "Iteration 4, loss = 0.19127380\n",
            "Iteration 5, loss = 0.17726738\n",
            "Iteration 6, loss = 0.16640602\n",
            "Iteration 7, loss = 0.15832089\n",
            "Iteration 8, loss = 0.15247686\n",
            "Iteration 9, loss = 0.14718309\n",
            "Iteration 10, loss = 0.14381228\n",
            "Iteration 11, loss = 0.14169459\n",
            "Iteration 12, loss = 0.13958432\n",
            "Iteration 13, loss = 0.13721399\n",
            "Iteration 14, loss = 0.13682999\n",
            "Iteration 15, loss = 0.13575671\n",
            "Iteration 16, loss = 0.13508681\n",
            "Iteration 17, loss = 0.13424253\n",
            "Iteration 18, loss = 0.13492318\n",
            "Iteration 19, loss = 0.13379495\n",
            "Iteration 20, loss = 0.13356813\n",
            "Iteration 21, loss = 0.13318513\n",
            "Iteration 22, loss = 0.13294932\n",
            "Iteration 23, loss = 0.13307513\n",
            "Iteration 24, loss = 0.13341254\n",
            "Iteration 25, loss = 0.13268159\n",
            "Iteration 26, loss = 0.13268010\n",
            "Iteration 27, loss = 0.13248536\n",
            "Iteration 28, loss = 0.13339931\n",
            "Iteration 29, loss = 0.13289821\n",
            "Iteration 30, loss = 0.13300663\n",
            "Iteration 31, loss = 0.13208961\n",
            "Iteration 32, loss = 0.13316914\n",
            "Iteration 33, loss = 0.13255996\n",
            "Iteration 34, loss = 0.13267471\n",
            "Iteration 35, loss = 0.13277310\n",
            "Iteration 36, loss = 0.13283638\n",
            "Iteration 37, loss = 0.13270978\n",
            "Iteration 38, loss = 0.13283320\n",
            "Iteration 39, loss = 0.13245027\n",
            "Iteration 40, loss = 0.13266106\n",
            "Iteration 41, loss = 0.13265830\n",
            "Iteration 42, loss = 0.13277073\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.12982347\n",
            "Iteration 44, loss = 0.12979114\n",
            "Iteration 45, loss = 0.12959843\n",
            "Iteration 46, loss = 0.12956031\n",
            "Iteration 47, loss = 0.12922588\n",
            "Iteration 48, loss = 0.12919087\n",
            "Iteration 49, loss = 0.12923521\n",
            "Iteration 50, loss = 0.12919937\n",
            "Iteration 51, loss = 0.12912544\n",
            "Iteration 52, loss = 0.12910938\n",
            "Iteration 53, loss = 0.12900709\n",
            "Iteration 54, loss = 0.12901733\n",
            "Iteration 55, loss = 0.12891798\n",
            "Iteration 56, loss = 0.12875133\n",
            "Iteration 57, loss = 0.12888089\n",
            "Iteration 58, loss = 0.12882507\n",
            "Iteration 59, loss = 0.12873728\n",
            "Iteration 60, loss = 0.12852988\n",
            "Iteration 61, loss = 0.12866621\n",
            "Iteration 62, loss = 0.12869592\n",
            "Iteration 63, loss = 0.12849991\n",
            "Iteration 64, loss = 0.12870193\n",
            "Iteration 65, loss = 0.12836072\n",
            "Iteration 66, loss = 0.12840975\n",
            "Iteration 67, loss = 0.12817305\n",
            "Iteration 68, loss = 0.12826462\n",
            "Iteration 69, loss = 0.12856624\n",
            "Iteration 70, loss = 0.12831370\n",
            "Iteration 71, loss = 0.12830060\n",
            "Iteration 72, loss = 0.12821753\n",
            "Iteration 73, loss = 0.12817777\n",
            "Iteration 74, loss = 0.12829579\n",
            "Iteration 75, loss = 0.12846625\n",
            "Iteration 76, loss = 0.12841578\n",
            "Iteration 77, loss = 0.12827995\n",
            "Iteration 78, loss = 0.12805325\n",
            "Iteration 79, loss = 0.12816832\n",
            "Iteration 80, loss = 0.12816030\n",
            "Iteration 81, loss = 0.12806093\n",
            "Iteration 82, loss = 0.12812104\n",
            "Iteration 83, loss = 0.12803036\n",
            "Iteration 84, loss = 0.12772638\n",
            "Iteration 85, loss = 0.12786665\n",
            "Iteration 86, loss = 0.12799435\n",
            "Iteration 87, loss = 0.12783875\n",
            "Iteration 88, loss = 0.12784281\n",
            "Iteration 89, loss = 0.12804409\n",
            "Iteration 90, loss = 0.12782469\n",
            "Iteration 91, loss = 0.12767525\n",
            "Iteration 92, loss = 0.12780030\n",
            "Iteration 93, loss = 0.12770462\n",
            "Iteration 94, loss = 0.12780306\n",
            "Iteration 95, loss = 0.12792122\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 96, loss = 0.12664166\n",
            "Iteration 97, loss = 0.12659825\n",
            "Iteration 98, loss = 0.12652302\n",
            "Iteration 99, loss = 0.12650101\n",
            "Iteration 100, loss = 0.12643796\n",
            "Iteration 101, loss = 0.12650160\n",
            "Iteration 102, loss = 0.12653789\n",
            "Iteration 103, loss = 0.12655772\n",
            "Iteration 104, loss = 0.12651732\n",
            "Iteration 105, loss = 0.12645492\n",
            "Iteration 106, loss = 0.12641807\n",
            "Iteration 107, loss = 0.12636118\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 108, loss = 0.12607706\n",
            "Iteration 109, loss = 0.12599823\n",
            "Iteration 110, loss = 0.12607796\n",
            "Iteration 111, loss = 0.12600618\n",
            "Iteration 112, loss = 0.12604819\n",
            "Iteration 113, loss = 0.12603977\n",
            "Iteration 114, loss = 0.12603477\n",
            "Iteration 115, loss = 0.12603558\n",
            "Iteration 116, loss = 0.12602614\n",
            "Iteration 117, loss = 0.12599463\n",
            "Iteration 118, loss = 0.12603351\n",
            "Iteration 119, loss = 0.12600661\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.12591365\n",
            "Iteration 121, loss = 0.12591031\n",
            "Iteration 122, loss = 0.12591034\n",
            "Iteration 123, loss = 0.12593047\n",
            "Iteration 124, loss = 0.12592858\n",
            "Iteration 125, loss = 0.12591523\n",
            "Iteration 126, loss = 0.12591767\n",
            "Iteration 127, loss = 0.12590447\n",
            "Iteration 128, loss = 0.12591535\n",
            "Iteration 129, loss = 0.12590933\n",
            "Iteration 130, loss = 0.12590897\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.12589144\n",
            "Iteration 132, loss = 0.12588603\n",
            "Iteration 133, loss = 0.12588811\n",
            "Iteration 134, loss = 0.12588704\n",
            "Iteration 135, loss = 0.12588589\n",
            "Iteration 136, loss = 0.12589039\n",
            "Iteration 137, loss = 0.12588376\n",
            "Iteration 138, loss = 0.12588907\n",
            "Iteration 139, loss = 0.12588773\n",
            "Iteration 140, loss = 0.12587966\n",
            "Iteration 141, loss = 0.12589069\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.25027207\n",
            "Iteration 3, loss = 0.22113952\n",
            "Iteration 4, loss = 0.19971607\n",
            "Iteration 5, loss = 0.18375638\n",
            "Iteration 6, loss = 0.17210734\n",
            "Iteration 7, loss = 0.16269633\n",
            "Iteration 8, loss = 0.15546379\n",
            "Iteration 9, loss = 0.15035414\n",
            "Iteration 10, loss = 0.14575718\n",
            "Iteration 11, loss = 0.14259778\n",
            "Iteration 12, loss = 0.14073820\n",
            "Iteration 13, loss = 0.13883228\n",
            "Iteration 14, loss = 0.13716365\n",
            "Iteration 15, loss = 0.13624324\n",
            "Iteration 16, loss = 0.13570682\n",
            "Iteration 17, loss = 0.13479191\n",
            "Iteration 18, loss = 0.13439545\n",
            "Iteration 19, loss = 0.13396631\n",
            "Iteration 20, loss = 0.13420063\n",
            "Iteration 21, loss = 0.13360399\n",
            "Iteration 22, loss = 0.13347364\n",
            "Iteration 23, loss = 0.13307203\n",
            "Iteration 24, loss = 0.13353534\n",
            "Iteration 25, loss = 0.13293753\n",
            "Iteration 26, loss = 0.13332127\n",
            "Iteration 27, loss = 0.13297009\n",
            "Iteration 28, loss = 0.13268111\n",
            "Iteration 29, loss = 0.13276264\n",
            "Iteration 30, loss = 0.13282471\n",
            "Iteration 31, loss = 0.13350775\n",
            "Iteration 32, loss = 0.13261774\n",
            "Iteration 33, loss = 0.13246988\n",
            "Iteration 34, loss = 0.13262843\n",
            "Iteration 35, loss = 0.13244921\n",
            "Iteration 36, loss = 0.13313884\n",
            "Iteration 37, loss = 0.13254198\n",
            "Iteration 38, loss = 0.13266882\n",
            "Iteration 39, loss = 0.13310641\n",
            "Iteration 40, loss = 0.13268732\n",
            "Iteration 41, loss = 0.13250740\n",
            "Iteration 42, loss = 0.13243385\n",
            "Iteration 43, loss = 0.13302763\n",
            "Iteration 44, loss = 0.13187175\n",
            "Iteration 45, loss = 0.13268598\n",
            "Iteration 46, loss = 0.13321851\n",
            "Iteration 47, loss = 0.13270165\n",
            "Iteration 48, loss = 0.13283936\n",
            "Iteration 49, loss = 0.13286292\n",
            "Iteration 50, loss = 0.13227581\n",
            "Iteration 51, loss = 0.13276247\n",
            "Iteration 52, loss = 0.13293418\n",
            "Iteration 53, loss = 0.13267650\n",
            "Iteration 54, loss = 0.13265482\n",
            "Iteration 55, loss = 0.13287836\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 56, loss = 0.12992282\n",
            "Iteration 57, loss = 0.12950367\n",
            "Iteration 58, loss = 0.12945845\n",
            "Iteration 59, loss = 0.12939910\n",
            "Iteration 60, loss = 0.12951913\n",
            "Iteration 61, loss = 0.12930120\n",
            "Iteration 62, loss = 0.12912830\n",
            "Iteration 63, loss = 0.12919228\n",
            "Iteration 64, loss = 0.12908655\n",
            "Iteration 65, loss = 0.12932568\n",
            "Iteration 66, loss = 0.12892896\n",
            "Iteration 67, loss = 0.12910978\n",
            "Iteration 68, loss = 0.12899151\n",
            "Iteration 69, loss = 0.12872366\n",
            "Iteration 70, loss = 0.12877296\n",
            "Iteration 71, loss = 0.12883072\n",
            "Iteration 72, loss = 0.12885206\n",
            "Iteration 73, loss = 0.12871887\n",
            "Iteration 74, loss = 0.12858547\n",
            "Iteration 75, loss = 0.12849621\n",
            "Iteration 76, loss = 0.12893732\n",
            "Iteration 77, loss = 0.12842099\n",
            "Iteration 78, loss = 0.12841408\n",
            "Iteration 79, loss = 0.12859774\n",
            "Iteration 80, loss = 0.12823136\n",
            "Iteration 81, loss = 0.12852900\n",
            "Iteration 82, loss = 0.12858493\n",
            "Iteration 83, loss = 0.12838520\n",
            "Iteration 84, loss = 0.12837214\n",
            "Iteration 85, loss = 0.12847290\n",
            "Iteration 86, loss = 0.12842409\n",
            "Iteration 87, loss = 0.12826277\n",
            "Iteration 88, loss = 0.12847335\n",
            "Iteration 89, loss = 0.12841176\n",
            "Iteration 90, loss = 0.12823739\n",
            "Iteration 91, loss = 0.12835635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 92, loss = 0.12718258\n",
            "Iteration 93, loss = 0.12717521\n",
            "Iteration 94, loss = 0.12723801\n",
            "Iteration 95, loss = 0.12714829\n",
            "Iteration 96, loss = 0.12708683\n",
            "Iteration 97, loss = 0.12714526\n",
            "Iteration 98, loss = 0.12710696\n",
            "Iteration 99, loss = 0.12713006\n",
            "Iteration 100, loss = 0.12697900\n",
            "Iteration 101, loss = 0.12700765\n",
            "Iteration 102, loss = 0.12704988\n",
            "Iteration 103, loss = 0.12700716\n",
            "Iteration 104, loss = 0.12704240\n",
            "Iteration 105, loss = 0.12702946\n",
            "Iteration 106, loss = 0.12692830\n",
            "Iteration 107, loss = 0.12698565\n",
            "Iteration 108, loss = 0.12696980\n",
            "Iteration 109, loss = 0.12717331\n",
            "Iteration 110, loss = 0.12710624\n",
            "Iteration 111, loss = 0.12700055\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 112, loss = 0.12663216\n",
            "Iteration 113, loss = 0.12657394\n",
            "Iteration 114, loss = 0.12657380\n",
            "Iteration 115, loss = 0.12660944\n",
            "Iteration 116, loss = 0.12660186\n",
            "Iteration 117, loss = 0.12662260\n",
            "Iteration 118, loss = 0.12654515\n",
            "Iteration 119, loss = 0.12656010\n",
            "Iteration 120, loss = 0.12654939\n",
            "Iteration 121, loss = 0.12655415\n",
            "Iteration 122, loss = 0.12653368\n",
            "Iteration 123, loss = 0.12653308\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 124, loss = 0.12648271\n",
            "Iteration 125, loss = 0.12647486\n",
            "Iteration 126, loss = 0.12646886\n",
            "Iteration 127, loss = 0.12647279\n",
            "Iteration 128, loss = 0.12646840\n",
            "Iteration 129, loss = 0.12645709\n",
            "Iteration 130, loss = 0.12647844\n",
            "Iteration 131, loss = 0.12646320\n",
            "Iteration 132, loss = 0.12646658\n",
            "Iteration 133, loss = 0.12647649\n",
            "Iteration 134, loss = 0.12647405\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 135, loss = 0.12645997\n",
            "Iteration 136, loss = 0.12644604\n",
            "Iteration 137, loss = 0.12644621\n",
            "Iteration 138, loss = 0.12643966\n",
            "Iteration 139, loss = 0.12644884\n",
            "Iteration 140, loss = 0.12644529\n",
            "Iteration 141, loss = 0.12644731\n",
            "Iteration 142, loss = 0.12644705\n",
            "Iteration 143, loss = 0.12644757\n",
            "Iteration 144, loss = 0.12644710\n",
            "Iteration 145, loss = 0.12644790\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.26655381\n",
            "Iteration 3, loss = 0.23416932\n",
            "Iteration 4, loss = 0.21020262\n",
            "Iteration 5, loss = 0.19159932\n",
            "Iteration 6, loss = 0.17748364\n",
            "Iteration 7, loss = 0.16674903\n",
            "Iteration 8, loss = 0.15909420\n",
            "Iteration 9, loss = 0.15289591\n",
            "Iteration 10, loss = 0.14832683\n",
            "Iteration 11, loss = 0.14399201\n",
            "Iteration 12, loss = 0.14181272\n",
            "Iteration 13, loss = 0.13979200\n",
            "Iteration 14, loss = 0.13783777\n",
            "Iteration 15, loss = 0.13646942\n",
            "Iteration 16, loss = 0.13652462\n",
            "Iteration 17, loss = 0.13560007\n",
            "Iteration 18, loss = 0.13490278\n",
            "Iteration 19, loss = 0.13444349\n",
            "Iteration 20, loss = 0.13379020\n",
            "Iteration 21, loss = 0.13335481\n",
            "Iteration 22, loss = 0.13326892\n",
            "Iteration 23, loss = 0.13331658\n",
            "Iteration 24, loss = 0.13335434\n",
            "Iteration 25, loss = 0.13292788\n",
            "Iteration 26, loss = 0.13256732\n",
            "Iteration 27, loss = 0.13303133\n",
            "Iteration 28, loss = 0.13312091\n",
            "Iteration 29, loss = 0.13303448\n",
            "Iteration 30, loss = 0.13312864\n",
            "Iteration 31, loss = 0.13303961\n",
            "Iteration 32, loss = 0.13264715\n",
            "Iteration 33, loss = 0.13235099\n",
            "Iteration 34, loss = 0.13266929\n",
            "Iteration 35, loss = 0.13208746\n",
            "Iteration 36, loss = 0.13203052\n",
            "Iteration 37, loss = 0.13282899\n",
            "Iteration 38, loss = 0.13260656\n",
            "Iteration 39, loss = 0.13274316\n",
            "Iteration 40, loss = 0.13243765\n",
            "Iteration 41, loss = 0.13254215\n",
            "Iteration 42, loss = 0.13244301\n",
            "Iteration 43, loss = 0.13251493\n",
            "Iteration 44, loss = 0.13274195\n",
            "Iteration 45, loss = 0.13267019\n",
            "Iteration 46, loss = 0.13236708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.12972236\n",
            "Iteration 48, loss = 0.12936593\n",
            "Iteration 49, loss = 0.12921560\n",
            "Iteration 50, loss = 0.12919041\n",
            "Iteration 51, loss = 0.12921948\n",
            "Iteration 52, loss = 0.12917014\n",
            "Iteration 53, loss = 0.12897785\n",
            "Iteration 54, loss = 0.12899805\n",
            "Iteration 55, loss = 0.12894184\n",
            "Iteration 56, loss = 0.12871749\n",
            "Iteration 57, loss = 0.12892783\n",
            "Iteration 58, loss = 0.12891359\n",
            "Iteration 59, loss = 0.12879070\n",
            "Iteration 60, loss = 0.12853174\n",
            "Iteration 61, loss = 0.12864600\n",
            "Iteration 62, loss = 0.12853090\n",
            "Iteration 63, loss = 0.12837311\n",
            "Iteration 64, loss = 0.12851243\n",
            "Iteration 65, loss = 0.12849709\n",
            "Iteration 66, loss = 0.12837267\n",
            "Iteration 67, loss = 0.12842714\n",
            "Iteration 68, loss = 0.12841238\n",
            "Iteration 69, loss = 0.12827351\n",
            "Iteration 70, loss = 0.12820658\n",
            "Iteration 71, loss = 0.12824224\n",
            "Iteration 72, loss = 0.12811980\n",
            "Iteration 73, loss = 0.12819316\n",
            "Iteration 74, loss = 0.12826662\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 75, loss = 0.12716166\n",
            "Iteration 76, loss = 0.12709391\n",
            "Iteration 77, loss = 0.12705430\n",
            "Iteration 78, loss = 0.12713120\n",
            "Iteration 79, loss = 0.12706686\n",
            "Iteration 80, loss = 0.12706781\n",
            "Iteration 81, loss = 0.12698790\n",
            "Iteration 82, loss = 0.12711137\n",
            "Iteration 83, loss = 0.12711186\n",
            "Iteration 84, loss = 0.12703572\n",
            "Iteration 85, loss = 0.12707712\n",
            "Iteration 86, loss = 0.12707464\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 87, loss = 0.12673279\n",
            "Iteration 88, loss = 0.12664548\n",
            "Iteration 89, loss = 0.12667949\n",
            "Iteration 90, loss = 0.12668948\n",
            "Iteration 91, loss = 0.12665571\n",
            "Iteration 92, loss = 0.12662673\n",
            "Iteration 93, loss = 0.12662467\n",
            "Iteration 94, loss = 0.12663103\n",
            "Iteration 95, loss = 0.12664952\n",
            "Iteration 96, loss = 0.12657890\n",
            "Iteration 97, loss = 0.12664172\n",
            "Iteration 98, loss = 0.12664600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 99, loss = 0.12658178\n",
            "Iteration 100, loss = 0.12654097\n",
            "Iteration 101, loss = 0.12653893\n",
            "Iteration 102, loss = 0.12653224\n",
            "Iteration 103, loss = 0.12654358\n",
            "Iteration 104, loss = 0.12653160\n",
            "Iteration 105, loss = 0.12654135\n",
            "Iteration 106, loss = 0.12652486\n",
            "Iteration 107, loss = 0.12654789\n",
            "Iteration 108, loss = 0.12652706\n",
            "Iteration 109, loss = 0.12652799\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 110, loss = 0.12653378\n",
            "Iteration 111, loss = 0.12651616\n",
            "Iteration 112, loss = 0.12651351\n",
            "Iteration 113, loss = 0.12651600\n",
            "Iteration 114, loss = 0.12651423\n",
            "Iteration 115, loss = 0.12651405\n",
            "Iteration 116, loss = 0.12651264\n",
            "Iteration 117, loss = 0.12651308\n",
            "Iteration 118, loss = 0.12651695\n",
            "Iteration 119, loss = 0.12651158\n",
            "Iteration 120, loss = 0.12651460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.50535068\n",
            "Iteration 2, loss = 0.21655468\n",
            "Iteration 3, loss = 0.19581630\n",
            "Iteration 4, loss = 0.18044352\n",
            "Iteration 5, loss = 0.16900929\n",
            "Iteration 6, loss = 0.16057805\n",
            "Iteration 7, loss = 0.15386858\n",
            "Iteration 8, loss = 0.14895313\n",
            "Iteration 9, loss = 0.14495310\n",
            "Iteration 10, loss = 0.14180377\n",
            "Iteration 11, loss = 0.14000265\n",
            "Iteration 12, loss = 0.13818841\n",
            "Iteration 13, loss = 0.13721352\n",
            "Iteration 14, loss = 0.13577249\n",
            "Iteration 15, loss = 0.13502106\n",
            "Iteration 16, loss = 0.13441798\n",
            "Iteration 17, loss = 0.13402870\n",
            "Iteration 18, loss = 0.13348687\n",
            "Iteration 19, loss = 0.13373727\n",
            "Iteration 20, loss = 0.13327194\n",
            "Iteration 21, loss = 0.13307326\n",
            "Iteration 22, loss = 0.13334469\n",
            "Iteration 23, loss = 0.13267167\n",
            "Iteration 24, loss = 0.13291923\n",
            "Iteration 25, loss = 0.13272170\n",
            "Iteration 26, loss = 0.13315607\n",
            "Iteration 27, loss = 0.13211223\n",
            "Iteration 28, loss = 0.13278586\n",
            "Iteration 29, loss = 0.13272630\n",
            "Iteration 30, loss = 0.13252196\n",
            "Iteration 31, loss = 0.13274987\n",
            "Iteration 32, loss = 0.13270009\n",
            "Iteration 33, loss = 0.13241464\n",
            "Iteration 34, loss = 0.13264454\n",
            "Iteration 35, loss = 0.13251456\n",
            "Iteration 36, loss = 0.13260543\n",
            "Iteration 37, loss = 0.13261051\n",
            "Iteration 38, loss = 0.13264399\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.13001616\n",
            "Iteration 40, loss = 0.12973581\n",
            "Iteration 41, loss = 0.12969749\n",
            "Iteration 42, loss = 0.12939134\n",
            "Iteration 43, loss = 0.12933256\n",
            "Iteration 44, loss = 0.12942923\n",
            "Iteration 45, loss = 0.12926829\n",
            "Iteration 46, loss = 0.12918854\n",
            "Iteration 47, loss = 0.12896650\n",
            "Iteration 48, loss = 0.12895988\n",
            "Iteration 49, loss = 0.12896926\n",
            "Iteration 50, loss = 0.12902029\n",
            "Iteration 51, loss = 0.12886515\n",
            "Iteration 52, loss = 0.12897742\n",
            "Iteration 53, loss = 0.12888802\n",
            "Iteration 54, loss = 0.12877485\n",
            "Iteration 55, loss = 0.12873947\n",
            "Iteration 56, loss = 0.12882912\n",
            "Iteration 57, loss = 0.12855363\n",
            "Iteration 58, loss = 0.12862233\n",
            "Iteration 59, loss = 0.12870912\n",
            "Iteration 60, loss = 0.12859435\n",
            "Iteration 61, loss = 0.12856450\n",
            "Iteration 62, loss = 0.12858693\n",
            "Iteration 63, loss = 0.12855159\n",
            "Iteration 64, loss = 0.12867628\n",
            "Iteration 65, loss = 0.12844717\n",
            "Iteration 66, loss = 0.12876915\n",
            "Iteration 67, loss = 0.12836906\n",
            "Iteration 68, loss = 0.12837187\n",
            "Iteration 69, loss = 0.12835400\n",
            "Iteration 70, loss = 0.12838903\n",
            "Iteration 71, loss = 0.12828081\n",
            "Iteration 72, loss = 0.12831333\n",
            "Iteration 73, loss = 0.12823319\n",
            "Iteration 74, loss = 0.12806946\n",
            "Iteration 75, loss = 0.12820133\n",
            "Iteration 76, loss = 0.12801621\n",
            "Iteration 77, loss = 0.12806869\n",
            "Iteration 78, loss = 0.12789588\n",
            "Iteration 79, loss = 0.12840332\n",
            "Iteration 80, loss = 0.12808638\n",
            "Iteration 81, loss = 0.12835664\n",
            "Iteration 82, loss = 0.12794894\n",
            "Iteration 83, loss = 0.12815980\n",
            "Iteration 84, loss = 0.12807458\n",
            "Iteration 85, loss = 0.12800442\n",
            "Iteration 86, loss = 0.12795262\n",
            "Iteration 87, loss = 0.12807535\n",
            "Iteration 88, loss = 0.12805292\n",
            "Iteration 89, loss = 0.12778591\n",
            "Iteration 90, loss = 0.12798641\n",
            "Iteration 91, loss = 0.12767089\n",
            "Iteration 92, loss = 0.12777389\n",
            "Iteration 93, loss = 0.12776496\n",
            "Iteration 94, loss = 0.12780666\n",
            "Iteration 95, loss = 0.12776785\n",
            "Iteration 96, loss = 0.12762775\n",
            "Iteration 97, loss = 0.12769199\n",
            "Iteration 98, loss = 0.12778615\n",
            "Iteration 99, loss = 0.12745824\n",
            "Iteration 100, loss = 0.12754337\n",
            "Iteration 101, loss = 0.12775963\n",
            "Iteration 102, loss = 0.12735633\n",
            "Iteration 103, loss = 0.12732026\n",
            "Iteration 104, loss = 0.12745474\n",
            "Iteration 105, loss = 0.12780157\n",
            "Iteration 106, loss = 0.12753718\n",
            "Iteration 107, loss = 0.12737287\n",
            "Iteration 108, loss = 0.12722341\n",
            "Iteration 109, loss = 0.12748988\n",
            "Iteration 110, loss = 0.12719032\n",
            "Iteration 111, loss = 0.12733968\n",
            "Iteration 112, loss = 0.12755499\n",
            "Iteration 113, loss = 0.12706663\n",
            "Iteration 114, loss = 0.12713845\n",
            "Iteration 115, loss = 0.12708633\n",
            "Iteration 116, loss = 0.12725566\n",
            "Iteration 117, loss = 0.12697968\n",
            "Iteration 118, loss = 0.12711307\n",
            "Iteration 119, loss = 0.12679755\n",
            "Iteration 120, loss = 0.12720191\n",
            "Iteration 121, loss = 0.12678326\n",
            "Iteration 122, loss = 0.12717970\n",
            "Iteration 123, loss = 0.12706455\n",
            "Iteration 124, loss = 0.12720030\n",
            "Iteration 125, loss = 0.12694180\n",
            "Iteration 126, loss = 0.12710952\n",
            "Iteration 127, loss = 0.12718204\n",
            "Iteration 128, loss = 0.12702037\n",
            "Iteration 129, loss = 0.12682268\n",
            "Iteration 130, loss = 0.12696214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 131, loss = 0.12569070\n",
            "Iteration 132, loss = 0.12564759\n",
            "Iteration 133, loss = 0.12565307\n",
            "Iteration 134, loss = 0.12559845\n",
            "Iteration 135, loss = 0.12564226\n",
            "Iteration 136, loss = 0.12560736\n",
            "Iteration 137, loss = 0.12561198\n",
            "Iteration 138, loss = 0.12566692\n",
            "Iteration 139, loss = 0.12562254\n",
            "Iteration 140, loss = 0.12555627\n",
            "Iteration 141, loss = 0.12559606\n",
            "Iteration 142, loss = 0.12551931\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 143, loss = 0.12511273\n",
            "Iteration 144, loss = 0.12512210\n",
            "Iteration 145, loss = 0.12512959\n",
            "Iteration 146, loss = 0.12511170\n",
            "Iteration 147, loss = 0.12504396\n",
            "Iteration 148, loss = 0.12510355\n",
            "Iteration 149, loss = 0.12503951\n",
            "Iteration 150, loss = 0.12504238\n",
            "Iteration 151, loss = 0.12508484\n",
            "Iteration 152, loss = 0.12510385\n",
            "Iteration 153, loss = 0.12506119\n",
            "Iteration 154, loss = 0.12512192\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 155, loss = 0.12495787\n",
            "Iteration 156, loss = 0.12494197\n",
            "Iteration 157, loss = 0.12494439\n",
            "Iteration 158, loss = 0.12495526\n",
            "Iteration 159, loss = 0.12496416\n",
            "Iteration 160, loss = 0.12495939\n",
            "Iteration 161, loss = 0.12493702\n",
            "Iteration 162, loss = 0.12495564\n",
            "Iteration 163, loss = 0.12495254\n",
            "Iteration 164, loss = 0.12491595\n",
            "Iteration 165, loss = 0.12495578\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 166, loss = 0.12491544\n",
            "Iteration 167, loss = 0.12492112\n",
            "Iteration 168, loss = 0.12490482\n",
            "Iteration 169, loss = 0.12492166\n",
            "Iteration 170, loss = 0.12491960\n",
            "Iteration 171, loss = 0.12491909\n",
            "Iteration 172, loss = 0.12491918\n",
            "Iteration 173, loss = 0.12491802\n",
            "Iteration 174, loss = 0.12491964\n",
            "Iteration 175, loss = 0.12491732\n",
            "Iteration 176, loss = 0.12491241\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.60949547\n",
            "Iteration 2, loss = 0.22485186\n",
            "Iteration 3, loss = 0.20158051\n",
            "Iteration 4, loss = 0.18437543\n",
            "Iteration 5, loss = 0.17203221\n",
            "Iteration 6, loss = 0.16235624\n",
            "Iteration 7, loss = 0.15465817\n",
            "Iteration 8, loss = 0.14958888\n",
            "Iteration 9, loss = 0.14617340\n",
            "Iteration 10, loss = 0.14264737\n",
            "Iteration 11, loss = 0.14058002\n",
            "Iteration 12, loss = 0.13863758\n",
            "Iteration 13, loss = 0.13742558\n",
            "Iteration 14, loss = 0.13676882\n",
            "Iteration 15, loss = 0.13511650\n",
            "Iteration 16, loss = 0.13487935\n",
            "Iteration 17, loss = 0.13411745\n",
            "Iteration 18, loss = 0.13419262\n",
            "Iteration 19, loss = 0.13373245\n",
            "Iteration 20, loss = 0.13331013\n",
            "Iteration 21, loss = 0.13310599\n",
            "Iteration 22, loss = 0.13333235\n",
            "Iteration 23, loss = 0.13322605\n",
            "Iteration 24, loss = 0.13306326\n",
            "Iteration 25, loss = 0.13328332\n",
            "Iteration 26, loss = 0.13309616\n",
            "Iteration 27, loss = 0.13342145\n",
            "Iteration 28, loss = 0.13279174\n",
            "Iteration 29, loss = 0.13349247\n",
            "Iteration 30, loss = 0.13278504\n",
            "Iteration 31, loss = 0.13265027\n",
            "Iteration 32, loss = 0.13284854\n",
            "Iteration 33, loss = 0.13274065\n",
            "Iteration 34, loss = 0.13278293\n",
            "Iteration 35, loss = 0.13283416\n",
            "Iteration 36, loss = 0.13263384\n",
            "Iteration 37, loss = 0.13315465\n",
            "Iteration 38, loss = 0.13291831\n",
            "Iteration 39, loss = 0.13322076\n",
            "Iteration 40, loss = 0.13318301\n",
            "Iteration 41, loss = 0.13295317\n",
            "Iteration 42, loss = 0.13322513\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.13004820\n",
            "Iteration 44, loss = 0.12990027\n",
            "Iteration 45, loss = 0.12976804\n",
            "Iteration 46, loss = 0.12978602\n",
            "Iteration 47, loss = 0.12980777\n",
            "Iteration 48, loss = 0.12970152\n",
            "Iteration 49, loss = 0.12953297\n",
            "Iteration 50, loss = 0.12963331\n",
            "Iteration 51, loss = 0.12937781\n",
            "Iteration 52, loss = 0.12924054\n",
            "Iteration 53, loss = 0.12949721\n",
            "Iteration 54, loss = 0.12941126\n",
            "Iteration 55, loss = 0.12914402\n",
            "Iteration 56, loss = 0.12923156\n",
            "Iteration 57, loss = 0.12911665\n",
            "Iteration 58, loss = 0.12912737\n",
            "Iteration 59, loss = 0.12932055\n",
            "Iteration 60, loss = 0.12908408\n",
            "Iteration 61, loss = 0.12907242\n",
            "Iteration 62, loss = 0.12901534\n",
            "Iteration 63, loss = 0.12895599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 64, loss = 0.12811327\n",
            "Iteration 65, loss = 0.12782199\n",
            "Iteration 66, loss = 0.12799040\n",
            "Iteration 67, loss = 0.12792556\n",
            "Iteration 68, loss = 0.12794556\n",
            "Iteration 69, loss = 0.12795154\n",
            "Iteration 70, loss = 0.12783089\n",
            "Iteration 71, loss = 0.12791392\n",
            "Iteration 72, loss = 0.12795137\n",
            "Iteration 73, loss = 0.12780847\n",
            "Iteration 74, loss = 0.12784056\n",
            "Iteration 75, loss = 0.12785640\n",
            "Iteration 76, loss = 0.12781611\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 77, loss = 0.12752383\n",
            "Iteration 78, loss = 0.12749252\n",
            "Iteration 79, loss = 0.12746513\n",
            "Iteration 80, loss = 0.12744044\n",
            "Iteration 81, loss = 0.12747049\n",
            "Iteration 82, loss = 0.12748962\n",
            "Iteration 83, loss = 0.12748282\n",
            "Iteration 84, loss = 0.12746331\n",
            "Iteration 85, loss = 0.12745685\n",
            "Iteration 86, loss = 0.12744469\n",
            "Iteration 87, loss = 0.12747120\n",
            "Iteration 88, loss = 0.12744302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 89, loss = 0.12737154\n",
            "Iteration 90, loss = 0.12737527\n",
            "Iteration 91, loss = 0.12737530\n",
            "Iteration 92, loss = 0.12736817\n",
            "Iteration 93, loss = 0.12736674\n",
            "Iteration 94, loss = 0.12736122\n",
            "Iteration 95, loss = 0.12736428\n",
            "Iteration 96, loss = 0.12736225\n",
            "Iteration 97, loss = 0.12736214\n",
            "Iteration 98, loss = 0.12735923\n",
            "Iteration 99, loss = 0.12736661\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 100, loss = 0.12734809\n",
            "Iteration 101, loss = 0.12734291\n",
            "Iteration 102, loss = 0.12734451\n",
            "Iteration 103, loss = 0.12734386\n",
            "Iteration 104, loss = 0.12734158\n",
            "Iteration 105, loss = 0.12734594\n",
            "Iteration 106, loss = 0.12734126\n",
            "Iteration 107, loss = 0.12734540\n",
            "Iteration 108, loss = 0.12734288\n",
            "Iteration 109, loss = 0.12734375\n",
            "Iteration 110, loss = 0.12734332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 19.4min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.40455784\n",
            "Iteration 2, loss = 0.27991892\n",
            "Iteration 3, loss = 0.26608319\n",
            "Iteration 4, loss = 0.25406112\n",
            "Iteration 5, loss = 0.24247110\n",
            "Iteration 6, loss = 0.23272399\n",
            "Iteration 7, loss = 0.22398497\n",
            "Iteration 8, loss = 0.21619478\n",
            "Iteration 9, loss = 0.20882719\n",
            "Iteration 10, loss = 0.20269689\n",
            "Iteration 11, loss = 0.19708624\n",
            "Iteration 12, loss = 0.19186843\n",
            "Iteration 13, loss = 0.18642042\n",
            "Iteration 14, loss = 0.18244117\n",
            "Iteration 15, loss = 0.17814429\n",
            "Iteration 16, loss = 0.17392524\n",
            "Iteration 17, loss = 0.17001011\n",
            "Iteration 18, loss = 0.16718022\n",
            "Iteration 19, loss = 0.16425720\n",
            "Iteration 20, loss = 0.16211528\n",
            "Iteration 21, loss = 0.15858487\n",
            "Iteration 22, loss = 0.15645526\n",
            "Iteration 23, loss = 0.15461317\n",
            "Iteration 24, loss = 0.15235114\n",
            "Iteration 25, loss = 0.15145710\n",
            "Iteration 26, loss = 0.14923746\n",
            "Iteration 27, loss = 0.14678963\n",
            "Iteration 28, loss = 0.14614391\n",
            "Iteration 29, loss = 0.14425440\n",
            "Iteration 30, loss = 0.14243629\n",
            "Iteration 31, loss = 0.14234073\n",
            "Iteration 32, loss = 0.14112576\n",
            "Iteration 33, loss = 0.14013981\n",
            "Iteration 34, loss = 0.13923823\n",
            "Iteration 35, loss = 0.13842127\n",
            "Iteration 36, loss = 0.13727004\n",
            "Iteration 37, loss = 0.13629718\n",
            "Iteration 38, loss = 0.13562150\n",
            "Iteration 39, loss = 0.13509165\n",
            "Iteration 40, loss = 0.13432701\n",
            "Iteration 41, loss = 0.13366235\n",
            "Iteration 42, loss = 0.13361483\n",
            "Iteration 43, loss = 0.13305112\n",
            "Iteration 44, loss = 0.13280979\n",
            "Iteration 45, loss = 0.13342792\n",
            "Iteration 46, loss = 0.13217432\n",
            "Iteration 47, loss = 0.13177693\n",
            "Iteration 48, loss = 0.13158873\n",
            "Iteration 49, loss = 0.13157809\n",
            "Iteration 50, loss = 0.13029482\n",
            "Iteration 51, loss = 0.13080925\n",
            "Iteration 52, loss = 0.13144022\n",
            "Iteration 53, loss = 0.12947980\n",
            "Iteration 54, loss = 0.12933281\n",
            "Iteration 55, loss = 0.12998488\n",
            "Iteration 56, loss = 0.13008794\n",
            "Iteration 57, loss = 0.12982908\n",
            "Iteration 58, loss = 0.12907633\n",
            "Iteration 59, loss = 0.12901424\n",
            "Iteration 60, loss = 0.12934719\n",
            "Iteration 61, loss = 0.12823865\n",
            "Iteration 62, loss = 0.12888623\n",
            "Iteration 63, loss = 0.12999993\n",
            "Iteration 64, loss = 0.12778308\n",
            "Iteration 65, loss = 0.12936872\n",
            "Iteration 66, loss = 0.12954289\n",
            "Iteration 67, loss = 0.12960837\n",
            "Iteration 68, loss = 0.12921953\n",
            "Iteration 69, loss = 0.12804155\n",
            "Iteration 70, loss = 0.12865167\n",
            "Iteration 71, loss = 0.12869100\n",
            "Iteration 72, loss = 0.12827338\n",
            "Iteration 73, loss = 0.12811964\n",
            "Iteration 74, loss = 0.12772451\n",
            "Iteration 75, loss = 0.12834773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 76, loss = 0.12546338\n",
            "Iteration 77, loss = 0.12471770\n",
            "Iteration 78, loss = 0.12492009\n",
            "Iteration 79, loss = 0.12510642\n",
            "Iteration 80, loss = 0.12455412\n",
            "Iteration 81, loss = 0.12426663\n",
            "Iteration 82, loss = 0.12437773\n",
            "Iteration 83, loss = 0.12450908\n",
            "Iteration 84, loss = 0.12443209\n",
            "Iteration 85, loss = 0.12386871\n",
            "Iteration 86, loss = 0.12420088\n",
            "Iteration 87, loss = 0.12451474\n",
            "Iteration 88, loss = 0.12411141\n",
            "Iteration 89, loss = 0.12417120\n",
            "Iteration 90, loss = 0.12442101\n",
            "Iteration 91, loss = 0.12445376\n",
            "Iteration 92, loss = 0.12454296\n",
            "Iteration 93, loss = 0.12403323\n",
            "Iteration 94, loss = 0.12402961\n",
            "Iteration 95, loss = 0.12408396\n",
            "Iteration 96, loss = 0.12431951\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 97, loss = 0.12340773\n",
            "Iteration 98, loss = 0.12314999\n",
            "Iteration 99, loss = 0.12334076\n",
            "Iteration 100, loss = 0.12327026\n",
            "Iteration 101, loss = 0.12307312\n",
            "Iteration 102, loss = 0.12306137\n",
            "Iteration 103, loss = 0.12311126\n",
            "Iteration 104, loss = 0.12337900\n",
            "Iteration 105, loss = 0.12322381\n",
            "Iteration 106, loss = 0.12299506\n",
            "Iteration 107, loss = 0.12320880\n",
            "Iteration 108, loss = 0.12314207\n",
            "Iteration 109, loss = 0.12311932\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 110, loss = 0.12290745\n",
            "Iteration 111, loss = 0.12284315\n",
            "Iteration 112, loss = 0.12284055\n",
            "Iteration 113, loss = 0.12280540\n",
            "Iteration 114, loss = 0.12277356\n",
            "Iteration 115, loss = 0.12282305\n",
            "Iteration 116, loss = 0.12280247\n",
            "Iteration 117, loss = 0.12282458\n",
            "Iteration 118, loss = 0.12276323\n",
            "Iteration 119, loss = 0.12278208\n",
            "Iteration 120, loss = 0.12279974\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 121, loss = 0.12274270\n",
            "Iteration 122, loss = 0.12274095\n",
            "Iteration 123, loss = 0.12273052\n",
            "Iteration 124, loss = 0.12272315\n",
            "Iteration 125, loss = 0.12272809\n",
            "Iteration 126, loss = 0.12272540\n",
            "Iteration 127, loss = 0.12272155\n",
            "Iteration 128, loss = 0.12273700\n",
            "Iteration 129, loss = 0.12272626\n",
            "Iteration 130, loss = 0.12272088\n",
            "Iteration 131, loss = 0.12271881\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 132, loss = 0.12270537\n",
            "Iteration 133, loss = 0.12270602\n",
            "Iteration 134, loss = 0.12270821\n",
            "Iteration 135, loss = 0.12270746\n",
            "Iteration 136, loss = 0.12270557\n",
            "Iteration 137, loss = 0.12270620\n",
            "Iteration 138, loss = 0.12270828\n",
            "Iteration 139, loss = 0.12270533\n",
            "Iteration 140, loss = 0.12270615\n",
            "Iteration 141, loss = 0.12270540\n",
            "Iteration 142, loss = 0.12270578\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.32459422\n",
            "Iteration 3, loss = 0.30538807\n",
            "Iteration 4, loss = 0.28931063\n",
            "Iteration 5, loss = 0.27417711\n",
            "Iteration 6, loss = 0.26222466\n",
            "Iteration 7, loss = 0.25130463\n",
            "Iteration 8, loss = 0.24107281\n",
            "Iteration 9, loss = 0.23193127\n",
            "Iteration 10, loss = 0.22567626\n",
            "Iteration 11, loss = 0.21696079\n",
            "Iteration 12, loss = 0.21053221\n",
            "Iteration 13, loss = 0.20361263\n",
            "Iteration 14, loss = 0.19796425\n",
            "Iteration 15, loss = 0.19194065\n",
            "Iteration 16, loss = 0.18717634\n",
            "Iteration 17, loss = 0.18266685\n",
            "Iteration 18, loss = 0.17876550\n",
            "Iteration 19, loss = 0.17388377\n",
            "Iteration 20, loss = 0.17053711\n",
            "Iteration 21, loss = 0.16791673\n",
            "Iteration 22, loss = 0.16384631\n",
            "Iteration 23, loss = 0.16121462\n",
            "Iteration 24, loss = 0.15922984\n",
            "Iteration 25, loss = 0.15642639\n",
            "Iteration 26, loss = 0.15465880\n",
            "Iteration 27, loss = 0.15217085\n",
            "Iteration 28, loss = 0.15011504\n",
            "Iteration 29, loss = 0.14841306\n",
            "Iteration 30, loss = 0.14694170\n",
            "Iteration 31, loss = 0.14529377\n",
            "Iteration 32, loss = 0.14409948\n",
            "Iteration 33, loss = 0.14335442\n",
            "Iteration 34, loss = 0.14128313\n",
            "Iteration 35, loss = 0.14033554\n",
            "Iteration 36, loss = 0.13987948\n",
            "Iteration 37, loss = 0.13952099\n",
            "Iteration 38, loss = 0.13790404\n",
            "Iteration 39, loss = 0.13679975\n",
            "Iteration 40, loss = 0.13627170\n",
            "Iteration 41, loss = 0.13575994\n",
            "Iteration 42, loss = 0.13485607\n",
            "Iteration 43, loss = 0.13419210\n",
            "Iteration 44, loss = 0.13348558\n",
            "Iteration 45, loss = 0.13335788\n",
            "Iteration 46, loss = 0.13381565\n",
            "Iteration 47, loss = 0.13297843\n",
            "Iteration 48, loss = 0.13182066\n",
            "Iteration 49, loss = 0.13186296\n",
            "Iteration 50, loss = 0.13059722\n",
            "Iteration 51, loss = 0.13051485\n",
            "Iteration 52, loss = 0.13103770\n",
            "Iteration 53, loss = 0.13035092\n",
            "Iteration 54, loss = 0.13129705\n",
            "Iteration 55, loss = 0.12972761\n",
            "Iteration 56, loss = 0.13045756\n",
            "Iteration 57, loss = 0.12895468\n",
            "Iteration 58, loss = 0.12900596\n",
            "Iteration 59, loss = 0.12920497\n",
            "Iteration 60, loss = 0.13019761\n",
            "Iteration 61, loss = 0.12912206\n",
            "Iteration 62, loss = 0.12978347\n",
            "Iteration 63, loss = 0.12910473\n",
            "Iteration 64, loss = 0.12960641\n",
            "Iteration 65, loss = 0.12795420\n",
            "Iteration 66, loss = 0.12775016\n",
            "Iteration 67, loss = 0.12870160\n",
            "Iteration 68, loss = 0.12829774\n",
            "Iteration 69, loss = 0.12862096\n",
            "Iteration 70, loss = 0.12773645\n",
            "Iteration 71, loss = 0.12924354\n",
            "Iteration 72, loss = 0.12799953\n",
            "Iteration 73, loss = 0.12746636\n",
            "Iteration 74, loss = 0.12812386\n",
            "Iteration 75, loss = 0.12815964\n",
            "Iteration 76, loss = 0.12808282\n",
            "Iteration 77, loss = 0.12742976\n",
            "Iteration 78, loss = 0.12698783\n",
            "Iteration 79, loss = 0.12800463\n",
            "Iteration 80, loss = 0.12908346\n",
            "Iteration 81, loss = 0.12740688\n",
            "Iteration 82, loss = 0.12726610\n",
            "Iteration 83, loss = 0.12691813\n",
            "Iteration 84, loss = 0.12878061\n",
            "Iteration 85, loss = 0.12671933\n",
            "Iteration 86, loss = 0.12729292\n",
            "Iteration 87, loss = 0.12768059\n",
            "Iteration 88, loss = 0.12701228\n",
            "Iteration 89, loss = 0.12869598\n",
            "Iteration 90, loss = 0.12835633\n",
            "Iteration 91, loss = 0.12696109\n",
            "Iteration 92, loss = 0.12701281\n",
            "Iteration 93, loss = 0.12839530\n",
            "Iteration 94, loss = 0.12747360\n",
            "Iteration 95, loss = 0.12755168\n",
            "Iteration 96, loss = 0.12813493\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 97, loss = 0.12413824\n",
            "Iteration 98, loss = 0.12389602\n",
            "Iteration 99, loss = 0.12384396\n",
            "Iteration 100, loss = 0.12406044\n",
            "Iteration 101, loss = 0.12361178\n",
            "Iteration 102, loss = 0.12363060\n",
            "Iteration 103, loss = 0.12391152\n",
            "Iteration 104, loss = 0.12377158\n",
            "Iteration 105, loss = 0.12315429\n",
            "Iteration 106, loss = 0.12341214\n",
            "Iteration 107, loss = 0.12393667\n",
            "Iteration 108, loss = 0.12399339\n",
            "Iteration 109, loss = 0.12354630\n",
            "Iteration 110, loss = 0.12355977\n",
            "Iteration 111, loss = 0.12345716\n",
            "Iteration 112, loss = 0.12323164\n",
            "Iteration 113, loss = 0.12348124\n",
            "Iteration 114, loss = 0.12319228\n",
            "Iteration 115, loss = 0.12326461\n",
            "Iteration 116, loss = 0.12379066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 117, loss = 0.12266698\n",
            "Iteration 118, loss = 0.12248610\n",
            "Iteration 119, loss = 0.12251875\n",
            "Iteration 120, loss = 0.12253031\n",
            "Iteration 121, loss = 0.12271281\n",
            "Iteration 122, loss = 0.12257302\n",
            "Iteration 123, loss = 0.12256989\n",
            "Iteration 124, loss = 0.12234787\n",
            "Iteration 125, loss = 0.12234191\n",
            "Iteration 126, loss = 0.12236398\n",
            "Iteration 127, loss = 0.12239040\n",
            "Iteration 128, loss = 0.12224366\n",
            "Iteration 129, loss = 0.12248107\n",
            "Iteration 130, loss = 0.12241196\n",
            "Iteration 131, loss = 0.12252106\n",
            "Iteration 132, loss = 0.12231850\n",
            "Iteration 133, loss = 0.12232747\n",
            "Iteration 134, loss = 0.12230181\n",
            "Iteration 135, loss = 0.12215865\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 136, loss = 0.12214685\n",
            "Iteration 137, loss = 0.12202746\n",
            "Iteration 138, loss = 0.12202843\n",
            "Iteration 139, loss = 0.12200022\n",
            "Iteration 140, loss = 0.12204125\n",
            "Iteration 141, loss = 0.12207695\n",
            "Iteration 142, loss = 0.12203055\n",
            "Iteration 143, loss = 0.12204702\n",
            "Iteration 144, loss = 0.12207395\n",
            "Iteration 145, loss = 0.12206152\n",
            "Iteration 146, loss = 0.12201505\n",
            "Iteration 147, loss = 0.12203572\n",
            "Iteration 148, loss = 0.12197884\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 149, loss = 0.12206921\n",
            "Iteration 150, loss = 0.12197967\n",
            "Iteration 151, loss = 0.12195805\n",
            "Iteration 152, loss = 0.12195668\n",
            "Iteration 153, loss = 0.12195496\n",
            "Iteration 154, loss = 0.12195523\n",
            "Iteration 155, loss = 0.12193549\n",
            "Iteration 156, loss = 0.12198260\n",
            "Iteration 157, loss = 0.12195927\n",
            "Iteration 158, loss = 0.12195897\n",
            "Iteration 159, loss = 0.12195368\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 160, loss = 0.12194372\n",
            "Iteration 161, loss = 0.12194014\n",
            "Iteration 162, loss = 0.12193615\n",
            "Iteration 163, loss = 0.12193740\n",
            "Iteration 164, loss = 0.12193362\n",
            "Iteration 165, loss = 0.12193437\n",
            "Iteration 166, loss = 0.12193453\n",
            "Iteration 167, loss = 0.12193540\n",
            "Iteration 168, loss = 0.12193380\n",
            "Iteration 169, loss = 0.12193593\n",
            "Iteration 170, loss = 0.12193528\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.76133231\n",
            "Iteration 2, loss = 0.24177989\n",
            "Iteration 3, loss = 0.23007753\n",
            "Iteration 4, loss = 0.22183568\n",
            "Iteration 5, loss = 0.21320727\n",
            "Iteration 6, loss = 0.20416542\n",
            "Iteration 7, loss = 0.19866718\n",
            "Iteration 8, loss = 0.19296295\n",
            "Iteration 9, loss = 0.18821748\n",
            "Iteration 10, loss = 0.18302548\n",
            "Iteration 11, loss = 0.17856625\n",
            "Iteration 12, loss = 0.17433339\n",
            "Iteration 13, loss = 0.17101417\n",
            "Iteration 14, loss = 0.16693715\n",
            "Iteration 15, loss = 0.16430846\n",
            "Iteration 16, loss = 0.16114807\n",
            "Iteration 17, loss = 0.15847116\n",
            "Iteration 18, loss = 0.15771205\n",
            "Iteration 19, loss = 0.15397669\n",
            "Iteration 20, loss = 0.15239114\n",
            "Iteration 21, loss = 0.15049823\n",
            "Iteration 22, loss = 0.14992928\n",
            "Iteration 23, loss = 0.14761731\n",
            "Iteration 24, loss = 0.14554851\n",
            "Iteration 25, loss = 0.14487873\n",
            "Iteration 26, loss = 0.14235369\n",
            "Iteration 27, loss = 0.14236340\n",
            "Iteration 28, loss = 0.14003779\n",
            "Iteration 29, loss = 0.14064558\n",
            "Iteration 30, loss = 0.13868351\n",
            "Iteration 31, loss = 0.13783746\n",
            "Iteration 32, loss = 0.13746358\n",
            "Iteration 33, loss = 0.13608291\n",
            "Iteration 34, loss = 0.13642207\n",
            "Iteration 35, loss = 0.13605013\n",
            "Iteration 36, loss = 0.13495174\n",
            "Iteration 37, loss = 0.13311701\n",
            "Iteration 38, loss = 0.13448967\n",
            "Iteration 39, loss = 0.13277953\n",
            "Iteration 40, loss = 0.13253973\n",
            "Iteration 41, loss = 0.13157735\n",
            "Iteration 42, loss = 0.13341755\n",
            "Iteration 43, loss = 0.13254344\n",
            "Iteration 44, loss = 0.13149368\n",
            "Iteration 45, loss = 0.13172308\n",
            "Iteration 46, loss = 0.13099294\n",
            "Iteration 47, loss = 0.13094639\n",
            "Iteration 48, loss = 0.13120790\n",
            "Iteration 49, loss = 0.13017362\n",
            "Iteration 50, loss = 0.13118349\n",
            "Iteration 51, loss = 0.13005898\n",
            "Iteration 52, loss = 0.12986018\n",
            "Iteration 53, loss = 0.12959749\n",
            "Iteration 54, loss = 0.12950643\n",
            "Iteration 55, loss = 0.12899334\n",
            "Iteration 56, loss = 0.12936647\n",
            "Iteration 57, loss = 0.12941548\n",
            "Iteration 58, loss = 0.12968380\n",
            "Iteration 59, loss = 0.12984391\n",
            "Iteration 60, loss = 0.12859169\n",
            "Iteration 61, loss = 0.12909632\n",
            "Iteration 62, loss = 0.13010200\n",
            "Iteration 63, loss = 0.12931886\n",
            "Iteration 64, loss = 0.12908363\n",
            "Iteration 65, loss = 0.12740041\n",
            "Iteration 66, loss = 0.12811244\n",
            "Iteration 67, loss = 0.12842864\n",
            "Iteration 68, loss = 0.12822481\n",
            "Iteration 69, loss = 0.12856729\n",
            "Iteration 70, loss = 0.12813908\n",
            "Iteration 71, loss = 0.12862022\n",
            "Iteration 72, loss = 0.12786026\n",
            "Iteration 73, loss = 0.12860835\n",
            "Iteration 74, loss = 0.12776982\n",
            "Iteration 75, loss = 0.12858035\n",
            "Iteration 76, loss = 0.12811096\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.12493701\n",
            "Iteration 78, loss = 0.12443004\n",
            "Iteration 79, loss = 0.12435032\n",
            "Iteration 80, loss = 0.12458929\n",
            "Iteration 81, loss = 0.12462557\n",
            "Iteration 82, loss = 0.12405950\n",
            "Iteration 83, loss = 0.12395143\n",
            "Iteration 84, loss = 0.12424909\n",
            "Iteration 85, loss = 0.12413419\n",
            "Iteration 86, loss = 0.12445836\n",
            "Iteration 87, loss = 0.12398874\n",
            "Iteration 88, loss = 0.12392460\n",
            "Iteration 89, loss = 0.12391387\n",
            "Iteration 90, loss = 0.12403484\n",
            "Iteration 91, loss = 0.12386020\n",
            "Iteration 92, loss = 0.12401114\n",
            "Iteration 93, loss = 0.12425967\n",
            "Iteration 94, loss = 0.12392628\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 95, loss = 0.12322826\n",
            "Iteration 96, loss = 0.12294524\n",
            "Iteration 97, loss = 0.12310921\n",
            "Iteration 98, loss = 0.12294145\n",
            "Iteration 99, loss = 0.12291698\n",
            "Iteration 100, loss = 0.12297182\n",
            "Iteration 101, loss = 0.12283608\n",
            "Iteration 102, loss = 0.12291453\n",
            "Iteration 103, loss = 0.12283984\n",
            "Iteration 104, loss = 0.12280599\n",
            "Iteration 105, loss = 0.12292698\n",
            "Iteration 106, loss = 0.12291521\n",
            "Iteration 107, loss = 0.12287452\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 108, loss = 0.12258827\n",
            "Iteration 109, loss = 0.12251593\n",
            "Iteration 110, loss = 0.12255748\n",
            "Iteration 111, loss = 0.12253783\n",
            "Iteration 112, loss = 0.12255043\n",
            "Iteration 113, loss = 0.12255502\n",
            "Iteration 114, loss = 0.12260572\n",
            "Iteration 115, loss = 0.12262803\n",
            "Iteration 116, loss = 0.12254891\n",
            "Iteration 117, loss = 0.12254680\n",
            "Iteration 118, loss = 0.12257725\n",
            "Iteration 119, loss = 0.12259175\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.12248796\n",
            "Iteration 121, loss = 0.12246574\n",
            "Iteration 122, loss = 0.12246533\n",
            "Iteration 123, loss = 0.12246457\n",
            "Iteration 124, loss = 0.12248533\n",
            "Iteration 125, loss = 0.12247768\n",
            "Iteration 126, loss = 0.12247461\n",
            "Iteration 127, loss = 0.12246369\n",
            "Iteration 128, loss = 0.12247590\n",
            "Iteration 129, loss = 0.12246370\n",
            "Iteration 130, loss = 0.12246651\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.12244980\n",
            "Iteration 132, loss = 0.12245059\n",
            "Iteration 133, loss = 0.12244918\n",
            "Iteration 134, loss = 0.12244566\n",
            "Iteration 135, loss = 0.12244730\n",
            "Iteration 136, loss = 0.12244921\n",
            "Iteration 137, loss = 0.12244963\n",
            "Iteration 138, loss = 0.12244575\n",
            "Iteration 139, loss = 0.12244582\n",
            "Iteration 140, loss = 0.12244744\n",
            "Iteration 141, loss = 0.12244680\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.33170225\n",
            "Iteration 3, loss = 0.31399326\n",
            "Iteration 4, loss = 0.29841250\n",
            "Iteration 5, loss = 0.28403676\n",
            "Iteration 6, loss = 0.27087621\n",
            "Iteration 7, loss = 0.26005913\n",
            "Iteration 8, loss = 0.24879371\n",
            "Iteration 9, loss = 0.24015298\n",
            "Iteration 10, loss = 0.23088879\n",
            "Iteration 11, loss = 0.22389746\n",
            "Iteration 12, loss = 0.21620802\n",
            "Iteration 13, loss = 0.21049616\n",
            "Iteration 14, loss = 0.20319763\n",
            "Iteration 15, loss = 0.19771863\n",
            "Iteration 16, loss = 0.19312657\n",
            "Iteration 17, loss = 0.18773713\n",
            "Iteration 18, loss = 0.18354140\n",
            "Iteration 19, loss = 0.17929567\n",
            "Iteration 20, loss = 0.17615973\n",
            "Iteration 21, loss = 0.17197752\n",
            "Iteration 22, loss = 0.16905061\n",
            "Iteration 23, loss = 0.16680830\n",
            "Iteration 24, loss = 0.16219474\n",
            "Iteration 25, loss = 0.16053376\n",
            "Iteration 26, loss = 0.15688926\n",
            "Iteration 27, loss = 0.15479647\n",
            "Iteration 28, loss = 0.15497696\n",
            "Iteration 29, loss = 0.15237207\n",
            "Iteration 30, loss = 0.15085580\n",
            "Iteration 31, loss = 0.14945877\n",
            "Iteration 32, loss = 0.14684854\n",
            "Iteration 33, loss = 0.14738733\n",
            "Iteration 34, loss = 0.14510541\n",
            "Iteration 35, loss = 0.14309680\n",
            "Iteration 36, loss = 0.14195430\n",
            "Iteration 37, loss = 0.14208655\n",
            "Iteration 38, loss = 0.14269700\n",
            "Iteration 39, loss = 0.14065040\n",
            "Iteration 40, loss = 0.13946437\n",
            "Iteration 41, loss = 0.14012898\n",
            "Iteration 42, loss = 0.13757240\n",
            "Iteration 43, loss = 0.13760822\n",
            "Iteration 44, loss = 0.13658589\n",
            "Iteration 45, loss = 0.13779323\n",
            "Iteration 46, loss = 0.13395483\n",
            "Iteration 47, loss = 0.13495539\n",
            "Iteration 48, loss = 0.13461363\n",
            "Iteration 49, loss = 0.13418849\n",
            "Iteration 50, loss = 0.13399286\n",
            "Iteration 51, loss = 0.13328988\n",
            "Iteration 52, loss = 0.13300402\n",
            "Iteration 53, loss = 0.13348580\n",
            "Iteration 54, loss = 0.13269539\n",
            "Iteration 55, loss = 0.13217412\n",
            "Iteration 56, loss = 0.13308831\n",
            "Iteration 57, loss = 0.13202290\n",
            "Iteration 58, loss = 0.13237312\n",
            "Iteration 59, loss = 0.13193827\n",
            "Iteration 60, loss = 0.13193264\n",
            "Iteration 61, loss = 0.13087181\n",
            "Iteration 62, loss = 0.13157018\n",
            "Iteration 63, loss = 0.13155090\n",
            "Iteration 64, loss = 0.13177108\n",
            "Iteration 65, loss = 0.13131529\n",
            "Iteration 66, loss = 0.13100717\n",
            "Iteration 67, loss = 0.13085372\n",
            "Iteration 68, loss = 0.13006318\n",
            "Iteration 69, loss = 0.13037364\n",
            "Iteration 70, loss = 0.13050403\n",
            "Iteration 71, loss = 0.13094366\n",
            "Iteration 72, loss = 0.13048264\n",
            "Iteration 73, loss = 0.13051282\n",
            "Iteration 74, loss = 0.12973397\n",
            "Iteration 75, loss = 0.13006040\n",
            "Iteration 76, loss = 0.13063090\n",
            "Iteration 77, loss = 0.12933259\n",
            "Iteration 78, loss = 0.13012953\n",
            "Iteration 79, loss = 0.12833542\n",
            "Iteration 80, loss = 0.12886151\n",
            "Iteration 81, loss = 0.12969724\n",
            "Iteration 82, loss = 0.12925368\n",
            "Iteration 83, loss = 0.12927130\n",
            "Iteration 84, loss = 0.13024947\n",
            "Iteration 85, loss = 0.12942293\n",
            "Iteration 86, loss = 0.13000112\n",
            "Iteration 87, loss = 0.12927971\n",
            "Iteration 88, loss = 0.12942360\n",
            "Iteration 89, loss = 0.13021667\n",
            "Iteration 90, loss = 0.12920459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 91, loss = 0.12531714\n",
            "Iteration 92, loss = 0.12538404\n",
            "Iteration 93, loss = 0.12548752\n",
            "Iteration 94, loss = 0.12540368\n",
            "Iteration 95, loss = 0.12554311\n",
            "Iteration 96, loss = 0.12513318\n",
            "Iteration 97, loss = 0.12511407\n",
            "Iteration 98, loss = 0.12567651\n",
            "Iteration 99, loss = 0.12482367\n",
            "Iteration 100, loss = 0.12507565\n",
            "Iteration 101, loss = 0.12515801\n",
            "Iteration 102, loss = 0.12504743\n",
            "Iteration 103, loss = 0.12502807\n",
            "Iteration 104, loss = 0.12513202\n",
            "Iteration 105, loss = 0.12522759\n",
            "Iteration 106, loss = 0.12499563\n",
            "Iteration 107, loss = 0.12505627\n",
            "Iteration 108, loss = 0.12481761\n",
            "Iteration 109, loss = 0.12460687\n",
            "Iteration 110, loss = 0.12475475\n",
            "Iteration 111, loss = 0.12472720\n",
            "Iteration 112, loss = 0.12455142\n",
            "Iteration 113, loss = 0.12439563\n",
            "Iteration 114, loss = 0.12464937\n",
            "Iteration 115, loss = 0.12445278\n",
            "Iteration 116, loss = 0.12454766\n",
            "Iteration 117, loss = 0.12426097\n",
            "Iteration 118, loss = 0.12468913\n",
            "Iteration 119, loss = 0.12453929\n",
            "Iteration 120, loss = 0.12437546\n",
            "Iteration 121, loss = 0.12454729\n",
            "Iteration 122, loss = 0.12463594\n",
            "Iteration 123, loss = 0.12466627\n",
            "Iteration 124, loss = 0.12470405\n",
            "Iteration 125, loss = 0.12451603\n",
            "Iteration 126, loss = 0.12447502\n",
            "Iteration 127, loss = 0.12428674\n",
            "Iteration 128, loss = 0.12449052\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 129, loss = 0.12346886\n",
            "Iteration 130, loss = 0.12331110\n",
            "Iteration 131, loss = 0.12351602\n",
            "Iteration 132, loss = 0.12327964\n",
            "Iteration 133, loss = 0.12333738\n",
            "Iteration 134, loss = 0.12346772\n",
            "Iteration 135, loss = 0.12324377\n",
            "Iteration 136, loss = 0.12319996\n",
            "Iteration 137, loss = 0.12323157\n",
            "Iteration 138, loss = 0.12338919\n",
            "Iteration 139, loss = 0.12317482\n",
            "Iteration 140, loss = 0.12317486\n",
            "Iteration 141, loss = 0.12322990\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 142, loss = 0.12302528\n",
            "Iteration 143, loss = 0.12295262\n",
            "Iteration 144, loss = 0.12293010\n",
            "Iteration 145, loss = 0.12301367\n",
            "Iteration 146, loss = 0.12294007\n",
            "Iteration 147, loss = 0.12298080\n",
            "Iteration 148, loss = 0.12298275\n",
            "Iteration 149, loss = 0.12294674\n",
            "Iteration 150, loss = 0.12300674\n",
            "Iteration 151, loss = 0.12297268\n",
            "Iteration 152, loss = 0.12293829\n",
            "Iteration 153, loss = 0.12291784\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 154, loss = 0.12290808\n",
            "Iteration 155, loss = 0.12285426\n",
            "Iteration 156, loss = 0.12285854\n",
            "Iteration 157, loss = 0.12286758\n",
            "Iteration 158, loss = 0.12286547\n",
            "Iteration 159, loss = 0.12287227\n",
            "Iteration 160, loss = 0.12287360\n",
            "Iteration 161, loss = 0.12284818\n",
            "Iteration 162, loss = 0.12284859\n",
            "Iteration 163, loss = 0.12284495\n",
            "Iteration 164, loss = 0.12288108\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 165, loss = 0.12284206\n",
            "Iteration 166, loss = 0.12283461\n",
            "Iteration 167, loss = 0.12283469\n",
            "Iteration 168, loss = 0.12284255\n",
            "Iteration 169, loss = 0.12283715\n",
            "Iteration 170, loss = 0.12283837\n",
            "Iteration 171, loss = 0.12283663\n",
            "Iteration 172, loss = 0.12283658\n",
            "Iteration 173, loss = 0.12283398\n",
            "Iteration 174, loss = 0.12283588\n",
            "Iteration 175, loss = 0.12283554\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.90927032\n",
            "Iteration 2, loss = 0.23443866\n",
            "Iteration 3, loss = 0.21979693\n",
            "Iteration 4, loss = 0.21119097\n",
            "Iteration 5, loss = 0.20342327\n",
            "Iteration 6, loss = 0.19720433\n",
            "Iteration 7, loss = 0.18932097\n",
            "Iteration 8, loss = 0.18539915\n",
            "Iteration 9, loss = 0.17956632\n",
            "Iteration 10, loss = 0.17644595\n",
            "Iteration 11, loss = 0.17197378\n",
            "Iteration 12, loss = 0.16788642\n",
            "Iteration 13, loss = 0.16505735\n",
            "Iteration 14, loss = 0.16151115\n",
            "Iteration 15, loss = 0.16003691\n",
            "Iteration 16, loss = 0.15735773\n",
            "Iteration 17, loss = 0.15435297\n",
            "Iteration 18, loss = 0.15285731\n",
            "Iteration 19, loss = 0.14973037\n",
            "Iteration 20, loss = 0.14903481\n",
            "Iteration 21, loss = 0.14693879\n",
            "Iteration 22, loss = 0.14692216\n",
            "Iteration 23, loss = 0.14355260\n",
            "Iteration 24, loss = 0.14244210\n",
            "Iteration 25, loss = 0.14285651\n",
            "Iteration 26, loss = 0.14125936\n",
            "Iteration 27, loss = 0.14042333\n",
            "Iteration 28, loss = 0.14078730\n",
            "Iteration 29, loss = 0.13712690\n",
            "Iteration 30, loss = 0.13901216\n",
            "Iteration 31, loss = 0.13719531\n",
            "Iteration 32, loss = 0.13558611\n",
            "Iteration 33, loss = 0.13578095\n",
            "Iteration 34, loss = 0.13545593\n",
            "Iteration 35, loss = 0.13657812\n",
            "Iteration 36, loss = 0.13481400\n",
            "Iteration 37, loss = 0.13306630\n",
            "Iteration 38, loss = 0.13250493\n",
            "Iteration 39, loss = 0.13276812\n",
            "Iteration 40, loss = 0.13284416\n",
            "Iteration 41, loss = 0.13233296\n",
            "Iteration 42, loss = 0.13279442\n",
            "Iteration 43, loss = 0.13199349\n",
            "Iteration 44, loss = 0.13238000\n",
            "Iteration 45, loss = 0.13172902\n",
            "Iteration 46, loss = 0.13153992\n",
            "Iteration 47, loss = 0.13059639\n",
            "Iteration 48, loss = 0.13093603\n",
            "Iteration 49, loss = 0.13118228\n",
            "Iteration 50, loss = 0.13028901\n",
            "Iteration 51, loss = 0.13027760\n",
            "Iteration 52, loss = 0.13110421\n",
            "Iteration 53, loss = 0.13014910\n",
            "Iteration 54, loss = 0.13040852\n",
            "Iteration 55, loss = 0.13003214\n",
            "Iteration 56, loss = 0.13113674\n",
            "Iteration 57, loss = 0.12973263\n",
            "Iteration 58, loss = 0.13080986\n",
            "Iteration 59, loss = 0.12990116\n",
            "Iteration 60, loss = 0.12929204\n",
            "Iteration 61, loss = 0.13027317\n",
            "Iteration 62, loss = 0.12939494\n",
            "Iteration 63, loss = 0.12873162\n",
            "Iteration 64, loss = 0.12832137\n",
            "Iteration 65, loss = 0.12948773\n",
            "Iteration 66, loss = 0.12917319\n",
            "Iteration 67, loss = 0.12874214\n",
            "Iteration 68, loss = 0.12845902\n",
            "Iteration 69, loss = 0.12781976\n",
            "Iteration 70, loss = 0.12942038\n",
            "Iteration 71, loss = 0.12894890\n",
            "Iteration 72, loss = 0.12813260\n",
            "Iteration 73, loss = 0.12845823\n",
            "Iteration 74, loss = 0.13002366\n",
            "Iteration 75, loss = 0.12692897\n",
            "Iteration 76, loss = 0.12843869\n",
            "Iteration 77, loss = 0.12896639\n",
            "Iteration 78, loss = 0.12853908\n",
            "Iteration 79, loss = 0.12932852\n",
            "Iteration 80, loss = 0.12820373\n",
            "Iteration 81, loss = 0.12884278\n",
            "Iteration 82, loss = 0.12793034\n",
            "Iteration 83, loss = 0.12813289\n",
            "Iteration 84, loss = 0.12837144\n",
            "Iteration 85, loss = 0.12887823\n",
            "Iteration 86, loss = 0.12846195\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 87, loss = 0.12485694\n",
            "Iteration 88, loss = 0.12458455\n",
            "Iteration 89, loss = 0.12501079\n",
            "Iteration 90, loss = 0.12446221\n",
            "Iteration 91, loss = 0.12461674\n",
            "Iteration 92, loss = 0.12419484\n",
            "Iteration 93, loss = 0.12434673\n",
            "Iteration 94, loss = 0.12472648\n",
            "Iteration 95, loss = 0.12414302\n",
            "Iteration 96, loss = 0.12415184\n",
            "Iteration 97, loss = 0.12454091\n",
            "Iteration 98, loss = 0.12412026\n",
            "Iteration 99, loss = 0.12413101\n",
            "Iteration 100, loss = 0.12392084\n",
            "Iteration 101, loss = 0.12441281\n",
            "Iteration 102, loss = 0.12398006\n",
            "Iteration 103, loss = 0.12419163\n",
            "Iteration 104, loss = 0.12391334\n",
            "Iteration 105, loss = 0.12420222\n",
            "Iteration 106, loss = 0.12408017\n",
            "Iteration 107, loss = 0.12380091\n",
            "Iteration 108, loss = 0.12430874\n",
            "Iteration 109, loss = 0.12427829\n",
            "Iteration 110, loss = 0.12383018\n",
            "Iteration 111, loss = 0.12373204\n",
            "Iteration 112, loss = 0.12396106\n",
            "Iteration 113, loss = 0.12422515\n",
            "Iteration 114, loss = 0.12387871\n",
            "Iteration 115, loss = 0.12386149\n",
            "Iteration 116, loss = 0.12365549\n",
            "Iteration 117, loss = 0.12365808\n",
            "Iteration 118, loss = 0.12410515\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 119, loss = 0.12288459\n",
            "Iteration 120, loss = 0.12276701\n",
            "Iteration 121, loss = 0.12261121\n",
            "Iteration 122, loss = 0.12269468\n",
            "Iteration 123, loss = 0.12264647\n",
            "Iteration 124, loss = 0.12270474\n",
            "Iteration 125, loss = 0.12275071\n",
            "Iteration 126, loss = 0.12278946\n",
            "Iteration 127, loss = 0.12279541\n",
            "Iteration 128, loss = 0.12256789\n",
            "Iteration 129, loss = 0.12255799\n",
            "Iteration 130, loss = 0.12254550\n",
            "Iteration 131, loss = 0.12264898\n",
            "Iteration 132, loss = 0.12262495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 133, loss = 0.12240874\n",
            "Iteration 134, loss = 0.12233759\n",
            "Iteration 135, loss = 0.12237031\n",
            "Iteration 136, loss = 0.12240468\n",
            "Iteration 137, loss = 0.12233897\n",
            "Iteration 138, loss = 0.12236276\n",
            "Iteration 139, loss = 0.12235761\n",
            "Iteration 140, loss = 0.12236776\n",
            "Iteration 141, loss = 0.12232048\n",
            "Iteration 142, loss = 0.12236170\n",
            "Iteration 143, loss = 0.12231387\n",
            "Iteration 144, loss = 0.12235580\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 145, loss = 0.12229435\n",
            "Iteration 146, loss = 0.12227053\n",
            "Iteration 147, loss = 0.12226267\n",
            "Iteration 148, loss = 0.12226557\n",
            "Iteration 149, loss = 0.12226663\n",
            "Iteration 150, loss = 0.12225771\n",
            "Iteration 151, loss = 0.12225273\n",
            "Iteration 152, loss = 0.12227275\n",
            "Iteration 153, loss = 0.12227100\n",
            "Iteration 154, loss = 0.12226524\n",
            "Iteration 155, loss = 0.12226527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 156, loss = 0.12224330\n",
            "Iteration 157, loss = 0.12224377\n",
            "Iteration 158, loss = 0.12224442\n",
            "Iteration 159, loss = 0.12224489\n",
            "Iteration 160, loss = 0.12224162\n",
            "Iteration 161, loss = 0.12224257\n",
            "Iteration 162, loss = 0.12224086\n",
            "Iteration 163, loss = 0.12224337\n",
            "Iteration 164, loss = 0.12224447\n",
            "Iteration 165, loss = 0.12224609\n",
            "Iteration 166, loss = 0.12224188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.19162291\n",
            "Iteration 2, loss = 0.24446849\n",
            "Iteration 3, loss = 0.23331569\n",
            "Iteration 4, loss = 0.22387980\n",
            "Iteration 5, loss = 0.21580284\n",
            "Iteration 6, loss = 0.20860751\n",
            "Iteration 7, loss = 0.20155071\n",
            "Iteration 8, loss = 0.19575025\n",
            "Iteration 9, loss = 0.19040365\n",
            "Iteration 10, loss = 0.18489110\n",
            "Iteration 11, loss = 0.18017516\n",
            "Iteration 12, loss = 0.17707026\n",
            "Iteration 13, loss = 0.17329066\n",
            "Iteration 14, loss = 0.16883600\n",
            "Iteration 15, loss = 0.16693813\n",
            "Iteration 16, loss = 0.16319240\n",
            "Iteration 17, loss = 0.16103136\n",
            "Iteration 18, loss = 0.15898787\n",
            "Iteration 19, loss = 0.15526948\n",
            "Iteration 20, loss = 0.15409551\n",
            "Iteration 21, loss = 0.15267757\n",
            "Iteration 22, loss = 0.15020176\n",
            "Iteration 23, loss = 0.14803897\n",
            "Iteration 24, loss = 0.14713578\n",
            "Iteration 25, loss = 0.14524763\n",
            "Iteration 26, loss = 0.14414532\n",
            "Iteration 27, loss = 0.14281460\n",
            "Iteration 28, loss = 0.14180768\n",
            "Iteration 29, loss = 0.14084281\n",
            "Iteration 30, loss = 0.14054676\n",
            "Iteration 31, loss = 0.13892957\n",
            "Iteration 32, loss = 0.13874654\n",
            "Iteration 33, loss = 0.13668968\n",
            "Iteration 34, loss = 0.13664817\n",
            "Iteration 35, loss = 0.13518556\n",
            "Iteration 36, loss = 0.13503463\n",
            "Iteration 37, loss = 0.13502355\n",
            "Iteration 38, loss = 0.13426631\n",
            "Iteration 39, loss = 0.13368763\n",
            "Iteration 40, loss = 0.13243969\n",
            "Iteration 41, loss = 0.13285807\n",
            "Iteration 42, loss = 0.13273861\n",
            "Iteration 43, loss = 0.13270613\n",
            "Iteration 44, loss = 0.13303142\n",
            "Iteration 45, loss = 0.13108563\n",
            "Iteration 46, loss = 0.13163025\n",
            "Iteration 47, loss = 0.13214957\n",
            "Iteration 48, loss = 0.13261132\n",
            "Iteration 49, loss = 0.13054834\n",
            "Iteration 50, loss = 0.12994899\n",
            "Iteration 51, loss = 0.13053424\n",
            "Iteration 52, loss = 0.13157581\n",
            "Iteration 53, loss = 0.13161819\n",
            "Iteration 54, loss = 0.12946156\n",
            "Iteration 55, loss = 0.13000750\n",
            "Iteration 56, loss = 0.12839844\n",
            "Iteration 57, loss = 0.13045205\n",
            "Iteration 58, loss = 0.13041046\n",
            "Iteration 59, loss = 0.13016816\n",
            "Iteration 60, loss = 0.12979873\n",
            "Iteration 61, loss = 0.12950543\n",
            "Iteration 62, loss = 0.13029658\n",
            "Iteration 63, loss = 0.13082982\n",
            "Iteration 64, loss = 0.12903942\n",
            "Iteration 65, loss = 0.12841150\n",
            "Iteration 66, loss = 0.12997070\n",
            "Iteration 67, loss = 0.12894129\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.12574770\n",
            "Iteration 69, loss = 0.12564490\n",
            "Iteration 70, loss = 0.12537991\n",
            "Iteration 71, loss = 0.12538255\n",
            "Iteration 72, loss = 0.12570723\n",
            "Iteration 73, loss = 0.12505120\n",
            "Iteration 74, loss = 0.12544320\n",
            "Iteration 75, loss = 0.12531763\n",
            "Iteration 76, loss = 0.12519077\n",
            "Iteration 77, loss = 0.12525272\n",
            "Iteration 78, loss = 0.12510567\n",
            "Iteration 79, loss = 0.12501766\n",
            "Iteration 80, loss = 0.12474597\n",
            "Iteration 81, loss = 0.12481244\n",
            "Iteration 82, loss = 0.12516319\n",
            "Iteration 83, loss = 0.12479168\n",
            "Iteration 84, loss = 0.12461985\n",
            "Iteration 85, loss = 0.12440754\n",
            "Iteration 86, loss = 0.12491500\n",
            "Iteration 87, loss = 0.12483310\n",
            "Iteration 88, loss = 0.12449619\n",
            "Iteration 89, loss = 0.12475329\n",
            "Iteration 90, loss = 0.12465516\n",
            "Iteration 91, loss = 0.12461849\n",
            "Iteration 92, loss = 0.12473382\n",
            "Iteration 93, loss = 0.12427174\n",
            "Iteration 94, loss = 0.12465626\n",
            "Iteration 95, loss = 0.12473708\n",
            "Iteration 96, loss = 0.12455079\n",
            "Iteration 97, loss = 0.12410815\n",
            "Iteration 98, loss = 0.12464628\n",
            "Iteration 99, loss = 0.12472533\n",
            "Iteration 100, loss = 0.12487686\n",
            "Iteration 101, loss = 0.12416656\n",
            "Iteration 102, loss = 0.12477749\n",
            "Iteration 103, loss = 0.12431098\n",
            "Iteration 104, loss = 0.12453167\n",
            "Iteration 105, loss = 0.12391443\n",
            "Iteration 106, loss = 0.12428022\n",
            "Iteration 107, loss = 0.12449814\n",
            "Iteration 108, loss = 0.12423750\n",
            "Iteration 109, loss = 0.12440447\n",
            "Iteration 110, loss = 0.12485970\n",
            "Iteration 111, loss = 0.12396473\n",
            "Iteration 112, loss = 0.12417017\n",
            "Iteration 113, loss = 0.12399679\n",
            "Iteration 114, loss = 0.12401467\n",
            "Iteration 115, loss = 0.12420388\n",
            "Iteration 116, loss = 0.12410005\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 117, loss = 0.12312575\n",
            "Iteration 118, loss = 0.12297897\n",
            "Iteration 119, loss = 0.12315365\n",
            "Iteration 120, loss = 0.12304756\n",
            "Iteration 121, loss = 0.12286604\n",
            "Iteration 122, loss = 0.12290462\n",
            "Iteration 123, loss = 0.12291764\n",
            "Iteration 124, loss = 0.12300156\n",
            "Iteration 125, loss = 0.12328911\n",
            "Iteration 126, loss = 0.12298866\n",
            "Iteration 127, loss = 0.12282783\n",
            "Iteration 128, loss = 0.12290560\n",
            "Iteration 129, loss = 0.12317479\n",
            "Iteration 130, loss = 0.12297203\n",
            "Iteration 131, loss = 0.12287808\n",
            "Iteration 132, loss = 0.12311144\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 133, loss = 0.12265731\n",
            "Iteration 134, loss = 0.12264356\n",
            "Iteration 135, loss = 0.12261723\n",
            "Iteration 136, loss = 0.12263692\n",
            "Iteration 137, loss = 0.12267250\n",
            "Iteration 138, loss = 0.12263077\n",
            "Iteration 139, loss = 0.12262571\n",
            "Iteration 140, loss = 0.12265223\n",
            "Iteration 141, loss = 0.12266689\n",
            "Iteration 142, loss = 0.12263223\n",
            "Iteration 143, loss = 0.12258084\n",
            "Iteration 144, loss = 0.12269123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 145, loss = 0.12264954\n",
            "Iteration 146, loss = 0.12256804\n",
            "Iteration 147, loss = 0.12257899\n",
            "Iteration 148, loss = 0.12257783\n",
            "Iteration 149, loss = 0.12256914\n",
            "Iteration 150, loss = 0.12257455\n",
            "Iteration 151, loss = 0.12258692\n",
            "Iteration 152, loss = 0.12258909\n",
            "Iteration 153, loss = 0.12257806\n",
            "Iteration 154, loss = 0.12256669\n",
            "Iteration 155, loss = 0.12257415\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 156, loss = 0.12255542\n",
            "Iteration 157, loss = 0.12255137\n",
            "Iteration 158, loss = 0.12255290\n",
            "Iteration 159, loss = 0.12254975\n",
            "Iteration 160, loss = 0.12255100\n",
            "Iteration 161, loss = 0.12255108\n",
            "Iteration 162, loss = 0.12255013\n",
            "Iteration 163, loss = 0.12254985\n",
            "Iteration 164, loss = 0.12255044\n",
            "Iteration 165, loss = 0.12255024\n",
            "Iteration 166, loss = 0.12255353\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 2.26574994\n",
            "Iteration 2, loss = 0.28699158\n",
            "Iteration 3, loss = 0.27226151\n",
            "Iteration 4, loss = 0.25927730\n",
            "Iteration 5, loss = 0.24854341\n",
            "Iteration 6, loss = 0.23845479\n",
            "Iteration 7, loss = 0.22876912\n",
            "Iteration 8, loss = 0.21999985\n",
            "Iteration 9, loss = 0.21399946\n",
            "Iteration 10, loss = 0.20676815\n",
            "Iteration 11, loss = 0.20029121\n",
            "Iteration 12, loss = 0.19516193\n",
            "Iteration 13, loss = 0.18964251\n",
            "Iteration 14, loss = 0.18489777\n",
            "Iteration 15, loss = 0.18045787\n",
            "Iteration 16, loss = 0.17616645\n",
            "Iteration 17, loss = 0.17211571\n",
            "Iteration 18, loss = 0.16843723\n",
            "Iteration 19, loss = 0.16555340\n",
            "Iteration 20, loss = 0.16224557\n",
            "Iteration 21, loss = 0.15990720\n",
            "Iteration 22, loss = 0.15798962\n",
            "Iteration 23, loss = 0.15516439\n",
            "Iteration 24, loss = 0.15331806\n",
            "Iteration 25, loss = 0.15120139\n",
            "Iteration 26, loss = 0.14935972\n",
            "Iteration 27, loss = 0.14823966\n",
            "Iteration 28, loss = 0.14555057\n",
            "Iteration 29, loss = 0.14488436\n",
            "Iteration 30, loss = 0.14321063\n",
            "Iteration 31, loss = 0.14177169\n",
            "Iteration 32, loss = 0.14122538\n",
            "Iteration 33, loss = 0.13980762\n",
            "Iteration 34, loss = 0.13922415\n",
            "Iteration 35, loss = 0.13774756\n",
            "Iteration 36, loss = 0.13764466\n",
            "Iteration 37, loss = 0.13533149\n",
            "Iteration 38, loss = 0.13608534\n",
            "Iteration 39, loss = 0.13504761\n",
            "Iteration 40, loss = 0.13521892\n",
            "Iteration 41, loss = 0.13482849\n",
            "Iteration 42, loss = 0.13348837\n",
            "Iteration 43, loss = 0.13172148\n",
            "Iteration 44, loss = 0.13211554\n",
            "Iteration 45, loss = 0.13151628\n",
            "Iteration 46, loss = 0.13135083\n",
            "Iteration 47, loss = 0.13196889\n",
            "Iteration 48, loss = 0.13115358\n",
            "Iteration 49, loss = 0.13009008\n",
            "Iteration 50, loss = 0.13097963\n",
            "Iteration 51, loss = 0.12965175\n",
            "Iteration 52, loss = 0.12955883\n",
            "Iteration 53, loss = 0.13035698\n",
            "Iteration 54, loss = 0.12989764\n",
            "Iteration 55, loss = 0.12934151\n",
            "Iteration 56, loss = 0.12906243\n",
            "Iteration 57, loss = 0.12901573\n",
            "Iteration 58, loss = 0.12902621\n",
            "Iteration 59, loss = 0.12925638\n",
            "Iteration 60, loss = 0.12737898\n",
            "Iteration 61, loss = 0.12858682\n",
            "Iteration 62, loss = 0.12892114\n",
            "Iteration 63, loss = 0.12804903\n",
            "Iteration 64, loss = 0.12814877\n",
            "Iteration 65, loss = 0.12921822\n",
            "Iteration 66, loss = 0.12873437\n",
            "Iteration 67, loss = 0.12803730\n",
            "Iteration 68, loss = 0.12895444\n",
            "Iteration 69, loss = 0.12766113\n",
            "Iteration 70, loss = 0.12766379\n",
            "Iteration 71, loss = 0.12803508\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 72, loss = 0.12532879\n",
            "Iteration 73, loss = 0.12454525\n",
            "Iteration 74, loss = 0.12431291\n",
            "Iteration 75, loss = 0.12434030\n",
            "Iteration 76, loss = 0.12417257\n",
            "Iteration 77, loss = 0.12411078\n",
            "Iteration 78, loss = 0.12384122\n",
            "Iteration 79, loss = 0.12409858\n",
            "Iteration 80, loss = 0.12397498\n",
            "Iteration 81, loss = 0.12404480\n",
            "Iteration 82, loss = 0.12385948\n",
            "Iteration 83, loss = 0.12408060\n",
            "Iteration 84, loss = 0.12401432\n",
            "Iteration 85, loss = 0.12412499\n",
            "Iteration 86, loss = 0.12392771\n",
            "Iteration 87, loss = 0.12400798\n",
            "Iteration 88, loss = 0.12389379\n",
            "Iteration 89, loss = 0.12390613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.12300843\n",
            "Iteration 91, loss = 0.12276340\n",
            "Iteration 92, loss = 0.12278904\n",
            "Iteration 93, loss = 0.12283321\n",
            "Iteration 94, loss = 0.12273811\n",
            "Iteration 95, loss = 0.12277481\n",
            "Iteration 96, loss = 0.12283449\n",
            "Iteration 97, loss = 0.12276020\n",
            "Iteration 98, loss = 0.12277528\n",
            "Iteration 99, loss = 0.12282550\n",
            "Iteration 100, loss = 0.12276461\n",
            "Iteration 101, loss = 0.12265456\n",
            "Iteration 102, loss = 0.12275690\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 103, loss = 0.12272878\n",
            "Iteration 104, loss = 0.12253184\n",
            "Iteration 105, loss = 0.12251477\n",
            "Iteration 106, loss = 0.12250844\n",
            "Iteration 107, loss = 0.12250821\n",
            "Iteration 108, loss = 0.12250444\n",
            "Iteration 109, loss = 0.12248742\n",
            "Iteration 110, loss = 0.12245696\n",
            "Iteration 111, loss = 0.12250829\n",
            "Iteration 112, loss = 0.12253033\n",
            "Iteration 113, loss = 0.12250555\n",
            "Iteration 114, loss = 0.12245529\n",
            "Iteration 115, loss = 0.12249931\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 116, loss = 0.12243246\n",
            "Iteration 117, loss = 0.12243075\n",
            "Iteration 118, loss = 0.12242944\n",
            "Iteration 119, loss = 0.12243093\n",
            "Iteration 120, loss = 0.12242763\n",
            "Iteration 121, loss = 0.12242683\n",
            "Iteration 122, loss = 0.12242683\n",
            "Iteration 123, loss = 0.12242363\n",
            "Iteration 124, loss = 0.12242760\n",
            "Iteration 125, loss = 0.12243061\n",
            "Iteration 126, loss = 0.12243462\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 127, loss = 0.12241066\n",
            "Iteration 128, loss = 0.12241129\n",
            "Iteration 129, loss = 0.12241451\n",
            "Iteration 130, loss = 0.12240893\n",
            "Iteration 131, loss = 0.12240930\n",
            "Iteration 132, loss = 0.12241009\n",
            "Iteration 133, loss = 0.12240952\n",
            "Iteration 134, loss = 0.12241206\n",
            "Iteration 135, loss = 0.12241205\n",
            "Iteration 136, loss = 0.12240988\n",
            "Iteration 137, loss = 0.12241078\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.11891896\n",
            "Iteration 2, loss = 0.24137026\n",
            "Iteration 3, loss = 0.23011321\n",
            "Iteration 4, loss = 0.22173031\n",
            "Iteration 5, loss = 0.21429145\n",
            "Iteration 6, loss = 0.20609613\n",
            "Iteration 7, loss = 0.20047572\n",
            "Iteration 8, loss = 0.19516696\n",
            "Iteration 9, loss = 0.18900263\n",
            "Iteration 10, loss = 0.18433229\n",
            "Iteration 11, loss = 0.17994502\n",
            "Iteration 12, loss = 0.17553791\n",
            "Iteration 13, loss = 0.17139443\n",
            "Iteration 14, loss = 0.16797261\n",
            "Iteration 15, loss = 0.16526709\n",
            "Iteration 16, loss = 0.16252184\n",
            "Iteration 17, loss = 0.15946612\n",
            "Iteration 18, loss = 0.15683114\n",
            "Iteration 19, loss = 0.15596712\n",
            "Iteration 20, loss = 0.15260070\n",
            "Iteration 21, loss = 0.15074568\n",
            "Iteration 22, loss = 0.14948402\n",
            "Iteration 23, loss = 0.14724892\n",
            "Iteration 24, loss = 0.14574105\n",
            "Iteration 25, loss = 0.14332770\n",
            "Iteration 26, loss = 0.14386551\n",
            "Iteration 27, loss = 0.14163654\n",
            "Iteration 28, loss = 0.14071010\n",
            "Iteration 29, loss = 0.14051783\n",
            "Iteration 30, loss = 0.14053358\n",
            "Iteration 31, loss = 0.13927803\n",
            "Iteration 32, loss = 0.13678860\n",
            "Iteration 33, loss = 0.13692248\n",
            "Iteration 34, loss = 0.13577244\n",
            "Iteration 35, loss = 0.13502148\n",
            "Iteration 36, loss = 0.13444286\n",
            "Iteration 37, loss = 0.13502665\n",
            "Iteration 38, loss = 0.13379409\n",
            "Iteration 39, loss = 0.13345292\n",
            "Iteration 40, loss = 0.13307886\n",
            "Iteration 41, loss = 0.13238625\n",
            "Iteration 42, loss = 0.13165037\n",
            "Iteration 43, loss = 0.13111836\n",
            "Iteration 44, loss = 0.13102967\n",
            "Iteration 45, loss = 0.13142669\n",
            "Iteration 46, loss = 0.13131483\n",
            "Iteration 47, loss = 0.13128246\n",
            "Iteration 48, loss = 0.13146092\n",
            "Iteration 49, loss = 0.13001547\n",
            "Iteration 50, loss = 0.13114244\n",
            "Iteration 51, loss = 0.12950525\n",
            "Iteration 52, loss = 0.12907175\n",
            "Iteration 53, loss = 0.13023609\n",
            "Iteration 54, loss = 0.12960662\n",
            "Iteration 55, loss = 0.12948859\n",
            "Iteration 56, loss = 0.12818078\n",
            "Iteration 57, loss = 0.12951487\n",
            "Iteration 58, loss = 0.12901502\n",
            "Iteration 59, loss = 0.12889558\n",
            "Iteration 60, loss = 0.12813745\n",
            "Iteration 61, loss = 0.12870980\n",
            "Iteration 62, loss = 0.12924138\n",
            "Iteration 63, loss = 0.12775508\n",
            "Iteration 64, loss = 0.12880930\n",
            "Iteration 65, loss = 0.12895660\n",
            "Iteration 66, loss = 0.12803171\n",
            "Iteration 67, loss = 0.12732017\n",
            "Iteration 68, loss = 0.12812787\n",
            "Iteration 69, loss = 0.12809739\n",
            "Iteration 70, loss = 0.12768956\n",
            "Iteration 71, loss = 0.13005237\n",
            "Iteration 72, loss = 0.12842368\n",
            "Iteration 73, loss = 0.12743605\n",
            "Iteration 74, loss = 0.12812623\n",
            "Iteration 75, loss = 0.12846240\n",
            "Iteration 76, loss = 0.12758267\n",
            "Iteration 77, loss = 0.12836188\n",
            "Iteration 78, loss = 0.12769066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 79, loss = 0.12463223\n",
            "Iteration 80, loss = 0.12399956\n",
            "Iteration 81, loss = 0.12407098\n",
            "Iteration 82, loss = 0.12417477\n",
            "Iteration 83, loss = 0.12400627\n",
            "Iteration 84, loss = 0.12431989\n",
            "Iteration 85, loss = 0.12395846\n",
            "Iteration 86, loss = 0.12383576\n",
            "Iteration 87, loss = 0.12390962\n",
            "Iteration 88, loss = 0.12369305\n",
            "Iteration 89, loss = 0.12378858\n",
            "Iteration 90, loss = 0.12399935\n",
            "Iteration 91, loss = 0.12390555\n",
            "Iteration 92, loss = 0.12357455\n",
            "Iteration 93, loss = 0.12376054\n",
            "Iteration 94, loss = 0.12357854\n",
            "Iteration 95, loss = 0.12335452\n",
            "Iteration 96, loss = 0.12387769\n",
            "Iteration 97, loss = 0.12340081\n",
            "Iteration 98, loss = 0.12361747\n",
            "Iteration 99, loss = 0.12354532\n",
            "Iteration 100, loss = 0.12363432\n",
            "Iteration 101, loss = 0.12322415\n",
            "Iteration 102, loss = 0.12321866\n",
            "Iteration 103, loss = 0.12336091\n",
            "Iteration 104, loss = 0.12309145\n",
            "Iteration 105, loss = 0.12292213\n",
            "Iteration 106, loss = 0.12350176\n",
            "Iteration 107, loss = 0.12351497\n",
            "Iteration 108, loss = 0.12348354\n",
            "Iteration 109, loss = 0.12305803\n",
            "Iteration 110, loss = 0.12294428\n",
            "Iteration 111, loss = 0.12312986\n",
            "Iteration 112, loss = 0.12303004\n",
            "Iteration 113, loss = 0.12300629\n",
            "Iteration 114, loss = 0.12279583\n",
            "Iteration 115, loss = 0.12276048\n",
            "Iteration 116, loss = 0.12358872\n",
            "Iteration 117, loss = 0.12336606\n",
            "Iteration 118, loss = 0.12305704\n",
            "Iteration 119, loss = 0.12345623\n",
            "Iteration 120, loss = 0.12333417\n",
            "Iteration 121, loss = 0.12313127\n",
            "Iteration 122, loss = 0.12303942\n",
            "Iteration 123, loss = 0.12289154\n",
            "Iteration 124, loss = 0.12283010\n",
            "Iteration 125, loss = 0.12331601\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 126, loss = 0.12279663\n",
            "Iteration 127, loss = 0.12190384\n",
            "Iteration 128, loss = 0.12193217\n",
            "Iteration 129, loss = 0.12213859\n",
            "Iteration 130, loss = 0.12209780\n",
            "Iteration 131, loss = 0.12189103\n",
            "Iteration 132, loss = 0.12184513\n",
            "Iteration 133, loss = 0.12204442\n",
            "Iteration 134, loss = 0.12196701\n",
            "Iteration 135, loss = 0.12190214\n",
            "Iteration 136, loss = 0.12179999\n",
            "Iteration 137, loss = 0.12192375\n",
            "Iteration 138, loss = 0.12188170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 139, loss = 0.12168366\n",
            "Iteration 140, loss = 0.12163525\n",
            "Iteration 141, loss = 0.12164165\n",
            "Iteration 142, loss = 0.12156562\n",
            "Iteration 143, loss = 0.12157377\n",
            "Iteration 144, loss = 0.12165148\n",
            "Iteration 145, loss = 0.12160773\n",
            "Iteration 146, loss = 0.12155946\n",
            "Iteration 147, loss = 0.12161116\n",
            "Iteration 148, loss = 0.12164700\n",
            "Iteration 149, loss = 0.12165668\n",
            "Iteration 150, loss = 0.12158888\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 151, loss = 0.12151485\n",
            "Iteration 152, loss = 0.12152221\n",
            "Iteration 153, loss = 0.12150900\n",
            "Iteration 154, loss = 0.12151793\n",
            "Iteration 155, loss = 0.12151497\n",
            "Iteration 156, loss = 0.12150363\n",
            "Iteration 157, loss = 0.12151286\n",
            "Iteration 158, loss = 0.12151298\n",
            "Iteration 159, loss = 0.12149821\n",
            "Iteration 160, loss = 0.12150964\n",
            "Iteration 161, loss = 0.12150819\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 162, loss = 0.12149271\n",
            "Iteration 163, loss = 0.12149300\n",
            "Iteration 164, loss = 0.12149009\n",
            "Iteration 165, loss = 0.12149138\n",
            "Iteration 166, loss = 0.12149052\n",
            "Iteration 167, loss = 0.12149357\n",
            "Iteration 168, loss = 0.12149413\n",
            "Iteration 169, loss = 0.12150049\n",
            "Iteration 170, loss = 0.12149382\n",
            "Iteration 171, loss = 0.12149069\n",
            "Iteration 172, loss = 0.12149082\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 2.39309850\n",
            "Iteration 2, loss = 0.30420851\n",
            "Iteration 3, loss = 0.28716087\n",
            "Iteration 4, loss = 0.27377904\n",
            "Iteration 5, loss = 0.26157707\n",
            "Iteration 6, loss = 0.25114813\n",
            "Iteration 7, loss = 0.24129038\n",
            "Iteration 8, loss = 0.23233644\n",
            "Iteration 9, loss = 0.22432661\n",
            "Iteration 10, loss = 0.21657195\n",
            "Iteration 11, loss = 0.20948286\n",
            "Iteration 12, loss = 0.20296980\n",
            "Iteration 13, loss = 0.19769478\n",
            "Iteration 14, loss = 0.19233926\n",
            "Iteration 15, loss = 0.18707350\n",
            "Iteration 16, loss = 0.18266417\n",
            "Iteration 17, loss = 0.17656643\n",
            "Iteration 18, loss = 0.17392904\n",
            "Iteration 19, loss = 0.17055340\n",
            "Iteration 20, loss = 0.16758175\n",
            "Iteration 21, loss = 0.16442223\n",
            "Iteration 22, loss = 0.16114480\n",
            "Iteration 23, loss = 0.15927164\n",
            "Iteration 24, loss = 0.15611828\n",
            "Iteration 25, loss = 0.15392113\n",
            "Iteration 26, loss = 0.15185311\n",
            "Iteration 27, loss = 0.15046671\n",
            "Iteration 28, loss = 0.14936804\n",
            "Iteration 29, loss = 0.14700503\n",
            "Iteration 30, loss = 0.14572155\n",
            "Iteration 31, loss = 0.14429821\n",
            "Iteration 32, loss = 0.14277016\n",
            "Iteration 33, loss = 0.14252623\n",
            "Iteration 34, loss = 0.13935423\n",
            "Iteration 35, loss = 0.13941013\n",
            "Iteration 36, loss = 0.13838233\n",
            "Iteration 37, loss = 0.13775226\n",
            "Iteration 38, loss = 0.13739860\n",
            "Iteration 39, loss = 0.13611754\n",
            "Iteration 40, loss = 0.13465184\n",
            "Iteration 41, loss = 0.13561478\n",
            "Iteration 42, loss = 0.13470766\n",
            "Iteration 43, loss = 0.13299985\n",
            "Iteration 44, loss = 0.13234917\n",
            "Iteration 45, loss = 0.13277159\n",
            "Iteration 46, loss = 0.13110510\n",
            "Iteration 47, loss = 0.13220434\n",
            "Iteration 48, loss = 0.13123921\n",
            "Iteration 49, loss = 0.13112892\n",
            "Iteration 50, loss = 0.13147325\n",
            "Iteration 51, loss = 0.13102317\n",
            "Iteration 52, loss = 0.13045329\n",
            "Iteration 53, loss = 0.13041347\n",
            "Iteration 54, loss = 0.12959431\n",
            "Iteration 55, loss = 0.12928979\n",
            "Iteration 56, loss = 0.13044127\n",
            "Iteration 57, loss = 0.12967643\n",
            "Iteration 58, loss = 0.13031967\n",
            "Iteration 59, loss = 0.12821192\n",
            "Iteration 60, loss = 0.12955592\n",
            "Iteration 61, loss = 0.12759531\n",
            "Iteration 62, loss = 0.12924686\n",
            "Iteration 63, loss = 0.12895354\n",
            "Iteration 64, loss = 0.12950689\n",
            "Iteration 65, loss = 0.12846327\n",
            "Iteration 66, loss = 0.12856378\n",
            "Iteration 67, loss = 0.12836802\n",
            "Iteration 68, loss = 0.12846330\n",
            "Iteration 69, loss = 0.13052863\n",
            "Iteration 70, loss = 0.12703497\n",
            "Iteration 71, loss = 0.12721787\n",
            "Iteration 72, loss = 0.12808202\n",
            "Iteration 73, loss = 0.12735371\n",
            "Iteration 74, loss = 0.12739352\n",
            "Iteration 75, loss = 0.12825628\n",
            "Iteration 76, loss = 0.12816142\n",
            "Iteration 77, loss = 0.12813958\n",
            "Iteration 78, loss = 0.12773789\n",
            "Iteration 79, loss = 0.12826764\n",
            "Iteration 80, loss = 0.12809658\n",
            "Iteration 81, loss = 0.12729570\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 82, loss = 0.12461937\n",
            "Iteration 83, loss = 0.12405330\n",
            "Iteration 84, loss = 0.12387510\n",
            "Iteration 85, loss = 0.12370542\n",
            "Iteration 86, loss = 0.12381365\n",
            "Iteration 87, loss = 0.12399846\n",
            "Iteration 88, loss = 0.12396413\n",
            "Iteration 89, loss = 0.12389986\n",
            "Iteration 90, loss = 0.12391859\n",
            "Iteration 91, loss = 0.12347070\n",
            "Iteration 92, loss = 0.12404927\n",
            "Iteration 93, loss = 0.12366651\n",
            "Iteration 94, loss = 0.12360186\n",
            "Iteration 95, loss = 0.12368929\n",
            "Iteration 96, loss = 0.12377978\n",
            "Iteration 97, loss = 0.12378491\n",
            "Iteration 98, loss = 0.12338725\n",
            "Iteration 99, loss = 0.12367399\n",
            "Iteration 100, loss = 0.12335247\n",
            "Iteration 101, loss = 0.12346978\n",
            "Iteration 102, loss = 0.12351888\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 103, loss = 0.12260963\n",
            "Iteration 104, loss = 0.12252857\n",
            "Iteration 105, loss = 0.12263387\n",
            "Iteration 106, loss = 0.12269329\n",
            "Iteration 107, loss = 0.12252963\n",
            "Iteration 108, loss = 0.12263315\n",
            "Iteration 109, loss = 0.12248470\n",
            "Iteration 110, loss = 0.12261989\n",
            "Iteration 111, loss = 0.12255263\n",
            "Iteration 112, loss = 0.12256567\n",
            "Iteration 113, loss = 0.12260308\n",
            "Iteration 114, loss = 0.12258496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 115, loss = 0.12236748\n",
            "Iteration 116, loss = 0.12223759\n",
            "Iteration 117, loss = 0.12223764\n",
            "Iteration 118, loss = 0.12228823\n",
            "Iteration 119, loss = 0.12227496\n",
            "Iteration 120, loss = 0.12227017\n",
            "Iteration 121, loss = 0.12224066\n",
            "Iteration 122, loss = 0.12226586\n",
            "Iteration 123, loss = 0.12226016\n",
            "Iteration 124, loss = 0.12219768\n",
            "Iteration 125, loss = 0.12222943\n",
            "Iteration 126, loss = 0.12230462\n",
            "Iteration 127, loss = 0.12223072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 128, loss = 0.12218146\n",
            "Iteration 129, loss = 0.12216984\n",
            "Iteration 130, loss = 0.12217020\n",
            "Iteration 131, loss = 0.12217579\n",
            "Iteration 132, loss = 0.12216542\n",
            "Iteration 133, loss = 0.12216676\n",
            "Iteration 134, loss = 0.12217379\n",
            "Iteration 135, loss = 0.12216931\n",
            "Iteration 136, loss = 0.12216898\n",
            "Iteration 137, loss = 0.12216455\n",
            "Iteration 138, loss = 0.12218587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 139, loss = 0.12214779\n",
            "Iteration 140, loss = 0.12215128\n",
            "Iteration 141, loss = 0.12215299\n",
            "Iteration 142, loss = 0.12215146\n",
            "Iteration 143, loss = 0.12214972\n",
            "Iteration 144, loss = 0.12215109\n",
            "Iteration 145, loss = 0.12214995\n",
            "Iteration 146, loss = 0.12215257\n",
            "Iteration 147, loss = 0.12214869\n",
            "Iteration 148, loss = 0.12214933\n",
            "Iteration 149, loss = 0.12215234\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.73164251\n",
            "Iteration 2, loss = 0.24709170\n",
            "Iteration 3, loss = 0.23397283\n",
            "Iteration 4, loss = 0.22482159\n",
            "Iteration 5, loss = 0.21772598\n",
            "Iteration 6, loss = 0.20991880\n",
            "Iteration 7, loss = 0.20337410\n",
            "Iteration 8, loss = 0.19780928\n",
            "Iteration 9, loss = 0.19136553\n",
            "Iteration 10, loss = 0.18693836\n",
            "Iteration 11, loss = 0.18173303\n",
            "Iteration 12, loss = 0.17731347\n",
            "Iteration 13, loss = 0.17381273\n",
            "Iteration 14, loss = 0.17046479\n",
            "Iteration 15, loss = 0.16701821\n",
            "Iteration 16, loss = 0.16303458\n",
            "Iteration 17, loss = 0.16133052\n",
            "Iteration 18, loss = 0.15787186\n",
            "Iteration 19, loss = 0.15592191\n",
            "Iteration 20, loss = 0.15323125\n",
            "Iteration 21, loss = 0.15207435\n",
            "Iteration 22, loss = 0.14903505\n",
            "Iteration 23, loss = 0.14802521\n",
            "Iteration 24, loss = 0.14712578\n",
            "Iteration 25, loss = 0.14577537\n",
            "Iteration 26, loss = 0.14475860\n",
            "Iteration 27, loss = 0.14273766\n",
            "Iteration 28, loss = 0.14136223\n",
            "Iteration 29, loss = 0.14108677\n",
            "Iteration 30, loss = 0.13897943\n",
            "Iteration 31, loss = 0.13851590\n",
            "Iteration 32, loss = 0.13799307\n",
            "Iteration 33, loss = 0.13897439\n",
            "Iteration 34, loss = 0.13592080\n",
            "Iteration 35, loss = 0.13667590\n",
            "Iteration 36, loss = 0.13585708\n",
            "Iteration 37, loss = 0.13534127\n",
            "Iteration 38, loss = 0.13487220\n",
            "Iteration 39, loss = 0.13520299\n",
            "Iteration 40, loss = 0.13429047\n",
            "Iteration 41, loss = 0.13339446\n",
            "Iteration 42, loss = 0.13267005\n",
            "Iteration 43, loss = 0.13240664\n",
            "Iteration 44, loss = 0.13302140\n",
            "Iteration 45, loss = 0.13237979\n",
            "Iteration 46, loss = 0.13157942\n",
            "Iteration 47, loss = 0.13215549\n",
            "Iteration 48, loss = 0.13006857\n",
            "Iteration 49, loss = 0.13064559\n",
            "Iteration 50, loss = 0.13058132\n",
            "Iteration 51, loss = 0.13044873\n",
            "Iteration 52, loss = 0.12893289\n",
            "Iteration 53, loss = 0.12959579\n",
            "Iteration 54, loss = 0.12974188\n",
            "Iteration 55, loss = 0.13005873\n",
            "Iteration 56, loss = 0.12911371\n",
            "Iteration 57, loss = 0.13021442\n",
            "Iteration 58, loss = 0.12997034\n",
            "Iteration 59, loss = 0.12965105\n",
            "Iteration 60, loss = 0.12882685\n",
            "Iteration 61, loss = 0.12856855\n",
            "Iteration 62, loss = 0.12943144\n",
            "Iteration 63, loss = 0.12875983\n",
            "Iteration 64, loss = 0.12885666\n",
            "Iteration 65, loss = 0.12867159\n",
            "Iteration 66, loss = 0.12758005\n",
            "Iteration 67, loss = 0.12905803\n",
            "Iteration 68, loss = 0.12947683\n",
            "Iteration 69, loss = 0.12783916\n",
            "Iteration 70, loss = 0.12893673\n",
            "Iteration 71, loss = 0.12822240\n",
            "Iteration 72, loss = 0.13010778\n",
            "Iteration 73, loss = 0.12811734\n",
            "Iteration 74, loss = 0.12893300\n",
            "Iteration 75, loss = 0.12910443\n",
            "Iteration 76, loss = 0.12842853\n",
            "Iteration 77, loss = 0.12812331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.12479057\n",
            "Iteration 79, loss = 0.12486548\n",
            "Iteration 80, loss = 0.12460451\n",
            "Iteration 81, loss = 0.12446333\n",
            "Iteration 82, loss = 0.12443031\n",
            "Iteration 83, loss = 0.12429205\n",
            "Iteration 84, loss = 0.12444063\n",
            "Iteration 85, loss = 0.12457141\n",
            "Iteration 86, loss = 0.12388383\n",
            "Iteration 87, loss = 0.12434011\n",
            "Iteration 88, loss = 0.12405306\n",
            "Iteration 89, loss = 0.12378860\n",
            "Iteration 90, loss = 0.12432579\n",
            "Iteration 91, loss = 0.12439064\n",
            "Iteration 92, loss = 0.12415253\n",
            "Iteration 93, loss = 0.12383229\n",
            "Iteration 94, loss = 0.12384159\n",
            "Iteration 95, loss = 0.12371366\n",
            "Iteration 96, loss = 0.12370561\n",
            "Iteration 97, loss = 0.12413220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 98, loss = 0.12334666\n",
            "Iteration 99, loss = 0.12300460\n",
            "Iteration 100, loss = 0.12300936\n",
            "Iteration 101, loss = 0.12305887\n",
            "Iteration 102, loss = 0.12289269\n",
            "Iteration 103, loss = 0.12289061\n",
            "Iteration 104, loss = 0.12282673\n",
            "Iteration 105, loss = 0.12281807\n",
            "Iteration 106, loss = 0.12284178\n",
            "Iteration 107, loss = 0.12275957\n",
            "Iteration 108, loss = 0.12293327\n",
            "Iteration 109, loss = 0.12281651\n",
            "Iteration 110, loss = 0.12274222\n",
            "Iteration 111, loss = 0.12291950\n",
            "Iteration 112, loss = 0.12294593\n",
            "Iteration 113, loss = 0.12299706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 114, loss = 0.12260961\n",
            "Iteration 115, loss = 0.12256303\n",
            "Iteration 116, loss = 0.12256860\n",
            "Iteration 117, loss = 0.12256623\n",
            "Iteration 118, loss = 0.12252817\n",
            "Iteration 119, loss = 0.12252989\n",
            "Iteration 120, loss = 0.12250713\n",
            "Iteration 121, loss = 0.12253401\n",
            "Iteration 122, loss = 0.12252424\n",
            "Iteration 123, loss = 0.12252663\n",
            "Iteration 124, loss = 0.12253898\n",
            "Iteration 125, loss = 0.12252638\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 126, loss = 0.12245896\n",
            "Iteration 127, loss = 0.12244934\n",
            "Iteration 128, loss = 0.12245854\n",
            "Iteration 129, loss = 0.12244558\n",
            "Iteration 130, loss = 0.12246344\n",
            "Iteration 131, loss = 0.12244704\n",
            "Iteration 132, loss = 0.12245279\n",
            "Iteration 133, loss = 0.12244014\n",
            "Iteration 134, loss = 0.12244201\n",
            "Iteration 135, loss = 0.12244849\n",
            "Iteration 136, loss = 0.12246170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 137, loss = 0.12243289\n",
            "Iteration 138, loss = 0.12242966\n",
            "Iteration 139, loss = 0.12243218\n",
            "Iteration 140, loss = 0.12242942\n",
            "Iteration 141, loss = 0.12242849\n",
            "Iteration 142, loss = 0.12242918\n",
            "Iteration 143, loss = 0.12243084\n",
            "Iteration 144, loss = 0.12243032\n",
            "Iteration 145, loss = 0.12243246\n",
            "Iteration 146, loss = 0.12242922\n",
            "Iteration 147, loss = 0.12242757\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33835    12]\n",
            " [ 2392  1382]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97     33847\n",
            "           1       0.99      0.37      0.53      3774\n",
            "\n",
            "    accuracy                           0.94     37621\n",
            "   macro avg       0.96      0.68      0.75     37621\n",
            "weighted avg       0.94      0.94      0.92     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 96.211% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 96.297% (0.000%)\n",
            "Iteration 1, loss = 0.61479625\n",
            "Iteration 2, loss = 0.38594037\n",
            "Iteration 3, loss = 0.36396065\n",
            "Iteration 4, loss = 0.34963035\n",
            "Iteration 5, loss = 0.33835762\n",
            "Iteration 6, loss = 0.33003923\n",
            "Iteration 7, loss = 0.32488576\n",
            "Iteration 8, loss = 0.32101436\n",
            "Iteration 9, loss = 0.31732601\n",
            "Iteration 10, loss = 0.31688576\n",
            "Iteration 11, loss = 0.31382611\n",
            "Iteration 12, loss = 0.31379622\n",
            "Iteration 13, loss = 0.31383352\n",
            "Iteration 14, loss = 0.31213194\n",
            "Iteration 15, loss = 0.31088868\n",
            "Iteration 16, loss = 0.31129257\n",
            "Iteration 17, loss = 0.31063424\n",
            "Iteration 18, loss = 0.31018592\n",
            "Iteration 19, loss = 0.30964091\n",
            "Iteration 20, loss = 0.31055294\n",
            "Iteration 21, loss = 0.31038008\n",
            "Iteration 22, loss = 0.30963020\n",
            "Iteration 23, loss = 0.30895897\n",
            "Iteration 24, loss = 0.31015593\n",
            "Iteration 25, loss = 0.30957366\n",
            "Iteration 26, loss = 0.30898760\n",
            "Iteration 27, loss = 0.30972446\n",
            "Iteration 28, loss = 0.30892455\n",
            "Iteration 29, loss = 0.30927050\n",
            "Iteration 30, loss = 0.30965731\n",
            "Iteration 31, loss = 0.31036069\n",
            "Iteration 32, loss = 0.30921811\n",
            "Iteration 33, loss = 0.30955545\n",
            "Iteration 34, loss = 0.30899887\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.30076106\n",
            "Iteration 36, loss = 0.29823974\n",
            "Iteration 37, loss = 0.29667956\n",
            "Iteration 38, loss = 0.29446803\n",
            "Iteration 39, loss = 0.29276799\n",
            "Iteration 40, loss = 0.29131764\n",
            "Iteration 41, loss = 0.28962464\n",
            "Iteration 42, loss = 0.28853804\n",
            "Iteration 43, loss = 0.28726669\n",
            "Iteration 44, loss = 0.28680957\n",
            "Iteration 45, loss = 0.28586053\n",
            "Iteration 46, loss = 0.28576356\n",
            "Iteration 47, loss = 0.28557746\n",
            "Iteration 48, loss = 0.28592110\n",
            "Iteration 49, loss = 0.28699962\n",
            "Iteration 50, loss = 0.28828022\n",
            "Iteration 51, loss = 0.28631073\n",
            "Iteration 52, loss = 0.29038996\n",
            "Iteration 53, loss = 0.29246808\n",
            "Iteration 54, loss = 0.28628729\n",
            "Iteration 55, loss = 0.29427111\n",
            "Iteration 56, loss = 0.29056208\n",
            "Iteration 57, loss = 0.29070694\n",
            "Iteration 58, loss = 0.29617721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.26804752\n",
            "Iteration 60, loss = 0.26712940\n",
            "Iteration 61, loss = 0.26651083\n",
            "Iteration 62, loss = 0.26614908\n",
            "Iteration 63, loss = 0.26540008\n",
            "Iteration 64, loss = 0.26491559\n",
            "Iteration 65, loss = 0.26410529\n",
            "Iteration 66, loss = 0.26333888\n",
            "Iteration 67, loss = 0.26279131\n",
            "Iteration 68, loss = 0.26238760\n",
            "Iteration 69, loss = 0.26193149\n",
            "Iteration 70, loss = 0.26115744\n",
            "Iteration 71, loss = 0.26074836\n",
            "Iteration 72, loss = 0.26018421\n",
            "Iteration 73, loss = 0.25957669\n",
            "Iteration 74, loss = 0.25923589\n",
            "Iteration 75, loss = 0.25864819\n",
            "Iteration 76, loss = 0.25802282\n",
            "Iteration 77, loss = 0.25763557\n",
            "Iteration 78, loss = 0.25692498\n",
            "Iteration 79, loss = 0.25666052\n",
            "Iteration 80, loss = 0.25610795\n",
            "Iteration 81, loss = 0.25578880\n",
            "Iteration 82, loss = 0.25506750\n",
            "Iteration 83, loss = 0.25483106\n",
            "Iteration 84, loss = 0.25417558\n",
            "Iteration 85, loss = 0.25372668\n",
            "Iteration 86, loss = 0.25311388\n",
            "Iteration 87, loss = 0.25302334\n",
            "Iteration 88, loss = 0.25234754\n",
            "Iteration 89, loss = 0.25209699\n",
            "Iteration 90, loss = 0.25149055\n",
            "Iteration 91, loss = 0.25129608\n",
            "Iteration 92, loss = 0.25066409\n",
            "Iteration 93, loss = 0.25031543\n",
            "Iteration 94, loss = 0.25003939\n",
            "Iteration 95, loss = 0.24969346\n",
            "Iteration 96, loss = 0.24882127\n",
            "Iteration 97, loss = 0.24887981\n",
            "Iteration 98, loss = 0.24831568\n",
            "Iteration 99, loss = 0.24779572\n",
            "Iteration 100, loss = 0.24766308\n",
            "Iteration 101, loss = 0.24724291\n",
            "Iteration 102, loss = 0.24682196\n",
            "Iteration 103, loss = 0.24668936\n",
            "Iteration 104, loss = 0.24637867\n",
            "Iteration 105, loss = 0.24612159\n",
            "Iteration 106, loss = 0.24566522\n",
            "Iteration 107, loss = 0.24516515\n",
            "Iteration 108, loss = 0.24505714\n",
            "Iteration 109, loss = 0.24454593\n",
            "Iteration 110, loss = 0.24438230\n",
            "Iteration 111, loss = 0.24430247\n",
            "Iteration 112, loss = 0.24370595\n",
            "Iteration 113, loss = 0.24299470\n",
            "Iteration 114, loss = 0.24311356\n",
            "Iteration 115, loss = 0.24283557\n",
            "Iteration 116, loss = 0.24249779\n",
            "Iteration 117, loss = 0.24256887\n",
            "Iteration 118, loss = 0.24201576\n",
            "Iteration 119, loss = 0.24169681\n",
            "Iteration 120, loss = 0.24176538\n",
            "Iteration 121, loss = 0.24156441\n",
            "Iteration 122, loss = 0.24095907\n",
            "Iteration 123, loss = 0.24064016\n",
            "Iteration 124, loss = 0.24085529\n",
            "Iteration 125, loss = 0.24043872\n",
            "Iteration 126, loss = 0.24009533\n",
            "Iteration 127, loss = 0.23990819\n",
            "Iteration 128, loss = 0.23996981\n",
            "Iteration 129, loss = 0.23965174\n",
            "Iteration 130, loss = 0.23965032\n",
            "Iteration 131, loss = 0.23909043\n",
            "Iteration 132, loss = 0.23888425\n",
            "Iteration 133, loss = 0.23873270\n",
            "Iteration 134, loss = 0.23842745\n",
            "Iteration 135, loss = 0.23849139\n",
            "Iteration 136, loss = 0.23832568\n",
            "Iteration 137, loss = 0.23825783\n",
            "Iteration 138, loss = 0.23785636\n",
            "Iteration 139, loss = 0.23780533\n",
            "Iteration 140, loss = 0.23728172\n",
            "Iteration 141, loss = 0.23744062\n",
            "Iteration 142, loss = 0.23734967\n",
            "Iteration 143, loss = 0.23685588\n",
            "Iteration 144, loss = 0.23692212\n",
            "Iteration 145, loss = 0.23672186\n",
            "Iteration 146, loss = 0.23632650\n",
            "Iteration 147, loss = 0.23658683\n",
            "Iteration 148, loss = 0.23616137\n",
            "Iteration 149, loss = 0.23614155\n",
            "Iteration 150, loss = 0.23629809\n",
            "Iteration 151, loss = 0.23595937\n",
            "Iteration 152, loss = 0.23557086\n",
            "Iteration 153, loss = 0.23582385\n",
            "Iteration 154, loss = 0.23558626\n",
            "Iteration 155, loss = 0.23551774\n",
            "Iteration 156, loss = 0.23547806\n",
            "Iteration 157, loss = 0.23490924\n",
            "Iteration 158, loss = 0.23509833\n",
            "Iteration 159, loss = 0.23487889\n",
            "Iteration 160, loss = 0.23465940\n",
            "Iteration 161, loss = 0.23453714\n",
            "Iteration 162, loss = 0.23464405\n",
            "Iteration 163, loss = 0.23433160\n",
            "Iteration 164, loss = 0.23484957\n",
            "Iteration 165, loss = 0.23445530\n",
            "Iteration 166, loss = 0.23427317\n",
            "Iteration 167, loss = 0.23432091\n",
            "Iteration 168, loss = 0.23422973\n",
            "Iteration 169, loss = 0.23390226\n",
            "Iteration 170, loss = 0.23382001\n",
            "Iteration 171, loss = 0.23390644\n",
            "Iteration 172, loss = 0.23370530\n",
            "Iteration 173, loss = 0.23336109\n",
            "Iteration 174, loss = 0.23374173\n",
            "Iteration 175, loss = 0.23371551\n",
            "Iteration 176, loss = 0.23356566\n",
            "Iteration 177, loss = 0.23348582\n",
            "Iteration 178, loss = 0.23315453\n",
            "Iteration 179, loss = 0.23338256\n",
            "Iteration 180, loss = 0.23371392\n",
            "Iteration 181, loss = 0.23320199\n",
            "Iteration 182, loss = 0.23335417\n",
            "Iteration 183, loss = 0.23319627\n",
            "Iteration 184, loss = 0.23316070\n",
            "Iteration 185, loss = 0.23312280\n",
            "Iteration 186, loss = 0.23290669\n",
            "Iteration 187, loss = 0.23332576\n",
            "Iteration 188, loss = 0.23293748\n",
            "Iteration 189, loss = 0.23299799\n",
            "Iteration 190, loss = 0.23274507\n",
            "Iteration 191, loss = 0.23288197\n",
            "Iteration 192, loss = 0.23322444\n",
            "Iteration 193, loss = 0.23286588\n",
            "Iteration 194, loss = 0.23265228\n",
            "Iteration 195, loss = 0.23275046\n",
            "Iteration 196, loss = 0.23245641\n",
            "Iteration 197, loss = 0.23257887\n",
            "Iteration 198, loss = 0.23230690\n",
            "Iteration 199, loss = 0.23303635\n",
            "Iteration 200, loss = 0.23250465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.59703872\n",
            "Iteration 2, loss = 0.19556943\n",
            "Iteration 3, loss = 0.17267808\n",
            "Iteration 4, loss = 0.15509827\n",
            "Iteration 5, loss = 0.14219070\n",
            "Iteration 6, loss = 0.13286940\n",
            "Iteration 7, loss = 0.12497524\n",
            "Iteration 8, loss = 0.11960885\n",
            "Iteration 9, loss = 0.11538403\n",
            "Iteration 10, loss = 0.11195552\n",
            "Iteration 11, loss = 0.10952218\n",
            "Iteration 12, loss = 0.10755985\n",
            "Iteration 13, loss = 0.10640787\n",
            "Iteration 14, loss = 0.10519771\n",
            "Iteration 15, loss = 0.10445914\n",
            "Iteration 16, loss = 0.10391488\n",
            "Iteration 17, loss = 0.10313672\n",
            "Iteration 18, loss = 0.10270284\n",
            "Iteration 19, loss = 0.10287297\n",
            "Iteration 20, loss = 0.10244010\n",
            "Iteration 21, loss = 0.10207123\n",
            "Iteration 22, loss = 0.10210050\n",
            "Iteration 23, loss = 0.10199335\n",
            "Iteration 24, loss = 0.10227769\n",
            "Iteration 25, loss = 0.10190930\n",
            "Iteration 26, loss = 0.10167426\n",
            "Iteration 27, loss = 0.10179904\n",
            "Iteration 28, loss = 0.10183307\n",
            "Iteration 29, loss = 0.10145035\n",
            "Iteration 30, loss = 0.10157908\n",
            "Iteration 31, loss = 0.10161580\n",
            "Iteration 32, loss = 0.10145915\n",
            "Iteration 33, loss = 0.10133681\n",
            "Iteration 34, loss = 0.10182427\n",
            "Iteration 35, loss = 0.10164785\n",
            "Iteration 36, loss = 0.10168847\n",
            "Iteration 37, loss = 0.10160264\n",
            "Iteration 38, loss = 0.10145847\n",
            "Iteration 39, loss = 0.10139652\n",
            "Iteration 40, loss = 0.10158036\n",
            "Iteration 41, loss = 0.10131019\n",
            "Iteration 42, loss = 0.10166143\n",
            "Iteration 43, loss = 0.10141743\n",
            "Iteration 44, loss = 0.10126389\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.10013194\n",
            "Iteration 46, loss = 0.10002396\n",
            "Iteration 47, loss = 0.09985927\n",
            "Iteration 48, loss = 0.09980240\n",
            "Iteration 49, loss = 0.09972585\n",
            "Iteration 50, loss = 0.09999503\n",
            "Iteration 51, loss = 0.09985869\n",
            "Iteration 52, loss = 0.09977894\n",
            "Iteration 53, loss = 0.09965890\n",
            "Iteration 54, loss = 0.09969574\n",
            "Iteration 55, loss = 0.09970647\n",
            "Iteration 56, loss = 0.09976202\n",
            "Iteration 57, loss = 0.09969754\n",
            "Iteration 58, loss = 0.09968388\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.09910011\n",
            "Iteration 60, loss = 0.09907276\n",
            "Iteration 61, loss = 0.09903617\n",
            "Iteration 62, loss = 0.09904643\n",
            "Iteration 63, loss = 0.09906425\n",
            "Iteration 64, loss = 0.09905486\n",
            "Iteration 65, loss = 0.09904565\n",
            "Iteration 66, loss = 0.09900339\n",
            "Iteration 67, loss = 0.09904829\n",
            "Iteration 68, loss = 0.09899406\n",
            "Iteration 69, loss = 0.09904642\n",
            "Iteration 70, loss = 0.09903347\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.09886870\n",
            "Iteration 72, loss = 0.09883492\n",
            "Iteration 73, loss = 0.09883865\n",
            "Iteration 74, loss = 0.09882684\n",
            "Iteration 75, loss = 0.09883242\n",
            "Iteration 76, loss = 0.09882899\n",
            "Iteration 77, loss = 0.09884337\n",
            "Iteration 78, loss = 0.09883089\n",
            "Iteration 79, loss = 0.09882124\n",
            "Iteration 80, loss = 0.09882968\n",
            "Iteration 81, loss = 0.09884492\n",
            "Iteration 82, loss = 0.09883916\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 83, loss = 0.09878193\n",
            "Iteration 84, loss = 0.09878179\n",
            "Iteration 85, loss = 0.09878301\n",
            "Iteration 86, loss = 0.09878112\n",
            "Iteration 87, loss = 0.09877815\n",
            "Iteration 88, loss = 0.09877472\n",
            "Iteration 89, loss = 0.09878206\n",
            "Iteration 90, loss = 0.09878135\n",
            "Iteration 91, loss = 0.09878153\n",
            "Iteration 92, loss = 0.09878084\n",
            "Iteration 93, loss = 0.09878184\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 94, loss = 0.09877469\n",
            "Iteration 95, loss = 0.09877131\n",
            "Iteration 96, loss = 0.09877087\n",
            "Iteration 97, loss = 0.09877166\n",
            "Iteration 98, loss = 0.09877105\n",
            "Iteration 99, loss = 0.09877114\n",
            "Iteration 100, loss = 0.09877184\n",
            "Iteration 101, loss = 0.09877082\n",
            "Iteration 102, loss = 0.09877131\n",
            "Iteration 103, loss = 0.09877077\n",
            "Iteration 104, loss = 0.09877130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.25597849\n",
            "Iteration 3, loss = 0.21817207\n",
            "Iteration 4, loss = 0.19020546\n",
            "Iteration 5, loss = 0.16888049\n",
            "Iteration 6, loss = 0.15221790\n",
            "Iteration 7, loss = 0.14026194\n",
            "Iteration 8, loss = 0.13085667\n",
            "Iteration 9, loss = 0.12430338\n",
            "Iteration 10, loss = 0.11887232\n",
            "Iteration 11, loss = 0.11456271\n",
            "Iteration 12, loss = 0.11137229\n",
            "Iteration 13, loss = 0.10913550\n",
            "Iteration 14, loss = 0.10720897\n",
            "Iteration 15, loss = 0.10603489\n",
            "Iteration 16, loss = 0.10495547\n",
            "Iteration 17, loss = 0.10413774\n",
            "Iteration 18, loss = 0.10330997\n",
            "Iteration 19, loss = 0.10307102\n",
            "Iteration 20, loss = 0.10258338\n",
            "Iteration 21, loss = 0.10240918\n",
            "Iteration 22, loss = 0.10197060\n",
            "Iteration 23, loss = 0.10224275\n",
            "Iteration 24, loss = 0.10225977\n",
            "Iteration 25, loss = 0.10180531\n",
            "Iteration 26, loss = 0.10189777\n",
            "Iteration 27, loss = 0.10186839\n",
            "Iteration 28, loss = 0.10197540\n",
            "Iteration 29, loss = 0.10163649\n",
            "Iteration 30, loss = 0.10153259\n",
            "Iteration 31, loss = 0.10167510\n",
            "Iteration 32, loss = 0.10152073\n",
            "Iteration 33, loss = 0.10126336\n",
            "Iteration 34, loss = 0.10176439\n",
            "Iteration 35, loss = 0.10167469\n",
            "Iteration 36, loss = 0.10142311\n",
            "Iteration 37, loss = 0.10165228\n",
            "Iteration 38, loss = 0.10170479\n",
            "Iteration 39, loss = 0.10158042\n",
            "Iteration 40, loss = 0.10160906\n",
            "Iteration 41, loss = 0.10156620\n",
            "Iteration 42, loss = 0.10127322\n",
            "Iteration 43, loss = 0.10141302\n",
            "Iteration 44, loss = 0.10158511\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.10023529\n",
            "Iteration 46, loss = 0.10026449\n",
            "Iteration 47, loss = 0.09998962\n",
            "Iteration 48, loss = 0.09992038\n",
            "Iteration 49, loss = 0.09987298\n",
            "Iteration 50, loss = 0.09981306\n",
            "Iteration 51, loss = 0.09993988\n",
            "Iteration 52, loss = 0.09982133\n",
            "Iteration 53, loss = 0.09979520\n",
            "Iteration 54, loss = 0.09979861\n",
            "Iteration 55, loss = 0.09987816\n",
            "Iteration 56, loss = 0.09990698\n",
            "Iteration 57, loss = 0.09982018\n",
            "Iteration 58, loss = 0.09968023\n",
            "Iteration 59, loss = 0.09974066\n",
            "Iteration 60, loss = 0.09972476\n",
            "Iteration 61, loss = 0.09979359\n",
            "Iteration 62, loss = 0.09965374\n",
            "Iteration 63, loss = 0.09975144\n",
            "Iteration 64, loss = 0.09962955\n",
            "Iteration 65, loss = 0.09968853\n",
            "Iteration 66, loss = 0.09960164\n",
            "Iteration 67, loss = 0.09963674\n",
            "Iteration 68, loss = 0.09971531\n",
            "Iteration 69, loss = 0.09968667\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 70, loss = 0.09906750\n",
            "Iteration 71, loss = 0.09900254\n",
            "Iteration 72, loss = 0.09901419\n",
            "Iteration 73, loss = 0.09905728\n",
            "Iteration 74, loss = 0.09901081\n",
            "Iteration 75, loss = 0.09899836\n",
            "Iteration 76, loss = 0.09902168\n",
            "Iteration 77, loss = 0.09899130\n",
            "Iteration 78, loss = 0.09898444\n",
            "Iteration 79, loss = 0.09898477\n",
            "Iteration 80, loss = 0.09895138\n",
            "Iteration 81, loss = 0.09894742\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 82, loss = 0.09885445\n",
            "Iteration 83, loss = 0.09881783\n",
            "Iteration 84, loss = 0.09881375\n",
            "Iteration 85, loss = 0.09882019\n",
            "Iteration 86, loss = 0.09880752\n",
            "Iteration 87, loss = 0.09879916\n",
            "Iteration 88, loss = 0.09879501\n",
            "Iteration 89, loss = 0.09880698\n",
            "Iteration 90, loss = 0.09880845\n",
            "Iteration 91, loss = 0.09878966\n",
            "Iteration 92, loss = 0.09882165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 93, loss = 0.09876682\n",
            "Iteration 94, loss = 0.09876727\n",
            "Iteration 95, loss = 0.09876098\n",
            "Iteration 96, loss = 0.09876343\n",
            "Iteration 97, loss = 0.09876095\n",
            "Iteration 98, loss = 0.09875739\n",
            "Iteration 99, loss = 0.09875840\n",
            "Iteration 100, loss = 0.09876318\n",
            "Iteration 101, loss = 0.09875904\n",
            "Iteration 102, loss = 0.09876622\n",
            "Iteration 103, loss = 0.09875639\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 104, loss = 0.09875506\n",
            "Iteration 105, loss = 0.09875212\n",
            "Iteration 106, loss = 0.09875223\n",
            "Iteration 107, loss = 0.09875174\n",
            "Iteration 108, loss = 0.09874903\n",
            "Iteration 109, loss = 0.09875346\n",
            "Iteration 110, loss = 0.09875169\n",
            "Iteration 111, loss = 0.09875160\n",
            "Iteration 112, loss = 0.09875077\n",
            "Iteration 113, loss = 0.09875228\n",
            "Iteration 114, loss = 0.09875076\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.87807760\n",
            "Iteration 2, loss = 0.22292975\n",
            "Iteration 3, loss = 0.19394912\n",
            "Iteration 4, loss = 0.17164937\n",
            "Iteration 5, loss = 0.15505240\n",
            "Iteration 6, loss = 0.14191499\n",
            "Iteration 7, loss = 0.13233615\n",
            "Iteration 8, loss = 0.12508492\n",
            "Iteration 9, loss = 0.11957559\n",
            "Iteration 10, loss = 0.11553267\n",
            "Iteration 11, loss = 0.11204582\n",
            "Iteration 12, loss = 0.10963305\n",
            "Iteration 13, loss = 0.10783812\n",
            "Iteration 14, loss = 0.10620421\n",
            "Iteration 15, loss = 0.10508424\n",
            "Iteration 16, loss = 0.10434876\n",
            "Iteration 17, loss = 0.10368431\n",
            "Iteration 18, loss = 0.10328441\n",
            "Iteration 19, loss = 0.10258320\n",
            "Iteration 20, loss = 0.10247275\n",
            "Iteration 21, loss = 0.10232780\n",
            "Iteration 22, loss = 0.10187273\n",
            "Iteration 23, loss = 0.10193902\n",
            "Iteration 24, loss = 0.10151773\n",
            "Iteration 25, loss = 0.10158024\n",
            "Iteration 26, loss = 0.10164424\n",
            "Iteration 27, loss = 0.10158665\n",
            "Iteration 28, loss = 0.10183078\n",
            "Iteration 29, loss = 0.10168952\n",
            "Iteration 30, loss = 0.10142618\n",
            "Iteration 31, loss = 0.10136776\n",
            "Iteration 32, loss = 0.10154144\n",
            "Iteration 33, loss = 0.10177743\n",
            "Iteration 34, loss = 0.10182364\n",
            "Iteration 35, loss = 0.10124503\n",
            "Iteration 36, loss = 0.10157996\n",
            "Iteration 37, loss = 0.10164398\n",
            "Iteration 38, loss = 0.10124802\n",
            "Iteration 39, loss = 0.10178383\n",
            "Iteration 40, loss = 0.10126194\n",
            "Iteration 41, loss = 0.10144444\n",
            "Iteration 42, loss = 0.10165441\n",
            "Iteration 43, loss = 0.10127913\n",
            "Iteration 44, loss = 0.10137964\n",
            "Iteration 45, loss = 0.10134529\n",
            "Iteration 46, loss = 0.10143488\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.09998989\n",
            "Iteration 48, loss = 0.09994272\n",
            "Iteration 49, loss = 0.10005926\n",
            "Iteration 50, loss = 0.09995110\n",
            "Iteration 51, loss = 0.09980981\n",
            "Iteration 52, loss = 0.09987102\n",
            "Iteration 53, loss = 0.09990349\n",
            "Iteration 54, loss = 0.09977652\n",
            "Iteration 55, loss = 0.09991710\n",
            "Iteration 56, loss = 0.09986280\n",
            "Iteration 57, loss = 0.09966200\n",
            "Iteration 58, loss = 0.09968670\n",
            "Iteration 59, loss = 0.09980098\n",
            "Iteration 60, loss = 0.09967409\n",
            "Iteration 61, loss = 0.09983397\n",
            "Iteration 62, loss = 0.09959411\n",
            "Iteration 63, loss = 0.09972678\n",
            "Iteration 64, loss = 0.09967103\n",
            "Iteration 65, loss = 0.09959708\n",
            "Iteration 66, loss = 0.09955382\n",
            "Iteration 67, loss = 0.09960394\n",
            "Iteration 68, loss = 0.09969882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 69, loss = 0.09908685\n",
            "Iteration 70, loss = 0.09899975\n",
            "Iteration 71, loss = 0.09898136\n",
            "Iteration 72, loss = 0.09901389\n",
            "Iteration 73, loss = 0.09900911\n",
            "Iteration 74, loss = 0.09898750\n",
            "Iteration 75, loss = 0.09899218\n",
            "Iteration 76, loss = 0.09907017\n",
            "Iteration 77, loss = 0.09903414\n",
            "Iteration 78, loss = 0.09903331\n",
            "Iteration 79, loss = 0.09906592\n",
            "Iteration 80, loss = 0.09898495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 81, loss = 0.09888661\n",
            "Iteration 82, loss = 0.09884205\n",
            "Iteration 83, loss = 0.09882318\n",
            "Iteration 84, loss = 0.09883999\n",
            "Iteration 85, loss = 0.09883927\n",
            "Iteration 86, loss = 0.09884885\n",
            "Iteration 87, loss = 0.09884434\n",
            "Iteration 88, loss = 0.09882590\n",
            "Iteration 89, loss = 0.09881880\n",
            "Iteration 90, loss = 0.09882153\n",
            "Iteration 91, loss = 0.09881215\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 92, loss = 0.09879022\n",
            "Iteration 93, loss = 0.09878189\n",
            "Iteration 94, loss = 0.09878449\n",
            "Iteration 95, loss = 0.09877657\n",
            "Iteration 96, loss = 0.09877782\n",
            "Iteration 97, loss = 0.09877690\n",
            "Iteration 98, loss = 0.09877681\n",
            "Iteration 99, loss = 0.09877738\n",
            "Iteration 100, loss = 0.09877767\n",
            "Iteration 101, loss = 0.09877539\n",
            "Iteration 102, loss = 0.09877285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 103, loss = 0.09877229\n",
            "Iteration 104, loss = 0.09876847\n",
            "Iteration 105, loss = 0.09876690\n",
            "Iteration 106, loss = 0.09876743\n",
            "Iteration 107, loss = 0.09876718\n",
            "Iteration 108, loss = 0.09876714\n",
            "Iteration 109, loss = 0.09876721\n",
            "Iteration 110, loss = 0.09876726\n",
            "Iteration 111, loss = 0.09876719\n",
            "Iteration 112, loss = 0.09876612\n",
            "Iteration 113, loss = 0.09876729\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23642434\n",
            "Iteration 3, loss = 0.20375367\n",
            "Iteration 4, loss = 0.17905597\n",
            "Iteration 5, loss = 0.16049051\n",
            "Iteration 6, loss = 0.14665206\n",
            "Iteration 7, loss = 0.13576160\n",
            "Iteration 8, loss = 0.12750433\n",
            "Iteration 9, loss = 0.12140518\n",
            "Iteration 10, loss = 0.11628101\n",
            "Iteration 11, loss = 0.11282306\n",
            "Iteration 12, loss = 0.11017313\n",
            "Iteration 13, loss = 0.10821831\n",
            "Iteration 14, loss = 0.10641908\n",
            "Iteration 15, loss = 0.10516883\n",
            "Iteration 16, loss = 0.10427424\n",
            "Iteration 17, loss = 0.10354855\n",
            "Iteration 18, loss = 0.10321432\n",
            "Iteration 19, loss = 0.10267727\n",
            "Iteration 20, loss = 0.10215837\n",
            "Iteration 21, loss = 0.10219194\n",
            "Iteration 22, loss = 0.10199657\n",
            "Iteration 23, loss = 0.10198539\n",
            "Iteration 24, loss = 0.10163498\n",
            "Iteration 25, loss = 0.10142672\n",
            "Iteration 26, loss = 0.10129237\n",
            "Iteration 27, loss = 0.10145621\n",
            "Iteration 28, loss = 0.10127818\n",
            "Iteration 29, loss = 0.10148496\n",
            "Iteration 30, loss = 0.10125232\n",
            "Iteration 31, loss = 0.10126518\n",
            "Iteration 32, loss = 0.10142916\n",
            "Iteration 33, loss = 0.10154596\n",
            "Iteration 34, loss = 0.10120095\n",
            "Iteration 35, loss = 0.10107155\n",
            "Iteration 36, loss = 0.10119108\n",
            "Iteration 37, loss = 0.10123757\n",
            "Iteration 38, loss = 0.10141984\n",
            "Iteration 39, loss = 0.10118644\n",
            "Iteration 40, loss = 0.10116293\n",
            "Iteration 41, loss = 0.10131413\n",
            "Iteration 42, loss = 0.10148351\n",
            "Iteration 43, loss = 0.10147547\n",
            "Iteration 44, loss = 0.10110385\n",
            "Iteration 45, loss = 0.10125412\n",
            "Iteration 46, loss = 0.10133659\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.09992811\n",
            "Iteration 48, loss = 0.09994241\n",
            "Iteration 49, loss = 0.09989910\n",
            "Iteration 50, loss = 0.09968063\n",
            "Iteration 51, loss = 0.09971756\n",
            "Iteration 52, loss = 0.09963946\n",
            "Iteration 53, loss = 0.09977799\n",
            "Iteration 54, loss = 0.09967619\n",
            "Iteration 55, loss = 0.09974418\n",
            "Iteration 56, loss = 0.09971155\n",
            "Iteration 57, loss = 0.09979665\n",
            "Iteration 58, loss = 0.09968468\n",
            "Iteration 59, loss = 0.09960752\n",
            "Iteration 60, loss = 0.09971204\n",
            "Iteration 61, loss = 0.09954252\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 62, loss = 0.09920107\n",
            "Iteration 63, loss = 0.09908967\n",
            "Iteration 64, loss = 0.09902526\n",
            "Iteration 65, loss = 0.09903116\n",
            "Iteration 66, loss = 0.09906374\n",
            "Iteration 67, loss = 0.09899262\n",
            "Iteration 68, loss = 0.09897738\n",
            "Iteration 69, loss = 0.09904297\n",
            "Iteration 70, loss = 0.09904713\n",
            "Iteration 71, loss = 0.09904553\n",
            "Iteration 72, loss = 0.09902748\n",
            "Iteration 73, loss = 0.09897351\n",
            "Iteration 74, loss = 0.09896724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 75, loss = 0.09886791\n",
            "Iteration 76, loss = 0.09883971\n",
            "Iteration 77, loss = 0.09884463\n",
            "Iteration 78, loss = 0.09883764\n",
            "Iteration 79, loss = 0.09884038\n",
            "Iteration 80, loss = 0.09881615\n",
            "Iteration 81, loss = 0.09882771\n",
            "Iteration 82, loss = 0.09882480\n",
            "Iteration 83, loss = 0.09883356\n",
            "Iteration 84, loss = 0.09882013\n",
            "Iteration 85, loss = 0.09881777\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 86, loss = 0.09879005\n",
            "Iteration 87, loss = 0.09878402\n",
            "Iteration 88, loss = 0.09878518\n",
            "Iteration 89, loss = 0.09877971\n",
            "Iteration 90, loss = 0.09878074\n",
            "Iteration 91, loss = 0.09878518\n",
            "Iteration 92, loss = 0.09877986\n",
            "Iteration 93, loss = 0.09878596\n",
            "Iteration 94, loss = 0.09877620\n",
            "Iteration 95, loss = 0.09877430\n",
            "Iteration 96, loss = 0.09878094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 97, loss = 0.09877051\n",
            "Iteration 98, loss = 0.09877098\n",
            "Iteration 99, loss = 0.09877154\n",
            "Iteration 100, loss = 0.09877090\n",
            "Iteration 101, loss = 0.09877128\n",
            "Iteration 102, loss = 0.09877006\n",
            "Iteration 103, loss = 0.09877021\n",
            "Iteration 104, loss = 0.09877016\n",
            "Iteration 105, loss = 0.09877037\n",
            "Iteration 106, loss = 0.09877143\n",
            "Iteration 107, loss = 0.09877025\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.24011047\n",
            "Iteration 3, loss = 0.20702509\n",
            "Iteration 4, loss = 0.18202121\n",
            "Iteration 5, loss = 0.16322380\n",
            "Iteration 6, loss = 0.14882883\n",
            "Iteration 7, loss = 0.13778450\n",
            "Iteration 8, loss = 0.12973301\n",
            "Iteration 9, loss = 0.12334635\n",
            "Iteration 10, loss = 0.11836867\n",
            "Iteration 11, loss = 0.11494788\n",
            "Iteration 12, loss = 0.11192425\n",
            "Iteration 13, loss = 0.10971118\n",
            "Iteration 14, loss = 0.10819651\n",
            "Iteration 15, loss = 0.10693135\n",
            "Iteration 16, loss = 0.10602928\n",
            "Iteration 17, loss = 0.10511423\n",
            "Iteration 18, loss = 0.10472378\n",
            "Iteration 19, loss = 0.10418535\n",
            "Iteration 20, loss = 0.10361309\n",
            "Iteration 21, loss = 0.10334473\n",
            "Iteration 22, loss = 0.10305813\n",
            "Iteration 23, loss = 0.10296477\n",
            "Iteration 24, loss = 0.10265124\n",
            "Iteration 25, loss = 0.10254020\n",
            "Iteration 26, loss = 0.10269632\n",
            "Iteration 27, loss = 0.10238607\n",
            "Iteration 28, loss = 0.10252045\n",
            "Iteration 29, loss = 0.10234483\n",
            "Iteration 30, loss = 0.10212327\n",
            "Iteration 31, loss = 0.10202978\n",
            "Iteration 32, loss = 0.10194224\n",
            "Iteration 33, loss = 0.10183160\n",
            "Iteration 34, loss = 0.10212093\n",
            "Iteration 35, loss = 0.10205737\n",
            "Iteration 36, loss = 0.10187408\n",
            "Iteration 37, loss = 0.10194251\n",
            "Iteration 38, loss = 0.10148565\n",
            "Iteration 39, loss = 0.10198606\n",
            "Iteration 40, loss = 0.10214765\n",
            "Iteration 41, loss = 0.10179366\n",
            "Iteration 42, loss = 0.10142887\n",
            "Iteration 43, loss = 0.10171169\n",
            "Iteration 44, loss = 0.10151454\n",
            "Iteration 45, loss = 0.10160607\n",
            "Iteration 46, loss = 0.10169233\n",
            "Iteration 47, loss = 0.10170210\n",
            "Iteration 48, loss = 0.10175715\n",
            "Iteration 49, loss = 0.10148893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 50, loss = 0.10033153\n",
            "Iteration 51, loss = 0.10019723\n",
            "Iteration 52, loss = 0.10009602\n",
            "Iteration 53, loss = 0.10000984\n",
            "Iteration 54, loss = 0.09995288\n",
            "Iteration 55, loss = 0.09998089\n",
            "Iteration 56, loss = 0.10007026\n",
            "Iteration 57, loss = 0.09994625\n",
            "Iteration 58, loss = 0.10002230\n",
            "Iteration 59, loss = 0.09985495\n",
            "Iteration 60, loss = 0.09987043\n",
            "Iteration 61, loss = 0.09993209\n",
            "Iteration 62, loss = 0.09996893\n",
            "Iteration 63, loss = 0.09978419\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 64, loss = 0.09944013\n",
            "Iteration 65, loss = 0.09923529\n",
            "Iteration 66, loss = 0.09915665\n",
            "Iteration 67, loss = 0.09917198\n",
            "Iteration 68, loss = 0.09912723\n",
            "Iteration 69, loss = 0.09913097\n",
            "Iteration 70, loss = 0.09915901\n",
            "Iteration 71, loss = 0.09911394\n",
            "Iteration 72, loss = 0.09918168\n",
            "Iteration 73, loss = 0.09915241\n",
            "Iteration 74, loss = 0.09909709\n",
            "Iteration 75, loss = 0.09907709\n",
            "Iteration 76, loss = 0.09910886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 77, loss = 0.09891875\n",
            "Iteration 78, loss = 0.09890169\n",
            "Iteration 79, loss = 0.09889850\n",
            "Iteration 80, loss = 0.09889541\n",
            "Iteration 81, loss = 0.09890614\n",
            "Iteration 82, loss = 0.09890809\n",
            "Iteration 83, loss = 0.09893101\n",
            "Iteration 84, loss = 0.09890478\n",
            "Iteration 85, loss = 0.09890897\n",
            "Iteration 86, loss = 0.09890736\n",
            "Iteration 87, loss = 0.09890590\n",
            "Iteration 88, loss = 0.09888285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 89, loss = 0.09887621\n",
            "Iteration 90, loss = 0.09886379\n",
            "Iteration 91, loss = 0.09886037\n",
            "Iteration 92, loss = 0.09886340\n",
            "Iteration 93, loss = 0.09886037\n",
            "Iteration 94, loss = 0.09886010\n",
            "Iteration 95, loss = 0.09885703\n",
            "Iteration 96, loss = 0.09885623\n",
            "Iteration 97, loss = 0.09885610\n",
            "Iteration 98, loss = 0.09886103\n",
            "Iteration 99, loss = 0.09886230\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 100, loss = 0.09885010\n",
            "Iteration 101, loss = 0.09884968\n",
            "Iteration 102, loss = 0.09884844\n",
            "Iteration 103, loss = 0.09884936\n",
            "Iteration 104, loss = 0.09884996\n",
            "Iteration 105, loss = 0.09884985\n",
            "Iteration 106, loss = 0.09885029\n",
            "Iteration 107, loss = 0.09884917\n",
            "Iteration 108, loss = 0.09884957\n",
            "Iteration 109, loss = 0.09884954\n",
            "Iteration 110, loss = 0.09884925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.35580230\n",
            "Iteration 2, loss = 0.18707481\n",
            "Iteration 3, loss = 0.16516951\n",
            "Iteration 4, loss = 0.14969832\n",
            "Iteration 5, loss = 0.13802994\n",
            "Iteration 6, loss = 0.12955691\n",
            "Iteration 7, loss = 0.12283862\n",
            "Iteration 8, loss = 0.11797848\n",
            "Iteration 9, loss = 0.11402556\n",
            "Iteration 10, loss = 0.11141737\n",
            "Iteration 11, loss = 0.10901307\n",
            "Iteration 12, loss = 0.10769735\n",
            "Iteration 13, loss = 0.10632964\n",
            "Iteration 14, loss = 0.10526914\n",
            "Iteration 15, loss = 0.10480105\n",
            "Iteration 16, loss = 0.10390051\n",
            "Iteration 17, loss = 0.10342907\n",
            "Iteration 18, loss = 0.10323012\n",
            "Iteration 19, loss = 0.10297192\n",
            "Iteration 20, loss = 0.10309961\n",
            "Iteration 21, loss = 0.10278110\n",
            "Iteration 22, loss = 0.10222945\n",
            "Iteration 23, loss = 0.10270576\n",
            "Iteration 24, loss = 0.10248659\n",
            "Iteration 25, loss = 0.10193294\n",
            "Iteration 26, loss = 0.10219210\n",
            "Iteration 27, loss = 0.10231391\n",
            "Iteration 28, loss = 0.10209599\n",
            "Iteration 29, loss = 0.10190880\n",
            "Iteration 30, loss = 0.10208823\n",
            "Iteration 31, loss = 0.10240394\n",
            "Iteration 32, loss = 0.10203526\n",
            "Iteration 33, loss = 0.10191195\n",
            "Iteration 34, loss = 0.10206856\n",
            "Iteration 35, loss = 0.10182290\n",
            "Iteration 36, loss = 0.10209318\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.10057567\n",
            "Iteration 38, loss = 0.10035704\n",
            "Iteration 39, loss = 0.10032634\n",
            "Iteration 40, loss = 0.10036730\n",
            "Iteration 41, loss = 0.10044581\n",
            "Iteration 42, loss = 0.10040730\n",
            "Iteration 43, loss = 0.10029386\n",
            "Iteration 44, loss = 0.10019718\n",
            "Iteration 45, loss = 0.10023934\n",
            "Iteration 46, loss = 0.10008640\n",
            "Iteration 47, loss = 0.10000112\n",
            "Iteration 48, loss = 0.10015165\n",
            "Iteration 49, loss = 0.10020835\n",
            "Iteration 50, loss = 0.10004274\n",
            "Iteration 51, loss = 0.10000921\n",
            "Iteration 52, loss = 0.10012657\n",
            "Iteration 53, loss = 0.10011626\n",
            "Iteration 54, loss = 0.10011196\n",
            "Iteration 55, loss = 0.10021090\n",
            "Iteration 56, loss = 0.10001870\n",
            "Iteration 57, loss = 0.09992093\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 58, loss = 0.09936877\n",
            "Iteration 59, loss = 0.09936155\n",
            "Iteration 60, loss = 0.09933694\n",
            "Iteration 61, loss = 0.09933925\n",
            "Iteration 62, loss = 0.09928520\n",
            "Iteration 63, loss = 0.09933100\n",
            "Iteration 64, loss = 0.09938465\n",
            "Iteration 65, loss = 0.09929336\n",
            "Iteration 66, loss = 0.09932931\n",
            "Iteration 67, loss = 0.09933810\n",
            "Iteration 68, loss = 0.09933086\n",
            "Iteration 69, loss = 0.09930530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 70, loss = 0.09913144\n",
            "Iteration 71, loss = 0.09911619\n",
            "Iteration 72, loss = 0.09913755\n",
            "Iteration 73, loss = 0.09913697\n",
            "Iteration 74, loss = 0.09911164\n",
            "Iteration 75, loss = 0.09912948\n",
            "Iteration 76, loss = 0.09912237\n",
            "Iteration 77, loss = 0.09913319\n",
            "Iteration 78, loss = 0.09913485\n",
            "Iteration 79, loss = 0.09914182\n",
            "Iteration 80, loss = 0.09910927\n",
            "Iteration 81, loss = 0.09913974\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 82, loss = 0.09909394\n",
            "Iteration 83, loss = 0.09907606\n",
            "Iteration 84, loss = 0.09907684\n",
            "Iteration 85, loss = 0.09908058\n",
            "Iteration 86, loss = 0.09907493\n",
            "Iteration 87, loss = 0.09908055\n",
            "Iteration 88, loss = 0.09908076\n",
            "Iteration 89, loss = 0.09907462\n",
            "Iteration 90, loss = 0.09908123\n",
            "Iteration 91, loss = 0.09908716\n",
            "Iteration 92, loss = 0.09907803\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 93, loss = 0.09907311\n",
            "Iteration 94, loss = 0.09906935\n",
            "Iteration 95, loss = 0.09906903\n",
            "Iteration 96, loss = 0.09906885\n",
            "Iteration 97, loss = 0.09906953\n",
            "Iteration 98, loss = 0.09906892\n",
            "Iteration 99, loss = 0.09906825\n",
            "Iteration 100, loss = 0.09906871\n",
            "Iteration 101, loss = 0.09906902\n",
            "Iteration 102, loss = 0.09906818\n",
            "Iteration 103, loss = 0.09906894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.71680168\n",
            "Iteration 2, loss = 0.20779821\n",
            "Iteration 3, loss = 0.18132295\n",
            "Iteration 4, loss = 0.16205172\n",
            "Iteration 5, loss = 0.14810361\n",
            "Iteration 6, loss = 0.13671356\n",
            "Iteration 7, loss = 0.12875165\n",
            "Iteration 8, loss = 0.12211180\n",
            "Iteration 9, loss = 0.11742507\n",
            "Iteration 10, loss = 0.11400597\n",
            "Iteration 11, loss = 0.11086745\n",
            "Iteration 12, loss = 0.10907884\n",
            "Iteration 13, loss = 0.10744708\n",
            "Iteration 14, loss = 0.10589894\n",
            "Iteration 15, loss = 0.10484503\n",
            "Iteration 16, loss = 0.10435130\n",
            "Iteration 17, loss = 0.10375703\n",
            "Iteration 18, loss = 0.10358802\n",
            "Iteration 19, loss = 0.10299979\n",
            "Iteration 20, loss = 0.10297565\n",
            "Iteration 21, loss = 0.10296900\n",
            "Iteration 22, loss = 0.10244987\n",
            "Iteration 23, loss = 0.10220236\n",
            "Iteration 24, loss = 0.10220969\n",
            "Iteration 25, loss = 0.10223186\n",
            "Iteration 26, loss = 0.10185802\n",
            "Iteration 27, loss = 0.10220164\n",
            "Iteration 28, loss = 0.10213372\n",
            "Iteration 29, loss = 0.10217310\n",
            "Iteration 30, loss = 0.10201626\n",
            "Iteration 31, loss = 0.10199714\n",
            "Iteration 32, loss = 0.10187288\n",
            "Iteration 33, loss = 0.10165376\n",
            "Iteration 34, loss = 0.10164659\n",
            "Iteration 35, loss = 0.10164636\n",
            "Iteration 36, loss = 0.10180679\n",
            "Iteration 37, loss = 0.10205301\n",
            "Iteration 38, loss = 0.10188706\n",
            "Iteration 39, loss = 0.10210099\n",
            "Iteration 40, loss = 0.10191706\n",
            "Iteration 41, loss = 0.10173781\n",
            "Iteration 42, loss = 0.10178928\n",
            "Iteration 43, loss = 0.10190578\n",
            "Iteration 44, loss = 0.10207356\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.10042985\n",
            "Iteration 46, loss = 0.10021623\n",
            "Iteration 47, loss = 0.10033677\n",
            "Iteration 48, loss = 0.10032381\n",
            "Iteration 49, loss = 0.10013221\n",
            "Iteration 50, loss = 0.10014177\n",
            "Iteration 51, loss = 0.10016492\n",
            "Iteration 52, loss = 0.10006647\n",
            "Iteration 53, loss = 0.10003049\n",
            "Iteration 54, loss = 0.10003196\n",
            "Iteration 55, loss = 0.10018521\n",
            "Iteration 56, loss = 0.09994127\n",
            "Iteration 57, loss = 0.10017498\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 58, loss = 0.09957607\n",
            "Iteration 59, loss = 0.09942771\n",
            "Iteration 60, loss = 0.09938619\n",
            "Iteration 61, loss = 0.09942348\n",
            "Iteration 62, loss = 0.09942520\n",
            "Iteration 63, loss = 0.09935855\n",
            "Iteration 64, loss = 0.09936469\n",
            "Iteration 65, loss = 0.09931821\n",
            "Iteration 66, loss = 0.09934049\n",
            "Iteration 67, loss = 0.09933912\n",
            "Iteration 68, loss = 0.09936051\n",
            "Iteration 69, loss = 0.09935773\n",
            "Iteration 70, loss = 0.09940121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.09923497\n",
            "Iteration 72, loss = 0.09919255\n",
            "Iteration 73, loss = 0.09919379\n",
            "Iteration 74, loss = 0.09918771\n",
            "Iteration 75, loss = 0.09918250\n",
            "Iteration 76, loss = 0.09918584\n",
            "Iteration 77, loss = 0.09918195\n",
            "Iteration 78, loss = 0.09918242\n",
            "Iteration 79, loss = 0.09917576\n",
            "Iteration 80, loss = 0.09917842\n",
            "Iteration 81, loss = 0.09918345\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 82, loss = 0.09914233\n",
            "Iteration 83, loss = 0.09913908\n",
            "Iteration 84, loss = 0.09914062\n",
            "Iteration 85, loss = 0.09913718\n",
            "Iteration 86, loss = 0.09913479\n",
            "Iteration 87, loss = 0.09914077\n",
            "Iteration 88, loss = 0.09913675\n",
            "Iteration 89, loss = 0.09913884\n",
            "Iteration 90, loss = 0.09913830\n",
            "Iteration 91, loss = 0.09913458\n",
            "Iteration 92, loss = 0.09913484\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 93, loss = 0.09912939\n",
            "Iteration 94, loss = 0.09912748\n",
            "Iteration 95, loss = 0.09912807\n",
            "Iteration 96, loss = 0.09912775\n",
            "Iteration 97, loss = 0.09912658\n",
            "Iteration 98, loss = 0.09912678\n",
            "Iteration 99, loss = 0.09912720\n",
            "Iteration 100, loss = 0.09912680\n",
            "Iteration 101, loss = 0.09912718\n",
            "Iteration 102, loss = 0.09912714\n",
            "Iteration 103, loss = 0.09912636\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.47222371\n",
            "Iteration 2, loss = 0.17722022\n",
            "Iteration 3, loss = 0.15894192\n",
            "Iteration 4, loss = 0.14480144\n",
            "Iteration 5, loss = 0.13452931\n",
            "Iteration 6, loss = 0.12710706\n",
            "Iteration 7, loss = 0.12097182\n",
            "Iteration 8, loss = 0.11629906\n",
            "Iteration 9, loss = 0.11326710\n",
            "Iteration 10, loss = 0.11059692\n",
            "Iteration 11, loss = 0.10846362\n",
            "Iteration 12, loss = 0.10719145\n",
            "Iteration 13, loss = 0.10601429\n",
            "Iteration 14, loss = 0.10496480\n",
            "Iteration 15, loss = 0.10448129\n",
            "Iteration 16, loss = 0.10423959\n",
            "Iteration 17, loss = 0.10386230\n",
            "Iteration 18, loss = 0.10318463\n",
            "Iteration 19, loss = 0.10292947\n",
            "Iteration 20, loss = 0.10310674\n",
            "Iteration 21, loss = 0.10252080\n",
            "Iteration 22, loss = 0.10282799\n",
            "Iteration 23, loss = 0.10249478\n",
            "Iteration 24, loss = 0.10240215\n",
            "Iteration 25, loss = 0.10236059\n",
            "Iteration 26, loss = 0.10214833\n",
            "Iteration 27, loss = 0.10231697\n",
            "Iteration 28, loss = 0.10220205\n",
            "Iteration 29, loss = 0.10221517\n",
            "Iteration 30, loss = 0.10189940\n",
            "Iteration 31, loss = 0.10160276\n",
            "Iteration 32, loss = 0.10177395\n",
            "Iteration 33, loss = 0.10229615\n",
            "Iteration 34, loss = 0.10214639\n",
            "Iteration 35, loss = 0.10173486\n",
            "Iteration 36, loss = 0.10187627\n",
            "Iteration 37, loss = 0.10162664\n",
            "Iteration 38, loss = 0.10193396\n",
            "Iteration 39, loss = 0.10184591\n",
            "Iteration 40, loss = 0.10205085\n",
            "Iteration 41, loss = 0.10176461\n",
            "Iteration 42, loss = 0.10188084\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 43, loss = 0.10043765\n",
            "Iteration 44, loss = 0.10021385\n",
            "Iteration 45, loss = 0.10034498\n",
            "Iteration 46, loss = 0.10017779\n",
            "Iteration 47, loss = 0.10019697\n",
            "Iteration 48, loss = 0.10013643\n",
            "Iteration 49, loss = 0.10023230\n",
            "Iteration 50, loss = 0.10004966\n",
            "Iteration 51, loss = 0.10007214\n",
            "Iteration 52, loss = 0.09996371\n",
            "Iteration 53, loss = 0.10007963\n",
            "Iteration 54, loss = 0.10010579\n",
            "Iteration 55, loss = 0.09980724\n",
            "Iteration 56, loss = 0.09994566\n",
            "Iteration 57, loss = 0.09980990\n",
            "Iteration 58, loss = 0.09991447\n",
            "Iteration 59, loss = 0.09980324\n",
            "Iteration 60, loss = 0.09992524\n",
            "Iteration 61, loss = 0.09977147\n",
            "Iteration 62, loss = 0.09987500\n",
            "Iteration 63, loss = 0.09967863\n",
            "Iteration 64, loss = 0.09978966\n",
            "Iteration 65, loss = 0.09989502\n",
            "Iteration 66, loss = 0.09968193\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.09916802\n",
            "Iteration 68, loss = 0.09912839\n",
            "Iteration 69, loss = 0.09913696\n",
            "Iteration 70, loss = 0.09910672\n",
            "Iteration 71, loss = 0.09915149\n",
            "Iteration 72, loss = 0.09906683\n",
            "Iteration 73, loss = 0.09910195\n",
            "Iteration 74, loss = 0.09907727\n",
            "Iteration 75, loss = 0.09901975\n",
            "Iteration 76, loss = 0.09908714\n",
            "Iteration 77, loss = 0.09901109\n",
            "Iteration 78, loss = 0.09910789\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 79, loss = 0.09891398\n",
            "Iteration 80, loss = 0.09888480\n",
            "Iteration 81, loss = 0.09888960\n",
            "Iteration 82, loss = 0.09889378\n",
            "Iteration 83, loss = 0.09888889\n",
            "Iteration 84, loss = 0.09888493\n",
            "Iteration 85, loss = 0.09889049\n",
            "Iteration 86, loss = 0.09888655\n",
            "Iteration 87, loss = 0.09889007\n",
            "Iteration 88, loss = 0.09887780\n",
            "Iteration 89, loss = 0.09888297\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 90, loss = 0.09884696\n",
            "Iteration 91, loss = 0.09883635\n",
            "Iteration 92, loss = 0.09884326\n",
            "Iteration 93, loss = 0.09883896\n",
            "Iteration 94, loss = 0.09884055\n",
            "Iteration 95, loss = 0.09883569\n",
            "Iteration 96, loss = 0.09884233\n",
            "Iteration 97, loss = 0.09883904\n",
            "Iteration 98, loss = 0.09883824\n",
            "Iteration 99, loss = 0.09883129\n",
            "Iteration 100, loss = 0.09883584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 101, loss = 0.09883185\n",
            "Iteration 102, loss = 0.09882941\n",
            "Iteration 103, loss = 0.09882820\n",
            "Iteration 104, loss = 0.09883004\n",
            "Iteration 105, loss = 0.09882872\n",
            "Iteration 106, loss = 0.09882978\n",
            "Iteration 107, loss = 0.09882924\n",
            "Iteration 108, loss = 0.09882915\n",
            "Iteration 109, loss = 0.09883007\n",
            "Iteration 110, loss = 0.09882914\n",
            "Iteration 111, loss = 0.09882946\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.57721478\n",
            "Iteration 2, loss = 0.19344524\n",
            "Iteration 3, loss = 0.17118918\n",
            "Iteration 4, loss = 0.15457168\n",
            "Iteration 5, loss = 0.14204076\n",
            "Iteration 6, loss = 0.13262115\n",
            "Iteration 7, loss = 0.12568431\n",
            "Iteration 8, loss = 0.11995466\n",
            "Iteration 9, loss = 0.11603379\n",
            "Iteration 10, loss = 0.11304807\n",
            "Iteration 11, loss = 0.11050315\n",
            "Iteration 12, loss = 0.10861061\n",
            "Iteration 13, loss = 0.10720008\n",
            "Iteration 14, loss = 0.10615589\n",
            "Iteration 15, loss = 0.10540012\n",
            "Iteration 16, loss = 0.10463191\n",
            "Iteration 17, loss = 0.10411795\n",
            "Iteration 18, loss = 0.10360614\n",
            "Iteration 19, loss = 0.10346634\n",
            "Iteration 20, loss = 0.10315973\n",
            "Iteration 21, loss = 0.10324530\n",
            "Iteration 22, loss = 0.10317953\n",
            "Iteration 23, loss = 0.10248924\n",
            "Iteration 24, loss = 0.10263602\n",
            "Iteration 25, loss = 0.10275722\n",
            "Iteration 26, loss = 0.10215433\n",
            "Iteration 27, loss = 0.10219286\n",
            "Iteration 28, loss = 0.10249028\n",
            "Iteration 29, loss = 0.10208417\n",
            "Iteration 30, loss = 0.10232826\n",
            "Iteration 31, loss = 0.10201192\n",
            "Iteration 32, loss = 0.10234080\n",
            "Iteration 33, loss = 0.10194997\n",
            "Iteration 34, loss = 0.10202030\n",
            "Iteration 35, loss = 0.10231302\n",
            "Iteration 36, loss = 0.10212848\n",
            "Iteration 37, loss = 0.10216724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.10080723\n",
            "Iteration 39, loss = 0.10051584\n",
            "Iteration 40, loss = 0.10063062\n",
            "Iteration 41, loss = 0.10045994\n",
            "Iteration 42, loss = 0.10049104\n",
            "Iteration 43, loss = 0.10040114\n",
            "Iteration 44, loss = 0.10037790\n",
            "Iteration 45, loss = 0.10032753\n",
            "Iteration 46, loss = 0.10043689\n",
            "Iteration 47, loss = 0.10015110\n",
            "Iteration 48, loss = 0.10025390\n",
            "Iteration 49, loss = 0.10041161\n",
            "Iteration 50, loss = 0.10006045\n",
            "Iteration 51, loss = 0.10019313\n",
            "Iteration 52, loss = 0.10022026\n",
            "Iteration 53, loss = 0.10003836\n",
            "Iteration 54, loss = 0.10022880\n",
            "Iteration 55, loss = 0.10001995\n",
            "Iteration 56, loss = 0.10014851\n",
            "Iteration 57, loss = 0.10003414\n",
            "Iteration 58, loss = 0.10007396\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.09956198\n",
            "Iteration 60, loss = 0.09945843\n",
            "Iteration 61, loss = 0.09947785\n",
            "Iteration 62, loss = 0.09941846\n",
            "Iteration 63, loss = 0.09953194\n",
            "Iteration 64, loss = 0.09933497\n",
            "Iteration 65, loss = 0.09941348\n",
            "Iteration 66, loss = 0.09943012\n",
            "Iteration 67, loss = 0.09939183\n",
            "Iteration 68, loss = 0.09945490\n",
            "Iteration 69, loss = 0.09942287\n",
            "Iteration 70, loss = 0.09940154\n",
            "Iteration 71, loss = 0.09942616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 72, loss = 0.09922834\n",
            "Iteration 73, loss = 0.09919941\n",
            "Iteration 74, loss = 0.09920091\n",
            "Iteration 75, loss = 0.09920050\n",
            "Iteration 76, loss = 0.09919444\n",
            "Iteration 77, loss = 0.09919974\n",
            "Iteration 78, loss = 0.09919658\n",
            "Iteration 79, loss = 0.09919879\n",
            "Iteration 80, loss = 0.09917641\n",
            "Iteration 81, loss = 0.09916290\n",
            "Iteration 82, loss = 0.09920678\n",
            "Iteration 83, loss = 0.09916749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 84, loss = 0.09916493\n",
            "Iteration 85, loss = 0.09915130\n",
            "Iteration 86, loss = 0.09913993\n",
            "Iteration 87, loss = 0.09915377\n",
            "Iteration 88, loss = 0.09915792\n",
            "Iteration 89, loss = 0.09915126\n",
            "Iteration 90, loss = 0.09915467\n",
            "Iteration 91, loss = 0.09915039\n",
            "Iteration 92, loss = 0.09914928\n",
            "Iteration 93, loss = 0.09914916\n",
            "Iteration 94, loss = 0.09915069\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 95, loss = 0.09914156\n",
            "Iteration 96, loss = 0.09913976\n",
            "Iteration 97, loss = 0.09914004\n",
            "Iteration 98, loss = 0.09914020\n",
            "Iteration 99, loss = 0.09913974\n",
            "Iteration 100, loss = 0.09914042\n",
            "Iteration 101, loss = 0.09914007\n",
            "Iteration 102, loss = 0.09914038\n",
            "Iteration 103, loss = 0.09914045\n",
            "Iteration 104, loss = 0.09913905\n",
            "Iteration 105, loss = 0.09914165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.51824361\n",
            "Iteration 2, loss = 0.19521894\n",
            "Iteration 3, loss = 0.17252948\n",
            "Iteration 4, loss = 0.15560610\n",
            "Iteration 5, loss = 0.14250786\n",
            "Iteration 6, loss = 0.13278352\n",
            "Iteration 7, loss = 0.12559184\n",
            "Iteration 8, loss = 0.11952993\n",
            "Iteration 9, loss = 0.11569985\n",
            "Iteration 10, loss = 0.11277576\n",
            "Iteration 11, loss = 0.11028549\n",
            "Iteration 12, loss = 0.10821886\n",
            "Iteration 13, loss = 0.10672647\n",
            "Iteration 14, loss = 0.10568420\n",
            "Iteration 15, loss = 0.10484984\n",
            "Iteration 16, loss = 0.10405205\n",
            "Iteration 17, loss = 0.10389414\n",
            "Iteration 18, loss = 0.10373441\n",
            "Iteration 19, loss = 0.10361093\n",
            "Iteration 20, loss = 0.10298302\n",
            "Iteration 21, loss = 0.10271457\n",
            "Iteration 22, loss = 0.10245426\n",
            "Iteration 23, loss = 0.10251514\n",
            "Iteration 24, loss = 0.10249967\n",
            "Iteration 25, loss = 0.10270570\n",
            "Iteration 26, loss = 0.10293704\n",
            "Iteration 27, loss = 0.10235936\n",
            "Iteration 28, loss = 0.10231256\n",
            "Iteration 29, loss = 0.10228075\n",
            "Iteration 30, loss = 0.10218037\n",
            "Iteration 31, loss = 0.10213123\n",
            "Iteration 32, loss = 0.10226269\n",
            "Iteration 33, loss = 0.10247286\n",
            "Iteration 34, loss = 0.10242726\n",
            "Iteration 35, loss = 0.10221383\n",
            "Iteration 36, loss = 0.10191489\n",
            "Iteration 37, loss = 0.10225384\n",
            "Iteration 38, loss = 0.10235633\n",
            "Iteration 39, loss = 0.10202288\n",
            "Iteration 40, loss = 0.10198880\n",
            "Iteration 41, loss = 0.10245817\n",
            "Iteration 42, loss = 0.10217702\n",
            "Iteration 43, loss = 0.10233070\n",
            "Iteration 44, loss = 0.10179401\n",
            "Iteration 45, loss = 0.10212737\n",
            "Iteration 46, loss = 0.10192308\n",
            "Iteration 47, loss = 0.10203292\n",
            "Iteration 48, loss = 0.10192680\n",
            "Iteration 49, loss = 0.10208742\n",
            "Iteration 50, loss = 0.10198794\n",
            "Iteration 51, loss = 0.10199000\n",
            "Iteration 52, loss = 0.10203105\n",
            "Iteration 53, loss = 0.10200171\n",
            "Iteration 54, loss = 0.10184928\n",
            "Iteration 55, loss = 0.10172508\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 56, loss = 0.10057851\n",
            "Iteration 57, loss = 0.10036218\n",
            "Iteration 58, loss = 0.10042017\n",
            "Iteration 59, loss = 0.10027216\n",
            "Iteration 60, loss = 0.10016263\n",
            "Iteration 61, loss = 0.10009322\n",
            "Iteration 62, loss = 0.10020234\n",
            "Iteration 63, loss = 0.10006472\n",
            "Iteration 64, loss = 0.10007961\n",
            "Iteration 65, loss = 0.10015320\n",
            "Iteration 66, loss = 0.10014989\n",
            "Iteration 67, loss = 0.09992501\n",
            "Iteration 68, loss = 0.09997174\n",
            "Iteration 69, loss = 0.10007530\n",
            "Iteration 70, loss = 0.10004450\n",
            "Iteration 71, loss = 0.10007227\n",
            "Iteration 72, loss = 0.10000099\n",
            "Iteration 73, loss = 0.09999329\n",
            "Iteration 74, loss = 0.10004387\n",
            "Iteration 75, loss = 0.09998538\n",
            "Iteration 76, loss = 0.10005770\n",
            "Iteration 77, loss = 0.09979448\n",
            "Iteration 78, loss = 0.09980233\n",
            "Iteration 79, loss = 0.09986491\n",
            "Iteration 80, loss = 0.09995008\n",
            "Iteration 81, loss = 0.09971640\n",
            "Iteration 82, loss = 0.09998734\n",
            "Iteration 83, loss = 0.09970962\n",
            "Iteration 84, loss = 0.09968799\n",
            "Iteration 85, loss = 0.09971821\n",
            "Iteration 86, loss = 0.09974588\n",
            "Iteration 87, loss = 0.09963942\n",
            "Iteration 88, loss = 0.09982998\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.09917930\n",
            "Iteration 90, loss = 0.09912431\n",
            "Iteration 91, loss = 0.09918394\n",
            "Iteration 92, loss = 0.09915681\n",
            "Iteration 93, loss = 0.09908476\n",
            "Iteration 94, loss = 0.09914449\n",
            "Iteration 95, loss = 0.09913699\n",
            "Iteration 96, loss = 0.09909678\n",
            "Iteration 97, loss = 0.09910488\n",
            "Iteration 98, loss = 0.09910141\n",
            "Iteration 99, loss = 0.09908776\n",
            "Iteration 100, loss = 0.09907179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.09891887\n",
            "Iteration 102, loss = 0.09891645\n",
            "Iteration 103, loss = 0.09889311\n",
            "Iteration 104, loss = 0.09891524\n",
            "Iteration 105, loss = 0.09890538\n",
            "Iteration 106, loss = 0.09890125\n",
            "Iteration 107, loss = 0.09887272\n",
            "Iteration 108, loss = 0.09890555\n",
            "Iteration 109, loss = 0.09889449\n",
            "Iteration 110, loss = 0.09891651\n",
            "Iteration 111, loss = 0.09890998\n",
            "Iteration 112, loss = 0.09888534\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.09885950\n",
            "Iteration 114, loss = 0.09885884\n",
            "Iteration 115, loss = 0.09884932\n",
            "Iteration 116, loss = 0.09885223\n",
            "Iteration 117, loss = 0.09885314\n",
            "Iteration 118, loss = 0.09885400\n",
            "Iteration 119, loss = 0.09885093\n",
            "Iteration 120, loss = 0.09884829\n",
            "Iteration 121, loss = 0.09885216\n",
            "Iteration 122, loss = 0.09884727\n",
            "Iteration 123, loss = 0.09885118\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 124, loss = 0.09885084\n",
            "Iteration 125, loss = 0.09884269\n",
            "Iteration 126, loss = 0.09884344\n",
            "Iteration 127, loss = 0.09883987\n",
            "Iteration 128, loss = 0.09884169\n",
            "Iteration 129, loss = 0.09884250\n",
            "Iteration 130, loss = 0.09884123\n",
            "Iteration 131, loss = 0.09884126\n",
            "Iteration 132, loss = 0.09884103\n",
            "Iteration 133, loss = 0.09883994\n",
            "Iteration 134, loss = 0.09884159\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 16.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.31135687\n",
            "Iteration 3, loss = 0.29431051\n",
            "Iteration 4, loss = 0.27807616\n",
            "Iteration 5, loss = 0.26347642\n",
            "Iteration 6, loss = 0.24997892\n",
            "Iteration 7, loss = 0.23817237\n",
            "Iteration 8, loss = 0.22696334\n",
            "Iteration 9, loss = 0.21597473\n",
            "Iteration 10, loss = 0.20722343\n",
            "Iteration 11, loss = 0.19899304\n",
            "Iteration 12, loss = 0.19079550\n",
            "Iteration 13, loss = 0.18333594\n",
            "Iteration 14, loss = 0.17695249\n",
            "Iteration 15, loss = 0.17091194\n",
            "Iteration 16, loss = 0.16524259\n",
            "Iteration 17, loss = 0.15994681\n",
            "Iteration 18, loss = 0.15537439\n",
            "Iteration 19, loss = 0.15076504\n",
            "Iteration 20, loss = 0.14671495\n",
            "Iteration 21, loss = 0.14280142\n",
            "Iteration 22, loss = 0.13947866\n",
            "Iteration 23, loss = 0.13668663\n",
            "Iteration 24, loss = 0.13445927\n",
            "Iteration 25, loss = 0.13082466\n",
            "Iteration 26, loss = 0.12850641\n",
            "Iteration 27, loss = 0.12629373\n",
            "Iteration 28, loss = 0.12397502\n",
            "Iteration 29, loss = 0.12186966\n",
            "Iteration 30, loss = 0.12031909\n",
            "Iteration 31, loss = 0.11791017\n",
            "Iteration 32, loss = 0.11701198\n",
            "Iteration 33, loss = 0.11574557\n",
            "Iteration 34, loss = 0.11375675\n",
            "Iteration 35, loss = 0.11308519\n",
            "Iteration 36, loss = 0.11152647\n",
            "Iteration 37, loss = 0.11066965\n",
            "Iteration 38, loss = 0.10980395\n",
            "Iteration 39, loss = 0.10839674\n",
            "Iteration 40, loss = 0.10797030\n",
            "Iteration 41, loss = 0.10687745\n",
            "Iteration 42, loss = 0.10629720\n",
            "Iteration 43, loss = 0.10616090\n",
            "Iteration 44, loss = 0.10540033\n",
            "Iteration 45, loss = 0.10496081\n",
            "Iteration 46, loss = 0.10416531\n",
            "Iteration 47, loss = 0.10378535\n",
            "Iteration 48, loss = 0.10305726\n",
            "Iteration 49, loss = 0.10291294\n",
            "Iteration 50, loss = 0.10209349\n",
            "Iteration 51, loss = 0.10199919\n",
            "Iteration 52, loss = 0.10165249\n",
            "Iteration 53, loss = 0.10076750\n",
            "Iteration 54, loss = 0.10093612\n",
            "Iteration 55, loss = 0.10087380\n",
            "Iteration 56, loss = 0.10113543\n",
            "Iteration 57, loss = 0.10039171\n",
            "Iteration 58, loss = 0.09997753\n",
            "Iteration 59, loss = 0.09985951\n",
            "Iteration 60, loss = 0.09909410\n",
            "Iteration 61, loss = 0.09951831\n",
            "Iteration 62, loss = 0.09939304\n",
            "Iteration 63, loss = 0.09990080\n",
            "Iteration 64, loss = 0.09901040\n",
            "Iteration 65, loss = 0.09907579\n",
            "Iteration 66, loss = 0.09887907\n",
            "Iteration 67, loss = 0.09858348\n",
            "Iteration 68, loss = 0.09876315\n",
            "Iteration 69, loss = 0.09837989\n",
            "Iteration 70, loss = 0.09875729\n",
            "Iteration 71, loss = 0.09825634\n",
            "Iteration 72, loss = 0.09892802\n",
            "Iteration 73, loss = 0.09799654\n",
            "Iteration 74, loss = 0.09846462\n",
            "Iteration 75, loss = 0.09880265\n",
            "Iteration 76, loss = 0.09899350\n",
            "Iteration 77, loss = 0.09807392\n",
            "Iteration 78, loss = 0.09773505\n",
            "Iteration 79, loss = 0.09851874\n",
            "Iteration 80, loss = 0.09760212\n",
            "Iteration 81, loss = 0.09788428\n",
            "Iteration 82, loss = 0.09854562\n",
            "Iteration 83, loss = 0.09907592\n",
            "Iteration 84, loss = 0.09756025\n",
            "Iteration 85, loss = 0.09796151\n",
            "Iteration 86, loss = 0.09818083\n",
            "Iteration 87, loss = 0.09836040\n",
            "Iteration 88, loss = 0.09817151\n",
            "Iteration 89, loss = 0.09795746\n",
            "Iteration 90, loss = 0.09789176\n",
            "Iteration 91, loss = 0.09789289\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 92, loss = 0.09636176\n",
            "Iteration 93, loss = 0.09614708\n",
            "Iteration 94, loss = 0.09608271\n",
            "Iteration 95, loss = 0.09656173\n",
            "Iteration 96, loss = 0.09624786\n",
            "Iteration 97, loss = 0.09584181\n",
            "Iteration 98, loss = 0.09621242\n",
            "Iteration 99, loss = 0.09614925\n",
            "Iteration 100, loss = 0.09617108\n",
            "Iteration 101, loss = 0.09594252\n",
            "Iteration 102, loss = 0.09624229\n",
            "Iteration 103, loss = 0.09632437\n",
            "Iteration 104, loss = 0.09616702\n",
            "Iteration 105, loss = 0.09590868\n",
            "Iteration 106, loss = 0.09605900\n",
            "Iteration 107, loss = 0.09594726\n",
            "Iteration 108, loss = 0.09604806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 109, loss = 0.09546037\n",
            "Iteration 110, loss = 0.09541420\n",
            "Iteration 111, loss = 0.09552576\n",
            "Iteration 112, loss = 0.09551927\n",
            "Iteration 113, loss = 0.09540612\n",
            "Iteration 114, loss = 0.09543891\n",
            "Iteration 115, loss = 0.09539310\n",
            "Iteration 116, loss = 0.09536445\n",
            "Iteration 117, loss = 0.09530597\n",
            "Iteration 118, loss = 0.09535187\n",
            "Iteration 119, loss = 0.09547362\n",
            "Iteration 120, loss = 0.09533521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 121, loss = 0.09518380\n",
            "Iteration 122, loss = 0.09521600\n",
            "Iteration 123, loss = 0.09522327\n",
            "Iteration 124, loss = 0.09521767\n",
            "Iteration 125, loss = 0.09522435\n",
            "Iteration 126, loss = 0.09521606\n",
            "Iteration 127, loss = 0.09522705\n",
            "Iteration 128, loss = 0.09524342\n",
            "Iteration 129, loss = 0.09522445\n",
            "Iteration 130, loss = 0.09518943\n",
            "Iteration 131, loss = 0.09521061\n",
            "Iteration 132, loss = 0.09521317\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 133, loss = 0.09517661\n",
            "Iteration 134, loss = 0.09516581\n",
            "Iteration 135, loss = 0.09517144\n",
            "Iteration 136, loss = 0.09516898\n",
            "Iteration 137, loss = 0.09517180\n",
            "Iteration 138, loss = 0.09516735\n",
            "Iteration 139, loss = 0.09516623\n",
            "Iteration 140, loss = 0.09517799\n",
            "Iteration 141, loss = 0.09517076\n",
            "Iteration 142, loss = 0.09517261\n",
            "Iteration 143, loss = 0.09517148\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 144, loss = 0.09516809\n",
            "Iteration 145, loss = 0.09516417\n",
            "Iteration 146, loss = 0.09516079\n",
            "Iteration 147, loss = 0.09515949\n",
            "Iteration 148, loss = 0.09515933\n",
            "Iteration 149, loss = 0.09515932\n",
            "Iteration 150, loss = 0.09515877\n",
            "Iteration 151, loss = 0.09515836\n",
            "Iteration 152, loss = 0.09515944\n",
            "Iteration 153, loss = 0.09515879\n",
            "Iteration 154, loss = 0.09516035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.81814579\n",
            "Iteration 2, loss = 0.21593096\n",
            "Iteration 3, loss = 0.20550791\n",
            "Iteration 4, loss = 0.19601099\n",
            "Iteration 5, loss = 0.18789959\n",
            "Iteration 6, loss = 0.18119262\n",
            "Iteration 7, loss = 0.17415628\n",
            "Iteration 8, loss = 0.16845611\n",
            "Iteration 9, loss = 0.16262491\n",
            "Iteration 10, loss = 0.15754661\n",
            "Iteration 11, loss = 0.15304822\n",
            "Iteration 12, loss = 0.14883588\n",
            "Iteration 13, loss = 0.14441651\n",
            "Iteration 14, loss = 0.14117823\n",
            "Iteration 15, loss = 0.13760911\n",
            "Iteration 16, loss = 0.13411264\n",
            "Iteration 17, loss = 0.13169588\n",
            "Iteration 18, loss = 0.12911180\n",
            "Iteration 19, loss = 0.12713148\n",
            "Iteration 20, loss = 0.12396429\n",
            "Iteration 21, loss = 0.12303509\n",
            "Iteration 22, loss = 0.12063653\n",
            "Iteration 23, loss = 0.11958205\n",
            "Iteration 24, loss = 0.11752166\n",
            "Iteration 25, loss = 0.11641207\n",
            "Iteration 26, loss = 0.11441871\n",
            "Iteration 27, loss = 0.11474564\n",
            "Iteration 28, loss = 0.11251067\n",
            "Iteration 29, loss = 0.11171854\n",
            "Iteration 30, loss = 0.11055685\n",
            "Iteration 31, loss = 0.10967193\n",
            "Iteration 32, loss = 0.11005309\n",
            "Iteration 33, loss = 0.10823739\n",
            "Iteration 34, loss = 0.10705534\n",
            "Iteration 35, loss = 0.10617272\n",
            "Iteration 36, loss = 0.10603898\n",
            "Iteration 37, loss = 0.10511214\n",
            "Iteration 38, loss = 0.10501323\n",
            "Iteration 39, loss = 0.10470964\n",
            "Iteration 40, loss = 0.10434581\n",
            "Iteration 41, loss = 0.10361401\n",
            "Iteration 42, loss = 0.10321971\n",
            "Iteration 43, loss = 0.10346637\n",
            "Iteration 44, loss = 0.10269463\n",
            "Iteration 45, loss = 0.10229310\n",
            "Iteration 46, loss = 0.10151760\n",
            "Iteration 47, loss = 0.10166257\n",
            "Iteration 48, loss = 0.10135439\n",
            "Iteration 49, loss = 0.10136486\n",
            "Iteration 50, loss = 0.10107951\n",
            "Iteration 51, loss = 0.10102864\n",
            "Iteration 52, loss = 0.10104331\n",
            "Iteration 53, loss = 0.10066780\n",
            "Iteration 54, loss = 0.10026239\n",
            "Iteration 55, loss = 0.10015638\n",
            "Iteration 56, loss = 0.10035982\n",
            "Iteration 57, loss = 0.10038323\n",
            "Iteration 58, loss = 0.10037865\n",
            "Iteration 59, loss = 0.10013771\n",
            "Iteration 60, loss = 0.09917977\n",
            "Iteration 61, loss = 0.10009642\n",
            "Iteration 62, loss = 0.09970979\n",
            "Iteration 63, loss = 0.09913488\n",
            "Iteration 64, loss = 0.09929599\n",
            "Iteration 65, loss = 0.10031964\n",
            "Iteration 66, loss = 0.09976054\n",
            "Iteration 67, loss = 0.09896862\n",
            "Iteration 68, loss = 0.09894406\n",
            "Iteration 69, loss = 0.09903024\n",
            "Iteration 70, loss = 0.09948651\n",
            "Iteration 71, loss = 0.09915228\n",
            "Iteration 72, loss = 0.09872650\n",
            "Iteration 73, loss = 0.09897159\n",
            "Iteration 74, loss = 0.09870946\n",
            "Iteration 75, loss = 0.09857723\n",
            "Iteration 76, loss = 0.09886146\n",
            "Iteration 77, loss = 0.09957025\n",
            "Iteration 78, loss = 0.09920270\n",
            "Iteration 79, loss = 0.09841608\n",
            "Iteration 80, loss = 0.09900375\n",
            "Iteration 81, loss = 0.09880789\n",
            "Iteration 82, loss = 0.09832646\n",
            "Iteration 83, loss = 0.09863624\n",
            "Iteration 84, loss = 0.09845371\n",
            "Iteration 85, loss = 0.09870405\n",
            "Iteration 86, loss = 0.09875569\n",
            "Iteration 87, loss = 0.09813933\n",
            "Iteration 88, loss = 0.09889720\n",
            "Iteration 89, loss = 0.09874152\n",
            "Iteration 90, loss = 0.09823815\n",
            "Iteration 91, loss = 0.09895418\n",
            "Iteration 92, loss = 0.09879025\n",
            "Iteration 93, loss = 0.09804410\n",
            "Iteration 94, loss = 0.09861731\n",
            "Iteration 95, loss = 0.09876625\n",
            "Iteration 96, loss = 0.09870166\n",
            "Iteration 97, loss = 0.09890465\n",
            "Iteration 98, loss = 0.09888904\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 99, loss = 0.09700648\n",
            "Iteration 100, loss = 0.09697775\n",
            "Iteration 101, loss = 0.09674185\n",
            "Iteration 102, loss = 0.09676991\n",
            "Iteration 103, loss = 0.09668766\n",
            "Iteration 104, loss = 0.09658923\n",
            "Iteration 105, loss = 0.09693137\n",
            "Iteration 106, loss = 0.09643510\n",
            "Iteration 107, loss = 0.09667267\n",
            "Iteration 108, loss = 0.09644615\n",
            "Iteration 109, loss = 0.09637780\n",
            "Iteration 110, loss = 0.09674516\n",
            "Iteration 111, loss = 0.09667832\n",
            "Iteration 112, loss = 0.09692119\n",
            "Iteration 113, loss = 0.09671635\n",
            "Iteration 114, loss = 0.09661460\n",
            "Iteration 115, loss = 0.09649356\n",
            "Iteration 116, loss = 0.09645318\n",
            "Iteration 117, loss = 0.09637332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 118, loss = 0.09611949\n",
            "Iteration 119, loss = 0.09584004\n",
            "Iteration 120, loss = 0.09593454\n",
            "Iteration 121, loss = 0.09592170\n",
            "Iteration 122, loss = 0.09580250\n",
            "Iteration 123, loss = 0.09582313\n",
            "Iteration 124, loss = 0.09581437\n",
            "Iteration 125, loss = 0.09586404\n",
            "Iteration 126, loss = 0.09587055\n",
            "Iteration 127, loss = 0.09591489\n",
            "Iteration 128, loss = 0.09586313\n",
            "Iteration 129, loss = 0.09573849\n",
            "Iteration 130, loss = 0.09577700\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 131, loss = 0.09573786\n",
            "Iteration 132, loss = 0.09568761\n",
            "Iteration 133, loss = 0.09571108\n",
            "Iteration 134, loss = 0.09569463\n",
            "Iteration 135, loss = 0.09566971\n",
            "Iteration 136, loss = 0.09566506\n",
            "Iteration 137, loss = 0.09568508\n",
            "Iteration 138, loss = 0.09566432\n",
            "Iteration 139, loss = 0.09565964\n",
            "Iteration 140, loss = 0.09566588\n",
            "Iteration 141, loss = 0.09564815\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 142, loss = 0.09561654\n",
            "Iteration 143, loss = 0.09562548\n",
            "Iteration 144, loss = 0.09561758\n",
            "Iteration 145, loss = 0.09562812\n",
            "Iteration 146, loss = 0.09561487\n",
            "Iteration 147, loss = 0.09561518\n",
            "Iteration 148, loss = 0.09561366\n",
            "Iteration 149, loss = 0.09561437\n",
            "Iteration 150, loss = 0.09560958\n",
            "Iteration 151, loss = 0.09561376\n",
            "Iteration 152, loss = 0.09561501\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 153, loss = 0.09560623\n",
            "Iteration 154, loss = 0.09560486\n",
            "Iteration 155, loss = 0.09560506\n",
            "Iteration 156, loss = 0.09560666\n",
            "Iteration 157, loss = 0.09560521\n",
            "Iteration 158, loss = 0.09560436\n",
            "Iteration 159, loss = 0.09560683\n",
            "Iteration 160, loss = 0.09560535\n",
            "Iteration 161, loss = 0.09560610\n",
            "Iteration 162, loss = 0.09560574\n",
            "Iteration 163, loss = 0.09560481\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.26227267\n",
            "Iteration 3, loss = 0.24740287\n",
            "Iteration 4, loss = 0.23540881\n",
            "Iteration 5, loss = 0.22403435\n",
            "Iteration 6, loss = 0.21404384\n",
            "Iteration 7, loss = 0.20439668\n",
            "Iteration 8, loss = 0.19612741\n",
            "Iteration 9, loss = 0.18838400\n",
            "Iteration 10, loss = 0.18171749\n",
            "Iteration 11, loss = 0.17536020\n",
            "Iteration 12, loss = 0.16873665\n",
            "Iteration 13, loss = 0.16406559\n",
            "Iteration 14, loss = 0.15826584\n",
            "Iteration 15, loss = 0.15407286\n",
            "Iteration 16, loss = 0.14905989\n",
            "Iteration 17, loss = 0.14601996\n",
            "Iteration 18, loss = 0.14200767\n",
            "Iteration 19, loss = 0.13807150\n",
            "Iteration 20, loss = 0.13560543\n",
            "Iteration 21, loss = 0.13314638\n",
            "Iteration 22, loss = 0.13022569\n",
            "Iteration 23, loss = 0.12765249\n",
            "Iteration 24, loss = 0.12495016\n",
            "Iteration 25, loss = 0.12316758\n",
            "Iteration 26, loss = 0.12155916\n",
            "Iteration 27, loss = 0.12004398\n",
            "Iteration 28, loss = 0.11778028\n",
            "Iteration 29, loss = 0.11684152\n",
            "Iteration 30, loss = 0.11525397\n",
            "Iteration 31, loss = 0.11328228\n",
            "Iteration 32, loss = 0.11296874\n",
            "Iteration 33, loss = 0.11208809\n",
            "Iteration 34, loss = 0.11056859\n",
            "Iteration 35, loss = 0.10938899\n",
            "Iteration 36, loss = 0.10863761\n",
            "Iteration 37, loss = 0.10756539\n",
            "Iteration 38, loss = 0.10780533\n",
            "Iteration 39, loss = 0.10667498\n",
            "Iteration 40, loss = 0.10633003\n",
            "Iteration 41, loss = 0.10505273\n",
            "Iteration 42, loss = 0.10477281\n",
            "Iteration 43, loss = 0.10440699\n",
            "Iteration 44, loss = 0.10447840\n",
            "Iteration 45, loss = 0.10425147\n",
            "Iteration 46, loss = 0.10307458\n",
            "Iteration 47, loss = 0.10338063\n",
            "Iteration 48, loss = 0.10234382\n",
            "Iteration 49, loss = 0.10215570\n",
            "Iteration 50, loss = 0.10224907\n",
            "Iteration 51, loss = 0.10149549\n",
            "Iteration 52, loss = 0.10102121\n",
            "Iteration 53, loss = 0.10162792\n",
            "Iteration 54, loss = 0.10059384\n",
            "Iteration 55, loss = 0.10125637\n",
            "Iteration 56, loss = 0.10026545\n",
            "Iteration 57, loss = 0.10049649\n",
            "Iteration 58, loss = 0.09943516\n",
            "Iteration 59, loss = 0.10128621\n",
            "Iteration 60, loss = 0.10087552\n",
            "Iteration 61, loss = 0.10039837\n",
            "Iteration 62, loss = 0.10012410\n",
            "Iteration 63, loss = 0.09992135\n",
            "Iteration 64, loss = 0.10016811\n",
            "Iteration 65, loss = 0.09955618\n",
            "Iteration 66, loss = 0.10003654\n",
            "Iteration 67, loss = 0.09949612\n",
            "Iteration 68, loss = 0.09973045\n",
            "Iteration 69, loss = 0.09893225\n",
            "Iteration 70, loss = 0.09919773\n",
            "Iteration 71, loss = 0.09941892\n",
            "Iteration 72, loss = 0.09916350\n",
            "Iteration 73, loss = 0.09960184\n",
            "Iteration 74, loss = 0.09860632\n",
            "Iteration 75, loss = 0.09908582\n",
            "Iteration 76, loss = 0.09883412\n",
            "Iteration 77, loss = 0.09920531\n",
            "Iteration 78, loss = 0.09793681\n",
            "Iteration 79, loss = 0.09814636\n",
            "Iteration 80, loss = 0.09825814\n",
            "Iteration 81, loss = 0.09871227\n",
            "Iteration 82, loss = 0.09790253\n",
            "Iteration 83, loss = 0.09828566\n",
            "Iteration 84, loss = 0.09817874\n",
            "Iteration 85, loss = 0.09873304\n",
            "Iteration 86, loss = 0.09850400\n",
            "Iteration 87, loss = 0.09797404\n",
            "Iteration 88, loss = 0.09875144\n",
            "Iteration 89, loss = 0.09832560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 90, loss = 0.09729479\n",
            "Iteration 91, loss = 0.09638811\n",
            "Iteration 92, loss = 0.09678917\n",
            "Iteration 93, loss = 0.09636007\n",
            "Iteration 94, loss = 0.09652577\n",
            "Iteration 95, loss = 0.09661446\n",
            "Iteration 96, loss = 0.09637452\n",
            "Iteration 97, loss = 0.09657749\n",
            "Iteration 98, loss = 0.09641122\n",
            "Iteration 99, loss = 0.09632256\n",
            "Iteration 100, loss = 0.09622482\n",
            "Iteration 101, loss = 0.09622432\n",
            "Iteration 102, loss = 0.09639764\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 103, loss = 0.09588132\n",
            "Iteration 104, loss = 0.09572607\n",
            "Iteration 105, loss = 0.09570188\n",
            "Iteration 106, loss = 0.09572772\n",
            "Iteration 107, loss = 0.09576420\n",
            "Iteration 108, loss = 0.09570373\n",
            "Iteration 109, loss = 0.09583036\n",
            "Iteration 110, loss = 0.09569268\n",
            "Iteration 111, loss = 0.09574743\n",
            "Iteration 112, loss = 0.09563985\n",
            "Iteration 113, loss = 0.09578145\n",
            "Iteration 114, loss = 0.09560994\n",
            "Iteration 115, loss = 0.09579268\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 116, loss = 0.09556429\n",
            "Iteration 117, loss = 0.09554460\n",
            "Iteration 118, loss = 0.09554859\n",
            "Iteration 119, loss = 0.09554443\n",
            "Iteration 120, loss = 0.09552828\n",
            "Iteration 121, loss = 0.09550754\n",
            "Iteration 122, loss = 0.09553754\n",
            "Iteration 123, loss = 0.09555830\n",
            "Iteration 124, loss = 0.09551845\n",
            "Iteration 125, loss = 0.09549883\n",
            "Iteration 126, loss = 0.09554468\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 127, loss = 0.09547911\n",
            "Iteration 128, loss = 0.09547772\n",
            "Iteration 129, loss = 0.09548137\n",
            "Iteration 130, loss = 0.09547110\n",
            "Iteration 131, loss = 0.09548159\n",
            "Iteration 132, loss = 0.09546785\n",
            "Iteration 133, loss = 0.09546731\n",
            "Iteration 134, loss = 0.09546473\n",
            "Iteration 135, loss = 0.09546833\n",
            "Iteration 136, loss = 0.09546807\n",
            "Iteration 137, loss = 0.09546005\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 138, loss = 0.09544433\n",
            "Iteration 139, loss = 0.09544014\n",
            "Iteration 140, loss = 0.09543734\n",
            "Iteration 141, loss = 0.09543419\n",
            "Iteration 142, loss = 0.09542961\n",
            "Iteration 143, loss = 0.09542744\n",
            "Iteration 144, loss = 0.09542517\n",
            "Iteration 145, loss = 0.09542473\n",
            "Iteration 146, loss = 0.09542347\n",
            "Iteration 147, loss = 0.09542426\n",
            "Iteration 148, loss = 0.09542419\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 2.43225745\n",
            "Iteration 2, loss = 0.25536069\n",
            "Iteration 3, loss = 0.24274312\n",
            "Iteration 4, loss = 0.23122087\n",
            "Iteration 5, loss = 0.22085587\n",
            "Iteration 6, loss = 0.21113641\n",
            "Iteration 7, loss = 0.20219211\n",
            "Iteration 8, loss = 0.19367067\n",
            "Iteration 9, loss = 0.18601343\n",
            "Iteration 10, loss = 0.17928091\n",
            "Iteration 11, loss = 0.17256628\n",
            "Iteration 12, loss = 0.16695522\n",
            "Iteration 13, loss = 0.16196580\n",
            "Iteration 14, loss = 0.15680666\n",
            "Iteration 15, loss = 0.15186064\n",
            "Iteration 16, loss = 0.14764084\n",
            "Iteration 17, loss = 0.14399399\n",
            "Iteration 18, loss = 0.14104028\n",
            "Iteration 19, loss = 0.13715906\n",
            "Iteration 20, loss = 0.13405927\n",
            "Iteration 21, loss = 0.13114017\n",
            "Iteration 22, loss = 0.12867424\n",
            "Iteration 23, loss = 0.12596435\n",
            "Iteration 24, loss = 0.12393700\n",
            "Iteration 25, loss = 0.12198447\n",
            "Iteration 26, loss = 0.12000410\n",
            "Iteration 27, loss = 0.11852994\n",
            "Iteration 28, loss = 0.11666329\n",
            "Iteration 29, loss = 0.11533098\n",
            "Iteration 30, loss = 0.11449144\n",
            "Iteration 31, loss = 0.11344860\n",
            "Iteration 32, loss = 0.11185781\n",
            "Iteration 33, loss = 0.11055462\n",
            "Iteration 34, loss = 0.11022525\n",
            "Iteration 35, loss = 0.10923043\n",
            "Iteration 36, loss = 0.10834234\n",
            "Iteration 37, loss = 0.10757904\n",
            "Iteration 38, loss = 0.10631721\n",
            "Iteration 39, loss = 0.10563913\n",
            "Iteration 40, loss = 0.10527524\n",
            "Iteration 41, loss = 0.10429574\n",
            "Iteration 42, loss = 0.10495476\n",
            "Iteration 43, loss = 0.10365183\n",
            "Iteration 44, loss = 0.10298899\n",
            "Iteration 45, loss = 0.10298179\n",
            "Iteration 46, loss = 0.10221137\n",
            "Iteration 47, loss = 0.10213900\n",
            "Iteration 48, loss = 0.10156633\n",
            "Iteration 49, loss = 0.10155483\n",
            "Iteration 50, loss = 0.10113851\n",
            "Iteration 51, loss = 0.10079642\n",
            "Iteration 52, loss = 0.10105097\n",
            "Iteration 53, loss = 0.10026194\n",
            "Iteration 54, loss = 0.10090796\n",
            "Iteration 55, loss = 0.10077309\n",
            "Iteration 56, loss = 0.10025281\n",
            "Iteration 57, loss = 0.09993224\n",
            "Iteration 58, loss = 0.09933960\n",
            "Iteration 59, loss = 0.09996851\n",
            "Iteration 60, loss = 0.09988628\n",
            "Iteration 61, loss = 0.09968470\n",
            "Iteration 62, loss = 0.09892886\n",
            "Iteration 63, loss = 0.09868250\n",
            "Iteration 64, loss = 0.09868474\n",
            "Iteration 65, loss = 0.09906685\n",
            "Iteration 66, loss = 0.09902429\n",
            "Iteration 67, loss = 0.09844565\n",
            "Iteration 68, loss = 0.09898244\n",
            "Iteration 69, loss = 0.09858907\n",
            "Iteration 70, loss = 0.09862384\n",
            "Iteration 71, loss = 0.09804815\n",
            "Iteration 72, loss = 0.09938453\n",
            "Iteration 73, loss = 0.09878574\n",
            "Iteration 74, loss = 0.09856228\n",
            "Iteration 75, loss = 0.09875720\n",
            "Iteration 76, loss = 0.09905967\n",
            "Iteration 77, loss = 0.09860428\n",
            "Iteration 78, loss = 0.09846269\n",
            "Iteration 79, loss = 0.09830627\n",
            "Iteration 80, loss = 0.09772006\n",
            "Iteration 81, loss = 0.09891330\n",
            "Iteration 82, loss = 0.09783589\n",
            "Iteration 83, loss = 0.09860253\n",
            "Iteration 84, loss = 0.09818083\n",
            "Iteration 85, loss = 0.09845538\n",
            "Iteration 86, loss = 0.09821011\n",
            "Iteration 87, loss = 0.09818466\n",
            "Iteration 88, loss = 0.09806863\n",
            "Iteration 89, loss = 0.09879658\n",
            "Iteration 90, loss = 0.09796228\n",
            "Iteration 91, loss = 0.09806135\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 92, loss = 0.09647784\n",
            "Iteration 93, loss = 0.09653470\n",
            "Iteration 94, loss = 0.09656035\n",
            "Iteration 95, loss = 0.09639999\n",
            "Iteration 96, loss = 0.09622475\n",
            "Iteration 97, loss = 0.09613470\n",
            "Iteration 98, loss = 0.09629754\n",
            "Iteration 99, loss = 0.09606125\n",
            "Iteration 100, loss = 0.09604224\n",
            "Iteration 101, loss = 0.09647471\n",
            "Iteration 102, loss = 0.09614951\n",
            "Iteration 103, loss = 0.09623966\n",
            "Iteration 104, loss = 0.09603409\n",
            "Iteration 105, loss = 0.09594415\n",
            "Iteration 106, loss = 0.09605425\n",
            "Iteration 107, loss = 0.09608388\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 108, loss = 0.09615548\n",
            "Iteration 109, loss = 0.09557793\n",
            "Iteration 110, loss = 0.09551136\n",
            "Iteration 111, loss = 0.09531432\n",
            "Iteration 112, loss = 0.09546584\n",
            "Iteration 113, loss = 0.09553610\n",
            "Iteration 114, loss = 0.09562300\n",
            "Iteration 115, loss = 0.09551176\n",
            "Iteration 116, loss = 0.09541039\n",
            "Iteration 117, loss = 0.09543718\n",
            "Iteration 118, loss = 0.09545476\n",
            "Iteration 119, loss = 0.09544638\n",
            "Iteration 120, loss = 0.09537893\n",
            "Iteration 121, loss = 0.09540035\n",
            "Iteration 122, loss = 0.09560675\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 123, loss = 0.09539893\n",
            "Iteration 124, loss = 0.09527835\n",
            "Iteration 125, loss = 0.09529846\n",
            "Iteration 126, loss = 0.09529280\n",
            "Iteration 127, loss = 0.09529481\n",
            "Iteration 128, loss = 0.09530301\n",
            "Iteration 129, loss = 0.09531077\n",
            "Iteration 130, loss = 0.09528761\n",
            "Iteration 131, loss = 0.09528769\n",
            "Iteration 132, loss = 0.09529501\n",
            "Iteration 133, loss = 0.09526435\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 134, loss = 0.09529328\n",
            "Iteration 135, loss = 0.09524858\n",
            "Iteration 136, loss = 0.09524905\n",
            "Iteration 137, loss = 0.09525391\n",
            "Iteration 138, loss = 0.09524505\n",
            "Iteration 139, loss = 0.09524588\n",
            "Iteration 140, loss = 0.09525285\n",
            "Iteration 141, loss = 0.09524852\n",
            "Iteration 142, loss = 0.09525317\n",
            "Iteration 143, loss = 0.09524508\n",
            "Iteration 144, loss = 0.09525554\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 145, loss = 0.09524053\n",
            "Iteration 146, loss = 0.09523911\n",
            "Iteration 147, loss = 0.09523883\n",
            "Iteration 148, loss = 0.09523933\n",
            "Iteration 149, loss = 0.09523880\n",
            "Iteration 150, loss = 0.09523920\n",
            "Iteration 151, loss = 0.09524030\n",
            "Iteration 152, loss = 0.09523934\n",
            "Iteration 153, loss = 0.09523807\n",
            "Iteration 154, loss = 0.09524098\n",
            "Iteration 155, loss = 0.09523886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.94562168\n",
            "Iteration 2, loss = 0.20701884\n",
            "Iteration 3, loss = 0.19562601\n",
            "Iteration 4, loss = 0.18730486\n",
            "Iteration 5, loss = 0.17937728\n",
            "Iteration 6, loss = 0.17315070\n",
            "Iteration 7, loss = 0.16662053\n",
            "Iteration 8, loss = 0.16146659\n",
            "Iteration 9, loss = 0.15680202\n",
            "Iteration 10, loss = 0.15175032\n",
            "Iteration 11, loss = 0.14821822\n",
            "Iteration 12, loss = 0.14331157\n",
            "Iteration 13, loss = 0.14022221\n",
            "Iteration 14, loss = 0.13658865\n",
            "Iteration 15, loss = 0.13380551\n",
            "Iteration 16, loss = 0.13121325\n",
            "Iteration 17, loss = 0.12886280\n",
            "Iteration 18, loss = 0.12615421\n",
            "Iteration 19, loss = 0.12444340\n",
            "Iteration 20, loss = 0.12196818\n",
            "Iteration 21, loss = 0.11998430\n",
            "Iteration 22, loss = 0.11852288\n",
            "Iteration 23, loss = 0.11714898\n",
            "Iteration 24, loss = 0.11482861\n",
            "Iteration 25, loss = 0.11456128\n",
            "Iteration 26, loss = 0.11247617\n",
            "Iteration 27, loss = 0.11234242\n",
            "Iteration 28, loss = 0.11107353\n",
            "Iteration 29, loss = 0.11010145\n",
            "Iteration 30, loss = 0.10936477\n",
            "Iteration 31, loss = 0.10842993\n",
            "Iteration 32, loss = 0.10784539\n",
            "Iteration 33, loss = 0.10706393\n",
            "Iteration 34, loss = 0.10637353\n",
            "Iteration 35, loss = 0.10586835\n",
            "Iteration 36, loss = 0.10494551\n",
            "Iteration 37, loss = 0.10426825\n",
            "Iteration 38, loss = 0.10413975\n",
            "Iteration 39, loss = 0.10400730\n",
            "Iteration 40, loss = 0.10380678\n",
            "Iteration 41, loss = 0.10281099\n",
            "Iteration 42, loss = 0.10250008\n",
            "Iteration 43, loss = 0.10240833\n",
            "Iteration 44, loss = 0.10218019\n",
            "Iteration 45, loss = 0.10195755\n",
            "Iteration 46, loss = 0.10153406\n",
            "Iteration 47, loss = 0.10209499\n",
            "Iteration 48, loss = 0.10115265\n",
            "Iteration 49, loss = 0.10088977\n",
            "Iteration 50, loss = 0.10110256\n",
            "Iteration 51, loss = 0.10016545\n",
            "Iteration 52, loss = 0.10022809\n",
            "Iteration 53, loss = 0.10050818\n",
            "Iteration 54, loss = 0.10014546\n",
            "Iteration 55, loss = 0.09959798\n",
            "Iteration 56, loss = 0.09968631\n",
            "Iteration 57, loss = 0.09947332\n",
            "Iteration 58, loss = 0.10047102\n",
            "Iteration 59, loss = 0.09986944\n",
            "Iteration 60, loss = 0.09981267\n",
            "Iteration 61, loss = 0.09907030\n",
            "Iteration 62, loss = 0.09931965\n",
            "Iteration 63, loss = 0.09966626\n",
            "Iteration 64, loss = 0.09857761\n",
            "Iteration 65, loss = 0.09869770\n",
            "Iteration 66, loss = 0.09854429\n",
            "Iteration 67, loss = 0.09925711\n",
            "Iteration 68, loss = 0.09885454\n",
            "Iteration 69, loss = 0.09882781\n",
            "Iteration 70, loss = 0.09885861\n",
            "Iteration 71, loss = 0.09920762\n",
            "Iteration 72, loss = 0.09916003\n",
            "Iteration 73, loss = 0.09825708\n",
            "Iteration 74, loss = 0.09883397\n",
            "Iteration 75, loss = 0.09867363\n",
            "Iteration 76, loss = 0.09844649\n",
            "Iteration 77, loss = 0.09852013\n",
            "Iteration 78, loss = 0.09858191\n",
            "Iteration 79, loss = 0.09813432\n",
            "Iteration 80, loss = 0.09791222\n",
            "Iteration 81, loss = 0.09951491\n",
            "Iteration 82, loss = 0.09858371\n",
            "Iteration 83, loss = 0.09890129\n",
            "Iteration 84, loss = 0.09908222\n",
            "Iteration 85, loss = 0.09862696\n",
            "Iteration 86, loss = 0.09827930\n",
            "Iteration 87, loss = 0.09835165\n",
            "Iteration 88, loss = 0.09867667\n",
            "Iteration 89, loss = 0.09815885\n",
            "Iteration 90, loss = 0.09840925\n",
            "Iteration 91, loss = 0.09823987\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 92, loss = 0.09786275\n",
            "Iteration 93, loss = 0.09658704\n",
            "Iteration 94, loss = 0.09676757\n",
            "Iteration 95, loss = 0.09652705\n",
            "Iteration 96, loss = 0.09660576\n",
            "Iteration 97, loss = 0.09638726\n",
            "Iteration 98, loss = 0.09664843\n",
            "Iteration 99, loss = 0.09678981\n",
            "Iteration 100, loss = 0.09659627\n",
            "Iteration 101, loss = 0.09653564\n",
            "Iteration 102, loss = 0.09665581\n",
            "Iteration 103, loss = 0.09668778\n",
            "Iteration 104, loss = 0.09646672\n",
            "Iteration 105, loss = 0.09639177\n",
            "Iteration 106, loss = 0.09628689\n",
            "Iteration 107, loss = 0.09656043\n",
            "Iteration 108, loss = 0.09624317\n",
            "Iteration 109, loss = 0.09653688\n",
            "Iteration 110, loss = 0.09639106\n",
            "Iteration 111, loss = 0.09638960\n",
            "Iteration 112, loss = 0.09659689\n",
            "Iteration 113, loss = 0.09626803\n",
            "Iteration 114, loss = 0.09637719\n",
            "Iteration 115, loss = 0.09653138\n",
            "Iteration 116, loss = 0.09645617\n",
            "Iteration 117, loss = 0.09630204\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 118, loss = 0.09587348\n",
            "Iteration 119, loss = 0.09599986\n",
            "Iteration 120, loss = 0.09572068\n",
            "Iteration 121, loss = 0.09566011\n",
            "Iteration 122, loss = 0.09578673\n",
            "Iteration 123, loss = 0.09572403\n",
            "Iteration 124, loss = 0.09569877\n",
            "Iteration 125, loss = 0.09569166\n",
            "Iteration 126, loss = 0.09568859\n",
            "Iteration 127, loss = 0.09572272\n",
            "Iteration 128, loss = 0.09573359\n",
            "Iteration 129, loss = 0.09551272\n",
            "Iteration 130, loss = 0.09564950\n",
            "Iteration 131, loss = 0.09567984\n",
            "Iteration 132, loss = 0.09565627\n",
            "Iteration 133, loss = 0.09561704\n",
            "Iteration 134, loss = 0.09570562\n",
            "Iteration 135, loss = 0.09571916\n",
            "Iteration 136, loss = 0.09566764\n",
            "Iteration 137, loss = 0.09577950\n",
            "Iteration 138, loss = 0.09575505\n",
            "Iteration 139, loss = 0.09568557\n",
            "Iteration 140, loss = 0.09576632\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 141, loss = 0.09556999\n",
            "Iteration 142, loss = 0.09549504\n",
            "Iteration 143, loss = 0.09548074\n",
            "Iteration 144, loss = 0.09547617\n",
            "Iteration 145, loss = 0.09549532\n",
            "Iteration 146, loss = 0.09551828\n",
            "Iteration 147, loss = 0.09549874\n",
            "Iteration 148, loss = 0.09549355\n",
            "Iteration 149, loss = 0.09546757\n",
            "Iteration 150, loss = 0.09553051\n",
            "Iteration 151, loss = 0.09548570\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 152, loss = 0.09543935\n",
            "Iteration 153, loss = 0.09544477\n",
            "Iteration 154, loss = 0.09544355\n",
            "Iteration 155, loss = 0.09544196\n",
            "Iteration 156, loss = 0.09544773\n",
            "Iteration 157, loss = 0.09545100\n",
            "Iteration 158, loss = 0.09544528\n",
            "Iteration 159, loss = 0.09544871\n",
            "Iteration 160, loss = 0.09544473\n",
            "Iteration 161, loss = 0.09544308\n",
            "Iteration 162, loss = 0.09543825\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 163, loss = 0.09543682\n",
            "Iteration 164, loss = 0.09543567\n",
            "Iteration 165, loss = 0.09543262\n",
            "Iteration 166, loss = 0.09543420\n",
            "Iteration 167, loss = 0.09543514\n",
            "Iteration 168, loss = 0.09543392\n",
            "Iteration 169, loss = 0.09543338\n",
            "Iteration 170, loss = 0.09543364\n",
            "Iteration 171, loss = 0.09543326\n",
            "Iteration 172, loss = 0.09543222\n",
            "Iteration 173, loss = 0.09543209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.78348065\n",
            "Iteration 2, loss = 0.27580914\n",
            "Iteration 3, loss = 0.25911994\n",
            "Iteration 4, loss = 0.24450278\n",
            "Iteration 5, loss = 0.23185507\n",
            "Iteration 6, loss = 0.22089293\n",
            "Iteration 7, loss = 0.21192616\n",
            "Iteration 8, loss = 0.20249960\n",
            "Iteration 9, loss = 0.19436833\n",
            "Iteration 10, loss = 0.18733122\n",
            "Iteration 11, loss = 0.18063729\n",
            "Iteration 12, loss = 0.17374810\n",
            "Iteration 13, loss = 0.16797008\n",
            "Iteration 14, loss = 0.16244779\n",
            "Iteration 15, loss = 0.15729350\n",
            "Iteration 16, loss = 0.15309399\n",
            "Iteration 17, loss = 0.14890210\n",
            "Iteration 18, loss = 0.14460875\n",
            "Iteration 19, loss = 0.14131780\n",
            "Iteration 20, loss = 0.13813380\n",
            "Iteration 21, loss = 0.13499138\n",
            "Iteration 22, loss = 0.13164248\n",
            "Iteration 23, loss = 0.12892690\n",
            "Iteration 24, loss = 0.12683657\n",
            "Iteration 25, loss = 0.12457354\n",
            "Iteration 26, loss = 0.12237540\n",
            "Iteration 27, loss = 0.12078963\n",
            "Iteration 28, loss = 0.11890522\n",
            "Iteration 29, loss = 0.11757532\n",
            "Iteration 30, loss = 0.11592612\n",
            "Iteration 31, loss = 0.11474502\n",
            "Iteration 32, loss = 0.11369462\n",
            "Iteration 33, loss = 0.11209508\n",
            "Iteration 34, loss = 0.11132815\n",
            "Iteration 35, loss = 0.11015851\n",
            "Iteration 36, loss = 0.10951668\n",
            "Iteration 37, loss = 0.10872707\n",
            "Iteration 38, loss = 0.10786821\n",
            "Iteration 39, loss = 0.10699126\n",
            "Iteration 40, loss = 0.10602023\n",
            "Iteration 41, loss = 0.10538692\n",
            "Iteration 42, loss = 0.10479372\n",
            "Iteration 43, loss = 0.10438298\n",
            "Iteration 44, loss = 0.10410348\n",
            "Iteration 45, loss = 0.10327885\n",
            "Iteration 46, loss = 0.10305602\n",
            "Iteration 47, loss = 0.10231411\n",
            "Iteration 48, loss = 0.10274807\n",
            "Iteration 49, loss = 0.10261421\n",
            "Iteration 50, loss = 0.10183263\n",
            "Iteration 51, loss = 0.10118259\n",
            "Iteration 52, loss = 0.10145828\n",
            "Iteration 53, loss = 0.10074996\n",
            "Iteration 54, loss = 0.10058580\n",
            "Iteration 55, loss = 0.09993708\n",
            "Iteration 56, loss = 0.10046707\n",
            "Iteration 57, loss = 0.10023513\n",
            "Iteration 58, loss = 0.09962615\n",
            "Iteration 59, loss = 0.09979570\n",
            "Iteration 60, loss = 0.09935629\n",
            "Iteration 61, loss = 0.09909096\n",
            "Iteration 62, loss = 0.09937677\n",
            "Iteration 63, loss = 0.10003689\n",
            "Iteration 64, loss = 0.09925093\n",
            "Iteration 65, loss = 0.09886224\n",
            "Iteration 66, loss = 0.09943958\n",
            "Iteration 67, loss = 0.09920594\n",
            "Iteration 68, loss = 0.09852747\n",
            "Iteration 69, loss = 0.09835998\n",
            "Iteration 70, loss = 0.09871557\n",
            "Iteration 71, loss = 0.09922146\n",
            "Iteration 72, loss = 0.09876859\n",
            "Iteration 73, loss = 0.09866115\n",
            "Iteration 74, loss = 0.09826731\n",
            "Iteration 75, loss = 0.09761888\n",
            "Iteration 76, loss = 0.09856285\n",
            "Iteration 77, loss = 0.09856361\n",
            "Iteration 78, loss = 0.09845758\n",
            "Iteration 79, loss = 0.09812287\n",
            "Iteration 80, loss = 0.09795233\n",
            "Iteration 81, loss = 0.09845441\n",
            "Iteration 82, loss = 0.09770494\n",
            "Iteration 83, loss = 0.09869808\n",
            "Iteration 84, loss = 0.09758425\n",
            "Iteration 85, loss = 0.09784563\n",
            "Iteration 86, loss = 0.09798398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 87, loss = 0.09634892\n",
            "Iteration 88, loss = 0.09616131\n",
            "Iteration 89, loss = 0.09641235\n",
            "Iteration 90, loss = 0.09591096\n",
            "Iteration 91, loss = 0.09617449\n",
            "Iteration 92, loss = 0.09637795\n",
            "Iteration 93, loss = 0.09623687\n",
            "Iteration 94, loss = 0.09631649\n",
            "Iteration 95, loss = 0.09652409\n",
            "Iteration 96, loss = 0.09597482\n",
            "Iteration 97, loss = 0.09621694\n",
            "Iteration 98, loss = 0.09616141\n",
            "Iteration 99, loss = 0.09597220\n",
            "Iteration 100, loss = 0.09580712\n",
            "Iteration 101, loss = 0.09612369\n",
            "Iteration 102, loss = 0.09590174\n",
            "Iteration 103, loss = 0.09594444\n",
            "Iteration 104, loss = 0.09629021\n",
            "Iteration 105, loss = 0.09621944\n",
            "Iteration 106, loss = 0.09607006\n",
            "Iteration 107, loss = 0.09590689\n",
            "Iteration 108, loss = 0.09584742\n",
            "Iteration 109, loss = 0.09601539\n",
            "Iteration 110, loss = 0.09586266\n",
            "Iteration 111, loss = 0.09591121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 112, loss = 0.09525626\n",
            "Iteration 113, loss = 0.09530855\n",
            "Iteration 114, loss = 0.09534684\n",
            "Iteration 115, loss = 0.09528477\n",
            "Iteration 116, loss = 0.09531687\n",
            "Iteration 117, loss = 0.09523153\n",
            "Iteration 118, loss = 0.09527854\n",
            "Iteration 119, loss = 0.09525390\n",
            "Iteration 120, loss = 0.09535631\n",
            "Iteration 121, loss = 0.09533445\n",
            "Iteration 122, loss = 0.09521732\n",
            "Iteration 123, loss = 0.09523517\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 124, loss = 0.09521528\n",
            "Iteration 125, loss = 0.09508759\n",
            "Iteration 126, loss = 0.09508566\n",
            "Iteration 127, loss = 0.09507344\n",
            "Iteration 128, loss = 0.09512238\n",
            "Iteration 129, loss = 0.09511067\n",
            "Iteration 130, loss = 0.09510019\n",
            "Iteration 131, loss = 0.09510742\n",
            "Iteration 132, loss = 0.09509573\n",
            "Iteration 133, loss = 0.09510665\n",
            "Iteration 134, loss = 0.09509714\n",
            "Iteration 135, loss = 0.09511838\n",
            "Iteration 136, loss = 0.09509129\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 137, loss = 0.09506510\n",
            "Iteration 138, loss = 0.09505728\n",
            "Iteration 139, loss = 0.09506445\n",
            "Iteration 140, loss = 0.09506031\n",
            "Iteration 141, loss = 0.09506296\n",
            "Iteration 142, loss = 0.09505673\n",
            "Iteration 143, loss = 0.09506402\n",
            "Iteration 144, loss = 0.09505977\n",
            "Iteration 145, loss = 0.09505617\n",
            "Iteration 146, loss = 0.09506018\n",
            "Iteration 147, loss = 0.09505852\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 148, loss = 0.09505052\n",
            "Iteration 149, loss = 0.09504994\n",
            "Iteration 150, loss = 0.09504807\n",
            "Iteration 151, loss = 0.09504863\n",
            "Iteration 152, loss = 0.09504727\n",
            "Iteration 153, loss = 0.09504746\n",
            "Iteration 154, loss = 0.09504771\n",
            "Iteration 155, loss = 0.09504845\n",
            "Iteration 156, loss = 0.09504754\n",
            "Iteration 157, loss = 0.09504825\n",
            "Iteration 158, loss = 0.09504775\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.26476460\n",
            "Iteration 2, loss = 0.21493733\n",
            "Iteration 3, loss = 0.20358271\n",
            "Iteration 4, loss = 0.19421132\n",
            "Iteration 5, loss = 0.18611211\n",
            "Iteration 6, loss = 0.17880977\n",
            "Iteration 7, loss = 0.17254010\n",
            "Iteration 8, loss = 0.16661431\n",
            "Iteration 9, loss = 0.16086874\n",
            "Iteration 10, loss = 0.15586055\n",
            "Iteration 11, loss = 0.15100761\n",
            "Iteration 12, loss = 0.14666390\n",
            "Iteration 13, loss = 0.14310810\n",
            "Iteration 14, loss = 0.13923199\n",
            "Iteration 15, loss = 0.13649698\n",
            "Iteration 16, loss = 0.13273513\n",
            "Iteration 17, loss = 0.13025359\n",
            "Iteration 18, loss = 0.12785717\n",
            "Iteration 19, loss = 0.12562552\n",
            "Iteration 20, loss = 0.12352760\n",
            "Iteration 21, loss = 0.12123082\n",
            "Iteration 22, loss = 0.11945433\n",
            "Iteration 23, loss = 0.11782398\n",
            "Iteration 24, loss = 0.11622525\n",
            "Iteration 25, loss = 0.11493053\n",
            "Iteration 26, loss = 0.11342709\n",
            "Iteration 27, loss = 0.11221646\n",
            "Iteration 28, loss = 0.11107309\n",
            "Iteration 29, loss = 0.10994994\n",
            "Iteration 30, loss = 0.10918415\n",
            "Iteration 31, loss = 0.10876908\n",
            "Iteration 32, loss = 0.10793920\n",
            "Iteration 33, loss = 0.10672034\n",
            "Iteration 34, loss = 0.10637706\n",
            "Iteration 35, loss = 0.10568857\n",
            "Iteration 36, loss = 0.10426056\n",
            "Iteration 37, loss = 0.10446400\n",
            "Iteration 38, loss = 0.10333734\n",
            "Iteration 39, loss = 0.10338362\n",
            "Iteration 40, loss = 0.10266450\n",
            "Iteration 41, loss = 0.10197816\n",
            "Iteration 42, loss = 0.10169457\n",
            "Iteration 43, loss = 0.10178216\n",
            "Iteration 44, loss = 0.10141184\n",
            "Iteration 45, loss = 0.10103290\n",
            "Iteration 46, loss = 0.10116065\n",
            "Iteration 47, loss = 0.10047572\n",
            "Iteration 48, loss = 0.10035032\n",
            "Iteration 49, loss = 0.09970201\n",
            "Iteration 50, loss = 0.10016005\n",
            "Iteration 51, loss = 0.10011943\n",
            "Iteration 52, loss = 0.09975452\n",
            "Iteration 53, loss = 0.09979806\n",
            "Iteration 54, loss = 0.09907493\n",
            "Iteration 55, loss = 0.09897073\n",
            "Iteration 56, loss = 0.09933661\n",
            "Iteration 57, loss = 0.09883533\n",
            "Iteration 58, loss = 0.09894810\n",
            "Iteration 59, loss = 0.09845339\n",
            "Iteration 60, loss = 0.09866513\n",
            "Iteration 61, loss = 0.09875611\n",
            "Iteration 62, loss = 0.09853935\n",
            "Iteration 63, loss = 0.09797181\n",
            "Iteration 64, loss = 0.09852462\n",
            "Iteration 65, loss = 0.09885710\n",
            "Iteration 66, loss = 0.09782671\n",
            "Iteration 67, loss = 0.09876052\n",
            "Iteration 68, loss = 0.09793901\n",
            "Iteration 69, loss = 0.09813545\n",
            "Iteration 70, loss = 0.09785018\n",
            "Iteration 71, loss = 0.09821235\n",
            "Iteration 72, loss = 0.09791832\n",
            "Iteration 73, loss = 0.09818043\n",
            "Iteration 74, loss = 0.09797758\n",
            "Iteration 75, loss = 0.09823039\n",
            "Iteration 76, loss = 0.09763122\n",
            "Iteration 77, loss = 0.09794692\n",
            "Iteration 78, loss = 0.09772768\n",
            "Iteration 79, loss = 0.09826644\n",
            "Iteration 80, loss = 0.09808206\n",
            "Iteration 81, loss = 0.09808145\n",
            "Iteration 82, loss = 0.09784673\n",
            "Iteration 83, loss = 0.09801910\n",
            "Iteration 84, loss = 0.09699596\n",
            "Iteration 85, loss = 0.09797055\n",
            "Iteration 86, loss = 0.09767442\n",
            "Iteration 87, loss = 0.09819736\n",
            "Iteration 88, loss = 0.09780483\n",
            "Iteration 89, loss = 0.09782610\n",
            "Iteration 90, loss = 0.09753182\n",
            "Iteration 91, loss = 0.09784382\n",
            "Iteration 92, loss = 0.09733742\n",
            "Iteration 93, loss = 0.09821757\n",
            "Iteration 94, loss = 0.09758532\n",
            "Iteration 95, loss = 0.09790322\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 96, loss = 0.09579655\n",
            "Iteration 97, loss = 0.09592515\n",
            "Iteration 98, loss = 0.09578298\n",
            "Iteration 99, loss = 0.09595290\n",
            "Iteration 100, loss = 0.09594180\n",
            "Iteration 101, loss = 0.09605547\n",
            "Iteration 102, loss = 0.09590696\n",
            "Iteration 103, loss = 0.09589385\n",
            "Iteration 104, loss = 0.09578629\n",
            "Iteration 105, loss = 0.09560546\n",
            "Iteration 106, loss = 0.09580392\n",
            "Iteration 107, loss = 0.09606395\n",
            "Iteration 108, loss = 0.09608351\n",
            "Iteration 109, loss = 0.09587571\n",
            "Iteration 110, loss = 0.09590793\n",
            "Iteration 111, loss = 0.09559366\n",
            "Iteration 112, loss = 0.09568118\n",
            "Iteration 113, loss = 0.09589691\n",
            "Iteration 114, loss = 0.09593216\n",
            "Iteration 115, loss = 0.09554314\n",
            "Iteration 116, loss = 0.09608226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 117, loss = 0.09510809\n",
            "Iteration 118, loss = 0.09522459\n",
            "Iteration 119, loss = 0.09501837\n",
            "Iteration 120, loss = 0.09524858\n",
            "Iteration 121, loss = 0.09510117\n",
            "Iteration 122, loss = 0.09502335\n",
            "Iteration 123, loss = 0.09507392\n",
            "Iteration 124, loss = 0.09507914\n",
            "Iteration 125, loss = 0.09507818\n",
            "Iteration 126, loss = 0.09515010\n",
            "Iteration 127, loss = 0.09512663\n",
            "Iteration 128, loss = 0.09514907\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 129, loss = 0.09490313\n",
            "Iteration 130, loss = 0.09494661\n",
            "Iteration 131, loss = 0.09492031\n",
            "Iteration 132, loss = 0.09493251\n",
            "Iteration 133, loss = 0.09491499\n",
            "Iteration 134, loss = 0.09490147\n",
            "Iteration 135, loss = 0.09491987\n",
            "Iteration 136, loss = 0.09489008\n",
            "Iteration 137, loss = 0.09493244\n",
            "Iteration 138, loss = 0.09491919\n",
            "Iteration 139, loss = 0.09494188\n",
            "Iteration 140, loss = 0.09491502\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 141, loss = 0.09490949\n",
            "Iteration 142, loss = 0.09488339\n",
            "Iteration 143, loss = 0.09488120\n",
            "Iteration 144, loss = 0.09487548\n",
            "Iteration 145, loss = 0.09487512\n",
            "Iteration 146, loss = 0.09488338\n",
            "Iteration 147, loss = 0.09487989\n",
            "Iteration 148, loss = 0.09487609\n",
            "Iteration 149, loss = 0.09487743\n",
            "Iteration 150, loss = 0.09488872\n",
            "Iteration 151, loss = 0.09487886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 152, loss = 0.09487536\n",
            "Iteration 153, loss = 0.09487043\n",
            "Iteration 154, loss = 0.09486878\n",
            "Iteration 155, loss = 0.09486942\n",
            "Iteration 156, loss = 0.09486929\n",
            "Iteration 157, loss = 0.09486865\n",
            "Iteration 158, loss = 0.09487058\n",
            "Iteration 159, loss = 0.09486977\n",
            "Iteration 160, loss = 0.09486849\n",
            "Iteration 161, loss = 0.09486952\n",
            "Iteration 162, loss = 0.09486810\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.29099367\n",
            "Iteration 3, loss = 0.27596251\n",
            "Iteration 4, loss = 0.26208295\n",
            "Iteration 5, loss = 0.24855153\n",
            "Iteration 6, loss = 0.23654062\n",
            "Iteration 7, loss = 0.22548042\n",
            "Iteration 8, loss = 0.21567151\n",
            "Iteration 9, loss = 0.20638570\n",
            "Iteration 10, loss = 0.19757903\n",
            "Iteration 11, loss = 0.19027747\n",
            "Iteration 12, loss = 0.18284096\n",
            "Iteration 13, loss = 0.17604387\n",
            "Iteration 14, loss = 0.16974961\n",
            "Iteration 15, loss = 0.16424738\n",
            "Iteration 16, loss = 0.15915521\n",
            "Iteration 17, loss = 0.15476302\n",
            "Iteration 18, loss = 0.15016935\n",
            "Iteration 19, loss = 0.14651872\n",
            "Iteration 20, loss = 0.14234377\n",
            "Iteration 21, loss = 0.13916984\n",
            "Iteration 22, loss = 0.13600641\n",
            "Iteration 23, loss = 0.13363826\n",
            "Iteration 24, loss = 0.12999834\n",
            "Iteration 25, loss = 0.12787230\n",
            "Iteration 26, loss = 0.12516705\n",
            "Iteration 27, loss = 0.12314784\n",
            "Iteration 28, loss = 0.12191591\n",
            "Iteration 29, loss = 0.11962264\n",
            "Iteration 30, loss = 0.11771531\n",
            "Iteration 31, loss = 0.11676592\n",
            "Iteration 32, loss = 0.11573276\n",
            "Iteration 33, loss = 0.11399949\n",
            "Iteration 34, loss = 0.11276483\n",
            "Iteration 35, loss = 0.11168987\n",
            "Iteration 36, loss = 0.11076191\n",
            "Iteration 37, loss = 0.10918234\n",
            "Iteration 38, loss = 0.10834173\n",
            "Iteration 39, loss = 0.10766949\n",
            "Iteration 40, loss = 0.10612929\n",
            "Iteration 41, loss = 0.10642804\n",
            "Iteration 42, loss = 0.10552129\n",
            "Iteration 43, loss = 0.10520953\n",
            "Iteration 44, loss = 0.10436756\n",
            "Iteration 45, loss = 0.10430515\n",
            "Iteration 46, loss = 0.10422296\n",
            "Iteration 47, loss = 0.10321753\n",
            "Iteration 48, loss = 0.10308433\n",
            "Iteration 49, loss = 0.10281091\n",
            "Iteration 50, loss = 0.10161146\n",
            "Iteration 51, loss = 0.10176286\n",
            "Iteration 52, loss = 0.10102624\n",
            "Iteration 53, loss = 0.10126012\n",
            "Iteration 54, loss = 0.10077006\n",
            "Iteration 55, loss = 0.10028220\n",
            "Iteration 56, loss = 0.10114811\n",
            "Iteration 57, loss = 0.09980071\n",
            "Iteration 58, loss = 0.09995973\n",
            "Iteration 59, loss = 0.09978473\n",
            "Iteration 60, loss = 0.10021116\n",
            "Iteration 61, loss = 0.09944775\n",
            "Iteration 62, loss = 0.09946458\n",
            "Iteration 63, loss = 0.09900815\n",
            "Iteration 64, loss = 0.09908886\n",
            "Iteration 65, loss = 0.09903726\n",
            "Iteration 66, loss = 0.09920535\n",
            "Iteration 67, loss = 0.09909777\n",
            "Iteration 68, loss = 0.09841607\n",
            "Iteration 69, loss = 0.09906858\n",
            "Iteration 70, loss = 0.09870942\n",
            "Iteration 71, loss = 0.09860237\n",
            "Iteration 72, loss = 0.09843799\n",
            "Iteration 73, loss = 0.09855468\n",
            "Iteration 74, loss = 0.09882149\n",
            "Iteration 75, loss = 0.09813428\n",
            "Iteration 76, loss = 0.09825569\n",
            "Iteration 77, loss = 0.09813271\n",
            "Iteration 78, loss = 0.09802727\n",
            "Iteration 79, loss = 0.09819081\n",
            "Iteration 80, loss = 0.09814069\n",
            "Iteration 81, loss = 0.09767911\n",
            "Iteration 82, loss = 0.09786251\n",
            "Iteration 83, loss = 0.09835922\n",
            "Iteration 84, loss = 0.09802116\n",
            "Iteration 85, loss = 0.09869810\n",
            "Iteration 86, loss = 0.09877925\n",
            "Iteration 87, loss = 0.09767433\n",
            "Iteration 88, loss = 0.09752834\n",
            "Iteration 89, loss = 0.09812118\n",
            "Iteration 90, loss = 0.09785533\n",
            "Iteration 91, loss = 0.09817584\n",
            "Iteration 92, loss = 0.09784776\n",
            "Iteration 93, loss = 0.09796794\n",
            "Iteration 94, loss = 0.09773025\n",
            "Iteration 95, loss = 0.09720504\n",
            "Iteration 96, loss = 0.09772975\n",
            "Iteration 97, loss = 0.09760790\n",
            "Iteration 98, loss = 0.09750561\n",
            "Iteration 99, loss = 0.09790189\n",
            "Iteration 100, loss = 0.09792347\n",
            "Iteration 101, loss = 0.09776892\n",
            "Iteration 102, loss = 0.09805476\n",
            "Iteration 103, loss = 0.09767820\n",
            "Iteration 104, loss = 0.09826695\n",
            "Iteration 105, loss = 0.09810676\n",
            "Iteration 106, loss = 0.09790874\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 107, loss = 0.09659440\n",
            "Iteration 108, loss = 0.09587861\n",
            "Iteration 109, loss = 0.09573950\n",
            "Iteration 110, loss = 0.09593418\n",
            "Iteration 111, loss = 0.09579583\n",
            "Iteration 112, loss = 0.09589268\n",
            "Iteration 113, loss = 0.09588440\n",
            "Iteration 114, loss = 0.09585528\n",
            "Iteration 115, loss = 0.09576994\n",
            "Iteration 116, loss = 0.09589629\n",
            "Iteration 117, loss = 0.09603651\n",
            "Iteration 118, loss = 0.09567354\n",
            "Iteration 119, loss = 0.09581100\n",
            "Iteration 120, loss = 0.09571235\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 121, loss = 0.09520234\n",
            "Iteration 122, loss = 0.09512579\n",
            "Iteration 123, loss = 0.09525060\n",
            "Iteration 124, loss = 0.09514825\n",
            "Iteration 125, loss = 0.09512524\n",
            "Iteration 126, loss = 0.09523088\n",
            "Iteration 127, loss = 0.09515896\n",
            "Iteration 128, loss = 0.09509293\n",
            "Iteration 129, loss = 0.09527420\n",
            "Iteration 130, loss = 0.09516314\n",
            "Iteration 131, loss = 0.09519763\n",
            "Iteration 132, loss = 0.09515112\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 133, loss = 0.09504945\n",
            "Iteration 134, loss = 0.09500520\n",
            "Iteration 135, loss = 0.09499125\n",
            "Iteration 136, loss = 0.09501402\n",
            "Iteration 137, loss = 0.09500941\n",
            "Iteration 138, loss = 0.09504341\n",
            "Iteration 139, loss = 0.09499277\n",
            "Iteration 140, loss = 0.09498402\n",
            "Iteration 141, loss = 0.09493042\n",
            "Iteration 142, loss = 0.09499783\n",
            "Iteration 143, loss = 0.09496591\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 144, loss = 0.09497652\n",
            "Iteration 145, loss = 0.09494511\n",
            "Iteration 146, loss = 0.09494446\n",
            "Iteration 147, loss = 0.09494476\n",
            "Iteration 148, loss = 0.09495384\n",
            "Iteration 149, loss = 0.09495119\n",
            "Iteration 150, loss = 0.09494976\n",
            "Iteration 151, loss = 0.09494378\n",
            "Iteration 152, loss = 0.09494652\n",
            "Iteration 153, loss = 0.09494474\n",
            "Iteration 154, loss = 0.09494565\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 155, loss = 0.09493919\n",
            "Iteration 156, loss = 0.09493724\n",
            "Iteration 157, loss = 0.09493848\n",
            "Iteration 158, loss = 0.09493796\n",
            "Iteration 159, loss = 0.09493783\n",
            "Iteration 160, loss = 0.09493753\n",
            "Iteration 161, loss = 0.09493710\n",
            "Iteration 162, loss = 0.09493691\n",
            "Iteration 163, loss = 0.09493656\n",
            "Iteration 164, loss = 0.09493671\n",
            "Iteration 165, loss = 0.09493707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.65504810\n",
            "Iteration 2, loss = 0.22132212\n",
            "Iteration 3, loss = 0.20840001\n",
            "Iteration 4, loss = 0.19146278\n",
            "Iteration 5, loss = 0.18349473\n",
            "Iteration 6, loss = 0.17512795\n",
            "Iteration 7, loss = 0.16794323\n",
            "Iteration 8, loss = 0.16228427\n",
            "Iteration 9, loss = 0.15684510\n",
            "Iteration 10, loss = 0.15148051\n",
            "Iteration 11, loss = 0.14695253\n",
            "Iteration 12, loss = 0.14389642\n",
            "Iteration 13, loss = 0.13901321\n",
            "Iteration 14, loss = 0.13578387\n",
            "Iteration 15, loss = 0.13326156\n",
            "Iteration 16, loss = 0.13064218\n",
            "Iteration 17, loss = 0.12831317\n",
            "Iteration 18, loss = 0.12591286\n",
            "Iteration 19, loss = 0.12374058\n",
            "Iteration 20, loss = 0.12183660\n",
            "Iteration 21, loss = 0.11914768\n",
            "Iteration 22, loss = 0.11885446\n",
            "Iteration 23, loss = 0.11692481\n",
            "Iteration 24, loss = 0.11546729\n",
            "Iteration 25, loss = 0.11417764\n",
            "Iteration 26, loss = 0.11288313\n",
            "Iteration 27, loss = 0.11206187\n",
            "Iteration 28, loss = 0.11107571\n",
            "Iteration 29, loss = 0.10966206\n",
            "Iteration 30, loss = 0.10862636\n",
            "Iteration 31, loss = 0.10839543\n",
            "Iteration 32, loss = 0.10776987\n",
            "Iteration 33, loss = 0.10690600\n",
            "Iteration 34, loss = 0.10595574\n",
            "Iteration 35, loss = 0.10563132\n",
            "Iteration 36, loss = 0.10508123\n",
            "Iteration 37, loss = 0.10472106\n",
            "Iteration 38, loss = 0.10424290\n",
            "Iteration 39, loss = 0.10409543\n",
            "Iteration 40, loss = 0.10336304\n",
            "Iteration 41, loss = 0.10239513\n",
            "Iteration 42, loss = 0.10257611\n",
            "Iteration 43, loss = 0.10204965\n",
            "Iteration 44, loss = 0.10283155\n",
            "Iteration 45, loss = 0.10199113\n",
            "Iteration 46, loss = 0.10140889\n",
            "Iteration 47, loss = 0.10127941\n",
            "Iteration 48, loss = 0.10086821\n",
            "Iteration 49, loss = 0.10124203\n",
            "Iteration 50, loss = 0.10039432\n",
            "Iteration 51, loss = 0.10092178\n",
            "Iteration 52, loss = 0.10070686\n",
            "Iteration 53, loss = 0.10060418\n",
            "Iteration 54, loss = 0.10038452\n",
            "Iteration 55, loss = 0.09978394\n",
            "Iteration 56, loss = 0.09976869\n",
            "Iteration 57, loss = 0.09989147\n",
            "Iteration 58, loss = 0.10032323\n",
            "Iteration 59, loss = 0.09987215\n",
            "Iteration 60, loss = 0.09993097\n",
            "Iteration 61, loss = 0.09952761\n",
            "Iteration 62, loss = 0.09922906\n",
            "Iteration 63, loss = 0.09961572\n",
            "Iteration 64, loss = 0.09895425\n",
            "Iteration 65, loss = 0.09925603\n",
            "Iteration 66, loss = 0.09931912\n",
            "Iteration 67, loss = 0.09899799\n",
            "Iteration 68, loss = 0.09907136\n",
            "Iteration 69, loss = 0.09923565\n",
            "Iteration 70, loss = 0.09884311\n",
            "Iteration 71, loss = 0.09896900\n",
            "Iteration 72, loss = 0.09886557\n",
            "Iteration 73, loss = 0.09871875\n",
            "Iteration 74, loss = 0.09906819\n",
            "Iteration 75, loss = 0.09821061\n",
            "Iteration 76, loss = 0.09895923\n",
            "Iteration 77, loss = 0.09864815\n",
            "Iteration 78, loss = 0.09866386\n",
            "Iteration 79, loss = 0.09836211\n",
            "Iteration 80, loss = 0.09871643\n",
            "Iteration 81, loss = 0.09923002\n",
            "Iteration 82, loss = 0.09843025\n",
            "Iteration 83, loss = 0.09841624\n",
            "Iteration 84, loss = 0.09862161\n",
            "Iteration 85, loss = 0.09849491\n",
            "Iteration 86, loss = 0.09834749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 87, loss = 0.09722040\n",
            "Iteration 88, loss = 0.09689669\n",
            "Iteration 89, loss = 0.09708686\n",
            "Iteration 90, loss = 0.09676495\n",
            "Iteration 91, loss = 0.09649067\n",
            "Iteration 92, loss = 0.09640791\n",
            "Iteration 93, loss = 0.09660687\n",
            "Iteration 94, loss = 0.09660844\n",
            "Iteration 95, loss = 0.09663003\n",
            "Iteration 96, loss = 0.09677320\n",
            "Iteration 97, loss = 0.09655572\n",
            "Iteration 98, loss = 0.09667976\n",
            "Iteration 99, loss = 0.09669185\n",
            "Iteration 100, loss = 0.09644087\n",
            "Iteration 101, loss = 0.09664121\n",
            "Iteration 102, loss = 0.09637265\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 103, loss = 0.09667184\n",
            "Iteration 104, loss = 0.09600725\n",
            "Iteration 105, loss = 0.09601717\n",
            "Iteration 106, loss = 0.09595449\n",
            "Iteration 107, loss = 0.09601543\n",
            "Iteration 108, loss = 0.09601732\n",
            "Iteration 109, loss = 0.09598199\n",
            "Iteration 110, loss = 0.09585331\n",
            "Iteration 111, loss = 0.09606533\n",
            "Iteration 112, loss = 0.09590141\n",
            "Iteration 113, loss = 0.09593358\n",
            "Iteration 114, loss = 0.09583953\n",
            "Iteration 115, loss = 0.09594561\n",
            "Iteration 116, loss = 0.09595967\n",
            "Iteration 117, loss = 0.09586951\n",
            "Iteration 118, loss = 0.09595388\n",
            "Iteration 119, loss = 0.09592772\n",
            "Iteration 120, loss = 0.09587366\n",
            "Iteration 121, loss = 0.09587934\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 122, loss = 0.09584299\n",
            "Iteration 123, loss = 0.09573766\n",
            "Iteration 124, loss = 0.09575872\n",
            "Iteration 125, loss = 0.09574726\n",
            "Iteration 126, loss = 0.09576959\n",
            "Iteration 127, loss = 0.09570938\n",
            "Iteration 128, loss = 0.09575254\n",
            "Iteration 129, loss = 0.09574661\n",
            "Iteration 130, loss = 0.09579448\n",
            "Iteration 131, loss = 0.09573212\n",
            "Iteration 132, loss = 0.09574703\n",
            "Iteration 133, loss = 0.09577075\n",
            "Iteration 134, loss = 0.09575602\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 135, loss = 0.09571268\n",
            "Iteration 136, loss = 0.09571424\n",
            "Iteration 137, loss = 0.09570601\n",
            "Iteration 138, loss = 0.09570519\n",
            "Iteration 139, loss = 0.09570062\n",
            "Iteration 140, loss = 0.09570879\n",
            "Iteration 141, loss = 0.09570425\n",
            "Iteration 142, loss = 0.09570917\n",
            "Iteration 143, loss = 0.09570643\n",
            "Iteration 144, loss = 0.09570620\n",
            "Iteration 145, loss = 0.09570353\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 146, loss = 0.09569448\n",
            "Iteration 147, loss = 0.09569359\n",
            "Iteration 148, loss = 0.09569418\n",
            "Iteration 149, loss = 0.09569268\n",
            "Iteration 150, loss = 0.09569379\n",
            "Iteration 151, loss = 0.09569329\n",
            "Iteration 152, loss = 0.09569315\n",
            "Iteration 153, loss = 0.09569349\n",
            "Iteration 154, loss = 0.09569282\n",
            "Iteration 155, loss = 0.09569227\n",
            "Iteration 156, loss = 0.09569418\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.98585571\n",
            "Iteration 2, loss = 0.26226663\n",
            "Iteration 3, loss = 0.24909162\n",
            "Iteration 4, loss = 0.23727451\n",
            "Iteration 5, loss = 0.22522009\n",
            "Iteration 6, loss = 0.21478508\n",
            "Iteration 7, loss = 0.20533992\n",
            "Iteration 8, loss = 0.19673809\n",
            "Iteration 9, loss = 0.18945869\n",
            "Iteration 10, loss = 0.18199284\n",
            "Iteration 11, loss = 0.17555458\n",
            "Iteration 12, loss = 0.16985859\n",
            "Iteration 13, loss = 0.16440868\n",
            "Iteration 14, loss = 0.15882870\n",
            "Iteration 15, loss = 0.15429758\n",
            "Iteration 16, loss = 0.14991764\n",
            "Iteration 17, loss = 0.14615709\n",
            "Iteration 18, loss = 0.14167351\n",
            "Iteration 19, loss = 0.13825053\n",
            "Iteration 20, loss = 0.13558530\n",
            "Iteration 21, loss = 0.13234506\n",
            "Iteration 22, loss = 0.13004950\n",
            "Iteration 23, loss = 0.12778358\n",
            "Iteration 24, loss = 0.12525598\n",
            "Iteration 25, loss = 0.12310871\n",
            "Iteration 26, loss = 0.12119120\n",
            "Iteration 27, loss = 0.11910519\n",
            "Iteration 28, loss = 0.11747192\n",
            "Iteration 29, loss = 0.11608365\n",
            "Iteration 30, loss = 0.11473910\n",
            "Iteration 31, loss = 0.11388660\n",
            "Iteration 32, loss = 0.11236201\n",
            "Iteration 33, loss = 0.11097841\n",
            "Iteration 34, loss = 0.11044023\n",
            "Iteration 35, loss = 0.10908052\n",
            "Iteration 36, loss = 0.10793516\n",
            "Iteration 37, loss = 0.10710011\n",
            "Iteration 38, loss = 0.10657282\n",
            "Iteration 39, loss = 0.10561306\n",
            "Iteration 40, loss = 0.10612637\n",
            "Iteration 41, loss = 0.10512276\n",
            "Iteration 42, loss = 0.10394618\n",
            "Iteration 43, loss = 0.10431318\n",
            "Iteration 44, loss = 0.10350618\n",
            "Iteration 45, loss = 0.10318891\n",
            "Iteration 46, loss = 0.10269328\n",
            "Iteration 47, loss = 0.10246318\n",
            "Iteration 48, loss = 0.10179224\n",
            "Iteration 49, loss = 0.10143039\n",
            "Iteration 50, loss = 0.10103914\n",
            "Iteration 51, loss = 0.10156950\n",
            "Iteration 52, loss = 0.10089443\n",
            "Iteration 53, loss = 0.10024977\n",
            "Iteration 54, loss = 0.10020576\n",
            "Iteration 55, loss = 0.10078725\n",
            "Iteration 56, loss = 0.10040673\n",
            "Iteration 57, loss = 0.09950439\n",
            "Iteration 58, loss = 0.09951675\n",
            "Iteration 59, loss = 0.09971630\n",
            "Iteration 60, loss = 0.09957972\n",
            "Iteration 61, loss = 0.09914696\n",
            "Iteration 62, loss = 0.09932132\n",
            "Iteration 63, loss = 0.09863099\n",
            "Iteration 64, loss = 0.09866406\n",
            "Iteration 65, loss = 0.09857511\n",
            "Iteration 66, loss = 0.09905532\n",
            "Iteration 67, loss = 0.09819078\n",
            "Iteration 68, loss = 0.09819763\n",
            "Iteration 69, loss = 0.09853906\n",
            "Iteration 70, loss = 0.09833512\n",
            "Iteration 71, loss = 0.09901122\n",
            "Iteration 72, loss = 0.09828301\n",
            "Iteration 73, loss = 0.09853374\n",
            "Iteration 74, loss = 0.09825337\n",
            "Iteration 75, loss = 0.09835406\n",
            "Iteration 76, loss = 0.09830753\n",
            "Iteration 77, loss = 0.09842394\n",
            "Iteration 78, loss = 0.09820597\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 79, loss = 0.09703302\n",
            "Iteration 80, loss = 0.09629307\n",
            "Iteration 81, loss = 0.09627446\n",
            "Iteration 82, loss = 0.09642267\n",
            "Iteration 83, loss = 0.09631952\n",
            "Iteration 84, loss = 0.09647864\n",
            "Iteration 85, loss = 0.09633468\n",
            "Iteration 86, loss = 0.09631072\n",
            "Iteration 87, loss = 0.09616674\n",
            "Iteration 88, loss = 0.09608422\n",
            "Iteration 89, loss = 0.09622607\n",
            "Iteration 90, loss = 0.09624249\n",
            "Iteration 91, loss = 0.09616371\n",
            "Iteration 92, loss = 0.09627827\n",
            "Iteration 93, loss = 0.09606122\n",
            "Iteration 94, loss = 0.09636426\n",
            "Iteration 95, loss = 0.09628676\n",
            "Iteration 96, loss = 0.09630974\n",
            "Iteration 97, loss = 0.09636068\n",
            "Iteration 98, loss = 0.09614404\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 99, loss = 0.09567153\n",
            "Iteration 100, loss = 0.09556794\n",
            "Iteration 101, loss = 0.09548774\n",
            "Iteration 102, loss = 0.09561169\n",
            "Iteration 103, loss = 0.09556021\n",
            "Iteration 104, loss = 0.09561451\n",
            "Iteration 105, loss = 0.09550391\n",
            "Iteration 106, loss = 0.09545071\n",
            "Iteration 107, loss = 0.09552168\n",
            "Iteration 108, loss = 0.09551848\n",
            "Iteration 109, loss = 0.09549753\n",
            "Iteration 110, loss = 0.09546378\n",
            "Iteration 111, loss = 0.09552749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 112, loss = 0.09542056\n",
            "Iteration 113, loss = 0.09540569\n",
            "Iteration 114, loss = 0.09536274\n",
            "Iteration 115, loss = 0.09536204\n",
            "Iteration 116, loss = 0.09537329\n",
            "Iteration 117, loss = 0.09536097\n",
            "Iteration 118, loss = 0.09537033\n",
            "Iteration 119, loss = 0.09535054\n",
            "Iteration 120, loss = 0.09534605\n",
            "Iteration 121, loss = 0.09537167\n",
            "Iteration 122, loss = 0.09538885\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 123, loss = 0.09533520\n",
            "Iteration 124, loss = 0.09533577\n",
            "Iteration 125, loss = 0.09533203\n",
            "Iteration 126, loss = 0.09534050\n",
            "Iteration 127, loss = 0.09532863\n",
            "Iteration 128, loss = 0.09533472\n",
            "Iteration 129, loss = 0.09533062\n",
            "Iteration 130, loss = 0.09533369\n",
            "Iteration 131, loss = 0.09533022\n",
            "Iteration 132, loss = 0.09533202\n",
            "Iteration 133, loss = 0.09533101\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 134, loss = 0.09532244\n",
            "Iteration 135, loss = 0.09532219\n",
            "Iteration 136, loss = 0.09532321\n",
            "Iteration 137, loss = 0.09532301\n",
            "Iteration 138, loss = 0.09532295\n",
            "Iteration 139, loss = 0.09532199\n",
            "Iteration 140, loss = 0.09532241\n",
            "Iteration 141, loss = 0.09532281\n",
            "Iteration 142, loss = 0.09532207\n",
            "Iteration 143, loss = 0.09532137\n",
            "Iteration 144, loss = 0.09532319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33835    12]\n",
            " [ 2853   921]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96     33847\n",
            "           1       0.99      0.24      0.39      3774\n",
            "\n",
            "    accuracy                           0.92     37621\n",
            "   macro avg       0.95      0.62      0.68     37621\n",
            "weighted avg       0.93      0.92      0.90     37621\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 97.472% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 97.519% (0.000%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rkaZ6ElkOd2",
        "colab_type": "code",
        "outputId": "c913436d-d4d4-455d-fd05-f1e1f567d73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"----------------------------------\")\n",
        "print(confusion_matrix(y_test,y_te_pred))\n",
        "print(\"----------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "[[33835    12]\n",
            " [ 2853   921]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76jASFS4kZwF",
        "colab_type": "code",
        "outputId": "1792bba7-cbc0-4ecf-8980-e7a66c99a209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print(classification_report(y_test,y_te_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96     33847\n",
            "           1       0.99      0.24      0.39      3774\n",
            "\n",
            "    accuracy                           0.92     37621\n",
            "   macro avg       0.95      0.62      0.68     37621\n",
            "weighted avg       0.93      0.92      0.90     37621\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7JWkF3OXpdR",
        "colab_type": "code",
        "outputId": "062bba07-5d79-44ba-8924-2f6bac847927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "iterations = list(range(epochs))\n",
        "plt.plot(iterations, training_accuracy, label='Train')\n",
        "plt.plot(iterations, testing_accuracy, label='Test')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESrFIiIiGiVAhEREa1SICIiotX/AJlsLn5+4fxkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}