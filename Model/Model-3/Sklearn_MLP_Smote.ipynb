{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sklearn_MLP_Smote.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "37P7Nn8j8-80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVO9ZrVq9INN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,KFold,ShuffleSplit,cross_val_score,StratifiedShuffleSplit\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaXht0V79JfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"drive/My Drive/Thesis/Wildfire.csv\")\n",
        "X = data.loc[:,['NDVI','LST','Burned_Area','SM','SUSM']].values.astype('float32')\n",
        "nor_X = preprocessing.normalize(X)\n",
        "Y = data.loc[:,['Class']].values.astype('int')\n",
        "labelencoder_y_1 = LabelEncoder()\n",
        "Y = labelencoder_y_1.fit_transform(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQwXJHAu9Lxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.value_counts(data['Class']).plot.bar()\n",
        "plt.title('class histogram')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='upper right')\n",
        "pd.value_counts(data['Class'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whgjVY8n9SW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm = SMOTE(random_state=42,ratio=0.5)\n",
        "X_OS, Y_OS = sm.fit_sample(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggh_IY26AByv",
        "colab_type": "code",
        "outputId": "705ed065-bf98-4517-ba67-43b3e4bbfe81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "print('Resampled dataset shape %s' % Counter(Y_OS))\n",
        "\n",
        "pd.value_counts(Y_OS).plot.bar()\n",
        "plt.title('class histogram')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 168819, 1: 84409})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtJilQ1t9P6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1080)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_OS,Y_OS,test_size = 0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCBHuAnaA2_P",
        "colab_type": "code",
        "outputId": "74437890-42f3-4a36-c1ac-6a9ae0d78a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "kfold = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=12)\n",
        "training_accuracy = []\n",
        "testing_accuracy = []\n",
        "epochs=10\n",
        "for train, test in kfold.split(X_train, y_train):\n",
        "\n",
        "  clf = MLPClassifier(solver='sgd',learning_rate='adaptive',momentum=0.9, activation='relu',alpha=5, batch_size='auto',verbose=True,n_iter_no_change = epochs)\n",
        "  clf.fit(X_train,y_train)\n",
        "  model = clf\n",
        "  \n",
        "  #training accuracy\n",
        "  y_tr_pred = clf.predict(X_train)\n",
        "  results_tr = cross_val_score(model, X_train ,y_tr_pred, cv = kfold,verbose=1)\n",
        "  training_accuracy.append(results_tr.max()*100.0)\n",
        "  \n",
        "  #testing Accuracy\n",
        "  y_te_pred = clf.predict(X_test)\n",
        "  results = cross_val_score(model, X_test ,y_te_pred, cv = kfold,verbose=1)\n",
        "  testing_accuracy.append(results.max()*100.0)\n",
        "  \n",
        "  print(\"Training Accuracy (Shuffle Split) : %.3f%% (%.3f%%)\" % (results_tr.max()*100.0, results_tr.std()*100.0))\n",
        "  print(\"Prediction Accuracy (Shuffle Split) : %.3f%% (%.3f%%)\" % (results.max()*100.0, results.std()*100.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65132261\n",
            "Iteration 3, loss = 0.62284825\n",
            "Iteration 4, loss = 0.60659814\n",
            "Iteration 5, loss = 0.59754426\n",
            "Iteration 6, loss = 0.59120979\n",
            "Iteration 7, loss = 0.58969643\n",
            "Iteration 8, loss = 0.62144730\n",
            "Iteration 9, loss = 0.60139514\n",
            "Iteration 10, loss = 0.59349927\n",
            "Iteration 11, loss = 0.59136687\n",
            "Iteration 12, loss = 0.64772722\n",
            "Iteration 13, loss = 0.63227082\n",
            "Iteration 14, loss = 0.60435107\n",
            "Iteration 15, loss = 0.59495870\n",
            "Iteration 16, loss = 0.58831552\n",
            "Iteration 17, loss = 0.58629542\n",
            "Iteration 18, loss = 0.58424509\n",
            "Iteration 19, loss = 0.60536018\n",
            "Iteration 20, loss = 0.59601182\n",
            "Iteration 21, loss = 0.58987849\n",
            "Iteration 22, loss = 0.58573901\n",
            "Iteration 23, loss = 0.60023582\n",
            "Iteration 24, loss = 0.60818039\n",
            "Iteration 25, loss = 0.60030878\n",
            "Iteration 26, loss = 0.59129361\n",
            "Iteration 27, loss = 0.58760845\n",
            "Iteration 28, loss = 0.58601199\n",
            "Iteration 29, loss = 0.58581587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.56143103\n",
            "Iteration 31, loss = 0.54956258\n",
            "Iteration 32, loss = 0.54237089\n",
            "Iteration 33, loss = 0.54960081\n",
            "Iteration 34, loss = 0.54933880\n",
            "Iteration 35, loss = 0.59865668\n",
            "Iteration 36, loss = 0.63315692\n",
            "Iteration 37, loss = 0.61689986\n",
            "Iteration 38, loss = 0.60036772\n",
            "Iteration 39, loss = 0.59192383\n",
            "Iteration 40, loss = 0.58466551\n",
            "Iteration 41, loss = 0.57803817\n",
            "Iteration 42, loss = 0.57094234\n",
            "Iteration 43, loss = 0.57116742\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.55649900\n",
            "Iteration 45, loss = 0.55243489\n",
            "Iteration 46, loss = 0.54853088\n",
            "Iteration 47, loss = 0.54441144\n",
            "Iteration 48, loss = 0.54022978\n",
            "Iteration 49, loss = 0.53576407\n",
            "Iteration 50, loss = 0.53102132\n",
            "Iteration 51, loss = 0.52626792\n",
            "Iteration 52, loss = 0.52114262\n",
            "Iteration 53, loss = 0.51663588\n",
            "Iteration 54, loss = 0.51127828\n",
            "Iteration 55, loss = 0.50595596\n",
            "Iteration 56, loss = 0.50069251\n",
            "Iteration 57, loss = 0.49571776\n",
            "Iteration 58, loss = 0.49053936\n",
            "Iteration 59, loss = 0.48543474\n",
            "Iteration 60, loss = 0.48107563\n",
            "Iteration 61, loss = 0.47660557\n",
            "Iteration 62, loss = 0.47316349\n",
            "Iteration 63, loss = 0.46953723\n",
            "Iteration 64, loss = 0.46610283\n",
            "Iteration 65, loss = 0.46347149\n",
            "Iteration 66, loss = 0.46222510\n",
            "Iteration 67, loss = 0.45901127\n",
            "Iteration 68, loss = 0.45748047\n",
            "Iteration 69, loss = 0.45695708\n",
            "Iteration 70, loss = 0.45303170\n",
            "Iteration 71, loss = 0.45336014\n",
            "Iteration 72, loss = 0.45360369\n",
            "Iteration 73, loss = 0.45119450\n",
            "Iteration 74, loss = 0.44799233\n",
            "Iteration 75, loss = 0.44713850\n",
            "Iteration 76, loss = 0.44301092\n",
            "Iteration 77, loss = 0.44260824\n",
            "Iteration 78, loss = 0.44381083\n",
            "Iteration 79, loss = 0.44086777\n",
            "Iteration 80, loss = 0.44324554\n",
            "Iteration 81, loss = 0.43777859\n",
            "Iteration 82, loss = 0.43596245\n",
            "Iteration 83, loss = 0.43517069\n",
            "Iteration 84, loss = 0.43498965\n",
            "Iteration 85, loss = 0.43523991\n",
            "Iteration 86, loss = 0.42842469\n",
            "Iteration 87, loss = 0.43097788\n",
            "Iteration 88, loss = 0.43026548\n",
            "Iteration 89, loss = 0.42395010\n",
            "Iteration 90, loss = 0.42700389\n",
            "Iteration 91, loss = 0.42498287\n",
            "Iteration 92, loss = 0.42352866\n",
            "Iteration 93, loss = 0.42008802\n",
            "Iteration 94, loss = 0.42263916\n",
            "Iteration 95, loss = 0.41958972\n",
            "Iteration 96, loss = 0.42010540\n",
            "Iteration 97, loss = 0.42055422\n",
            "Iteration 98, loss = 0.41462958\n",
            "Iteration 99, loss = 0.41220357\n",
            "Iteration 100, loss = 0.41453027\n",
            "Iteration 101, loss = 0.41170119\n",
            "Iteration 102, loss = 0.40870247\n",
            "Iteration 103, loss = 0.41022284\n",
            "Iteration 104, loss = 0.41515461\n",
            "Iteration 105, loss = 0.41036236\n",
            "Iteration 106, loss = 0.40637942\n",
            "Iteration 107, loss = 0.40622453\n",
            "Iteration 108, loss = 0.40693305\n",
            "Iteration 109, loss = 0.40484429\n",
            "Iteration 110, loss = 0.40595814\n",
            "Iteration 111, loss = 0.40340756\n",
            "Iteration 112, loss = 0.40099291\n",
            "Iteration 113, loss = 0.40136775\n",
            "Iteration 114, loss = 0.40192632\n",
            "Iteration 115, loss = 0.40536567\n",
            "Iteration 116, loss = 0.39845689\n",
            "Iteration 117, loss = 0.39476816\n",
            "Iteration 118, loss = 0.39702872\n",
            "Iteration 119, loss = 0.39550515\n",
            "Iteration 120, loss = 0.39788065\n",
            "Iteration 121, loss = 0.39488613\n",
            "Iteration 122, loss = 0.39422801\n",
            "Iteration 123, loss = 0.39339298\n",
            "Iteration 124, loss = 0.39643125\n",
            "Iteration 125, loss = 0.39492687\n",
            "Iteration 126, loss = 0.38828159\n",
            "Iteration 127, loss = 0.39132097\n",
            "Iteration 128, loss = 0.39267051\n",
            "Iteration 129, loss = 0.39047174\n",
            "Iteration 130, loss = 0.39369471\n",
            "Iteration 131, loss = 0.38619088\n",
            "Iteration 132, loss = 0.38690793\n",
            "Iteration 133, loss = 0.39243199\n",
            "Iteration 134, loss = 0.38795295\n",
            "Iteration 135, loss = 0.38865016\n",
            "Iteration 136, loss = 0.38642205\n",
            "Iteration 137, loss = 0.38299964\n",
            "Iteration 138, loss = 0.38664637\n",
            "Iteration 139, loss = 0.38298213\n",
            "Iteration 140, loss = 0.38445706\n",
            "Iteration 141, loss = 0.38644654\n",
            "Iteration 142, loss = 0.38493421\n",
            "Iteration 143, loss = 0.38536418\n",
            "Iteration 144, loss = 0.38090880\n",
            "Iteration 145, loss = 0.38455556\n",
            "Iteration 146, loss = 0.38734188\n",
            "Iteration 147, loss = 0.38537149\n",
            "Iteration 148, loss = 0.37887090\n",
            "Iteration 149, loss = 0.38094673\n",
            "Iteration 150, loss = 0.37838580\n",
            "Iteration 151, loss = 0.37953378\n",
            "Iteration 152, loss = 0.38204171\n",
            "Iteration 153, loss = 0.38253949\n",
            "Iteration 154, loss = 0.37770808\n",
            "Iteration 155, loss = 0.38110775\n",
            "Iteration 156, loss = 0.37898776\n",
            "Iteration 157, loss = 0.37557454\n",
            "Iteration 158, loss = 0.38749129\n",
            "Iteration 159, loss = 0.37953031\n",
            "Iteration 160, loss = 0.37546532\n",
            "Iteration 161, loss = 0.38030891\n",
            "Iteration 162, loss = 0.38234084\n",
            "Iteration 163, loss = 0.38029280\n",
            "Iteration 164, loss = 0.37592143\n",
            "Iteration 165, loss = 0.38042326\n",
            "Iteration 166, loss = 0.37794172\n",
            "Iteration 167, loss = 0.38088231\n",
            "Iteration 168, loss = 0.37888341\n",
            "Iteration 169, loss = 0.37947860\n",
            "Iteration 170, loss = 0.37444899\n",
            "Iteration 171, loss = 0.37792472\n",
            "Iteration 172, loss = 0.38235567\n",
            "Iteration 173, loss = 0.37148518\n",
            "Iteration 174, loss = 0.37293441\n",
            "Iteration 175, loss = 0.38205697\n",
            "Iteration 176, loss = 0.37004853\n",
            "Iteration 177, loss = 0.37389323\n",
            "Iteration 178, loss = 0.37453395\n",
            "Iteration 179, loss = 0.37615607\n",
            "Iteration 180, loss = 0.37905079\n",
            "Iteration 181, loss = 0.37628784\n",
            "Iteration 182, loss = 0.37596344\n",
            "Iteration 183, loss = 0.37162373\n",
            "Iteration 184, loss = 0.37317700\n",
            "Iteration 185, loss = 0.37427069\n",
            "Iteration 186, loss = 0.37589862\n",
            "Iteration 187, loss = 0.38026100\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 188, loss = 0.33433061\n",
            "Iteration 189, loss = 0.33392252\n",
            "Iteration 190, loss = 0.33381236\n",
            "Iteration 191, loss = 0.33377374\n",
            "Iteration 192, loss = 0.33364405\n",
            "Iteration 193, loss = 0.33356089\n",
            "Iteration 194, loss = 0.33341328\n",
            "Iteration 195, loss = 0.33340927\n",
            "Iteration 196, loss = 0.33330517\n",
            "Iteration 197, loss = 0.33320050\n",
            "Iteration 198, loss = 0.33307777\n",
            "Iteration 199, loss = 0.33307680\n",
            "Iteration 200, loss = 0.33301858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64144713\n",
            "Iteration 3, loss = 0.61014956\n",
            "Iteration 4, loss = 0.59327301\n",
            "Iteration 5, loss = 0.58040589\n",
            "Iteration 6, loss = 0.57421608\n",
            "Iteration 7, loss = 0.57159771\n",
            "Iteration 8, loss = 0.58059909\n",
            "Iteration 9, loss = 0.57678562\n",
            "Iteration 10, loss = 0.57146354\n",
            "Iteration 11, loss = 0.56890831\n",
            "Iteration 12, loss = 0.56741165\n",
            "Iteration 13, loss = 0.56489751\n",
            "Iteration 14, loss = 0.56327019\n",
            "Iteration 15, loss = 0.56518520\n",
            "Iteration 16, loss = 0.56481037\n",
            "Iteration 17, loss = 0.56469008\n",
            "Iteration 18, loss = 0.56501039\n",
            "Iteration 19, loss = 0.57283742\n",
            "Iteration 20, loss = 0.63904884\n",
            "Iteration 21, loss = 0.62370516\n",
            "Iteration 22, loss = 0.60823974\n",
            "Iteration 23, loss = 0.58959033\n",
            "Iteration 24, loss = 0.57240000\n",
            "Iteration 25, loss = 0.56630940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.53935913\n",
            "Iteration 27, loss = 0.52693319\n",
            "Iteration 28, loss = 0.51376165\n",
            "Iteration 29, loss = 0.51235978\n",
            "Iteration 30, loss = 0.52202164\n",
            "Iteration 31, loss = 0.52150980\n",
            "Iteration 32, loss = 0.51811975\n",
            "Iteration 33, loss = 0.51731901\n",
            "Iteration 34, loss = 0.51945284\n",
            "Iteration 35, loss = 0.51303001\n",
            "Iteration 36, loss = 0.51634228\n",
            "Iteration 37, loss = 0.51092954\n",
            "Iteration 38, loss = 0.51366700\n",
            "Iteration 39, loss = 0.51263149\n",
            "Iteration 40, loss = 0.51325117\n",
            "Iteration 41, loss = 0.51157140\n",
            "Iteration 42, loss = 0.51329603\n",
            "Iteration 43, loss = 0.51089443\n",
            "Iteration 44, loss = 0.51024766\n",
            "Iteration 45, loss = 0.51306305\n",
            "Iteration 46, loss = 0.51166610\n",
            "Iteration 47, loss = 0.51222549\n",
            "Iteration 48, loss = 0.51317307\n",
            "Iteration 49, loss = 0.51189975\n",
            "Iteration 50, loss = 0.51511225\n",
            "Iteration 51, loss = 0.50855972\n",
            "Iteration 52, loss = 0.51878640\n",
            "Iteration 53, loss = 0.51164776\n",
            "Iteration 54, loss = 0.51260160\n",
            "Iteration 55, loss = 0.51614333\n",
            "Iteration 56, loss = 0.51174030\n",
            "Iteration 57, loss = 0.51590211\n",
            "Iteration 58, loss = 0.51788574\n",
            "Iteration 59, loss = 0.51013090\n",
            "Iteration 60, loss = 0.51304337\n",
            "Iteration 61, loss = 0.51725201\n",
            "Iteration 62, loss = 0.51481133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 63, loss = 0.39909848\n",
            "Iteration 64, loss = 0.38287206\n",
            "Iteration 65, loss = 0.36918453\n",
            "Iteration 66, loss = 0.35714358\n",
            "Iteration 67, loss = 0.34665530\n",
            "Iteration 68, loss = 0.33712711\n",
            "Iteration 69, loss = 0.32904158\n",
            "Iteration 70, loss = 0.32177785\n",
            "Iteration 71, loss = 0.31548012\n",
            "Iteration 72, loss = 0.30974351\n",
            "Iteration 73, loss = 0.30491274\n",
            "Iteration 74, loss = 0.30046234\n",
            "Iteration 75, loss = 0.29653507\n",
            "Iteration 76, loss = 0.29303687\n",
            "Iteration 77, loss = 0.28986457\n",
            "Iteration 78, loss = 0.28688264\n",
            "Iteration 79, loss = 0.28415774\n",
            "Iteration 80, loss = 0.28173330\n",
            "Iteration 81, loss = 0.27977761\n",
            "Iteration 82, loss = 0.27735193\n",
            "Iteration 83, loss = 0.27586237\n",
            "Iteration 84, loss = 0.27441955\n",
            "Iteration 85, loss = 0.27274843\n",
            "Iteration 86, loss = 0.27143876\n",
            "Iteration 87, loss = 0.27002606\n",
            "Iteration 88, loss = 0.26911631\n",
            "Iteration 89, loss = 0.26760099\n",
            "Iteration 90, loss = 0.26762406\n",
            "Iteration 91, loss = 0.26660859\n",
            "Iteration 92, loss = 0.26534135\n",
            "Iteration 93, loss = 0.26518269\n",
            "Iteration 94, loss = 0.26411979\n",
            "Iteration 95, loss = 0.26305054\n",
            "Iteration 96, loss = 0.26312260\n",
            "Iteration 97, loss = 0.26210227\n",
            "Iteration 98, loss = 0.26130044\n",
            "Iteration 99, loss = 0.26158943\n",
            "Iteration 100, loss = 0.26095107\n",
            "Iteration 101, loss = 0.26075041\n",
            "Iteration 102, loss = 0.25952168\n",
            "Iteration 103, loss = 0.25908994\n",
            "Iteration 104, loss = 0.25961891\n",
            "Iteration 105, loss = 0.25900053\n",
            "Iteration 106, loss = 0.25831680\n",
            "Iteration 107, loss = 0.25963713\n",
            "Iteration 108, loss = 0.25783766\n",
            "Iteration 109, loss = 0.25785720\n",
            "Iteration 110, loss = 0.25957168\n",
            "Iteration 111, loss = 0.25888451\n",
            "Iteration 112, loss = 0.25798157\n",
            "Iteration 113, loss = 0.25795114\n",
            "Iteration 114, loss = 0.25779643\n",
            "Iteration 115, loss = 0.25807786\n",
            "Iteration 116, loss = 0.25665288\n",
            "Iteration 117, loss = 0.25977049\n",
            "Iteration 118, loss = 0.25690963\n",
            "Iteration 119, loss = 0.25801046\n",
            "Iteration 120, loss = 0.25814574\n",
            "Iteration 121, loss = 0.26028054\n",
            "Iteration 122, loss = 0.25962101\n",
            "Iteration 123, loss = 0.25924719\n",
            "Iteration 124, loss = 0.25804088\n",
            "Iteration 125, loss = 0.26015419\n",
            "Iteration 126, loss = 0.26301199\n",
            "Iteration 127, loss = 0.25924644\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 128, loss = 0.24988287\n",
            "Iteration 129, loss = 0.24987464\n",
            "Iteration 130, loss = 0.24982304\n",
            "Iteration 131, loss = 0.24976583\n",
            "Iteration 132, loss = 0.24979510\n",
            "Iteration 133, loss = 0.24971470\n",
            "Iteration 134, loss = 0.24971656\n",
            "Iteration 135, loss = 0.24969240\n",
            "Iteration 136, loss = 0.24967441\n",
            "Iteration 137, loss = 0.24962899\n",
            "Iteration 138, loss = 0.24959598\n",
            "Iteration 139, loss = 0.24956223\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 140, loss = 0.24928297\n",
            "Iteration 141, loss = 0.24927095\n",
            "Iteration 142, loss = 0.24925805\n",
            "Iteration 143, loss = 0.24925860\n",
            "Iteration 144, loss = 0.24926133\n",
            "Iteration 145, loss = 0.24923851\n",
            "Iteration 146, loss = 0.24926183\n",
            "Iteration 147, loss = 0.24925691\n",
            "Iteration 148, loss = 0.24924947\n",
            "Iteration 149, loss = 0.24924644\n",
            "Iteration 150, loss = 0.24923566\n",
            "Iteration 151, loss = 0.24920421\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 152, loss = 0.24913718\n",
            "Iteration 153, loss = 0.24912947\n",
            "Iteration 154, loss = 0.24913329\n",
            "Iteration 155, loss = 0.24913083\n",
            "Iteration 156, loss = 0.24912825\n",
            "Iteration 157, loss = 0.24912011\n",
            "Iteration 158, loss = 0.24913293\n",
            "Iteration 159, loss = 0.24912249\n",
            "Iteration 160, loss = 0.24912333\n",
            "Iteration 161, loss = 0.24911512\n",
            "Iteration 162, loss = 0.24911311\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62941116\n",
            "Iteration 3, loss = 0.60320658\n",
            "Iteration 4, loss = 0.58679348\n",
            "Iteration 5, loss = 0.57781520\n",
            "Iteration 6, loss = 0.57398357\n",
            "Iteration 7, loss = 0.56871723\n",
            "Iteration 8, loss = 0.56757299\n",
            "Iteration 9, loss = 0.56583348\n",
            "Iteration 10, loss = 0.56553937\n",
            "Iteration 11, loss = 0.56412697\n",
            "Iteration 12, loss = 0.56595724\n",
            "Iteration 13, loss = 0.56462383\n",
            "Iteration 14, loss = 0.56591566\n",
            "Iteration 15, loss = 0.56875065\n",
            "Iteration 16, loss = 0.56694432\n",
            "Iteration 17, loss = 0.56793510\n",
            "Iteration 18, loss = 0.56810243\n",
            "Iteration 19, loss = 0.56831517\n",
            "Iteration 20, loss = 0.56728895\n",
            "Iteration 21, loss = 0.56817625\n",
            "Iteration 22, loss = 0.56958870\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.50807528\n",
            "Iteration 24, loss = 0.48265387\n",
            "Iteration 25, loss = 0.50540056\n",
            "Iteration 26, loss = 0.51170821\n",
            "Iteration 27, loss = 0.50853563\n",
            "Iteration 28, loss = 0.51211536\n",
            "Iteration 29, loss = 0.50855514\n",
            "Iteration 30, loss = 0.51072002\n",
            "Iteration 31, loss = 0.51111230\n",
            "Iteration 32, loss = 0.50502191\n",
            "Iteration 33, loss = 0.51272155\n",
            "Iteration 34, loss = 0.51036948\n",
            "Iteration 35, loss = 0.51181829\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.40807818\n",
            "Iteration 37, loss = 0.39331639\n",
            "Iteration 38, loss = 0.38019626\n",
            "Iteration 39, loss = 0.36846553\n",
            "Iteration 40, loss = 0.35757269\n",
            "Iteration 41, loss = 0.34782158\n",
            "Iteration 42, loss = 0.33931601\n",
            "Iteration 43, loss = 0.33158685\n",
            "Iteration 44, loss = 0.32432534\n",
            "Iteration 45, loss = 0.31816757\n",
            "Iteration 46, loss = 0.31288849\n",
            "Iteration 47, loss = 0.30724720\n",
            "Iteration 48, loss = 0.30259280\n",
            "Iteration 49, loss = 0.29856058\n",
            "Iteration 50, loss = 0.29478072\n",
            "Iteration 51, loss = 0.29165892\n",
            "Iteration 52, loss = 0.28846535\n",
            "Iteration 53, loss = 0.28570396\n",
            "Iteration 54, loss = 0.28314953\n",
            "Iteration 55, loss = 0.28023747\n",
            "Iteration 56, loss = 0.27867163\n",
            "Iteration 57, loss = 0.27678294\n",
            "Iteration 58, loss = 0.27493498\n",
            "Iteration 59, loss = 0.27358606\n",
            "Iteration 60, loss = 0.27169665\n",
            "Iteration 61, loss = 0.27075857\n",
            "Iteration 62, loss = 0.27014408\n",
            "Iteration 63, loss = 0.26814138\n",
            "Iteration 64, loss = 0.26763113\n",
            "Iteration 65, loss = 0.26586994\n",
            "Iteration 66, loss = 0.26581337\n",
            "Iteration 67, loss = 0.26529288\n",
            "Iteration 68, loss = 0.26386893\n",
            "Iteration 69, loss = 0.26447966\n",
            "Iteration 70, loss = 0.26329224\n",
            "Iteration 71, loss = 0.26314426\n",
            "Iteration 72, loss = 0.26454663\n",
            "Iteration 73, loss = 0.26238873\n",
            "Iteration 74, loss = 0.26218423\n",
            "Iteration 75, loss = 0.26271966\n",
            "Iteration 76, loss = 0.26127647\n",
            "Iteration 77, loss = 0.26036666\n",
            "Iteration 78, loss = 0.26668815\n",
            "Iteration 79, loss = 0.25989872\n",
            "Iteration 80, loss = 0.26055767\n",
            "Iteration 81, loss = 0.26242042\n",
            "Iteration 82, loss = 0.25935983\n",
            "Iteration 83, loss = 0.26659444\n",
            "Iteration 84, loss = 0.26098096\n",
            "Iteration 85, loss = 0.26251009\n",
            "Iteration 86, loss = 0.25922438\n",
            "Iteration 87, loss = 0.25973944\n",
            "Iteration 88, loss = 0.26227232\n",
            "Iteration 89, loss = 0.26575095\n",
            "Iteration 90, loss = 0.26420303\n",
            "Iteration 91, loss = 0.26976995\n",
            "Iteration 92, loss = 0.26120125\n",
            "Iteration 93, loss = 0.26469190\n",
            "Iteration 94, loss = 0.26490485\n",
            "Iteration 95, loss = 0.26169359\n",
            "Iteration 96, loss = 0.26932242\n",
            "Iteration 97, loss = 0.26257999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.24903273\n",
            "Iteration 99, loss = 0.24890420\n",
            "Iteration 100, loss = 0.24889785\n",
            "Iteration 101, loss = 0.24881813\n",
            "Iteration 102, loss = 0.24879446\n",
            "Iteration 103, loss = 0.24874792\n",
            "Iteration 104, loss = 0.24874223\n",
            "Iteration 105, loss = 0.24871120\n",
            "Iteration 106, loss = 0.24867033\n",
            "Iteration 107, loss = 0.24864872\n",
            "Iteration 108, loss = 0.24863909\n",
            "Iteration 109, loss = 0.24861272\n",
            "Iteration 110, loss = 0.24855184\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.24829311\n",
            "Iteration 112, loss = 0.24828382\n",
            "Iteration 113, loss = 0.24825343\n",
            "Iteration 114, loss = 0.24824622\n",
            "Iteration 115, loss = 0.24825192\n",
            "Iteration 116, loss = 0.24826777\n",
            "Iteration 117, loss = 0.24824169\n",
            "Iteration 118, loss = 0.24822162\n",
            "Iteration 119, loss = 0.24822221\n",
            "Iteration 120, loss = 0.24821318\n",
            "Iteration 121, loss = 0.24823778\n",
            "Iteration 122, loss = 0.24822855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 123, loss = 0.24812500\n",
            "Iteration 124, loss = 0.24812109\n",
            "Iteration 125, loss = 0.24811948\n",
            "Iteration 126, loss = 0.24811491\n",
            "Iteration 127, loss = 0.24812562\n",
            "Iteration 128, loss = 0.24812155\n",
            "Iteration 129, loss = 0.24810669\n",
            "Iteration 130, loss = 0.24812071\n",
            "Iteration 131, loss = 0.24811566\n",
            "Iteration 132, loss = 0.24811937\n",
            "Iteration 133, loss = 0.24810915\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65551835\n",
            "Iteration 3, loss = 0.61876331\n",
            "Iteration 4, loss = 0.59708642\n",
            "Iteration 5, loss = 0.58488976\n",
            "Iteration 6, loss = 0.57823536\n",
            "Iteration 7, loss = 0.57226997\n",
            "Iteration 8, loss = 0.56866869\n",
            "Iteration 9, loss = 0.56902331\n",
            "Iteration 10, loss = 0.57564897\n",
            "Iteration 11, loss = 0.62873067\n",
            "Iteration 12, loss = 0.62258831\n",
            "Iteration 13, loss = 0.61670471\n",
            "Iteration 14, loss = 0.61236058\n",
            "Iteration 15, loss = 0.60571823\n",
            "Iteration 16, loss = 0.61565245\n",
            "Iteration 17, loss = 0.59643631\n",
            "Iteration 18, loss = 0.58335186\n",
            "Iteration 19, loss = 0.57666039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 20, loss = 0.55905202\n",
            "Iteration 21, loss = 0.55329538\n",
            "Iteration 22, loss = 0.54638876\n",
            "Iteration 23, loss = 0.53821563\n",
            "Iteration 24, loss = 0.53282147\n",
            "Iteration 25, loss = 0.53717722\n",
            "Iteration 26, loss = 0.53789628\n",
            "Iteration 27, loss = 0.53842463\n",
            "Iteration 28, loss = 0.53525204\n",
            "Iteration 29, loss = 0.53487536\n",
            "Iteration 30, loss = 0.53136962\n",
            "Iteration 31, loss = 0.53317504\n",
            "Iteration 32, loss = 0.52907162\n",
            "Iteration 33, loss = 0.53073505\n",
            "Iteration 34, loss = 0.52664453\n",
            "Iteration 35, loss = 0.52683835\n",
            "Iteration 36, loss = 0.52253936\n",
            "Iteration 37, loss = 0.52661556\n",
            "Iteration 38, loss = 0.52361404\n",
            "Iteration 39, loss = 0.52104454\n",
            "Iteration 40, loss = 0.52309726\n",
            "Iteration 41, loss = 0.52108247\n",
            "Iteration 42, loss = 0.51722559\n",
            "Iteration 43, loss = 0.52218241\n",
            "Iteration 44, loss = 0.51964532\n",
            "Iteration 45, loss = 0.52309479\n",
            "Iteration 46, loss = 0.52062106\n",
            "Iteration 47, loss = 0.52032599\n",
            "Iteration 48, loss = 0.51979380\n",
            "Iteration 49, loss = 0.51858039\n",
            "Iteration 50, loss = 0.52084278\n",
            "Iteration 51, loss = 0.52189731\n",
            "Iteration 52, loss = 0.51847089\n",
            "Iteration 53, loss = 0.51887516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 54, loss = 0.42458448\n",
            "Iteration 55, loss = 0.41060975\n",
            "Iteration 56, loss = 0.39818844\n",
            "Iteration 57, loss = 0.38723377\n",
            "Iteration 58, loss = 0.37732828\n",
            "Iteration 59, loss = 0.36832877\n",
            "Iteration 60, loss = 0.36083940\n",
            "Iteration 61, loss = 0.35386168\n",
            "Iteration 62, loss = 0.34783876\n",
            "Iteration 63, loss = 0.34265473\n",
            "Iteration 64, loss = 0.33811862\n",
            "Iteration 65, loss = 0.33393052\n",
            "Iteration 66, loss = 0.33010447\n",
            "Iteration 67, loss = 0.32732679\n",
            "Iteration 68, loss = 0.32464593\n",
            "Iteration 69, loss = 0.32165469\n",
            "Iteration 70, loss = 0.31955011\n",
            "Iteration 71, loss = 0.31674825\n",
            "Iteration 72, loss = 0.31587307\n",
            "Iteration 73, loss = 0.31460601\n",
            "Iteration 74, loss = 0.31293786\n",
            "Iteration 75, loss = 0.31176113\n",
            "Iteration 76, loss = 0.31195079\n",
            "Iteration 77, loss = 0.31052582\n",
            "Iteration 78, loss = 0.30977673\n",
            "Iteration 79, loss = 0.31041799\n",
            "Iteration 80, loss = 0.30774400\n",
            "Iteration 81, loss = 0.30782351\n",
            "Iteration 82, loss = 0.30872869\n",
            "Iteration 83, loss = 0.30723025\n",
            "Iteration 84, loss = 0.30665439\n",
            "Iteration 85, loss = 0.30585700\n",
            "Iteration 86, loss = 0.30455213\n",
            "Iteration 87, loss = 0.30700592\n",
            "Iteration 88, loss = 0.30998710\n",
            "Iteration 89, loss = 0.30836699\n",
            "Iteration 90, loss = 0.31171967\n",
            "Iteration 91, loss = 0.30527069\n",
            "Iteration 92, loss = 0.31075621\n",
            "Iteration 93, loss = 0.30927156\n",
            "Iteration 94, loss = 0.31114946\n",
            "Iteration 95, loss = 0.30704045\n",
            "Iteration 96, loss = 0.30730251\n",
            "Iteration 97, loss = 0.30856706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.28380141\n",
            "Iteration 99, loss = 0.28348942\n",
            "Iteration 100, loss = 0.28337176\n",
            "Iteration 101, loss = 0.28320566\n",
            "Iteration 102, loss = 0.28299805\n",
            "Iteration 103, loss = 0.28286067\n",
            "Iteration 104, loss = 0.28273317\n",
            "Iteration 105, loss = 0.28257751\n",
            "Iteration 106, loss = 0.28250444\n",
            "Iteration 107, loss = 0.28239362\n",
            "Iteration 108, loss = 0.28225577\n",
            "Iteration 109, loss = 0.28218442\n",
            "Iteration 110, loss = 0.28200676\n",
            "Iteration 111, loss = 0.28190973\n",
            "Iteration 112, loss = 0.28179098\n",
            "Iteration 113, loss = 0.28167627\n",
            "Iteration 114, loss = 0.28159167\n",
            "Iteration 115, loss = 0.28142071\n",
            "Iteration 116, loss = 0.28134865\n",
            "Iteration 117, loss = 0.28120252\n",
            "Iteration 118, loss = 0.28105365\n",
            "Iteration 119, loss = 0.28100786\n",
            "Iteration 120, loss = 0.28086324\n",
            "Iteration 121, loss = 0.28072032\n",
            "Iteration 122, loss = 0.28063038\n",
            "Iteration 123, loss = 0.28053574\n",
            "Iteration 124, loss = 0.28034944\n",
            "Iteration 125, loss = 0.28030043\n",
            "Iteration 126, loss = 0.28020378\n",
            "Iteration 127, loss = 0.28010721\n",
            "Iteration 128, loss = 0.27995722\n",
            "Iteration 129, loss = 0.27992988\n",
            "Iteration 130, loss = 0.27980162\n",
            "Iteration 131, loss = 0.27970008\n",
            "Iteration 132, loss = 0.27959525\n",
            "Iteration 133, loss = 0.27942639\n",
            "Iteration 134, loss = 0.27940041\n",
            "Iteration 135, loss = 0.27922213\n",
            "Iteration 136, loss = 0.27921107\n",
            "Iteration 137, loss = 0.27910974\n",
            "Iteration 138, loss = 0.27898726\n",
            "Iteration 139, loss = 0.27883331\n",
            "Iteration 140, loss = 0.27876864\n",
            "Iteration 141, loss = 0.27871999\n",
            "Iteration 142, loss = 0.27858419\n",
            "Iteration 143, loss = 0.27850798\n",
            "Iteration 144, loss = 0.27836694\n",
            "Iteration 145, loss = 0.27829168\n",
            "Iteration 146, loss = 0.27813711\n",
            "Iteration 147, loss = 0.27812319\n",
            "Iteration 148, loss = 0.27806667\n",
            "Iteration 149, loss = 0.27793383\n",
            "Iteration 150, loss = 0.27785983\n",
            "Iteration 151, loss = 0.27777728\n",
            "Iteration 152, loss = 0.27765095\n",
            "Iteration 153, loss = 0.27754077\n",
            "Iteration 154, loss = 0.27749114\n",
            "Iteration 155, loss = 0.27742121\n",
            "Iteration 156, loss = 0.27725769\n",
            "Iteration 157, loss = 0.27715767\n",
            "Iteration 158, loss = 0.27716733\n",
            "Iteration 159, loss = 0.27702974\n",
            "Iteration 160, loss = 0.27695373\n",
            "Iteration 161, loss = 0.27690498\n",
            "Iteration 162, loss = 0.27674478\n",
            "Iteration 163, loss = 0.27672352\n",
            "Iteration 164, loss = 0.27660539\n",
            "Iteration 165, loss = 0.27653598\n",
            "Iteration 166, loss = 0.27639539\n",
            "Iteration 167, loss = 0.27641163\n",
            "Iteration 168, loss = 0.27622498\n",
            "Iteration 169, loss = 0.27622228\n",
            "Iteration 170, loss = 0.27614870\n",
            "Iteration 171, loss = 0.27608762\n",
            "Iteration 172, loss = 0.27596519\n",
            "Iteration 173, loss = 0.27588547\n",
            "Iteration 174, loss = 0.27577654\n",
            "Iteration 175, loss = 0.27569265\n",
            "Iteration 176, loss = 0.27569833\n",
            "Iteration 177, loss = 0.27553030\n",
            "Iteration 178, loss = 0.27552207\n",
            "Iteration 179, loss = 0.27541247\n",
            "Iteration 180, loss = 0.27537079\n",
            "Iteration 181, loss = 0.27536330\n",
            "Iteration 182, loss = 0.27522107\n",
            "Iteration 183, loss = 0.27511875\n",
            "Iteration 184, loss = 0.27506281\n",
            "Iteration 185, loss = 0.27498743\n",
            "Iteration 186, loss = 0.27490944\n",
            "Iteration 187, loss = 0.27484883\n",
            "Iteration 188, loss = 0.27477208\n",
            "Iteration 189, loss = 0.27470943\n",
            "Iteration 190, loss = 0.27458128\n",
            "Iteration 191, loss = 0.27453509\n",
            "Iteration 192, loss = 0.27452192\n",
            "Iteration 193, loss = 0.27445375\n",
            "Iteration 194, loss = 0.27430612\n",
            "Iteration 195, loss = 0.27424476\n",
            "Iteration 196, loss = 0.27418527\n",
            "Iteration 197, loss = 0.27413151\n",
            "Iteration 198, loss = 0.27401467\n",
            "Iteration 199, loss = 0.27398972\n",
            "Iteration 200, loss = 0.27389725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.92593663\n",
            "Iteration 2, loss = 0.63301839\n",
            "Iteration 3, loss = 0.60573350\n",
            "Iteration 4, loss = 0.58944346\n",
            "Iteration 5, loss = 0.57896644\n",
            "Iteration 6, loss = 0.57149600\n",
            "Iteration 7, loss = 0.56784322\n",
            "Iteration 8, loss = 0.56662625\n",
            "Iteration 9, loss = 0.56367761\n",
            "Iteration 10, loss = 0.56354804\n",
            "Iteration 11, loss = 0.56475770\n",
            "Iteration 12, loss = 0.56231617\n",
            "Iteration 13, loss = 0.56364734\n",
            "Iteration 14, loss = 0.56236301\n",
            "Iteration 15, loss = 0.56411958\n",
            "Iteration 16, loss = 0.56400625\n",
            "Iteration 17, loss = 0.56358566\n",
            "Iteration 18, loss = 0.56333423\n",
            "Iteration 19, loss = 0.56272450\n",
            "Iteration 20, loss = 0.58005887\n",
            "Iteration 21, loss = 0.57756320\n",
            "Iteration 22, loss = 0.57214939\n",
            "Iteration 23, loss = 0.56836616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.51249993\n",
            "Iteration 25, loss = 0.48940516\n",
            "Iteration 26, loss = 0.51234740\n",
            "Iteration 27, loss = 0.51208496\n",
            "Iteration 28, loss = 0.51630449\n",
            "Iteration 29, loss = 0.51028842\n",
            "Iteration 30, loss = 0.51051091\n",
            "Iteration 31, loss = 0.51236366\n",
            "Iteration 32, loss = 0.51068419\n",
            "Iteration 33, loss = 0.50969268\n",
            "Iteration 34, loss = 0.50851296\n",
            "Iteration 35, loss = 0.51309763\n",
            "Iteration 36, loss = 0.51283838\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.41403912\n",
            "Iteration 38, loss = 0.39964209\n",
            "Iteration 39, loss = 0.38664205\n",
            "Iteration 40, loss = 0.37478384\n",
            "Iteration 41, loss = 0.36375711\n",
            "Iteration 42, loss = 0.35401898\n",
            "Iteration 43, loss = 0.34505658\n",
            "Iteration 44, loss = 0.33699385\n",
            "Iteration 45, loss = 0.32970672\n",
            "Iteration 46, loss = 0.32298733\n",
            "Iteration 47, loss = 0.31729695\n",
            "Iteration 48, loss = 0.31161031\n",
            "Iteration 49, loss = 0.30687639\n",
            "Iteration 50, loss = 0.30243757\n",
            "Iteration 51, loss = 0.29798550\n",
            "Iteration 52, loss = 0.29447001\n",
            "Iteration 53, loss = 0.29112237\n",
            "Iteration 54, loss = 0.28841636\n",
            "Iteration 55, loss = 0.28518900\n",
            "Iteration 56, loss = 0.28223603\n",
            "Iteration 57, loss = 0.28115827\n",
            "Iteration 58, loss = 0.27859200\n",
            "Iteration 59, loss = 0.27673869\n",
            "Iteration 60, loss = 0.27524615\n",
            "Iteration 61, loss = 0.27367089\n",
            "Iteration 62, loss = 0.27242979\n",
            "Iteration 63, loss = 0.27139195\n",
            "Iteration 64, loss = 0.26933002\n",
            "Iteration 65, loss = 0.26871034\n",
            "Iteration 66, loss = 0.26691381\n",
            "Iteration 67, loss = 0.26668976\n",
            "Iteration 68, loss = 0.26637017\n",
            "Iteration 69, loss = 0.26609127\n",
            "Iteration 70, loss = 0.26378093\n",
            "Iteration 71, loss = 0.26523100\n",
            "Iteration 72, loss = 0.26282241\n",
            "Iteration 73, loss = 0.26187511\n",
            "Iteration 74, loss = 0.26364698\n",
            "Iteration 75, loss = 0.26505504\n",
            "Iteration 76, loss = 0.26446415\n",
            "Iteration 77, loss = 0.26153818\n",
            "Iteration 78, loss = 0.26470240\n",
            "Iteration 79, loss = 0.26424194\n",
            "Iteration 80, loss = 0.26533840\n",
            "Iteration 81, loss = 0.26793603\n",
            "Iteration 82, loss = 0.26483014\n",
            "Iteration 83, loss = 0.26066201\n",
            "Iteration 84, loss = 0.26700691\n",
            "Iteration 85, loss = 0.26722689\n",
            "Iteration 86, loss = 0.26467911\n",
            "Iteration 87, loss = 0.27217978\n",
            "Iteration 88, loss = 0.26137433\n",
            "Iteration 89, loss = 0.26234765\n",
            "Iteration 90, loss = 0.26585661\n",
            "Iteration 91, loss = 0.26833977\n",
            "Iteration 92, loss = 0.25849599\n",
            "Iteration 93, loss = 0.26957608\n",
            "Iteration 94, loss = 0.26717895\n",
            "Iteration 95, loss = 0.26831701\n",
            "Iteration 96, loss = 0.26828663\n",
            "Iteration 97, loss = 0.26815009\n",
            "Iteration 98, loss = 0.26516053\n",
            "Iteration 99, loss = 0.26329564\n",
            "Iteration 100, loss = 0.27343388\n",
            "Iteration 101, loss = 0.26943661\n",
            "Iteration 102, loss = 0.27328795\n",
            "Iteration 103, loss = 0.26181550\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 104, loss = 0.24780526\n",
            "Iteration 105, loss = 0.24774456\n",
            "Iteration 106, loss = 0.24770489\n",
            "Iteration 107, loss = 0.24766438\n",
            "Iteration 108, loss = 0.24764130\n",
            "Iteration 109, loss = 0.24765706\n",
            "Iteration 110, loss = 0.24759156\n",
            "Iteration 111, loss = 0.24754539\n",
            "Iteration 112, loss = 0.24750894\n",
            "Iteration 113, loss = 0.24749395\n",
            "Iteration 114, loss = 0.24749636\n",
            "Iteration 115, loss = 0.24746285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 116, loss = 0.24718862\n",
            "Iteration 117, loss = 0.24716182\n",
            "Iteration 118, loss = 0.24716327\n",
            "Iteration 119, loss = 0.24715633\n",
            "Iteration 120, loss = 0.24715350\n",
            "Iteration 121, loss = 0.24716700\n",
            "Iteration 122, loss = 0.24712779\n",
            "Iteration 123, loss = 0.24710265\n",
            "Iteration 124, loss = 0.24711245\n",
            "Iteration 125, loss = 0.24709102\n",
            "Iteration 126, loss = 0.24707865\n",
            "Iteration 127, loss = 0.24711783\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 128, loss = 0.24702847\n",
            "Iteration 129, loss = 0.24701513\n",
            "Iteration 130, loss = 0.24701363\n",
            "Iteration 131, loss = 0.24702074\n",
            "Iteration 132, loss = 0.24700979\n",
            "Iteration 133, loss = 0.24700754\n",
            "Iteration 134, loss = 0.24700036\n",
            "Iteration 135, loss = 0.24701059\n",
            "Iteration 136, loss = 0.24700697\n",
            "Iteration 137, loss = 0.24700734\n",
            "Iteration 138, loss = 0.24700364\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62862394\n",
            "Iteration 3, loss = 0.60265094\n",
            "Iteration 4, loss = 0.58672236\n",
            "Iteration 5, loss = 0.57916666\n",
            "Iteration 6, loss = 0.57197776\n",
            "Iteration 7, loss = 0.56939049\n",
            "Iteration 8, loss = 0.56780269\n",
            "Iteration 9, loss = 0.56722239\n",
            "Iteration 10, loss = 0.56733759\n",
            "Iteration 11, loss = 0.56513163\n",
            "Iteration 12, loss = 0.56578479\n",
            "Iteration 13, loss = 0.56626400\n",
            "Iteration 14, loss = 0.56545816\n",
            "Iteration 15, loss = 0.56518868\n",
            "Iteration 16, loss = 0.56611259\n",
            "Iteration 17, loss = 0.56661594\n",
            "Iteration 18, loss = 0.56629319\n",
            "Iteration 19, loss = 0.56487438\n",
            "Iteration 20, loss = 0.56689942\n",
            "Iteration 21, loss = 0.56491127\n",
            "Iteration 22, loss = 0.56805642\n",
            "Iteration 23, loss = 0.56659573\n",
            "Iteration 24, loss = 0.56674937\n",
            "Iteration 25, loss = 0.56663243\n",
            "Iteration 26, loss = 0.56623148\n",
            "Iteration 27, loss = 0.56454697\n",
            "Iteration 28, loss = 0.56602417\n",
            "Iteration 29, loss = 0.56616343\n",
            "Iteration 30, loss = 0.56633396\n",
            "Iteration 31, loss = 0.56760582\n",
            "Iteration 32, loss = 0.56577732\n",
            "Iteration 33, loss = 0.56574321\n",
            "Iteration 34, loss = 0.57090637\n",
            "Iteration 35, loss = 0.56463991\n",
            "Iteration 36, loss = 0.56883609\n",
            "Iteration 37, loss = 0.56768177\n",
            "Iteration 38, loss = 0.56954577\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.50443385\n",
            "Iteration 40, loss = 0.48316635\n",
            "Iteration 41, loss = 0.50735084\n",
            "Iteration 42, loss = 0.51125972\n",
            "Iteration 43, loss = 0.50725435\n",
            "Iteration 44, loss = 0.51243409\n",
            "Iteration 45, loss = 0.51383854\n",
            "Iteration 46, loss = 0.50941530\n",
            "Iteration 47, loss = 0.51095942\n",
            "Iteration 48, loss = 0.51066309\n",
            "Iteration 49, loss = 0.50839302\n",
            "Iteration 50, loss = 0.51130946\n",
            "Iteration 51, loss = 0.51421183\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.40965190\n",
            "Iteration 53, loss = 0.39525351\n",
            "Iteration 54, loss = 0.38191051\n",
            "Iteration 55, loss = 0.37012413\n",
            "Iteration 56, loss = 0.35921015\n",
            "Iteration 57, loss = 0.34938718\n",
            "Iteration 58, loss = 0.34075300\n",
            "Iteration 59, loss = 0.33283472\n",
            "Iteration 60, loss = 0.32560845\n",
            "Iteration 61, loss = 0.31925872\n",
            "Iteration 62, loss = 0.31378598\n",
            "Iteration 63, loss = 0.30849554\n",
            "Iteration 64, loss = 0.30358116\n",
            "Iteration 65, loss = 0.29952345\n",
            "Iteration 66, loss = 0.29620572\n",
            "Iteration 67, loss = 0.29230701\n",
            "Iteration 68, loss = 0.28921013\n",
            "Iteration 69, loss = 0.28645015\n",
            "Iteration 70, loss = 0.28399517\n",
            "Iteration 71, loss = 0.28172515\n",
            "Iteration 72, loss = 0.27945983\n",
            "Iteration 73, loss = 0.27742139\n",
            "Iteration 74, loss = 0.27558662\n",
            "Iteration 75, loss = 0.27483771\n",
            "Iteration 76, loss = 0.27259761\n",
            "Iteration 77, loss = 0.27107156\n",
            "Iteration 78, loss = 0.27059125\n",
            "Iteration 79, loss = 0.26914192\n",
            "Iteration 80, loss = 0.26820384\n",
            "Iteration 81, loss = 0.26687375\n",
            "Iteration 82, loss = 0.26649331\n",
            "Iteration 83, loss = 0.26587810\n",
            "Iteration 84, loss = 0.26623034\n",
            "Iteration 85, loss = 0.26342867\n",
            "Iteration 86, loss = 0.26461388\n",
            "Iteration 87, loss = 0.26497289\n",
            "Iteration 88, loss = 0.26335553\n",
            "Iteration 89, loss = 0.26314052\n",
            "Iteration 90, loss = 0.26288660\n",
            "Iteration 91, loss = 0.26308916\n",
            "Iteration 92, loss = 0.26193865\n",
            "Iteration 93, loss = 0.26339673\n",
            "Iteration 94, loss = 0.26187536\n",
            "Iteration 95, loss = 0.26278365\n",
            "Iteration 96, loss = 0.26684019\n",
            "Iteration 97, loss = 0.26064666\n",
            "Iteration 98, loss = 0.26192473\n",
            "Iteration 99, loss = 0.26210898\n",
            "Iteration 100, loss = 0.26323346\n",
            "Iteration 101, loss = 0.26831415\n",
            "Iteration 102, loss = 0.26234653\n",
            "Iteration 103, loss = 0.26220228\n",
            "Iteration 104, loss = 0.26010883\n",
            "Iteration 105, loss = 0.27456155\n",
            "Iteration 106, loss = 0.26471041\n",
            "Iteration 107, loss = 0.26689899\n",
            "Iteration 108, loss = 0.26712902\n",
            "Iteration 109, loss = 0.26494931\n",
            "Iteration 110, loss = 0.26500094\n",
            "Iteration 111, loss = 0.27112313\n",
            "Iteration 112, loss = 0.26408901\n",
            "Iteration 113, loss = 0.27973474\n",
            "Iteration 114, loss = 0.26655141\n",
            "Iteration 115, loss = 0.26735817\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 116, loss = 0.24935076\n",
            "Iteration 117, loss = 0.24934355\n",
            "Iteration 118, loss = 0.24925394\n",
            "Iteration 119, loss = 0.24922967\n",
            "Iteration 120, loss = 0.24917627\n",
            "Iteration 121, loss = 0.24918784\n",
            "Iteration 122, loss = 0.24914529\n",
            "Iteration 123, loss = 0.24907112\n",
            "Iteration 124, loss = 0.24910736\n",
            "Iteration 125, loss = 0.24900529\n",
            "Iteration 126, loss = 0.24902989\n",
            "Iteration 127, loss = 0.24896552\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 128, loss = 0.24872041\n",
            "Iteration 129, loss = 0.24867574\n",
            "Iteration 130, loss = 0.24865467\n",
            "Iteration 131, loss = 0.24866340\n",
            "Iteration 132, loss = 0.24869591\n",
            "Iteration 133, loss = 0.24869105\n",
            "Iteration 134, loss = 0.24863919\n",
            "Iteration 135, loss = 0.24863034\n",
            "Iteration 136, loss = 0.24864148\n",
            "Iteration 137, loss = 0.24865175\n",
            "Iteration 138, loss = 0.24861875\n",
            "Iteration 139, loss = 0.24862450\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 140, loss = 0.24853746\n",
            "Iteration 141, loss = 0.24853010\n",
            "Iteration 142, loss = 0.24852995\n",
            "Iteration 143, loss = 0.24853972\n",
            "Iteration 144, loss = 0.24852402\n",
            "Iteration 145, loss = 0.24853397\n",
            "Iteration 146, loss = 0.24852091\n",
            "Iteration 147, loss = 0.24851275\n",
            "Iteration 148, loss = 0.24851729\n",
            "Iteration 149, loss = 0.24852975\n",
            "Iteration 150, loss = 0.24851829\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63191246\n",
            "Iteration 3, loss = 0.60590797\n",
            "Iteration 4, loss = 0.59117095\n",
            "Iteration 5, loss = 0.58481519\n",
            "Iteration 6, loss = 0.58096426\n",
            "Iteration 7, loss = 0.57385107\n",
            "Iteration 8, loss = 0.57081379\n",
            "Iteration 9, loss = 0.56978804\n",
            "Iteration 10, loss = 0.56804204\n",
            "Iteration 11, loss = 0.56749091\n",
            "Iteration 12, loss = 0.56639074\n",
            "Iteration 13, loss = 0.56859587\n",
            "Iteration 14, loss = 0.56851257\n",
            "Iteration 15, loss = 0.58019237\n",
            "Iteration 16, loss = 0.61566957\n",
            "Iteration 17, loss = 0.58686857\n",
            "Iteration 18, loss = 0.57396053\n",
            "Iteration 19, loss = 0.56915847\n",
            "Iteration 20, loss = 0.56460579\n",
            "Iteration 21, loss = 0.56304271\n",
            "Iteration 22, loss = 0.56369815\n",
            "Iteration 23, loss = 0.56292053\n",
            "Iteration 24, loss = 0.56366542\n",
            "Iteration 25, loss = 0.56480922\n",
            "Iteration 26, loss = 0.56281262\n",
            "Iteration 27, loss = 0.56508076\n",
            "Iteration 28, loss = 0.56248281\n",
            "Iteration 29, loss = 0.56583229\n",
            "Iteration 30, loss = 0.56591800\n",
            "Iteration 31, loss = 0.56534357\n",
            "Iteration 32, loss = 0.56335659\n",
            "Iteration 33, loss = 0.56509024\n",
            "Iteration 34, loss = 0.56588335\n",
            "Iteration 35, loss = 0.56469418\n",
            "Iteration 36, loss = 0.56390689\n",
            "Iteration 37, loss = 0.56635711\n",
            "Iteration 38, loss = 0.56461525\n",
            "Iteration 39, loss = 0.56519639\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 40, loss = 0.50069279\n",
            "Iteration 41, loss = 0.48187728\n",
            "Iteration 42, loss = 0.50304541\n",
            "Iteration 43, loss = 0.50487584\n",
            "Iteration 44, loss = 0.50682448\n",
            "Iteration 45, loss = 0.50790299\n",
            "Iteration 46, loss = 0.50738872\n",
            "Iteration 47, loss = 0.50125586\n",
            "Iteration 48, loss = 0.50914110\n",
            "Iteration 49, loss = 0.50719004\n",
            "Iteration 50, loss = 0.50318051\n",
            "Iteration 51, loss = 0.51285672\n",
            "Iteration 52, loss = 0.50599193\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 53, loss = 0.40833701\n",
            "Iteration 54, loss = 0.39276046\n",
            "Iteration 55, loss = 0.37940491\n",
            "Iteration 56, loss = 0.36757897\n",
            "Iteration 57, loss = 0.35668705\n",
            "Iteration 58, loss = 0.34687896\n",
            "Iteration 59, loss = 0.33823108\n",
            "Iteration 60, loss = 0.33043542\n",
            "Iteration 61, loss = 0.32322959\n",
            "Iteration 62, loss = 0.31703564\n",
            "Iteration 63, loss = 0.31139517\n",
            "Iteration 64, loss = 0.30612274\n",
            "Iteration 65, loss = 0.30178320\n",
            "Iteration 66, loss = 0.29739693\n",
            "Iteration 67, loss = 0.29382701\n",
            "Iteration 68, loss = 0.29082583\n",
            "Iteration 69, loss = 0.28685948\n",
            "Iteration 70, loss = 0.28432501\n",
            "Iteration 71, loss = 0.28198047\n",
            "Iteration 72, loss = 0.27914196\n",
            "Iteration 73, loss = 0.27748894\n",
            "Iteration 74, loss = 0.27542481\n",
            "Iteration 75, loss = 0.27384976\n",
            "Iteration 76, loss = 0.27175370\n",
            "Iteration 77, loss = 0.27034298\n",
            "Iteration 78, loss = 0.26954787\n",
            "Iteration 79, loss = 0.26871145\n",
            "Iteration 80, loss = 0.26682110\n",
            "Iteration 81, loss = 0.26578206\n",
            "Iteration 82, loss = 0.26582263\n",
            "Iteration 83, loss = 0.26455974\n",
            "Iteration 84, loss = 0.26373478\n",
            "Iteration 85, loss = 0.26363278\n",
            "Iteration 86, loss = 0.26243261\n",
            "Iteration 87, loss = 0.26248890\n",
            "Iteration 88, loss = 0.26117537\n",
            "Iteration 89, loss = 0.26084241\n",
            "Iteration 90, loss = 0.26161325\n",
            "Iteration 91, loss = 0.26105447\n",
            "Iteration 92, loss = 0.26077997\n",
            "Iteration 93, loss = 0.25952631\n",
            "Iteration 94, loss = 0.25830303\n",
            "Iteration 95, loss = 0.26128931\n",
            "Iteration 96, loss = 0.25878374\n",
            "Iteration 97, loss = 0.25945515\n",
            "Iteration 98, loss = 0.25911737\n",
            "Iteration 99, loss = 0.26064044\n",
            "Iteration 100, loss = 0.25824960\n",
            "Iteration 101, loss = 0.26515352\n",
            "Iteration 102, loss = 0.25929255\n",
            "Iteration 103, loss = 0.25768893\n",
            "Iteration 104, loss = 0.26112703\n",
            "Iteration 105, loss = 0.26257375\n",
            "Iteration 106, loss = 0.25977778\n",
            "Iteration 107, loss = 0.25650291\n",
            "Iteration 108, loss = 0.25888501\n",
            "Iteration 109, loss = 0.25844296\n",
            "Iteration 110, loss = 0.25901393\n",
            "Iteration 111, loss = 0.26311213\n",
            "Iteration 112, loss = 0.26546018\n",
            "Iteration 113, loss = 0.26654683\n",
            "Iteration 114, loss = 0.26554348\n",
            "Iteration 115, loss = 0.26540015\n",
            "Iteration 116, loss = 0.27136280\n",
            "Iteration 117, loss = 0.26146655\n",
            "Iteration 118, loss = 0.26572935\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 119, loss = 0.24752506\n",
            "Iteration 120, loss = 0.24743728\n",
            "Iteration 121, loss = 0.24741382\n",
            "Iteration 122, loss = 0.24740774\n",
            "Iteration 123, loss = 0.24738852\n",
            "Iteration 124, loss = 0.24734566\n",
            "Iteration 125, loss = 0.24731934\n",
            "Iteration 126, loss = 0.24731148\n",
            "Iteration 127, loss = 0.24727631\n",
            "Iteration 128, loss = 0.24723122\n",
            "Iteration 129, loss = 0.24718632\n",
            "Iteration 130, loss = 0.24720054\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 131, loss = 0.24689897\n",
            "Iteration 132, loss = 0.24690635\n",
            "Iteration 133, loss = 0.24689636\n",
            "Iteration 134, loss = 0.24689369\n",
            "Iteration 135, loss = 0.24691136\n",
            "Iteration 136, loss = 0.24689635\n",
            "Iteration 137, loss = 0.24686815\n",
            "Iteration 138, loss = 0.24686637\n",
            "Iteration 139, loss = 0.24685672\n",
            "Iteration 140, loss = 0.24685275\n",
            "Iteration 141, loss = 0.24686392\n",
            "Iteration 142, loss = 0.24684346\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 143, loss = 0.24676894\n",
            "Iteration 144, loss = 0.24676944\n",
            "Iteration 145, loss = 0.24675906\n",
            "Iteration 146, loss = 0.24676270\n",
            "Iteration 147, loss = 0.24676145\n",
            "Iteration 148, loss = 0.24676055\n",
            "Iteration 149, loss = 0.24676532\n",
            "Iteration 150, loss = 0.24676588\n",
            "Iteration 151, loss = 0.24675543\n",
            "Iteration 152, loss = 0.24675885\n",
            "Iteration 153, loss = 0.24675523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64485788\n",
            "Iteration 3, loss = 0.61511432\n",
            "Iteration 4, loss = 0.59619906\n",
            "Iteration 5, loss = 0.58521951\n",
            "Iteration 6, loss = 0.57667286\n",
            "Iteration 7, loss = 0.57391218\n",
            "Iteration 8, loss = 0.56984557\n",
            "Iteration 9, loss = 0.56873927\n",
            "Iteration 10, loss = 0.56979739\n",
            "Iteration 11, loss = 0.56592117\n",
            "Iteration 12, loss = 0.60383555\n",
            "Iteration 13, loss = 0.61736047\n",
            "Iteration 14, loss = 0.60669094\n",
            "Iteration 15, loss = 0.59081704\n",
            "Iteration 16, loss = 0.57885598\n",
            "Iteration 17, loss = 0.57425493\n",
            "Iteration 18, loss = 0.57156033\n",
            "Iteration 19, loss = 0.56959520\n",
            "Iteration 20, loss = 0.56948103\n",
            "Iteration 21, loss = 0.56784453\n",
            "Iteration 22, loss = 0.56901975\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.53129790\n",
            "Iteration 24, loss = 0.51467827\n",
            "Iteration 25, loss = 0.51150702\n",
            "Iteration 26, loss = 0.52642843\n",
            "Iteration 27, loss = 0.52354809\n",
            "Iteration 28, loss = 0.52749873\n",
            "Iteration 29, loss = 0.52566163\n",
            "Iteration 30, loss = 0.52431445\n",
            "Iteration 31, loss = 0.52619234\n",
            "Iteration 32, loss = 0.52058290\n",
            "Iteration 33, loss = 0.52314158\n",
            "Iteration 34, loss = 0.52281831\n",
            "Iteration 35, loss = 0.52072192\n",
            "Iteration 36, loss = 0.52033600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.44345830\n",
            "Iteration 38, loss = 0.43150493\n",
            "Iteration 39, loss = 0.42095452\n",
            "Iteration 40, loss = 0.41148919\n",
            "Iteration 41, loss = 0.40227915\n",
            "Iteration 42, loss = 0.39391074\n",
            "Iteration 43, loss = 0.38658447\n",
            "Iteration 44, loss = 0.37964354\n",
            "Iteration 45, loss = 0.37347838\n",
            "Iteration 46, loss = 0.36771250\n",
            "Iteration 47, loss = 0.36328294\n",
            "Iteration 48, loss = 0.35839596\n",
            "Iteration 49, loss = 0.35522903\n",
            "Iteration 50, loss = 0.35168709\n",
            "Iteration 51, loss = 0.34808427\n",
            "Iteration 52, loss = 0.34552721\n",
            "Iteration 53, loss = 0.34502665\n",
            "Iteration 54, loss = 0.34151232\n",
            "Iteration 55, loss = 0.34066889\n",
            "Iteration 56, loss = 0.33251433\n",
            "Iteration 57, loss = 0.31484968\n",
            "Iteration 58, loss = 0.30924719\n",
            "Iteration 59, loss = 0.30625135\n",
            "Iteration 60, loss = 0.30322887\n",
            "Iteration 61, loss = 0.30243524\n",
            "Iteration 62, loss = 0.29818122\n",
            "Iteration 63, loss = 0.29577935\n",
            "Iteration 64, loss = 0.29303521\n",
            "Iteration 65, loss = 0.29257837\n",
            "Iteration 66, loss = 0.29360370\n",
            "Iteration 67, loss = 0.29068448\n",
            "Iteration 68, loss = 0.28968497\n",
            "Iteration 69, loss = 0.28867241\n",
            "Iteration 70, loss = 0.29549619\n",
            "Iteration 71, loss = 0.28862005\n",
            "Iteration 72, loss = 0.28680479\n",
            "Iteration 73, loss = 0.28664799\n",
            "Iteration 74, loss = 0.28697682\n",
            "Iteration 75, loss = 0.30229278\n",
            "Iteration 76, loss = 0.29225718\n",
            "Iteration 77, loss = 0.29138393\n",
            "Iteration 78, loss = 0.28914366\n",
            "Iteration 79, loss = 0.28701870\n",
            "Iteration 80, loss = 0.28929156\n",
            "Iteration 81, loss = 0.28675213\n",
            "Iteration 82, loss = 0.28626041\n",
            "Iteration 83, loss = 0.30930454\n",
            "Iteration 84, loss = 0.29123998\n",
            "Iteration 85, loss = 0.29264021\n",
            "Iteration 86, loss = 0.29302539\n",
            "Iteration 87, loss = 0.29174591\n",
            "Iteration 88, loss = 0.29137449\n",
            "Iteration 89, loss = 0.28933663\n",
            "Iteration 90, loss = 0.28985588\n",
            "Iteration 91, loss = 0.29422557\n",
            "Iteration 92, loss = 0.28941601\n",
            "Iteration 93, loss = 0.28672394\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.25939262\n",
            "Iteration 95, loss = 0.25921334\n",
            "Iteration 96, loss = 0.25916952\n",
            "Iteration 97, loss = 0.25907685\n",
            "Iteration 98, loss = 0.25903178\n",
            "Iteration 99, loss = 0.25894050\n",
            "Iteration 100, loss = 0.25883488\n",
            "Iteration 101, loss = 0.25876752\n",
            "Iteration 102, loss = 0.25872268\n",
            "Iteration 103, loss = 0.25859394\n",
            "Iteration 104, loss = 0.25852698\n",
            "Iteration 105, loss = 0.25849357\n",
            "Iteration 106, loss = 0.25838765\n",
            "Iteration 107, loss = 0.25827728\n",
            "Iteration 108, loss = 0.25831184\n",
            "Iteration 109, loss = 0.25821336\n",
            "Iteration 110, loss = 0.25809074\n",
            "Iteration 111, loss = 0.25806008\n",
            "Iteration 112, loss = 0.25802362\n",
            "Iteration 113, loss = 0.25800782\n",
            "Iteration 114, loss = 0.25790508\n",
            "Iteration 115, loss = 0.25785659\n",
            "Iteration 116, loss = 0.25776199\n",
            "Iteration 117, loss = 0.25767560\n",
            "Iteration 118, loss = 0.25767111\n",
            "Iteration 119, loss = 0.25755000\n",
            "Iteration 120, loss = 0.25751764\n",
            "Iteration 121, loss = 0.25743475\n",
            "Iteration 122, loss = 0.25741878\n",
            "Iteration 123, loss = 0.25736444\n",
            "Iteration 124, loss = 0.25732957\n",
            "Iteration 125, loss = 0.25722576\n",
            "Iteration 126, loss = 0.25724617\n",
            "Iteration 127, loss = 0.25712405\n",
            "Iteration 128, loss = 0.25708119\n",
            "Iteration 129, loss = 0.25702981\n",
            "Iteration 130, loss = 0.25696498\n",
            "Iteration 131, loss = 0.25695054\n",
            "Iteration 132, loss = 0.25694473\n",
            "Iteration 133, loss = 0.25677378\n",
            "Iteration 134, loss = 0.25676830\n",
            "Iteration 135, loss = 0.25675512\n",
            "Iteration 136, loss = 0.25668101\n",
            "Iteration 137, loss = 0.25665431\n",
            "Iteration 138, loss = 0.25666879\n",
            "Iteration 139, loss = 0.25654274\n",
            "Iteration 140, loss = 0.25649778\n",
            "Iteration 141, loss = 0.25641114\n",
            "Iteration 142, loss = 0.25640799\n",
            "Iteration 143, loss = 0.25638075\n",
            "Iteration 144, loss = 0.25636102\n",
            "Iteration 145, loss = 0.25632263\n",
            "Iteration 146, loss = 0.25625044\n",
            "Iteration 147, loss = 0.25614812\n",
            "Iteration 148, loss = 0.25619364\n",
            "Iteration 149, loss = 0.25612673\n",
            "Iteration 150, loss = 0.25609969\n",
            "Iteration 151, loss = 0.25604090\n",
            "Iteration 152, loss = 0.25601554\n",
            "Iteration 153, loss = 0.25596822\n",
            "Iteration 154, loss = 0.25596400\n",
            "Iteration 155, loss = 0.25584893\n",
            "Iteration 156, loss = 0.25585287\n",
            "Iteration 157, loss = 0.25577971\n",
            "Iteration 158, loss = 0.25576968\n",
            "Iteration 159, loss = 0.25570789\n",
            "Iteration 160, loss = 0.25566263\n",
            "Iteration 161, loss = 0.25568828\n",
            "Iteration 162, loss = 0.25557175\n",
            "Iteration 163, loss = 0.25563663\n",
            "Iteration 164, loss = 0.25555967\n",
            "Iteration 165, loss = 0.25551372\n",
            "Iteration 166, loss = 0.25547196\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 167, loss = 0.25516510\n",
            "Iteration 168, loss = 0.25510763\n",
            "Iteration 169, loss = 0.25513019\n",
            "Iteration 170, loss = 0.25514655\n",
            "Iteration 171, loss = 0.25512491\n",
            "Iteration 172, loss = 0.25511207\n",
            "Iteration 173, loss = 0.25511343\n",
            "Iteration 174, loss = 0.25509275\n",
            "Iteration 175, loss = 0.25509558\n",
            "Iteration 176, loss = 0.25507829\n",
            "Iteration 177, loss = 0.25508252\n",
            "Iteration 178, loss = 0.25510000\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 179, loss = 0.25497059\n",
            "Iteration 180, loss = 0.25496849\n",
            "Iteration 181, loss = 0.25496668\n",
            "Iteration 182, loss = 0.25495461\n",
            "Iteration 183, loss = 0.25493623\n",
            "Iteration 184, loss = 0.25495098\n",
            "Iteration 185, loss = 0.25494865\n",
            "Iteration 186, loss = 0.25495136\n",
            "Iteration 187, loss = 0.25494631\n",
            "Iteration 188, loss = 0.25494296\n",
            "Iteration 189, loss = 0.25494464\n",
            "Iteration 190, loss = 0.25494624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64027647\n",
            "Iteration 3, loss = 0.61363174\n",
            "Iteration 4, loss = 0.59620753\n",
            "Iteration 5, loss = 0.58507409\n",
            "Iteration 6, loss = 0.58027842\n",
            "Iteration 7, loss = 0.57373009\n",
            "Iteration 8, loss = 0.57329140\n",
            "Iteration 9, loss = 0.57143977\n",
            "Iteration 10, loss = 0.57045176\n",
            "Iteration 11, loss = 0.56870113\n",
            "Iteration 12, loss = 0.56934777\n",
            "Iteration 13, loss = 0.56826975\n",
            "Iteration 14, loss = 0.56944110\n",
            "Iteration 15, loss = 0.56966999\n",
            "Iteration 16, loss = 0.57022055\n",
            "Iteration 17, loss = 0.56945125\n",
            "Iteration 18, loss = 0.57052762\n",
            "Iteration 19, loss = 0.57036174\n",
            "Iteration 20, loss = 0.56889211\n",
            "Iteration 21, loss = 0.57054010\n",
            "Iteration 22, loss = 0.57037760\n",
            "Iteration 23, loss = 0.57117264\n",
            "Iteration 24, loss = 0.57090843\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.51598530\n",
            "Iteration 26, loss = 0.49820721\n",
            "Iteration 27, loss = 0.51334048\n",
            "Iteration 28, loss = 0.52005576\n",
            "Iteration 29, loss = 0.52179413\n",
            "Iteration 30, loss = 0.52008977\n",
            "Iteration 31, loss = 0.51691053\n",
            "Iteration 32, loss = 0.52086898\n",
            "Iteration 33, loss = 0.51911399\n",
            "Iteration 34, loss = 0.52262919\n",
            "Iteration 35, loss = 0.51977834\n",
            "Iteration 36, loss = 0.52017877\n",
            "Iteration 37, loss = 0.51943663\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.43389576\n",
            "Iteration 39, loss = 0.42025601\n",
            "Iteration 40, loss = 0.40874046\n",
            "Iteration 41, loss = 0.39791098\n",
            "Iteration 42, loss = 0.38821871\n",
            "Iteration 43, loss = 0.37921835\n",
            "Iteration 44, loss = 0.37132554\n",
            "Iteration 45, loss = 0.36430832\n",
            "Iteration 46, loss = 0.35744442\n",
            "Iteration 47, loss = 0.35227613\n",
            "Iteration 48, loss = 0.34756260\n",
            "Iteration 49, loss = 0.34212979\n",
            "Iteration 50, loss = 0.33931125\n",
            "Iteration 51, loss = 0.33564876\n",
            "Iteration 52, loss = 0.32745722\n",
            "Iteration 53, loss = 0.31586530\n",
            "Iteration 54, loss = 0.31069583\n",
            "Iteration 55, loss = 0.30639574\n",
            "Iteration 56, loss = 0.30247312\n",
            "Iteration 57, loss = 0.29912019\n",
            "Iteration 58, loss = 0.29625427\n",
            "Iteration 59, loss = 0.29277184\n",
            "Iteration 60, loss = 0.29093659\n",
            "Iteration 61, loss = 0.28855177\n",
            "Iteration 62, loss = 0.28709914\n",
            "Iteration 63, loss = 0.28596110\n",
            "Iteration 64, loss = 0.28357459\n",
            "Iteration 65, loss = 0.28073469\n",
            "Iteration 66, loss = 0.28012051\n",
            "Iteration 67, loss = 0.28129493\n",
            "Iteration 68, loss = 0.27898924\n",
            "Iteration 69, loss = 0.27689510\n",
            "Iteration 70, loss = 0.27793795\n",
            "Iteration 71, loss = 0.27604783\n",
            "Iteration 72, loss = 0.27607638\n",
            "Iteration 73, loss = 0.27897946\n",
            "Iteration 74, loss = 0.27701488\n",
            "Iteration 75, loss = 0.27290902\n",
            "Iteration 76, loss = 0.27554580\n",
            "Iteration 77, loss = 0.27290438\n",
            "Iteration 78, loss = 0.28312490\n",
            "Iteration 79, loss = 0.28585888\n",
            "Iteration 80, loss = 0.28078509\n",
            "Iteration 81, loss = 0.28012026\n",
            "Iteration 82, loss = 0.28214910\n",
            "Iteration 83, loss = 0.27303669\n",
            "Iteration 84, loss = 0.28631243\n",
            "Iteration 85, loss = 0.28425304\n",
            "Iteration 86, loss = 0.28123797\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 87, loss = 0.25724305\n",
            "Iteration 88, loss = 0.25695665\n",
            "Iteration 89, loss = 0.25689524\n",
            "Iteration 90, loss = 0.25678900\n",
            "Iteration 91, loss = 0.25670333\n",
            "Iteration 92, loss = 0.25663293\n",
            "Iteration 93, loss = 0.25653778\n",
            "Iteration 94, loss = 0.25644082\n",
            "Iteration 95, loss = 0.25639009\n",
            "Iteration 96, loss = 0.25633521\n",
            "Iteration 97, loss = 0.25616950\n",
            "Iteration 98, loss = 0.25610773\n",
            "Iteration 99, loss = 0.25606732\n",
            "Iteration 100, loss = 0.25601144\n",
            "Iteration 101, loss = 0.25589442\n",
            "Iteration 102, loss = 0.25581098\n",
            "Iteration 103, loss = 0.25580579\n",
            "Iteration 104, loss = 0.25569010\n",
            "Iteration 105, loss = 0.25560821\n",
            "Iteration 106, loss = 0.25557822\n",
            "Iteration 107, loss = 0.25548306\n",
            "Iteration 108, loss = 0.25544971\n",
            "Iteration 109, loss = 0.25537671\n",
            "Iteration 110, loss = 0.25525943\n",
            "Iteration 111, loss = 0.25522188\n",
            "Iteration 112, loss = 0.25510879\n",
            "Iteration 113, loss = 0.25509673\n",
            "Iteration 114, loss = 0.25500172\n",
            "Iteration 115, loss = 0.25498711\n",
            "Iteration 116, loss = 0.25484493\n",
            "Iteration 117, loss = 0.25486254\n",
            "Iteration 118, loss = 0.25481043\n",
            "Iteration 119, loss = 0.25474170\n",
            "Iteration 120, loss = 0.25464888\n",
            "Iteration 121, loss = 0.25467301\n",
            "Iteration 122, loss = 0.25459017\n",
            "Iteration 123, loss = 0.25450670\n",
            "Iteration 124, loss = 0.25447191\n",
            "Iteration 125, loss = 0.25442471\n",
            "Iteration 126, loss = 0.25433448\n",
            "Iteration 127, loss = 0.25432019\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 128, loss = 0.25397159\n",
            "Iteration 129, loss = 0.25393834\n",
            "Iteration 130, loss = 0.25393251\n",
            "Iteration 131, loss = 0.25391352\n",
            "Iteration 132, loss = 0.25390897\n",
            "Iteration 133, loss = 0.25390876\n",
            "Iteration 134, loss = 0.25390475\n",
            "Iteration 135, loss = 0.25390037\n",
            "Iteration 136, loss = 0.25386063\n",
            "Iteration 137, loss = 0.25384560\n",
            "Iteration 138, loss = 0.25386456\n",
            "Iteration 139, loss = 0.25381880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 140, loss = 0.25374333\n",
            "Iteration 141, loss = 0.25373851\n",
            "Iteration 142, loss = 0.25373845\n",
            "Iteration 143, loss = 0.25373702\n",
            "Iteration 144, loss = 0.25373030\n",
            "Iteration 145, loss = 0.25372115\n",
            "Iteration 146, loss = 0.25372521\n",
            "Iteration 147, loss = 0.25372506\n",
            "Iteration 148, loss = 0.25372223\n",
            "Iteration 149, loss = 0.25372303\n",
            "Iteration 150, loss = 0.25372177\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64444054\n",
            "Iteration 3, loss = 0.61348454\n",
            "Iteration 4, loss = 0.59423629\n",
            "Iteration 5, loss = 0.58313700\n",
            "Iteration 6, loss = 0.57694873\n",
            "Iteration 7, loss = 0.57174778\n",
            "Iteration 8, loss = 0.56909049\n",
            "Iteration 9, loss = 0.56857212\n",
            "Iteration 10, loss = 0.56867905\n",
            "Iteration 11, loss = 0.56768875\n",
            "Iteration 12, loss = 0.56727869\n",
            "Iteration 13, loss = 0.56851954\n",
            "Iteration 14, loss = 0.56724315\n",
            "Iteration 15, loss = 0.56668148\n",
            "Iteration 16, loss = 0.56935267\n",
            "Iteration 17, loss = 0.56732458\n",
            "Iteration 18, loss = 0.56816835\n",
            "Iteration 19, loss = 0.56745514\n",
            "Iteration 20, loss = 0.56719521\n",
            "Iteration 21, loss = 0.56787628\n",
            "Iteration 22, loss = 0.56690519\n",
            "Iteration 23, loss = 0.56610498\n",
            "Iteration 24, loss = 0.56581207\n",
            "Iteration 25, loss = 0.56724512\n",
            "Iteration 26, loss = 0.56647541\n",
            "Iteration 27, loss = 0.56600018\n",
            "Iteration 28, loss = 0.56544919\n",
            "Iteration 29, loss = 0.56529836\n",
            "Iteration 30, loss = 0.56697765\n",
            "Iteration 31, loss = 0.56525894\n",
            "Iteration 32, loss = 0.56624539\n",
            "Iteration 33, loss = 0.56547267\n",
            "Iteration 34, loss = 0.56363303\n",
            "Iteration 35, loss = 0.56461251\n",
            "Iteration 36, loss = 0.59288854\n",
            "Iteration 37, loss = 0.60234658\n",
            "Iteration 38, loss = 0.58038268\n",
            "Iteration 39, loss = 0.56996078\n",
            "Iteration 40, loss = 0.56393167\n",
            "Iteration 41, loss = 0.56207159\n",
            "Iteration 42, loss = 0.56094343\n",
            "Iteration 43, loss = 0.56034170\n",
            "Iteration 44, loss = 0.56022547\n",
            "Iteration 45, loss = 0.56060795\n",
            "Iteration 46, loss = 0.55915639\n",
            "Iteration 47, loss = 0.55891232\n",
            "Iteration 48, loss = 0.56229504\n",
            "Iteration 49, loss = 0.59373607\n",
            "Iteration 50, loss = 0.59423463\n",
            "Iteration 51, loss = 0.57483139\n",
            "Iteration 52, loss = 0.56614323\n",
            "Iteration 53, loss = 0.56248349\n",
            "Iteration 54, loss = 0.55956013\n",
            "Iteration 55, loss = 0.55657774\n",
            "Iteration 56, loss = 0.56671667\n",
            "Iteration 57, loss = 0.56338165\n",
            "Iteration 58, loss = 0.56191536\n",
            "Iteration 59, loss = 0.55967705\n",
            "Iteration 60, loss = 0.60725158\n",
            "Iteration 61, loss = 0.57577247\n",
            "Iteration 62, loss = 0.56488186\n",
            "Iteration 63, loss = 0.56020189\n",
            "Iteration 64, loss = 0.55745728\n",
            "Iteration 65, loss = 0.55589354\n",
            "Iteration 66, loss = 0.55572657\n",
            "Iteration 67, loss = 0.56955647\n",
            "Iteration 68, loss = 0.56464604\n",
            "Iteration 69, loss = 0.55895006\n",
            "Iteration 70, loss = 0.55786506\n",
            "Iteration 71, loss = 0.56162210\n",
            "Iteration 72, loss = 0.55957255\n",
            "Iteration 73, loss = 0.55702436\n",
            "Iteration 74, loss = 0.55605945\n",
            "Iteration 75, loss = 0.55679895\n",
            "Iteration 76, loss = 0.55941997\n",
            "Iteration 77, loss = 0.55694809\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 78, loss = 0.50533279\n",
            "Iteration 79, loss = 0.48364238\n",
            "Iteration 80, loss = 0.50222028\n",
            "Iteration 81, loss = 0.50653883\n",
            "Iteration 82, loss = 0.50435081\n",
            "Iteration 83, loss = 0.50431374\n",
            "Iteration 84, loss = 0.50203664\n",
            "Iteration 85, loss = 0.50220251\n",
            "Iteration 86, loss = 0.51115371\n",
            "Iteration 87, loss = 0.51057779\n",
            "Iteration 88, loss = 0.56751668\n",
            "Iteration 89, loss = 0.60351701\n",
            "Iteration 90, loss = 0.57692407\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 91, loss = 0.56176875\n",
            "Iteration 92, loss = 0.55803238\n",
            "Iteration 93, loss = 0.55427954\n",
            "Iteration 94, loss = 0.55002506\n",
            "Iteration 95, loss = 0.54584402\n",
            "Iteration 96, loss = 0.54176534\n",
            "Iteration 97, loss = 0.53688263\n",
            "Iteration 98, loss = 0.53212405\n",
            "Iteration 99, loss = 0.52684896\n",
            "Iteration 100, loss = 0.52135380\n",
            "Iteration 101, loss = 0.51583039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 102, loss = 0.51097081\n",
            "Iteration 103, loss = 0.50973526\n",
            "Iteration 104, loss = 0.50866396\n",
            "Iteration 105, loss = 0.50731473\n",
            "Iteration 106, loss = 0.50613659\n",
            "Iteration 107, loss = 0.50472842\n",
            "Iteration 108, loss = 0.50346947\n",
            "Iteration 109, loss = 0.50230706\n",
            "Iteration 110, loss = 0.50105651\n",
            "Iteration 111, loss = 0.49967037\n",
            "Iteration 112, loss = 0.49830601\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.49714751\n",
            "Iteration 114, loss = 0.49684950\n",
            "Iteration 115, loss = 0.49661491\n",
            "Iteration 116, loss = 0.49630024\n",
            "Iteration 117, loss = 0.49606562\n",
            "Iteration 118, loss = 0.49577203\n",
            "Iteration 119, loss = 0.49550030\n",
            "Iteration 120, loss = 0.49526493\n",
            "Iteration 121, loss = 0.49497398\n",
            "Iteration 122, loss = 0.49474464\n",
            "Iteration 123, loss = 0.49438370\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 124, loss = 0.49414990\n",
            "Iteration 125, loss = 0.49408863\n",
            "Iteration 126, loss = 0.49402906\n",
            "Iteration 127, loss = 0.49398668\n",
            "Iteration 128, loss = 0.49392115\n",
            "Iteration 129, loss = 0.49387607\n",
            "Iteration 130, loss = 0.49382023\n",
            "Iteration 131, loss = 0.49376035\n",
            "Iteration 132, loss = 0.49371089\n",
            "Iteration 133, loss = 0.49365807\n",
            "Iteration 134, loss = 0.49358332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64227986\n",
            "Iteration 3, loss = 0.62834666\n",
            "Iteration 4, loss = 0.63430197\n",
            "Iteration 5, loss = 0.59859667\n",
            "Iteration 6, loss = 0.58541254\n",
            "Iteration 7, loss = 0.57789668\n",
            "Iteration 8, loss = 0.57303931\n",
            "Iteration 9, loss = 0.57084920\n",
            "Iteration 10, loss = 0.56995162\n",
            "Iteration 11, loss = 0.56928363\n",
            "Iteration 12, loss = 0.56919506\n",
            "Iteration 13, loss = 0.56964355\n",
            "Iteration 14, loss = 0.56951168\n",
            "Iteration 15, loss = 0.57039227\n",
            "Iteration 16, loss = 0.56898665\n",
            "Iteration 17, loss = 0.56856733\n",
            "Iteration 18, loss = 0.57015310\n",
            "Iteration 19, loss = 0.57078279\n",
            "Iteration 20, loss = 0.57058690\n",
            "Iteration 21, loss = 0.56973199\n",
            "Iteration 22, loss = 0.56987594\n",
            "Iteration 23, loss = 0.57133691\n",
            "Iteration 24, loss = 0.57062129\n",
            "Iteration 25, loss = 0.57237368\n",
            "Iteration 26, loss = 0.57027710\n",
            "Iteration 27, loss = 0.57197334\n",
            "Iteration 28, loss = 0.57110871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.51653522\n",
            "Iteration 30, loss = 0.49742238\n",
            "Iteration 31, loss = 0.51737095\n",
            "Iteration 32, loss = 0.51946182\n",
            "Iteration 33, loss = 0.52117232\n",
            "Iteration 34, loss = 0.52091122\n",
            "Iteration 35, loss = 0.52376084\n",
            "Iteration 36, loss = 0.52009538\n",
            "Iteration 37, loss = 0.51974593\n",
            "Iteration 38, loss = 0.51978857\n",
            "Iteration 39, loss = 0.51751133\n",
            "Iteration 40, loss = 0.52174536\n",
            "Iteration 41, loss = 0.52131242\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 42, loss = 0.43512680\n",
            "Iteration 43, loss = 0.42218360\n",
            "Iteration 44, loss = 0.41075922\n",
            "Iteration 45, loss = 0.39992315\n",
            "Iteration 46, loss = 0.39038998\n",
            "Iteration 47, loss = 0.38153781\n",
            "Iteration 48, loss = 0.37366910\n",
            "Iteration 49, loss = 0.36636307\n",
            "Iteration 50, loss = 0.36004030\n",
            "Iteration 51, loss = 0.35470524\n",
            "Iteration 52, loss = 0.34953523\n",
            "Iteration 53, loss = 0.34527423\n",
            "Iteration 54, loss = 0.34143120\n",
            "Iteration 55, loss = 0.33839877\n",
            "Iteration 56, loss = 0.33494492\n",
            "Iteration 57, loss = 0.33274557\n",
            "Iteration 58, loss = 0.33002157\n",
            "Iteration 59, loss = 0.32730398\n",
            "Iteration 60, loss = 0.32728414\n",
            "Iteration 61, loss = 0.32474347\n",
            "Iteration 62, loss = 0.32357130\n",
            "Iteration 63, loss = 0.32396192\n",
            "Iteration 64, loss = 0.32156429\n",
            "Iteration 65, loss = 0.32233299\n",
            "Iteration 66, loss = 0.32078718\n",
            "Iteration 67, loss = 0.32023833\n",
            "Iteration 68, loss = 0.32249427\n",
            "Iteration 69, loss = 0.32254297\n",
            "Iteration 70, loss = 0.32275512\n",
            "Iteration 71, loss = 0.32044181\n",
            "Iteration 72, loss = 0.31995720\n",
            "Iteration 73, loss = 0.32185098\n",
            "Iteration 74, loss = 0.32297716\n",
            "Iteration 75, loss = 0.32155420\n",
            "Iteration 76, loss = 0.31950613\n",
            "Iteration 77, loss = 0.32268196\n",
            "Iteration 78, loss = 0.32044016\n",
            "Iteration 79, loss = 0.31955325\n",
            "Iteration 80, loss = 0.32505162\n",
            "Iteration 81, loss = 0.32057530\n",
            "Iteration 82, loss = 0.32590472\n",
            "Iteration 83, loss = 0.32409767\n",
            "Iteration 84, loss = 0.32427531\n",
            "Iteration 85, loss = 0.31772240\n",
            "Iteration 86, loss = 0.32125601\n",
            "Iteration 87, loss = 0.32502633\n",
            "Iteration 88, loss = 0.32422989\n",
            "Iteration 89, loss = 0.31996390\n",
            "Iteration 90, loss = 0.31940596\n",
            "Iteration 91, loss = 0.32558660\n",
            "Iteration 92, loss = 0.32372805\n",
            "Iteration 93, loss = 0.32279021\n",
            "Iteration 94, loss = 0.32346469\n",
            "Iteration 95, loss = 0.31295476\n",
            "Iteration 96, loss = 0.32035155\n",
            "Iteration 97, loss = 0.32332185\n",
            "Iteration 98, loss = 0.31862333\n",
            "Iteration 99, loss = 0.32485331\n",
            "Iteration 100, loss = 0.31230118\n",
            "Iteration 101, loss = 0.31520597\n",
            "Iteration 102, loss = 0.31887887\n",
            "Iteration 103, loss = 0.32288530\n",
            "Iteration 104, loss = 0.31507682\n",
            "Iteration 105, loss = 0.32264538\n",
            "Iteration 106, loss = 0.32629133\n",
            "Iteration 107, loss = 0.32155526\n",
            "Iteration 108, loss = 0.32249611\n",
            "Iteration 109, loss = 0.31797424\n",
            "Iteration 110, loss = 0.31738590\n",
            "Iteration 111, loss = 0.32257529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 112, loss = 0.27914596\n",
            "Iteration 113, loss = 0.27888503\n",
            "Iteration 114, loss = 0.27870450\n",
            "Iteration 115, loss = 0.27855530\n",
            "Iteration 116, loss = 0.27847728\n",
            "Iteration 117, loss = 0.27836382\n",
            "Iteration 118, loss = 0.27823625\n",
            "Iteration 119, loss = 0.27813816\n",
            "Iteration 120, loss = 0.27807520\n",
            "Iteration 121, loss = 0.27799057\n",
            "Iteration 122, loss = 0.27792000\n",
            "Iteration 123, loss = 0.27777519\n",
            "Iteration 124, loss = 0.27774063\n",
            "Iteration 125, loss = 0.27765974\n",
            "Iteration 126, loss = 0.27750462\n",
            "Iteration 127, loss = 0.27748772\n",
            "Iteration 128, loss = 0.27736051\n",
            "Iteration 129, loss = 0.27733861\n",
            "Iteration 130, loss = 0.27726963\n",
            "Iteration 131, loss = 0.27711844\n",
            "Iteration 132, loss = 0.27708889\n",
            "Iteration 133, loss = 0.27701774\n",
            "Iteration 134, loss = 0.27693581\n",
            "Iteration 135, loss = 0.27684456\n",
            "Iteration 136, loss = 0.27677829\n",
            "Iteration 137, loss = 0.27661132\n",
            "Iteration 138, loss = 0.27659081\n",
            "Iteration 139, loss = 0.27650391\n",
            "Iteration 140, loss = 0.27645045\n",
            "Iteration 141, loss = 0.27641146\n",
            "Iteration 142, loss = 0.27629209\n",
            "Iteration 143, loss = 0.27625044\n",
            "Iteration 144, loss = 0.27610925\n",
            "Iteration 145, loss = 0.27609825\n",
            "Iteration 146, loss = 0.27598691\n",
            "Iteration 147, loss = 0.27589234\n",
            "Iteration 148, loss = 0.27585330\n",
            "Iteration 149, loss = 0.27584348\n",
            "Iteration 150, loss = 0.27572836\n",
            "Iteration 151, loss = 0.27563587\n",
            "Iteration 152, loss = 0.27558574\n",
            "Iteration 153, loss = 0.27551399\n",
            "Iteration 154, loss = 0.27544663\n",
            "Iteration 155, loss = 0.27540551\n",
            "Iteration 156, loss = 0.27533389\n",
            "Iteration 157, loss = 0.27520867\n",
            "Iteration 158, loss = 0.27515476\n",
            "Iteration 159, loss = 0.27506608\n",
            "Iteration 160, loss = 0.27500840\n",
            "Iteration 161, loss = 0.27498067\n",
            "Iteration 162, loss = 0.27486111\n",
            "Iteration 163, loss = 0.27478645\n",
            "Iteration 164, loss = 0.27474358\n",
            "Iteration 165, loss = 0.27465875\n",
            "Iteration 166, loss = 0.27466812\n",
            "Iteration 167, loss = 0.27455897\n",
            "Iteration 168, loss = 0.27445804\n",
            "Iteration 169, loss = 0.27444417\n",
            "Iteration 170, loss = 0.27435770\n",
            "Iteration 171, loss = 0.27429663\n",
            "Iteration 172, loss = 0.27428224\n",
            "Iteration 173, loss = 0.27413869\n",
            "Iteration 174, loss = 0.27412750\n",
            "Iteration 175, loss = 0.27405214\n",
            "Iteration 176, loss = 0.27408548\n",
            "Iteration 177, loss = 0.27394206\n",
            "Iteration 178, loss = 0.27391743\n",
            "Iteration 179, loss = 0.27374473\n",
            "Iteration 180, loss = 0.27374646\n",
            "Iteration 181, loss = 0.27365973\n",
            "Iteration 182, loss = 0.27365201\n",
            "Iteration 183, loss = 0.27351449\n",
            "Iteration 184, loss = 0.27353594\n",
            "Iteration 185, loss = 0.27343886\n",
            "Iteration 186, loss = 0.27336873\n",
            "Iteration 187, loss = 0.27335896\n",
            "Iteration 188, loss = 0.27322925\n",
            "Iteration 189, loss = 0.27317753\n",
            "Iteration 190, loss = 0.27320576\n",
            "Iteration 191, loss = 0.27306586\n",
            "Iteration 192, loss = 0.27303807\n",
            "Iteration 193, loss = 0.27299433\n",
            "Iteration 194, loss = 0.27289092\n",
            "Iteration 195, loss = 0.27289804\n",
            "Iteration 196, loss = 0.27282278\n",
            "Iteration 197, loss = 0.27277951\n",
            "Iteration 198, loss = 0.27268873\n",
            "Iteration 199, loss = 0.27264331\n",
            "Iteration 200, loss = 0.27259304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 58.8min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68171525\n",
            "Iteration 3, loss = 0.67011339\n",
            "Iteration 4, loss = 0.66563650\n",
            "Iteration 5, loss = 0.65074094\n",
            "Iteration 6, loss = 0.63701749\n",
            "Iteration 7, loss = 0.63252123\n",
            "Iteration 8, loss = 0.62115049\n",
            "Iteration 9, loss = 0.61646876\n",
            "Iteration 10, loss = 0.60523876\n",
            "Iteration 11, loss = 0.60602610\n",
            "Iteration 12, loss = 0.60211752\n",
            "Iteration 13, loss = 0.59898779\n",
            "Iteration 14, loss = 0.59332125\n",
            "Iteration 15, loss = 0.59037516\n",
            "Iteration 16, loss = 0.58539844\n",
            "Iteration 17, loss = 0.58437597\n",
            "Iteration 18, loss = 0.58150616\n",
            "Iteration 19, loss = 0.57755999\n",
            "Iteration 20, loss = 0.58072465\n",
            "Iteration 21, loss = 0.57411799\n",
            "Iteration 22, loss = 0.57509488\n",
            "Iteration 23, loss = 0.57606852\n",
            "Iteration 24, loss = 0.57107262\n",
            "Iteration 25, loss = 0.57252118\n",
            "Iteration 26, loss = 0.57213272\n",
            "Iteration 27, loss = 0.57003363\n",
            "Iteration 28, loss = 0.56645478\n",
            "Iteration 29, loss = 0.57096490\n",
            "Iteration 30, loss = 0.56806157\n",
            "Iteration 31, loss = 0.56620042\n",
            "Iteration 32, loss = 0.56717692\n",
            "Iteration 33, loss = 0.56717239\n",
            "Iteration 34, loss = 0.56630961\n",
            "Iteration 35, loss = 0.56887367\n",
            "Iteration 36, loss = 0.56790218\n",
            "Iteration 37, loss = 0.56489681\n",
            "Iteration 38, loss = 0.56989415\n",
            "Iteration 39, loss = 0.56499236\n",
            "Iteration 40, loss = 0.57059499\n",
            "Iteration 41, loss = 0.56551794\n",
            "Iteration 42, loss = 0.56361292\n",
            "Iteration 43, loss = 0.56526404\n",
            "Iteration 44, loss = 0.57074970\n",
            "Iteration 45, loss = 0.56245436\n",
            "Iteration 46, loss = 0.56470221\n",
            "Iteration 47, loss = 0.56517771\n",
            "Iteration 48, loss = 0.57280064\n",
            "Iteration 49, loss = 0.56917505\n",
            "Iteration 50, loss = 0.56722338\n",
            "Iteration 51, loss = 0.56825157\n",
            "Iteration 52, loss = 0.56562095\n",
            "Iteration 53, loss = 0.56490853\n",
            "Iteration 54, loss = 0.56940245\n",
            "Iteration 55, loss = 0.56678945\n",
            "Iteration 56, loss = 0.56734750\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.52540499\n",
            "Iteration 58, loss = 0.51529164\n",
            "Iteration 59, loss = 0.50831508\n",
            "Iteration 60, loss = 0.50189627\n",
            "Iteration 61, loss = 0.49781831\n",
            "Iteration 62, loss = 0.49273673\n",
            "Iteration 63, loss = 0.48835900\n",
            "Iteration 64, loss = 0.48573089\n",
            "Iteration 65, loss = 0.48716773\n",
            "Iteration 66, loss = 0.50285806\n",
            "Iteration 67, loss = 0.49914272\n",
            "Iteration 68, loss = 0.51099180\n",
            "Iteration 69, loss = 0.50618890\n",
            "Iteration 70, loss = 0.51850442\n",
            "Iteration 71, loss = 0.51416390\n",
            "Iteration 72, loss = 0.52902815\n",
            "Iteration 73, loss = 0.53363460\n",
            "Iteration 74, loss = 0.51572438\n",
            "Iteration 75, loss = 0.52901086\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.44464722\n",
            "Iteration 77, loss = 0.44168886\n",
            "Iteration 78, loss = 0.43878665\n",
            "Iteration 79, loss = 0.43614190\n",
            "Iteration 80, loss = 0.43324149\n",
            "Iteration 81, loss = 0.43036944\n",
            "Iteration 82, loss = 0.42780986\n",
            "Iteration 83, loss = 0.42480402\n",
            "Iteration 84, loss = 0.42227722\n",
            "Iteration 85, loss = 0.41935922\n",
            "Iteration 86, loss = 0.41670548\n",
            "Iteration 87, loss = 0.41456991\n",
            "Iteration 88, loss = 0.41164256\n",
            "Iteration 89, loss = 0.40932965\n",
            "Iteration 90, loss = 0.40681730\n",
            "Iteration 91, loss = 0.40416789\n",
            "Iteration 92, loss = 0.40152673\n",
            "Iteration 93, loss = 0.39915556\n",
            "Iteration 94, loss = 0.39657219\n",
            "Iteration 95, loss = 0.39478886\n",
            "Iteration 96, loss = 0.39188479\n",
            "Iteration 97, loss = 0.38919893\n",
            "Iteration 98, loss = 0.38774131\n",
            "Iteration 99, loss = 0.38459200\n",
            "Iteration 100, loss = 0.38292486\n",
            "Iteration 101, loss = 0.38142400\n",
            "Iteration 102, loss = 0.37860650\n",
            "Iteration 103, loss = 0.37651796\n",
            "Iteration 104, loss = 0.37436754\n",
            "Iteration 105, loss = 0.37259593\n",
            "Iteration 106, loss = 0.37024684\n",
            "Iteration 107, loss = 0.36875899\n",
            "Iteration 108, loss = 0.36637913\n",
            "Iteration 109, loss = 0.36385078\n",
            "Iteration 110, loss = 0.36241611\n",
            "Iteration 111, loss = 0.36076268\n",
            "Iteration 112, loss = 0.35833194\n",
            "Iteration 113, loss = 0.35692530\n",
            "Iteration 114, loss = 0.35477039\n",
            "Iteration 115, loss = 0.35332920\n",
            "Iteration 116, loss = 0.35201525\n",
            "Iteration 117, loss = 0.34973294\n",
            "Iteration 118, loss = 0.34819601\n",
            "Iteration 119, loss = 0.34680131\n",
            "Iteration 120, loss = 0.34518563\n",
            "Iteration 121, loss = 0.34435785\n",
            "Iteration 122, loss = 0.34136834\n",
            "Iteration 123, loss = 0.33980176\n",
            "Iteration 124, loss = 0.33924407\n",
            "Iteration 125, loss = 0.33700693\n",
            "Iteration 126, loss = 0.33684145\n",
            "Iteration 127, loss = 0.33512930\n",
            "Iteration 128, loss = 0.33241499\n",
            "Iteration 129, loss = 0.33159348\n",
            "Iteration 130, loss = 0.33139117\n",
            "Iteration 131, loss = 0.32904862\n",
            "Iteration 132, loss = 0.32783628\n",
            "Iteration 133, loss = 0.32785312\n",
            "Iteration 134, loss = 0.32571269\n",
            "Iteration 135, loss = 0.32357114\n",
            "Iteration 136, loss = 0.32335676\n",
            "Iteration 137, loss = 0.32229009\n",
            "Iteration 138, loss = 0.32068236\n",
            "Iteration 139, loss = 0.31992857\n",
            "Iteration 140, loss = 0.31760683\n",
            "Iteration 141, loss = 0.31726500\n",
            "Iteration 142, loss = 0.31719213\n",
            "Iteration 143, loss = 0.31419100\n",
            "Iteration 144, loss = 0.31361246\n",
            "Iteration 145, loss = 0.31276013\n",
            "Iteration 146, loss = 0.31259544\n",
            "Iteration 147, loss = 0.31285493\n",
            "Iteration 148, loss = 0.30973352\n",
            "Iteration 149, loss = 0.30963204\n",
            "Iteration 150, loss = 0.30743236\n",
            "Iteration 151, loss = 0.30736592\n",
            "Iteration 152, loss = 0.30828966\n",
            "Iteration 153, loss = 0.30661581\n",
            "Iteration 154, loss = 0.30590931\n",
            "Iteration 155, loss = 0.30518079\n",
            "Iteration 156, loss = 0.30462701\n",
            "Iteration 157, loss = 0.30476511\n",
            "Iteration 158, loss = 0.30377708\n",
            "Iteration 159, loss = 0.30070944\n",
            "Iteration 160, loss = 0.30069833\n",
            "Iteration 161, loss = 0.30069892\n",
            "Iteration 162, loss = 0.30067317\n",
            "Iteration 163, loss = 0.29947663\n",
            "Iteration 164, loss = 0.29723453\n",
            "Iteration 165, loss = 0.29678228\n",
            "Iteration 166, loss = 0.29701292\n",
            "Iteration 167, loss = 0.29582422\n",
            "Iteration 168, loss = 0.29796469\n",
            "Iteration 169, loss = 0.29780742\n",
            "Iteration 170, loss = 0.29387941\n",
            "Iteration 171, loss = 0.30093672\n",
            "Iteration 172, loss = 0.29444861\n",
            "Iteration 173, loss = 0.29714486\n",
            "Iteration 174, loss = 0.29104493\n",
            "Iteration 175, loss = 0.29344960\n",
            "Iteration 176, loss = 0.29292361\n",
            "Iteration 177, loss = 0.29190903\n",
            "Iteration 178, loss = 0.29950799\n",
            "Iteration 179, loss = 0.29313574\n",
            "Iteration 180, loss = 0.29058927\n",
            "Iteration 181, loss = 0.29045936\n",
            "Iteration 182, loss = 0.28971237\n",
            "Iteration 183, loss = 0.29391158\n",
            "Iteration 184, loss = 0.28944144\n",
            "Iteration 185, loss = 0.28881566\n",
            "Iteration 186, loss = 0.29403386\n",
            "Iteration 187, loss = 0.29645009\n",
            "Iteration 188, loss = 0.28617721\n",
            "Iteration 189, loss = 0.29114246\n",
            "Iteration 190, loss = 0.28968571\n",
            "Iteration 191, loss = 0.28436242\n",
            "Iteration 192, loss = 0.29458557\n",
            "Iteration 193, loss = 0.28874658\n",
            "Iteration 194, loss = 0.28388460\n",
            "Iteration 195, loss = 0.28933490\n",
            "Iteration 196, loss = 0.28566980\n",
            "Iteration 197, loss = 0.28936648\n",
            "Iteration 198, loss = 0.30075425\n",
            "Iteration 199, loss = 0.29843357\n",
            "Iteration 200, loss = 0.28671545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72122352\n",
            "Iteration 3, loss = 0.70131451\n",
            "Iteration 4, loss = 0.68159670\n",
            "Iteration 5, loss = 0.66941472\n",
            "Iteration 6, loss = 0.65735476\n",
            "Iteration 7, loss = 0.64797051\n",
            "Iteration 8, loss = 0.63988056\n",
            "Iteration 9, loss = 0.63280379\n",
            "Iteration 10, loss = 0.62371352\n",
            "Iteration 11, loss = 0.61767230\n",
            "Iteration 12, loss = 0.61545696\n",
            "Iteration 13, loss = 0.60681406\n",
            "Iteration 14, loss = 0.60251932\n",
            "Iteration 15, loss = 0.60210846\n",
            "Iteration 16, loss = 0.60138518\n",
            "Iteration 17, loss = 0.59594845\n",
            "Iteration 18, loss = 0.59185611\n",
            "Iteration 19, loss = 0.59288654\n",
            "Iteration 20, loss = 0.58856108\n",
            "Iteration 21, loss = 0.58435884\n",
            "Iteration 22, loss = 0.58354524\n",
            "Iteration 23, loss = 0.58325541\n",
            "Iteration 24, loss = 0.58027488\n",
            "Iteration 25, loss = 0.58070237\n",
            "Iteration 26, loss = 0.57809102\n",
            "Iteration 27, loss = 0.58040369\n",
            "Iteration 28, loss = 0.74561857\n",
            "Iteration 29, loss = 0.68184856\n",
            "Iteration 30, loss = 0.65868769\n",
            "Iteration 31, loss = 0.64925941\n",
            "Iteration 32, loss = 0.64463214\n",
            "Iteration 33, loss = 0.64185742\n",
            "Iteration 34, loss = 0.63988329\n",
            "Iteration 35, loss = 0.63828889\n",
            "Iteration 36, loss = 0.63693364\n",
            "Iteration 37, loss = 0.63574579\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.63505462\n",
            "Iteration 39, loss = 0.63484758\n",
            "Iteration 40, loss = 0.63464722\n",
            "Iteration 41, loss = 0.63445082\n",
            "Iteration 42, loss = 0.63425944\n",
            "Iteration 43, loss = 0.63407243\n",
            "Iteration 44, loss = 0.63388907\n",
            "Iteration 45, loss = 0.63371006\n",
            "Iteration 46, loss = 0.63353458\n",
            "Iteration 47, loss = 0.63336306\n",
            "Iteration 48, loss = 0.63319548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.63308998\n",
            "Iteration 50, loss = 0.63305707\n",
            "Iteration 51, loss = 0.63302464\n",
            "Iteration 52, loss = 0.63299234\n",
            "Iteration 53, loss = 0.63296021\n",
            "Iteration 54, loss = 0.63292820\n",
            "Iteration 55, loss = 0.63289639\n",
            "Iteration 56, loss = 0.63286450\n",
            "Iteration 57, loss = 0.63283293\n",
            "Iteration 58, loss = 0.63280139\n",
            "Iteration 59, loss = 0.63277015\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 60, loss = 0.63275010\n",
            "Iteration 61, loss = 0.63274384\n",
            "Iteration 62, loss = 0.63273762\n",
            "Iteration 63, loss = 0.63273139\n",
            "Iteration 64, loss = 0.63272518\n",
            "Iteration 65, loss = 0.63271896\n",
            "Iteration 66, loss = 0.63271277\n",
            "Iteration 67, loss = 0.63270657\n",
            "Iteration 68, loss = 0.63270040\n",
            "Iteration 69, loss = 0.63269420\n",
            "Iteration 70, loss = 0.63268802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 71, loss = 0.63268407\n",
            "Iteration 72, loss = 0.63268282\n",
            "Iteration 73, loss = 0.63268158\n",
            "Iteration 74, loss = 0.63268035\n",
            "Iteration 75, loss = 0.63267911\n",
            "Iteration 76, loss = 0.63267788\n",
            "Iteration 77, loss = 0.63267665\n",
            "Iteration 78, loss = 0.63267541\n",
            "Iteration 79, loss = 0.63267418\n",
            "Iteration 80, loss = 0.63267295\n",
            "Iteration 81, loss = 0.63267171\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 82, loss = 0.63267092\n",
            "Iteration 83, loss = 0.63267067\n",
            "Iteration 84, loss = 0.63267043\n",
            "Iteration 85, loss = 0.63267018\n",
            "Iteration 86, loss = 0.63266994\n",
            "Iteration 87, loss = 0.63266969\n",
            "Iteration 88, loss = 0.63266944\n",
            "Iteration 89, loss = 0.63266920\n",
            "Iteration 90, loss = 0.63266895\n",
            "Iteration 91, loss = 0.63266870\n",
            "Iteration 92, loss = 0.63266846\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.40854015\n",
            "Iteration 2, loss = 0.68084570\n",
            "Iteration 3, loss = 0.66393971\n",
            "Iteration 4, loss = 0.65138778\n",
            "Iteration 5, loss = 0.64170367\n",
            "Iteration 6, loss = 0.63409668\n",
            "Iteration 7, loss = 0.62214148\n",
            "Iteration 8, loss = 0.61970299\n",
            "Iteration 9, loss = 0.60763657\n",
            "Iteration 10, loss = 0.60819976\n",
            "Iteration 11, loss = 0.60187676\n",
            "Iteration 12, loss = 0.59550362\n",
            "Iteration 13, loss = 0.59304250\n",
            "Iteration 14, loss = 0.58964663\n",
            "Iteration 15, loss = 0.58985839\n",
            "Iteration 16, loss = 0.58460528\n",
            "Iteration 17, loss = 0.58265464\n",
            "Iteration 18, loss = 0.57920111\n",
            "Iteration 19, loss = 0.58013978\n",
            "Iteration 20, loss = 0.57961754\n",
            "Iteration 21, loss = 0.57871761\n",
            "Iteration 22, loss = 0.57525181\n",
            "Iteration 23, loss = 0.57493458\n",
            "Iteration 24, loss = 0.57668615\n",
            "Iteration 25, loss = 0.57168620\n",
            "Iteration 26, loss = 0.57125135\n",
            "Iteration 27, loss = 0.56977344\n",
            "Iteration 28, loss = 0.57024921\n",
            "Iteration 29, loss = 0.56858510\n",
            "Iteration 30, loss = 0.56811706\n",
            "Iteration 31, loss = 0.56270756\n",
            "Iteration 32, loss = 0.56716142\n",
            "Iteration 33, loss = 0.57092530\n",
            "Iteration 34, loss = 0.56622868\n",
            "Iteration 35, loss = 0.56599714\n",
            "Iteration 36, loss = 0.56200333\n",
            "Iteration 37, loss = 0.56791183\n",
            "Iteration 38, loss = 0.56206920\n",
            "Iteration 39, loss = 0.56718101\n",
            "Iteration 40, loss = 0.56765555\n",
            "Iteration 41, loss = 0.56285520\n",
            "Iteration 42, loss = 0.56720767\n",
            "Iteration 43, loss = 0.56370745\n",
            "Iteration 44, loss = 0.56515984\n",
            "Iteration 45, loss = 0.56279489\n",
            "Iteration 46, loss = 0.56731726\n",
            "Iteration 47, loss = 0.56719087\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.52653807\n",
            "Iteration 49, loss = 0.51414743\n",
            "Iteration 50, loss = 0.50776498\n",
            "Iteration 51, loss = 0.50194519\n",
            "Iteration 52, loss = 0.49637025\n",
            "Iteration 53, loss = 0.48919997\n",
            "Iteration 54, loss = 0.48400301\n",
            "Iteration 55, loss = 0.48650934\n",
            "Iteration 56, loss = 0.48870552\n",
            "Iteration 57, loss = 0.49796084\n",
            "Iteration 58, loss = 0.49634939\n",
            "Iteration 59, loss = 0.49815884\n",
            "Iteration 60, loss = 0.50863794\n",
            "Iteration 61, loss = 0.50590923\n",
            "Iteration 62, loss = 0.50908719\n",
            "Iteration 63, loss = 0.52088367\n",
            "Iteration 64, loss = 0.50530684\n",
            "Iteration 65, loss = 0.51191643\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.44260683\n",
            "Iteration 67, loss = 0.44021979\n",
            "Iteration 68, loss = 0.43781203\n",
            "Iteration 69, loss = 0.43600049\n",
            "Iteration 70, loss = 0.43366111\n",
            "Iteration 71, loss = 0.43180242\n",
            "Iteration 72, loss = 0.42985448\n",
            "Iteration 73, loss = 0.42761761\n",
            "Iteration 74, loss = 0.42535611\n",
            "Iteration 75, loss = 0.42373924\n",
            "Iteration 76, loss = 0.42127833\n",
            "Iteration 77, loss = 0.41934453\n",
            "Iteration 78, loss = 0.41761894\n",
            "Iteration 79, loss = 0.41556051\n",
            "Iteration 80, loss = 0.41369283\n",
            "Iteration 81, loss = 0.41155018\n",
            "Iteration 82, loss = 0.40963890\n",
            "Iteration 83, loss = 0.40776213\n",
            "Iteration 84, loss = 0.40604208\n",
            "Iteration 85, loss = 0.40393686\n",
            "Iteration 86, loss = 0.40226266\n",
            "Iteration 87, loss = 0.40015456\n",
            "Iteration 88, loss = 0.39862481\n",
            "Iteration 89, loss = 0.39650459\n",
            "Iteration 90, loss = 0.39495107\n",
            "Iteration 91, loss = 0.39259668\n",
            "Iteration 92, loss = 0.39123145\n",
            "Iteration 93, loss = 0.38940907\n",
            "Iteration 94, loss = 0.38731804\n",
            "Iteration 95, loss = 0.38583729\n",
            "Iteration 96, loss = 0.38473947\n",
            "Iteration 97, loss = 0.38319603\n",
            "Iteration 98, loss = 0.38165408\n",
            "Iteration 99, loss = 0.37891943\n",
            "Iteration 100, loss = 0.37776673\n",
            "Iteration 101, loss = 0.37553786\n",
            "Iteration 102, loss = 0.37508768\n",
            "Iteration 103, loss = 0.37356921\n",
            "Iteration 104, loss = 0.37099256\n",
            "Iteration 105, loss = 0.37000504\n",
            "Iteration 106, loss = 0.36889738\n",
            "Iteration 107, loss = 0.36782809\n",
            "Iteration 108, loss = 0.36612579\n",
            "Iteration 109, loss = 0.36492092\n",
            "Iteration 110, loss = 0.36335166\n",
            "Iteration 111, loss = 0.36204104\n",
            "Iteration 112, loss = 0.36039222\n",
            "Iteration 113, loss = 0.36049230\n",
            "Iteration 114, loss = 0.35781106\n",
            "Iteration 115, loss = 0.35004272\n",
            "Iteration 116, loss = 0.34447631\n",
            "Iteration 117, loss = 0.34351730\n",
            "Iteration 118, loss = 0.34103497\n",
            "Iteration 119, loss = 0.33989513\n",
            "Iteration 120, loss = 0.33801551\n",
            "Iteration 121, loss = 0.33652337\n",
            "Iteration 122, loss = 0.33676813\n",
            "Iteration 123, loss = 0.33430331\n",
            "Iteration 124, loss = 0.33214525\n",
            "Iteration 125, loss = 0.33111957\n",
            "Iteration 126, loss = 0.32951307\n",
            "Iteration 127, loss = 0.32835900\n",
            "Iteration 128, loss = 0.32742621\n",
            "Iteration 129, loss = 0.32584177\n",
            "Iteration 130, loss = 0.32631367\n",
            "Iteration 131, loss = 0.32401050\n",
            "Iteration 132, loss = 0.32271900\n",
            "Iteration 133, loss = 0.32242576\n",
            "Iteration 134, loss = 0.31951762\n",
            "Iteration 135, loss = 0.32049522\n",
            "Iteration 136, loss = 0.32052949\n",
            "Iteration 137, loss = 0.31703365\n",
            "Iteration 138, loss = 0.31622415\n",
            "Iteration 139, loss = 0.31435362\n",
            "Iteration 140, loss = 0.31424647\n",
            "Iteration 141, loss = 0.31309617\n",
            "Iteration 142, loss = 0.31328512\n",
            "Iteration 143, loss = 0.31237402\n",
            "Iteration 144, loss = 0.30992200\n",
            "Iteration 145, loss = 0.30930670\n",
            "Iteration 146, loss = 0.30733441\n",
            "Iteration 147, loss = 0.30749101\n",
            "Iteration 148, loss = 0.30850171\n",
            "Iteration 149, loss = 0.30802809\n",
            "Iteration 150, loss = 0.30934513\n",
            "Iteration 151, loss = 0.30326714\n",
            "Iteration 152, loss = 0.30468267\n",
            "Iteration 153, loss = 0.30259490\n",
            "Iteration 154, loss = 0.30447536\n",
            "Iteration 155, loss = 0.30322627\n",
            "Iteration 156, loss = 0.30291899\n",
            "Iteration 157, loss = 0.30078137\n",
            "Iteration 158, loss = 0.29919315\n",
            "Iteration 159, loss = 0.30224313\n",
            "Iteration 160, loss = 0.29905729\n",
            "Iteration 161, loss = 0.29842648\n",
            "Iteration 162, loss = 0.29985213\n",
            "Iteration 163, loss = 0.30138663\n",
            "Iteration 164, loss = 0.29574693\n",
            "Iteration 165, loss = 0.29731663\n",
            "Iteration 166, loss = 0.29943535\n",
            "Iteration 167, loss = 0.29682932\n",
            "Iteration 168, loss = 0.29531200\n",
            "Iteration 169, loss = 0.29449726\n",
            "Iteration 170, loss = 0.29579840\n",
            "Iteration 171, loss = 0.29321195\n",
            "Iteration 172, loss = 0.30015972\n",
            "Iteration 173, loss = 0.29455081\n",
            "Iteration 174, loss = 0.29649052\n",
            "Iteration 175, loss = 0.29755709\n",
            "Iteration 176, loss = 0.29472845\n",
            "Iteration 177, loss = 0.29573662\n",
            "Iteration 178, loss = 0.29051739\n",
            "Iteration 179, loss = 0.29169142\n",
            "Iteration 180, loss = 0.29085711\n",
            "Iteration 181, loss = 0.29398202\n",
            "Iteration 182, loss = 0.29299767\n",
            "Iteration 183, loss = 0.29284591\n",
            "Iteration 184, loss = 0.30040390\n",
            "Iteration 185, loss = 0.29298806\n",
            "Iteration 186, loss = 0.28967193\n",
            "Iteration 187, loss = 0.29621012\n",
            "Iteration 188, loss = 0.29056452\n",
            "Iteration 189, loss = 0.28507790\n",
            "Iteration 190, loss = 0.29306441\n",
            "Iteration 191, loss = 0.30366493\n",
            "Iteration 192, loss = 0.29320043\n",
            "Iteration 193, loss = 0.28677529\n",
            "Iteration 194, loss = 0.28714553\n",
            "Iteration 195, loss = 0.29358681\n",
            "Iteration 196, loss = 0.30336426\n",
            "Iteration 197, loss = 0.28511113\n",
            "Iteration 198, loss = 0.28191808\n",
            "Iteration 199, loss = 0.29087904\n",
            "Iteration 200, loss = 0.28927147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70218891\n",
            "Iteration 3, loss = 0.68233380\n",
            "Iteration 4, loss = 0.67277053\n",
            "Iteration 5, loss = 0.66209597\n",
            "Iteration 6, loss = 0.64972000\n",
            "Iteration 7, loss = 0.63841135\n",
            "Iteration 8, loss = 0.63166532\n",
            "Iteration 9, loss = 0.62353883\n",
            "Iteration 10, loss = 0.61815926\n",
            "Iteration 11, loss = 0.60989907\n",
            "Iteration 12, loss = 0.60641239\n",
            "Iteration 13, loss = 0.60408394\n",
            "Iteration 14, loss = 0.60104448\n",
            "Iteration 15, loss = 0.59317764\n",
            "Iteration 16, loss = 0.59353248\n",
            "Iteration 17, loss = 0.59050768\n",
            "Iteration 18, loss = 0.58238100\n",
            "Iteration 19, loss = 0.59040654\n",
            "Iteration 20, loss = 0.58476591\n",
            "Iteration 21, loss = 0.58059559\n",
            "Iteration 22, loss = 0.58224159\n",
            "Iteration 23, loss = 0.57943726\n",
            "Iteration 24, loss = 0.57513047\n",
            "Iteration 25, loss = 0.57683187\n",
            "Iteration 26, loss = 0.57663206\n",
            "Iteration 27, loss = 0.57328651\n",
            "Iteration 28, loss = 0.57425460\n",
            "Iteration 29, loss = 0.57284067\n",
            "Iteration 30, loss = 0.57277660\n",
            "Iteration 31, loss = 0.57060024\n",
            "Iteration 32, loss = 0.57285538\n",
            "Iteration 33, loss = 0.56939273\n",
            "Iteration 34, loss = 0.57265242\n",
            "Iteration 35, loss = 0.57130320\n",
            "Iteration 36, loss = 0.57294152\n",
            "Iteration 37, loss = 0.57107310\n",
            "Iteration 38, loss = 0.57191828\n",
            "Iteration 39, loss = 0.56733859\n",
            "Iteration 40, loss = 0.56889737\n",
            "Iteration 41, loss = 0.56826361\n",
            "Iteration 42, loss = 0.56882797\n",
            "Iteration 43, loss = 0.57164446\n",
            "Iteration 44, loss = 0.56929032\n",
            "Iteration 45, loss = 0.57074917\n",
            "Iteration 46, loss = 0.56711910\n",
            "Iteration 47, loss = 0.56916990\n",
            "Iteration 48, loss = 0.57035723\n",
            "Iteration 49, loss = 0.56785691\n",
            "Iteration 50, loss = 0.56789270\n",
            "Iteration 51, loss = 0.57107132\n",
            "Iteration 52, loss = 0.63437674\n",
            "Iteration 53, loss = 0.61667269\n",
            "Iteration 54, loss = 0.60485736\n",
            "Iteration 55, loss = 0.59471315\n",
            "Iteration 56, loss = 0.59239512\n",
            "Iteration 57, loss = 0.58676798\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 58, loss = 0.57456301\n",
            "Iteration 59, loss = 0.57036439\n",
            "Iteration 60, loss = 0.56807677\n",
            "Iteration 61, loss = 0.56675840\n",
            "Iteration 62, loss = 0.56505789\n",
            "Iteration 63, loss = 0.56311392\n",
            "Iteration 64, loss = 0.56176929\n",
            "Iteration 65, loss = 0.55971787\n",
            "Iteration 66, loss = 0.55835384\n",
            "Iteration 67, loss = 0.55644314\n",
            "Iteration 68, loss = 0.55510806\n",
            "Iteration 69, loss = 0.55280812\n",
            "Iteration 70, loss = 0.55085450\n",
            "Iteration 71, loss = 0.54797864\n",
            "Iteration 72, loss = 0.54606284\n",
            "Iteration 73, loss = 0.54413367\n",
            "Iteration 74, loss = 0.54217900\n",
            "Iteration 75, loss = 0.54112079\n",
            "Iteration 76, loss = 0.53850787\n",
            "Iteration 77, loss = 0.54206009\n",
            "Iteration 78, loss = 0.54176822\n",
            "Iteration 79, loss = 0.53653792\n",
            "Iteration 80, loss = 0.53877939\n",
            "Iteration 81, loss = 0.53713136\n",
            "Iteration 82, loss = 0.54296468\n",
            "Iteration 83, loss = 0.53998144\n",
            "Iteration 84, loss = 0.53886176\n",
            "Iteration 85, loss = 0.53825061\n",
            "Iteration 86, loss = 0.53995106\n",
            "Iteration 87, loss = 0.53739858\n",
            "Iteration 88, loss = 0.54008310\n",
            "Iteration 89, loss = 0.53861422\n",
            "Iteration 90, loss = 0.53529296\n",
            "Iteration 91, loss = 0.53798714\n",
            "Iteration 92, loss = 0.53749157\n",
            "Iteration 93, loss = 0.53560697\n",
            "Iteration 94, loss = 0.54361300\n",
            "Iteration 95, loss = 0.52975163\n",
            "Iteration 96, loss = 0.54212834\n",
            "Iteration 97, loss = 0.52873089\n",
            "Iteration 98, loss = 0.53933424\n",
            "Iteration 99, loss = 0.52753586\n",
            "Iteration 100, loss = 0.53690881\n",
            "Iteration 101, loss = 0.53015780\n",
            "Iteration 102, loss = 0.53639382\n",
            "Iteration 103, loss = 0.52918054\n",
            "Iteration 104, loss = 0.52963068\n",
            "Iteration 105, loss = 0.53103450\n",
            "Iteration 106, loss = 0.53272613\n",
            "Iteration 107, loss = 0.53253846\n",
            "Iteration 108, loss = 0.52704730\n",
            "Iteration 109, loss = 0.52921471\n",
            "Iteration 110, loss = 0.53413725\n",
            "Iteration 111, loss = 0.52720846\n",
            "Iteration 112, loss = 0.52756617\n",
            "Iteration 113, loss = 0.53130137\n",
            "Iteration 114, loss = 0.52164194\n",
            "Iteration 115, loss = 0.52600758\n",
            "Iteration 116, loss = 0.53886236\n",
            "Iteration 117, loss = 0.52525987\n",
            "Iteration 118, loss = 0.52667723\n",
            "Iteration 119, loss = 0.52282512\n",
            "Iteration 120, loss = 0.52529266\n",
            "Iteration 121, loss = 0.52103501\n",
            "Iteration 122, loss = 0.52615062\n",
            "Iteration 123, loss = 0.52329148\n",
            "Iteration 124, loss = 0.51748522\n",
            "Iteration 125, loss = 0.51845255\n",
            "Iteration 126, loss = 0.52999333\n",
            "Iteration 127, loss = 0.53386829\n",
            "Iteration 128, loss = 0.52575304\n",
            "Iteration 129, loss = 0.52198207\n",
            "Iteration 130, loss = 0.52473202\n",
            "Iteration 131, loss = 0.51382746\n",
            "Iteration 132, loss = 0.52435402\n",
            "Iteration 133, loss = 0.51966477\n",
            "Iteration 134, loss = 0.52585964\n",
            "Iteration 135, loss = 0.51987045\n",
            "Iteration 136, loss = 0.52483675\n",
            "Iteration 137, loss = 0.52168222\n",
            "Iteration 138, loss = 0.52273822\n",
            "Iteration 139, loss = 0.51778702\n",
            "Iteration 140, loss = 0.53329616\n",
            "Iteration 141, loss = 0.52346320\n",
            "Iteration 142, loss = 0.51788100\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 143, loss = 0.44875089\n",
            "Iteration 144, loss = 0.44557558\n",
            "Iteration 145, loss = 0.44259086\n",
            "Iteration 146, loss = 0.43997154\n",
            "Iteration 147, loss = 0.43690039\n",
            "Iteration 148, loss = 0.43388700\n",
            "Iteration 149, loss = 0.43135934\n",
            "Iteration 150, loss = 0.42845658\n",
            "Iteration 151, loss = 0.42579956\n",
            "Iteration 152, loss = 0.42313406\n",
            "Iteration 153, loss = 0.42106337\n",
            "Iteration 154, loss = 0.41831937\n",
            "Iteration 155, loss = 0.41573439\n",
            "Iteration 156, loss = 0.41318061\n",
            "Iteration 157, loss = 0.41110119\n",
            "Iteration 158, loss = 0.40899445\n",
            "Iteration 159, loss = 0.40649159\n",
            "Iteration 160, loss = 0.40417606\n",
            "Iteration 161, loss = 0.40201642\n",
            "Iteration 162, loss = 0.39960106\n",
            "Iteration 163, loss = 0.39815432\n",
            "Iteration 164, loss = 0.39539139\n",
            "Iteration 165, loss = 0.39363154\n",
            "Iteration 166, loss = 0.39224052\n",
            "Iteration 167, loss = 0.38958071\n",
            "Iteration 168, loss = 0.38777291\n",
            "Iteration 169, loss = 0.38667285\n",
            "Iteration 170, loss = 0.38428915\n",
            "Iteration 171, loss = 0.38264682\n",
            "Iteration 172, loss = 0.38092352\n",
            "Iteration 173, loss = 0.37838989\n",
            "Iteration 174, loss = 0.37699964\n",
            "Iteration 175, loss = 0.37637424\n",
            "Iteration 176, loss = 0.37385858\n",
            "Iteration 177, loss = 0.37248291\n",
            "Iteration 178, loss = 0.37095039\n",
            "Iteration 179, loss = 0.36982866\n",
            "Iteration 180, loss = 0.36861587\n",
            "Iteration 181, loss = 0.36723555\n",
            "Iteration 182, loss = 0.36621331\n",
            "Iteration 183, loss = 0.36387646\n",
            "Iteration 184, loss = 0.36299936\n",
            "Iteration 185, loss = 0.36142414\n",
            "Iteration 186, loss = 0.35954702\n",
            "Iteration 187, loss = 0.35981001\n",
            "Iteration 188, loss = 0.35920823\n",
            "Iteration 189, loss = 0.35847216\n",
            "Iteration 190, loss = 0.35679086\n",
            "Iteration 191, loss = 0.35535443\n",
            "Iteration 192, loss = 0.35269356\n",
            "Iteration 193, loss = 0.35360993\n",
            "Iteration 194, loss = 0.35278559\n",
            "Iteration 195, loss = 0.35165722\n",
            "Iteration 196, loss = 0.34992823\n",
            "Iteration 197, loss = 0.34816400\n",
            "Iteration 198, loss = 0.34547922\n",
            "Iteration 199, loss = 0.34052580\n",
            "Iteration 200, loss = 0.33618939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70129782\n",
            "Iteration 3, loss = 0.68048923\n",
            "Iteration 4, loss = 0.66692714\n",
            "Iteration 5, loss = 0.65087905\n",
            "Iteration 6, loss = 0.64320411\n",
            "Iteration 7, loss = 0.63099447\n",
            "Iteration 8, loss = 0.61997616\n",
            "Iteration 9, loss = 0.61760407\n",
            "Iteration 10, loss = 0.60813922\n",
            "Iteration 11, loss = 0.60068589\n",
            "Iteration 12, loss = 0.59946901\n",
            "Iteration 13, loss = 0.59335500\n",
            "Iteration 14, loss = 0.59059885\n",
            "Iteration 15, loss = 0.58804338\n",
            "Iteration 16, loss = 0.58458236\n",
            "Iteration 17, loss = 0.57979375\n",
            "Iteration 18, loss = 0.57806492\n",
            "Iteration 19, loss = 0.57768128\n",
            "Iteration 20, loss = 0.57603530\n",
            "Iteration 21, loss = 0.57137695\n",
            "Iteration 22, loss = 0.57341289\n",
            "Iteration 23, loss = 0.57341559\n",
            "Iteration 24, loss = 0.56924347\n",
            "Iteration 25, loss = 0.56973791\n",
            "Iteration 26, loss = 0.56740580\n",
            "Iteration 27, loss = 0.56749665\n",
            "Iteration 28, loss = 0.56878842\n",
            "Iteration 29, loss = 0.56706599\n",
            "Iteration 30, loss = 0.56546005\n",
            "Iteration 31, loss = 0.56660269\n",
            "Iteration 32, loss = 0.56531176\n",
            "Iteration 33, loss = 0.56315300\n",
            "Iteration 34, loss = 0.56335005\n",
            "Iteration 35, loss = 0.56278100\n",
            "Iteration 36, loss = 0.56603588\n",
            "Iteration 37, loss = 0.56368862\n",
            "Iteration 38, loss = 0.56525053\n",
            "Iteration 39, loss = 0.56474419\n",
            "Iteration 40, loss = 0.56168014\n",
            "Iteration 41, loss = 0.57029064\n",
            "Iteration 42, loss = 0.56037070\n",
            "Iteration 43, loss = 0.56238134\n",
            "Iteration 44, loss = 0.56386660\n",
            "Iteration 45, loss = 0.56129617\n",
            "Iteration 46, loss = 0.56901946\n",
            "Iteration 47, loss = 0.56009142\n",
            "Iteration 48, loss = 0.56551014\n",
            "Iteration 49, loss = 0.56163854\n",
            "Iteration 50, loss = 0.56367463\n",
            "Iteration 51, loss = 0.56562350\n",
            "Iteration 52, loss = 0.56382477\n",
            "Iteration 53, loss = 0.56184123\n",
            "Iteration 54, loss = 0.56577175\n",
            "Iteration 55, loss = 0.56253195\n",
            "Iteration 56, loss = 0.56141982\n",
            "Iteration 57, loss = 0.56082548\n",
            "Iteration 58, loss = 0.55998266\n",
            "Iteration 59, loss = 0.56899105\n",
            "Iteration 60, loss = 0.56292516\n",
            "Iteration 61, loss = 0.56031145\n",
            "Iteration 62, loss = 0.56560641\n",
            "Iteration 63, loss = 0.56386175\n",
            "Iteration 64, loss = 0.56170184\n",
            "Iteration 65, loss = 0.56541409\n",
            "Iteration 66, loss = 0.56148026\n",
            "Iteration 67, loss = 0.56378440\n",
            "Iteration 68, loss = 0.56254260\n",
            "Iteration 69, loss = 0.56643313\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 70, loss = 0.53587754\n",
            "Iteration 71, loss = 0.51057538\n",
            "Iteration 72, loss = 0.50048883\n",
            "Iteration 73, loss = 0.49159154\n",
            "Iteration 74, loss = 0.48406683\n",
            "Iteration 75, loss = 0.47483166\n",
            "Iteration 76, loss = 0.47371090\n",
            "Iteration 77, loss = 0.48005660\n",
            "Iteration 78, loss = 0.48836181\n",
            "Iteration 79, loss = 0.50070840\n",
            "Iteration 80, loss = 0.50508284\n",
            "Iteration 81, loss = 0.50356093\n",
            "Iteration 82, loss = 0.50048575\n",
            "Iteration 83, loss = 0.50287187\n",
            "Iteration 84, loss = 0.50105816\n",
            "Iteration 85, loss = 0.51326497\n",
            "Iteration 86, loss = 0.50671662\n",
            "Iteration 87, loss = 0.51104182\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.43104469\n",
            "Iteration 89, loss = 0.42795711\n",
            "Iteration 90, loss = 0.42537961\n",
            "Iteration 91, loss = 0.42217135\n",
            "Iteration 92, loss = 0.41935176\n",
            "Iteration 93, loss = 0.41702279\n",
            "Iteration 94, loss = 0.41391126\n",
            "Iteration 95, loss = 0.41120674\n",
            "Iteration 96, loss = 0.40862077\n",
            "Iteration 97, loss = 0.40621475\n",
            "Iteration 98, loss = 0.40316022\n",
            "Iteration 99, loss = 0.40085403\n",
            "Iteration 100, loss = 0.39811560\n",
            "Iteration 101, loss = 0.39589241\n",
            "Iteration 102, loss = 0.39263401\n",
            "Iteration 103, loss = 0.39079564\n",
            "Iteration 104, loss = 0.38771839\n",
            "Iteration 105, loss = 0.38570648\n",
            "Iteration 106, loss = 0.38346312\n",
            "Iteration 107, loss = 0.38093086\n",
            "Iteration 108, loss = 0.37844216\n",
            "Iteration 109, loss = 0.37676510\n",
            "Iteration 110, loss = 0.37446225\n",
            "Iteration 111, loss = 0.37252749\n",
            "Iteration 112, loss = 0.36976718\n",
            "Iteration 113, loss = 0.36771277\n",
            "Iteration 114, loss = 0.36504917\n",
            "Iteration 115, loss = 0.36351761\n",
            "Iteration 116, loss = 0.36175482\n",
            "Iteration 117, loss = 0.35947367\n",
            "Iteration 118, loss = 0.35798556\n",
            "Iteration 119, loss = 0.35564765\n",
            "Iteration 120, loss = 0.35341959\n",
            "Iteration 121, loss = 0.35208733\n",
            "Iteration 122, loss = 0.34999868\n",
            "Iteration 123, loss = 0.34809204\n",
            "Iteration 124, loss = 0.34693233\n",
            "Iteration 125, loss = 0.34484226\n",
            "Iteration 126, loss = 0.34320676\n",
            "Iteration 127, loss = 0.34140031\n",
            "Iteration 128, loss = 0.33994808\n",
            "Iteration 129, loss = 0.33829392\n",
            "Iteration 130, loss = 0.33758045\n",
            "Iteration 131, loss = 0.33588123\n",
            "Iteration 132, loss = 0.33379617\n",
            "Iteration 133, loss = 0.33233636\n",
            "Iteration 134, loss = 0.33047609\n",
            "Iteration 135, loss = 0.32896216\n",
            "Iteration 136, loss = 0.32776713\n",
            "Iteration 137, loss = 0.32670350\n",
            "Iteration 138, loss = 0.32450632\n",
            "Iteration 139, loss = 0.32509491\n",
            "Iteration 140, loss = 0.32301410\n",
            "Iteration 141, loss = 0.32020350\n",
            "Iteration 142, loss = 0.31966946\n",
            "Iteration 143, loss = 0.31932445\n",
            "Iteration 144, loss = 0.31830950\n",
            "Iteration 145, loss = 0.31739031\n",
            "Iteration 146, loss = 0.31575426\n",
            "Iteration 147, loss = 0.31505509\n",
            "Iteration 148, loss = 0.31297612\n",
            "Iteration 149, loss = 0.31211296\n",
            "Iteration 150, loss = 0.31027170\n",
            "Iteration 151, loss = 0.30975654\n",
            "Iteration 152, loss = 0.30870096\n",
            "Iteration 153, loss = 0.30767018\n",
            "Iteration 154, loss = 0.30781535\n",
            "Iteration 155, loss = 0.30616514\n",
            "Iteration 156, loss = 0.30730186\n",
            "Iteration 157, loss = 0.30342783\n",
            "Iteration 158, loss = 0.30334265\n",
            "Iteration 159, loss = 0.30238869\n",
            "Iteration 160, loss = 0.30053100\n",
            "Iteration 161, loss = 0.30063752\n",
            "Iteration 162, loss = 0.29855152\n",
            "Iteration 163, loss = 0.30008977\n",
            "Iteration 164, loss = 0.29810523\n",
            "Iteration 165, loss = 0.29816037\n",
            "Iteration 166, loss = 0.29608773\n",
            "Iteration 167, loss = 0.29613521\n",
            "Iteration 168, loss = 0.29552038\n",
            "Iteration 169, loss = 0.29804962\n",
            "Iteration 170, loss = 0.29475054\n",
            "Iteration 171, loss = 0.29224473\n",
            "Iteration 172, loss = 0.29308563\n",
            "Iteration 173, loss = 0.29129642\n",
            "Iteration 174, loss = 0.29389965\n",
            "Iteration 175, loss = 0.29030186\n",
            "Iteration 176, loss = 0.29029577\n",
            "Iteration 177, loss = 0.28984789\n",
            "Iteration 178, loss = 0.28882756\n",
            "Iteration 179, loss = 0.28720220\n",
            "Iteration 180, loss = 0.28706189\n",
            "Iteration 181, loss = 0.28572502\n",
            "Iteration 182, loss = 0.28770497\n",
            "Iteration 183, loss = 0.28579845\n",
            "Iteration 184, loss = 0.28604249\n",
            "Iteration 185, loss = 0.28593938\n",
            "Iteration 186, loss = 0.28571573\n",
            "Iteration 187, loss = 0.28398893\n",
            "Iteration 188, loss = 0.28621083\n",
            "Iteration 189, loss = 0.28526488\n",
            "Iteration 190, loss = 0.28350232\n",
            "Iteration 191, loss = 0.28159065\n",
            "Iteration 192, loss = 0.28554009\n",
            "Iteration 193, loss = 0.28309310\n",
            "Iteration 194, loss = 0.28024297\n",
            "Iteration 195, loss = 0.28210185\n",
            "Iteration 196, loss = 0.27920919\n",
            "Iteration 197, loss = 0.28011625\n",
            "Iteration 198, loss = 0.27932837\n",
            "Iteration 199, loss = 0.28117218\n",
            "Iteration 200, loss = 0.27786864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67480929\n",
            "Iteration 3, loss = 0.66164137\n",
            "Iteration 4, loss = 0.64566291\n",
            "Iteration 5, loss = 0.63841569\n",
            "Iteration 6, loss = 0.63065306\n",
            "Iteration 7, loss = 0.62213909\n",
            "Iteration 8, loss = 0.61249959\n",
            "Iteration 9, loss = 0.61441538\n",
            "Iteration 10, loss = 0.60583188\n",
            "Iteration 11, loss = 0.60062514\n",
            "Iteration 12, loss = 0.60044337\n",
            "Iteration 13, loss = 0.59422511\n",
            "Iteration 14, loss = 0.59006591\n",
            "Iteration 15, loss = 0.58668696\n",
            "Iteration 16, loss = 0.58302057\n",
            "Iteration 17, loss = 0.58102078\n",
            "Iteration 18, loss = 0.57688358\n",
            "Iteration 19, loss = 0.57690966\n",
            "Iteration 20, loss = 0.57517871\n",
            "Iteration 21, loss = 0.57188842\n",
            "Iteration 22, loss = 0.57404740\n",
            "Iteration 23, loss = 0.57232460\n",
            "Iteration 24, loss = 0.57280212\n",
            "Iteration 25, loss = 0.57019743\n",
            "Iteration 26, loss = 0.56554315\n",
            "Iteration 27, loss = 0.57291179\n",
            "Iteration 28, loss = 0.56612965\n",
            "Iteration 29, loss = 0.56886693\n",
            "Iteration 30, loss = 0.56640924\n",
            "Iteration 31, loss = 0.57402684\n",
            "Iteration 32, loss = 0.57356306\n",
            "Iteration 33, loss = 0.57333085\n",
            "Iteration 34, loss = 0.56964968\n",
            "Iteration 35, loss = 0.57037693\n",
            "Iteration 36, loss = 0.56827428\n",
            "Iteration 37, loss = 0.57023944\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.53237357\n",
            "Iteration 39, loss = 0.52298253\n",
            "Iteration 40, loss = 0.51762214\n",
            "Iteration 41, loss = 0.51288279\n",
            "Iteration 42, loss = 0.50810480\n",
            "Iteration 43, loss = 0.50306666\n",
            "Iteration 44, loss = 0.49891683\n",
            "Iteration 45, loss = 0.49435524\n",
            "Iteration 46, loss = 0.48792115\n",
            "Iteration 47, loss = 0.48826285\n",
            "Iteration 48, loss = 0.50441724\n",
            "Iteration 49, loss = 0.49574911\n",
            "Iteration 50, loss = 0.52017227\n",
            "Iteration 51, loss = 0.50417120\n",
            "Iteration 52, loss = 0.51724595\n",
            "Iteration 53, loss = 0.51505547\n",
            "Iteration 54, loss = 0.50842462\n",
            "Iteration 55, loss = 0.51533306\n",
            "Iteration 56, loss = 0.51050352\n",
            "Iteration 57, loss = 0.52545351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 58, loss = 0.45704717\n",
            "Iteration 59, loss = 0.45100942\n",
            "Iteration 60, loss = 0.44904605\n",
            "Iteration 61, loss = 0.44706469\n",
            "Iteration 62, loss = 0.44518004\n",
            "Iteration 63, loss = 0.44365189\n",
            "Iteration 64, loss = 0.44186829\n",
            "Iteration 65, loss = 0.43989057\n",
            "Iteration 66, loss = 0.43805282\n",
            "Iteration 67, loss = 0.43620007\n",
            "Iteration 68, loss = 0.43470419\n",
            "Iteration 69, loss = 0.43270961\n",
            "Iteration 70, loss = 0.42888253\n",
            "Iteration 71, loss = 0.41809190\n",
            "Iteration 72, loss = 0.41484199\n",
            "Iteration 73, loss = 0.41242271\n",
            "Iteration 74, loss = 0.41029571\n",
            "Iteration 75, loss = 0.40840264\n",
            "Iteration 76, loss = 0.40601301\n",
            "Iteration 77, loss = 0.40354001\n",
            "Iteration 78, loss = 0.40156047\n",
            "Iteration 79, loss = 0.39941365\n",
            "Iteration 80, loss = 0.39735394\n",
            "Iteration 81, loss = 0.39501135\n",
            "Iteration 82, loss = 0.39316187\n",
            "Iteration 83, loss = 0.39045046\n",
            "Iteration 84, loss = 0.38893298\n",
            "Iteration 85, loss = 0.38755030\n",
            "Iteration 86, loss = 0.38498069\n",
            "Iteration 87, loss = 0.38348007\n",
            "Iteration 88, loss = 0.38129791\n",
            "Iteration 89, loss = 0.37940176\n",
            "Iteration 90, loss = 0.37720889\n",
            "Iteration 91, loss = 0.37634683\n",
            "Iteration 92, loss = 0.37413453\n",
            "Iteration 93, loss = 0.37205103\n",
            "Iteration 94, loss = 0.36981731\n",
            "Iteration 95, loss = 0.36886224\n",
            "Iteration 96, loss = 0.36669852\n",
            "Iteration 97, loss = 0.36470295\n",
            "Iteration 98, loss = 0.36286863\n",
            "Iteration 99, loss = 0.36197841\n",
            "Iteration 100, loss = 0.36013454\n",
            "Iteration 101, loss = 0.35897956\n",
            "Iteration 102, loss = 0.35649730\n",
            "Iteration 103, loss = 0.35497705\n",
            "Iteration 104, loss = 0.35282691\n",
            "Iteration 105, loss = 0.35207671\n",
            "Iteration 106, loss = 0.35112833\n",
            "Iteration 107, loss = 0.34961820\n",
            "Iteration 108, loss = 0.34761195\n",
            "Iteration 109, loss = 0.34627419\n",
            "Iteration 110, loss = 0.34547896\n",
            "Iteration 111, loss = 0.34312275\n",
            "Iteration 112, loss = 0.34143929\n",
            "Iteration 113, loss = 0.34036412\n",
            "Iteration 114, loss = 0.34064656\n",
            "Iteration 115, loss = 0.33826948\n",
            "Iteration 116, loss = 0.33639112\n",
            "Iteration 117, loss = 0.33581534\n",
            "Iteration 118, loss = 0.33453156\n",
            "Iteration 119, loss = 0.33436680\n",
            "Iteration 120, loss = 0.33143672\n",
            "Iteration 121, loss = 0.33097454\n",
            "Iteration 122, loss = 0.32873391\n",
            "Iteration 123, loss = 0.32880068\n",
            "Iteration 124, loss = 0.32773126\n",
            "Iteration 125, loss = 0.32709994\n",
            "Iteration 126, loss = 0.32556634\n",
            "Iteration 127, loss = 0.32458580\n",
            "Iteration 128, loss = 0.32193912\n",
            "Iteration 129, loss = 0.32141780\n",
            "Iteration 130, loss = 0.32286726\n",
            "Iteration 131, loss = 0.32354126\n",
            "Iteration 132, loss = 0.32011101\n",
            "Iteration 133, loss = 0.32195133\n",
            "Iteration 134, loss = 0.31728540\n",
            "Iteration 135, loss = 0.31700486\n",
            "Iteration 136, loss = 0.31699820\n",
            "Iteration 137, loss = 0.31776447\n",
            "Iteration 138, loss = 0.31356569\n",
            "Iteration 139, loss = 0.31164105\n",
            "Iteration 140, loss = 0.31581803\n",
            "Iteration 141, loss = 0.31275317\n",
            "Iteration 142, loss = 0.31401142\n",
            "Iteration 143, loss = 0.30969086\n",
            "Iteration 144, loss = 0.31376296\n",
            "Iteration 145, loss = 0.30875952\n",
            "Iteration 146, loss = 0.31222892\n",
            "Iteration 147, loss = 0.30879647\n",
            "Iteration 148, loss = 0.30831581\n",
            "Iteration 149, loss = 0.30633715\n",
            "Iteration 150, loss = 0.30541183\n",
            "Iteration 151, loss = 0.31078407\n",
            "Iteration 152, loss = 0.31054085\n",
            "Iteration 153, loss = 0.30396426\n",
            "Iteration 154, loss = 0.30428218\n",
            "Iteration 155, loss = 0.30436987\n",
            "Iteration 156, loss = 0.30291285\n",
            "Iteration 157, loss = 0.30252805\n",
            "Iteration 158, loss = 0.30418595\n",
            "Iteration 159, loss = 0.30444262\n",
            "Iteration 160, loss = 0.30557736\n",
            "Iteration 161, loss = 0.29773558\n",
            "Iteration 162, loss = 0.30336456\n",
            "Iteration 163, loss = 0.30228196\n",
            "Iteration 164, loss = 0.30791491\n",
            "Iteration 165, loss = 0.30217120\n",
            "Iteration 166, loss = 0.30160623\n",
            "Iteration 167, loss = 0.30165782\n",
            "Iteration 168, loss = 0.29373660\n",
            "Iteration 169, loss = 0.29898162\n",
            "Iteration 170, loss = 0.30606945\n",
            "Iteration 171, loss = 0.30213868\n",
            "Iteration 172, loss = 0.30132755\n",
            "Iteration 173, loss = 0.30079249\n",
            "Iteration 174, loss = 0.30103174\n",
            "Iteration 175, loss = 0.29686940\n",
            "Iteration 176, loss = 0.30338500\n",
            "Iteration 177, loss = 0.30117782\n",
            "Iteration 178, loss = 0.29264233\n",
            "Iteration 179, loss = 0.30372181\n",
            "Iteration 180, loss = 0.30938324\n",
            "Iteration 181, loss = 0.30599372\n",
            "Iteration 182, loss = 0.29425468\n",
            "Iteration 183, loss = 0.29564661\n",
            "Iteration 184, loss = 0.29569569\n",
            "Iteration 185, loss = 0.29075840\n",
            "Iteration 186, loss = 0.30834249\n",
            "Iteration 187, loss = 0.30560841\n",
            "Iteration 188, loss = 0.30677792\n",
            "Iteration 189, loss = 0.30709737\n",
            "Iteration 190, loss = 0.28839095\n",
            "Iteration 191, loss = 0.30914184\n",
            "Iteration 192, loss = 0.31251534\n",
            "Iteration 193, loss = 0.29658836\n",
            "Iteration 194, loss = 0.29728519\n",
            "Iteration 195, loss = 0.30282059\n",
            "Iteration 196, loss = 0.29983924\n",
            "Iteration 197, loss = 0.30805247\n",
            "Iteration 198, loss = 0.30990573\n",
            "Iteration 199, loss = 0.29866385\n",
            "Iteration 200, loss = 0.28654033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69307554\n",
            "Iteration 3, loss = 0.67748704\n",
            "Iteration 4, loss = 0.66925954\n",
            "Iteration 5, loss = 0.67595187\n",
            "Iteration 6, loss = 0.65671970\n",
            "Iteration 7, loss = 0.64872507\n",
            "Iteration 8, loss = 0.63380670\n",
            "Iteration 9, loss = 0.62788052\n",
            "Iteration 10, loss = 0.61917291\n",
            "Iteration 11, loss = 0.61534357\n",
            "Iteration 12, loss = 0.60761355\n",
            "Iteration 13, loss = 0.60425658\n",
            "Iteration 14, loss = 0.59770295\n",
            "Iteration 15, loss = 0.59624952\n",
            "Iteration 16, loss = 0.64740579\n",
            "Iteration 17, loss = 0.63147908\n",
            "Iteration 18, loss = 0.61821071\n",
            "Iteration 19, loss = 0.60744531\n",
            "Iteration 20, loss = 0.60024772\n",
            "Iteration 21, loss = 0.59606831\n",
            "Iteration 22, loss = 0.59158541\n",
            "Iteration 23, loss = 0.58632915\n",
            "Iteration 24, loss = 0.58387805\n",
            "Iteration 25, loss = 0.58288043\n",
            "Iteration 26, loss = 0.57814992\n",
            "Iteration 27, loss = 0.57831583\n",
            "Iteration 28, loss = 0.57828823\n",
            "Iteration 29, loss = 0.57550468\n",
            "Iteration 30, loss = 0.57502498\n",
            "Iteration 31, loss = 0.57128651\n",
            "Iteration 32, loss = 0.57280331\n",
            "Iteration 33, loss = 0.57274076\n",
            "Iteration 34, loss = 0.57303620\n",
            "Iteration 35, loss = 0.56803953\n",
            "Iteration 36, loss = 0.56866268\n",
            "Iteration 37, loss = 0.57505802\n",
            "Iteration 38, loss = 0.56804303\n",
            "Iteration 39, loss = 0.56851451\n",
            "Iteration 40, loss = 0.56779083\n",
            "Iteration 41, loss = 0.57061232\n",
            "Iteration 42, loss = 0.56977467\n",
            "Iteration 43, loss = 0.56809442\n",
            "Iteration 44, loss = 0.56771480\n",
            "Iteration 45, loss = 0.56738667\n",
            "Iteration 46, loss = 0.56726864\n",
            "Iteration 47, loss = 0.56811677\n",
            "Iteration 48, loss = 0.57070341\n",
            "Iteration 49, loss = 0.56767605\n",
            "Iteration 50, loss = 0.56800557\n",
            "Iteration 51, loss = 0.56690574\n",
            "Iteration 52, loss = 0.56919835\n",
            "Iteration 53, loss = 0.56651601\n",
            "Iteration 54, loss = 0.56506279\n",
            "Iteration 55, loss = 0.57079700\n",
            "Iteration 56, loss = 0.56501553\n",
            "Iteration 57, loss = 0.56987360\n",
            "Iteration 58, loss = 0.56879760\n",
            "Iteration 59, loss = 0.56566932\n",
            "Iteration 60, loss = 0.64422683\n",
            "Iteration 61, loss = 0.61902165\n",
            "Iteration 62, loss = 0.60638477\n",
            "Iteration 63, loss = 0.59830046\n",
            "Iteration 64, loss = 0.59004574\n",
            "Iteration 65, loss = 0.58474960\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 66, loss = 0.57138473\n",
            "Iteration 67, loss = 0.56786552\n",
            "Iteration 68, loss = 0.56585843\n",
            "Iteration 69, loss = 0.56453631\n",
            "Iteration 70, loss = 0.56337470\n",
            "Iteration 71, loss = 0.56163001\n",
            "Iteration 72, loss = 0.56016235\n",
            "Iteration 73, loss = 0.55843264\n",
            "Iteration 74, loss = 0.55647710\n",
            "Iteration 75, loss = 0.55560745\n",
            "Iteration 76, loss = 0.55352190\n",
            "Iteration 77, loss = 0.55123863\n",
            "Iteration 78, loss = 0.54860088\n",
            "Iteration 79, loss = 0.54792467\n",
            "Iteration 80, loss = 0.54475647\n",
            "Iteration 81, loss = 0.54333103\n",
            "Iteration 82, loss = 0.54270669\n",
            "Iteration 83, loss = 0.53866826\n",
            "Iteration 84, loss = 0.53722408\n",
            "Iteration 85, loss = 0.53598550\n",
            "Iteration 86, loss = 0.53467488\n",
            "Iteration 87, loss = 0.53499179\n",
            "Iteration 88, loss = 0.53891904\n",
            "Iteration 89, loss = 0.53908566\n",
            "Iteration 90, loss = 0.53474905\n",
            "Iteration 91, loss = 0.53990986\n",
            "Iteration 92, loss = 0.54013788\n",
            "Iteration 93, loss = 0.54004790\n",
            "Iteration 94, loss = 0.53140341\n",
            "Iteration 95, loss = 0.54075949\n",
            "Iteration 96, loss = 0.53691933\n",
            "Iteration 97, loss = 0.53896240\n",
            "Iteration 98, loss = 0.53932895\n",
            "Iteration 99, loss = 0.53672168\n",
            "Iteration 100, loss = 0.53561870\n",
            "Iteration 101, loss = 0.52781387\n",
            "Iteration 102, loss = 0.53737969\n",
            "Iteration 103, loss = 0.53384031\n",
            "Iteration 104, loss = 0.53774225\n",
            "Iteration 105, loss = 0.53088822\n",
            "Iteration 106, loss = 0.53003077\n",
            "Iteration 107, loss = 0.53639706\n",
            "Iteration 108, loss = 0.53430516\n",
            "Iteration 109, loss = 0.53588586\n",
            "Iteration 110, loss = 0.52603386\n",
            "Iteration 111, loss = 0.53153199\n",
            "Iteration 112, loss = 0.52751160\n",
            "Iteration 113, loss = 0.52194515\n",
            "Iteration 114, loss = 0.52214691\n",
            "Iteration 115, loss = 0.51451924\n",
            "Iteration 116, loss = 0.53477329\n",
            "Iteration 117, loss = 0.53502123\n",
            "Iteration 118, loss = 0.52269669\n",
            "Iteration 119, loss = 0.52161333\n",
            "Iteration 120, loss = 0.52707670\n",
            "Iteration 121, loss = 0.51530818\n",
            "Iteration 122, loss = 0.52816333\n",
            "Iteration 123, loss = 0.52065674\n",
            "Iteration 124, loss = 0.52225030\n",
            "Iteration 125, loss = 0.52948200\n",
            "Iteration 126, loss = 0.51340368\n",
            "Iteration 127, loss = 0.52451134\n",
            "Iteration 128, loss = 0.52286107\n",
            "Iteration 129, loss = 0.51887401\n",
            "Iteration 130, loss = 0.51967918\n",
            "Iteration 131, loss = 0.53043435\n",
            "Iteration 132, loss = 0.51880091\n",
            "Iteration 133, loss = 0.51835317\n",
            "Iteration 134, loss = 0.50575767\n",
            "Iteration 135, loss = 0.53629617\n",
            "Iteration 136, loss = 0.51356186\n",
            "Iteration 137, loss = 0.53602332\n",
            "Iteration 138, loss = 0.51551072\n",
            "Iteration 139, loss = 0.52217167\n",
            "Iteration 140, loss = 0.51645554\n",
            "Iteration 141, loss = 0.52060896\n",
            "Iteration 142, loss = 0.52233892\n",
            "Iteration 143, loss = 0.51697598\n",
            "Iteration 144, loss = 0.52439455\n",
            "Iteration 145, loss = 0.52058516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 146, loss = 0.43781400\n",
            "Iteration 147, loss = 0.43390901\n",
            "Iteration 148, loss = 0.43055893\n",
            "Iteration 149, loss = 0.42712370\n",
            "Iteration 150, loss = 0.42411582\n",
            "Iteration 151, loss = 0.42129294\n",
            "Iteration 152, loss = 0.41812657\n",
            "Iteration 153, loss = 0.41494212\n",
            "Iteration 154, loss = 0.41165684\n",
            "Iteration 155, loss = 0.40925395\n",
            "Iteration 156, loss = 0.40613192\n",
            "Iteration 157, loss = 0.40332621\n",
            "Iteration 158, loss = 0.40018674\n",
            "Iteration 159, loss = 0.39778392\n",
            "Iteration 160, loss = 0.39512554\n",
            "Iteration 161, loss = 0.39233079\n",
            "Iteration 162, loss = 0.38957505\n",
            "Iteration 163, loss = 0.38686997\n",
            "Iteration 164, loss = 0.38493871\n",
            "Iteration 165, loss = 0.38215845\n",
            "Iteration 166, loss = 0.37948910\n",
            "Iteration 167, loss = 0.37745724\n",
            "Iteration 168, loss = 0.37518013\n",
            "Iteration 169, loss = 0.37333444\n",
            "Iteration 170, loss = 0.37048200\n",
            "Iteration 171, loss = 0.36819382\n",
            "Iteration 172, loss = 0.36580605\n",
            "Iteration 173, loss = 0.36377751\n",
            "Iteration 174, loss = 0.36135182\n",
            "Iteration 175, loss = 0.36052076\n",
            "Iteration 176, loss = 0.35817719\n",
            "Iteration 177, loss = 0.35568718\n",
            "Iteration 178, loss = 0.35398096\n",
            "Iteration 179, loss = 0.35293959\n",
            "Iteration 180, loss = 0.35001235\n",
            "Iteration 181, loss = 0.34834405\n",
            "Iteration 182, loss = 0.34644132\n",
            "Iteration 183, loss = 0.34509263\n",
            "Iteration 184, loss = 0.34326702\n",
            "Iteration 185, loss = 0.34186442\n",
            "Iteration 186, loss = 0.34008445\n",
            "Iteration 187, loss = 0.33858056\n",
            "Iteration 188, loss = 0.33654457\n",
            "Iteration 189, loss = 0.33512004\n",
            "Iteration 190, loss = 0.33331287\n",
            "Iteration 191, loss = 0.33263881\n",
            "Iteration 192, loss = 0.33063213\n",
            "Iteration 193, loss = 0.32972433\n",
            "Iteration 194, loss = 0.32729368\n",
            "Iteration 195, loss = 0.32729640\n",
            "Iteration 196, loss = 0.32481672\n",
            "Iteration 197, loss = 0.32384538\n",
            "Iteration 198, loss = 0.32388304\n",
            "Iteration 199, loss = 0.32159912\n",
            "Iteration 200, loss = 0.32048732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68820216\n",
            "Iteration 3, loss = 0.67438737\n",
            "Iteration 4, loss = 0.66141567\n",
            "Iteration 5, loss = 0.64895678\n",
            "Iteration 6, loss = 0.63698749\n",
            "Iteration 7, loss = 0.62840011\n",
            "Iteration 8, loss = 0.62125530\n",
            "Iteration 9, loss = 0.61879569\n",
            "Iteration 10, loss = 0.61008375\n",
            "Iteration 11, loss = 0.60561939\n",
            "Iteration 12, loss = 0.59822831\n",
            "Iteration 13, loss = 0.59819081\n",
            "Iteration 14, loss = 0.59191499\n",
            "Iteration 15, loss = 0.58631585\n",
            "Iteration 16, loss = 0.58632199\n",
            "Iteration 17, loss = 0.57982237\n",
            "Iteration 18, loss = 0.58427959\n",
            "Iteration 19, loss = 0.58286390\n",
            "Iteration 20, loss = 0.57692406\n",
            "Iteration 21, loss = 0.57882092\n",
            "Iteration 22, loss = 0.57558832\n",
            "Iteration 23, loss = 0.57574363\n",
            "Iteration 24, loss = 0.57348766\n",
            "Iteration 25, loss = 0.57281080\n",
            "Iteration 26, loss = 0.57067741\n",
            "Iteration 27, loss = 0.57100269\n",
            "Iteration 28, loss = 0.57195762\n",
            "Iteration 29, loss = 0.56440965\n",
            "Iteration 30, loss = 0.57078388\n",
            "Iteration 31, loss = 0.56903181\n",
            "Iteration 32, loss = 0.56894830\n",
            "Iteration 33, loss = 0.56839992\n",
            "Iteration 34, loss = 0.56668538\n",
            "Iteration 35, loss = 0.56513548\n",
            "Iteration 36, loss = 0.56526964\n",
            "Iteration 37, loss = 0.56913067\n",
            "Iteration 38, loss = 0.56498372\n",
            "Iteration 39, loss = 0.56726145\n",
            "Iteration 40, loss = 0.56682458\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 41, loss = 0.52769245\n",
            "Iteration 42, loss = 0.51668855\n",
            "Iteration 43, loss = 0.50877602\n",
            "Iteration 44, loss = 0.50328476\n",
            "Iteration 45, loss = 0.49967800\n",
            "Iteration 46, loss = 0.49118168\n",
            "Iteration 47, loss = 0.48767710\n",
            "Iteration 48, loss = 0.48868787\n",
            "Iteration 49, loss = 0.49123270\n",
            "Iteration 50, loss = 0.49923404\n",
            "Iteration 51, loss = 0.50765295\n",
            "Iteration 52, loss = 0.50879416\n",
            "Iteration 53, loss = 0.51794137\n",
            "Iteration 54, loss = 0.51210745\n",
            "Iteration 55, loss = 0.50731075\n",
            "Iteration 56, loss = 0.50348006\n",
            "Iteration 57, loss = 0.52016410\n",
            "Iteration 58, loss = 0.51528837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.44689021\n",
            "Iteration 60, loss = 0.44313514\n",
            "Iteration 61, loss = 0.44083981\n",
            "Iteration 62, loss = 0.43909752\n",
            "Iteration 63, loss = 0.43672662\n",
            "Iteration 64, loss = 0.43465218\n",
            "Iteration 65, loss = 0.43232704\n",
            "Iteration 66, loss = 0.42986331\n",
            "Iteration 67, loss = 0.42792988\n",
            "Iteration 68, loss = 0.42590731\n",
            "Iteration 69, loss = 0.42355275\n",
            "Iteration 70, loss = 0.42170736\n",
            "Iteration 71, loss = 0.41964876\n",
            "Iteration 72, loss = 0.41741541\n",
            "Iteration 73, loss = 0.41543430\n",
            "Iteration 74, loss = 0.41337718\n",
            "Iteration 75, loss = 0.41154690\n",
            "Iteration 76, loss = 0.40927446\n",
            "Iteration 77, loss = 0.40710822\n",
            "Iteration 78, loss = 0.40535648\n",
            "Iteration 79, loss = 0.40351459\n",
            "Iteration 80, loss = 0.40115695\n",
            "Iteration 81, loss = 0.39971550\n",
            "Iteration 82, loss = 0.39796280\n",
            "Iteration 83, loss = 0.39581919\n",
            "Iteration 84, loss = 0.39418170\n",
            "Iteration 85, loss = 0.39266079\n",
            "Iteration 86, loss = 0.39051366\n",
            "Iteration 87, loss = 0.38893177\n",
            "Iteration 88, loss = 0.38730112\n",
            "Iteration 89, loss = 0.38475420\n",
            "Iteration 90, loss = 0.38352360\n",
            "Iteration 91, loss = 0.38236117\n",
            "Iteration 92, loss = 0.38026214\n",
            "Iteration 93, loss = 0.37887657\n",
            "Iteration 94, loss = 0.37745341\n",
            "Iteration 95, loss = 0.37622098\n",
            "Iteration 96, loss = 0.37431332\n",
            "Iteration 97, loss = 0.37261932\n",
            "Iteration 98, loss = 0.37155181\n",
            "Iteration 99, loss = 0.36988218\n",
            "Iteration 100, loss = 0.36818050\n",
            "Iteration 101, loss = 0.36731173\n",
            "Iteration 102, loss = 0.36600030\n",
            "Iteration 103, loss = 0.36431421\n",
            "Iteration 104, loss = 0.36382713\n",
            "Iteration 105, loss = 0.36187988\n",
            "Iteration 106, loss = 0.36032986\n",
            "Iteration 107, loss = 0.35902600\n",
            "Iteration 108, loss = 0.35745499\n",
            "Iteration 109, loss = 0.35599788\n",
            "Iteration 110, loss = 0.35607167\n",
            "Iteration 111, loss = 0.35395176\n",
            "Iteration 112, loss = 0.35303422\n",
            "Iteration 113, loss = 0.35109533\n",
            "Iteration 114, loss = 0.35073194\n",
            "Iteration 115, loss = 0.34840883\n",
            "Iteration 116, loss = 0.34937835\n",
            "Iteration 117, loss = 0.34808412\n",
            "Iteration 118, loss = 0.34679395\n",
            "Iteration 119, loss = 0.34551763\n",
            "Iteration 120, loss = 0.34475345\n",
            "Iteration 121, loss = 0.34366471\n",
            "Iteration 122, loss = 0.34215842\n",
            "Iteration 123, loss = 0.34140280\n",
            "Iteration 124, loss = 0.34022512\n",
            "Iteration 125, loss = 0.33991007\n",
            "Iteration 126, loss = 0.33982400\n",
            "Iteration 127, loss = 0.33755259\n",
            "Iteration 128, loss = 0.33705400\n",
            "Iteration 129, loss = 0.33826605\n",
            "Iteration 130, loss = 0.33799454\n",
            "Iteration 131, loss = 0.33616559\n",
            "Iteration 132, loss = 0.33693248\n",
            "Iteration 133, loss = 0.33381498\n",
            "Iteration 134, loss = 0.33376720\n",
            "Iteration 135, loss = 0.33323159\n",
            "Iteration 136, loss = 0.33365222\n",
            "Iteration 137, loss = 0.32952418\n",
            "Iteration 138, loss = 0.32970532\n",
            "Iteration 139, loss = 0.33064082\n",
            "Iteration 140, loss = 0.33023248\n",
            "Iteration 141, loss = 0.32921458\n",
            "Iteration 142, loss = 0.32835725\n",
            "Iteration 143, loss = 0.33002524\n",
            "Iteration 144, loss = 0.32854881\n",
            "Iteration 145, loss = 0.32684564\n",
            "Iteration 146, loss = 0.32869219\n",
            "Iteration 147, loss = 0.32646322\n",
            "Iteration 148, loss = 0.32903176\n",
            "Iteration 149, loss = 0.32562617\n",
            "Iteration 150, loss = 0.32668320\n",
            "Iteration 151, loss = 0.32343807\n",
            "Iteration 152, loss = 0.32475414\n",
            "Iteration 153, loss = 0.32296227\n",
            "Iteration 154, loss = 0.32094818\n",
            "Iteration 155, loss = 0.32509648\n",
            "Iteration 156, loss = 0.32458793\n",
            "Iteration 157, loss = 0.32501366\n",
            "Iteration 158, loss = 0.32548442\n",
            "Iteration 159, loss = 0.32061490\n",
            "Iteration 160, loss = 0.32015668\n",
            "Iteration 161, loss = 0.32037892\n",
            "Iteration 162, loss = 0.31864807\n",
            "Iteration 163, loss = 0.32244057\n",
            "Iteration 164, loss = 0.32437540\n",
            "Iteration 165, loss = 0.32088149\n",
            "Iteration 166, loss = 0.32009239\n",
            "Iteration 167, loss = 0.31898170\n",
            "Iteration 168, loss = 0.31728623\n",
            "Iteration 169, loss = 0.32187801\n",
            "Iteration 170, loss = 0.32338392\n",
            "Iteration 171, loss = 0.31969413\n",
            "Iteration 172, loss = 0.32199374\n",
            "Iteration 173, loss = 0.31365164\n",
            "Iteration 174, loss = 0.32193220\n",
            "Iteration 175, loss = 0.31890262\n",
            "Iteration 176, loss = 0.32038813\n",
            "Iteration 177, loss = 0.32090192\n",
            "Iteration 178, loss = 0.31688776\n",
            "Iteration 179, loss = 0.31759441\n",
            "Iteration 180, loss = 0.31238051\n",
            "Iteration 181, loss = 0.31491088\n",
            "Iteration 182, loss = 0.31874799\n",
            "Iteration 183, loss = 0.31405523\n",
            "Iteration 184, loss = 0.31892672\n",
            "Iteration 185, loss = 0.31247595\n",
            "Iteration 186, loss = 0.31721047\n",
            "Iteration 187, loss = 0.32029816\n",
            "Iteration 188, loss = 0.32140853\n",
            "Iteration 189, loss = 0.31425842\n",
            "Iteration 190, loss = 0.31842677\n",
            "Iteration 191, loss = 0.31710588\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 192, loss = 0.30253739\n",
            "Iteration 193, loss = 0.29572337\n",
            "Iteration 194, loss = 0.29568737\n",
            "Iteration 195, loss = 0.29551651\n",
            "Iteration 196, loss = 0.29545669\n",
            "Iteration 197, loss = 0.29560578\n",
            "Iteration 198, loss = 0.29541272\n",
            "Iteration 199, loss = 0.29526256\n",
            "Iteration 200, loss = 0.29511955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.77126751\n",
            "Iteration 2, loss = 0.69217266\n",
            "Iteration 3, loss = 0.66768343\n",
            "Iteration 4, loss = 0.65570095\n",
            "Iteration 5, loss = 0.64251589\n",
            "Iteration 6, loss = 0.63188746\n",
            "Iteration 7, loss = 0.62514043\n",
            "Iteration 8, loss = 0.61537167\n",
            "Iteration 9, loss = 0.60773218\n",
            "Iteration 10, loss = 0.60583977\n",
            "Iteration 11, loss = 0.59697819\n",
            "Iteration 12, loss = 0.59640217\n",
            "Iteration 13, loss = 0.58884809\n",
            "Iteration 14, loss = 0.59072978\n",
            "Iteration 15, loss = 0.58484602\n",
            "Iteration 16, loss = 0.58412197\n",
            "Iteration 17, loss = 0.58280851\n",
            "Iteration 18, loss = 0.57705052\n",
            "Iteration 19, loss = 0.57541770\n",
            "Iteration 20, loss = 0.57617193\n",
            "Iteration 21, loss = 0.57344606\n",
            "Iteration 22, loss = 0.57223288\n",
            "Iteration 23, loss = 0.57019190\n",
            "Iteration 24, loss = 0.57093635\n",
            "Iteration 25, loss = 0.56873307\n",
            "Iteration 26, loss = 0.57055486\n",
            "Iteration 27, loss = 0.56871320\n",
            "Iteration 28, loss = 0.56608830\n",
            "Iteration 29, loss = 0.56431078\n",
            "Iteration 30, loss = 0.56734794\n",
            "Iteration 31, loss = 0.56608570\n",
            "Iteration 32, loss = 0.56652636\n",
            "Iteration 33, loss = 0.56884165\n",
            "Iteration 34, loss = 0.56528919\n",
            "Iteration 35, loss = 0.56606890\n",
            "Iteration 36, loss = 0.56535391\n",
            "Iteration 37, loss = 0.56135607\n",
            "Iteration 38, loss = 0.56560036\n",
            "Iteration 39, loss = 0.56317371\n",
            "Iteration 40, loss = 0.56663516\n",
            "Iteration 41, loss = 0.56944067\n",
            "Iteration 42, loss = 0.56626380\n",
            "Iteration 43, loss = 0.56550672\n",
            "Iteration 44, loss = 0.56433059\n",
            "Iteration 45, loss = 0.56627896\n",
            "Iteration 46, loss = 0.56518484\n",
            "Iteration 47, loss = 0.56750638\n",
            "Iteration 48, loss = 0.56524475\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.52279031\n",
            "Iteration 50, loss = 0.51358872\n",
            "Iteration 51, loss = 0.50704379\n",
            "Iteration 52, loss = 0.50096169\n",
            "Iteration 53, loss = 0.49580853\n",
            "Iteration 54, loss = 0.49052960\n",
            "Iteration 55, loss = 0.48396306\n",
            "Iteration 56, loss = 0.49116695\n",
            "Iteration 57, loss = 0.50006011\n",
            "Iteration 58, loss = 0.51474413\n",
            "Iteration 59, loss = 0.51315036\n",
            "Iteration 60, loss = 0.50986628\n",
            "Iteration 61, loss = 0.52418692\n",
            "Iteration 62, loss = 0.50041553\n",
            "Iteration 63, loss = 0.52585104\n",
            "Iteration 64, loss = 0.52151162\n",
            "Iteration 65, loss = 0.50964442\n",
            "Iteration 66, loss = 0.50233486\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.44225296\n",
            "Iteration 68, loss = 0.43788971\n",
            "Iteration 69, loss = 0.43488279\n",
            "Iteration 70, loss = 0.43232875\n",
            "Iteration 71, loss = 0.42959938\n",
            "Iteration 72, loss = 0.42708886\n",
            "Iteration 73, loss = 0.42438119\n",
            "Iteration 74, loss = 0.42202639\n",
            "Iteration 75, loss = 0.41884524\n",
            "Iteration 76, loss = 0.41657352\n",
            "Iteration 77, loss = 0.41411397\n",
            "Iteration 78, loss = 0.41177482\n",
            "Iteration 79, loss = 0.40884930\n",
            "Iteration 80, loss = 0.40641795\n",
            "Iteration 81, loss = 0.40395861\n",
            "Iteration 82, loss = 0.40140192\n",
            "Iteration 83, loss = 0.39885637\n",
            "Iteration 84, loss = 0.39676227\n",
            "Iteration 85, loss = 0.39443240\n",
            "Iteration 86, loss = 0.39224645\n",
            "Iteration 87, loss = 0.38992888\n",
            "Iteration 88, loss = 0.38778834\n",
            "Iteration 89, loss = 0.38558112\n",
            "Iteration 90, loss = 0.38332844\n",
            "Iteration 91, loss = 0.38127582\n",
            "Iteration 92, loss = 0.37900856\n",
            "Iteration 93, loss = 0.37730841\n",
            "Iteration 94, loss = 0.37469802\n",
            "Iteration 95, loss = 0.37286577\n",
            "Iteration 96, loss = 0.37126424\n",
            "Iteration 97, loss = 0.36888460\n",
            "Iteration 98, loss = 0.36693987\n",
            "Iteration 99, loss = 0.36482495\n",
            "Iteration 100, loss = 0.36305018\n",
            "Iteration 101, loss = 0.36199081\n",
            "Iteration 102, loss = 0.35923587\n",
            "Iteration 103, loss = 0.35797804\n",
            "Iteration 104, loss = 0.35581056\n",
            "Iteration 105, loss = 0.35349030\n",
            "Iteration 106, loss = 0.35191117\n",
            "Iteration 107, loss = 0.35097180\n",
            "Iteration 108, loss = 0.34921694\n",
            "Iteration 109, loss = 0.34752221\n",
            "Iteration 110, loss = 0.34661586\n",
            "Iteration 111, loss = 0.34430035\n",
            "Iteration 112, loss = 0.34308227\n",
            "Iteration 113, loss = 0.34173339\n",
            "Iteration 114, loss = 0.34002220\n",
            "Iteration 115, loss = 0.33892773\n",
            "Iteration 116, loss = 0.33731178\n",
            "Iteration 117, loss = 0.33520042\n",
            "Iteration 118, loss = 0.33420158\n",
            "Iteration 119, loss = 0.33252010\n",
            "Iteration 120, loss = 0.33175411\n",
            "Iteration 121, loss = 0.33042665\n",
            "Iteration 122, loss = 0.33021924\n",
            "Iteration 123, loss = 0.32802738\n",
            "Iteration 124, loss = 0.32594782\n",
            "Iteration 125, loss = 0.32573966\n",
            "Iteration 126, loss = 0.32428543\n",
            "Iteration 127, loss = 0.32307363\n",
            "Iteration 128, loss = 0.32453139\n",
            "Iteration 129, loss = 0.32241899\n",
            "Iteration 130, loss = 0.31947876\n",
            "Iteration 131, loss = 0.31848331\n",
            "Iteration 132, loss = 0.31721009\n",
            "Iteration 133, loss = 0.31603132\n",
            "Iteration 134, loss = 0.31469186\n",
            "Iteration 135, loss = 0.31378448\n",
            "Iteration 136, loss = 0.31513108\n",
            "Iteration 137, loss = 0.31195282\n",
            "Iteration 138, loss = 0.31410850\n",
            "Iteration 139, loss = 0.31320249\n",
            "Iteration 140, loss = 0.31054746\n",
            "Iteration 141, loss = 0.30967806\n",
            "Iteration 142, loss = 0.30754094\n",
            "Iteration 143, loss = 0.30853298\n",
            "Iteration 144, loss = 0.30903543\n",
            "Iteration 145, loss = 0.30605730\n",
            "Iteration 146, loss = 0.30643456\n",
            "Iteration 147, loss = 0.30731469\n",
            "Iteration 148, loss = 0.30377706\n",
            "Iteration 149, loss = 0.30458453\n",
            "Iteration 150, loss = 0.30424410\n",
            "Iteration 151, loss = 0.30461700\n",
            "Iteration 152, loss = 0.30177838\n",
            "Iteration 153, loss = 0.29983463\n",
            "Iteration 154, loss = 0.30313018\n",
            "Iteration 155, loss = 0.30019575\n",
            "Iteration 156, loss = 0.30205939\n",
            "Iteration 157, loss = 0.29849282\n",
            "Iteration 158, loss = 0.30093594\n",
            "Iteration 159, loss = 0.30150624\n",
            "Iteration 160, loss = 0.29957388\n",
            "Iteration 161, loss = 0.29794051\n",
            "Iteration 162, loss = 0.29826391\n",
            "Iteration 163, loss = 0.29883520\n",
            "Iteration 164, loss = 0.29902453\n",
            "Iteration 165, loss = 0.29568941\n",
            "Iteration 166, loss = 0.29760329\n",
            "Iteration 167, loss = 0.29183518\n",
            "Iteration 168, loss = 0.29569576\n",
            "Iteration 169, loss = 0.29386625\n",
            "Iteration 170, loss = 0.29408887\n",
            "Iteration 171, loss = 0.29168103\n",
            "Iteration 172, loss = 0.29660815\n",
            "Iteration 173, loss = 0.29410326\n",
            "Iteration 174, loss = 0.29245350\n",
            "Iteration 175, loss = 0.28927893\n",
            "Iteration 176, loss = 0.29068949\n",
            "Iteration 177, loss = 0.29597087\n",
            "Iteration 178, loss = 0.28942873\n",
            "Iteration 179, loss = 0.28583951\n",
            "Iteration 180, loss = 0.29556654\n",
            "Iteration 181, loss = 0.29336437\n",
            "Iteration 182, loss = 0.29333918\n",
            "Iteration 183, loss = 0.28868073\n",
            "Iteration 184, loss = 0.28638474\n",
            "Iteration 185, loss = 0.28710707\n",
            "Iteration 186, loss = 0.29259020\n",
            "Iteration 187, loss = 0.28738777\n",
            "Iteration 188, loss = 0.29425836\n",
            "Iteration 189, loss = 0.29356950\n",
            "Iteration 190, loss = 0.29637517\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 191, loss = 0.27399142\n",
            "Iteration 192, loss = 0.27395429\n",
            "Iteration 193, loss = 0.27373098\n",
            "Iteration 194, loss = 0.27385136\n",
            "Iteration 195, loss = 0.27365293\n",
            "Iteration 196, loss = 0.27362606\n",
            "Iteration 197, loss = 0.27348826\n",
            "Iteration 198, loss = 0.27358792\n",
            "Iteration 199, loss = 0.27337366\n",
            "Iteration 200, loss = 0.27333293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67476156\n",
            "Iteration 3, loss = 0.65773126\n",
            "Iteration 4, loss = 0.65050147\n",
            "Iteration 5, loss = 0.63626306\n",
            "Iteration 6, loss = 0.62839631\n",
            "Iteration 7, loss = 0.61992210\n",
            "Iteration 8, loss = 0.61300216\n",
            "Iteration 9, loss = 0.60557573\n",
            "Iteration 10, loss = 0.60097414\n",
            "Iteration 11, loss = 0.59775056\n",
            "Iteration 12, loss = 0.59131504\n",
            "Iteration 13, loss = 0.58842425\n",
            "Iteration 14, loss = 0.58625601\n",
            "Iteration 15, loss = 0.58293326\n",
            "Iteration 16, loss = 0.57833726\n",
            "Iteration 17, loss = 0.57853368\n",
            "Iteration 18, loss = 0.57723017\n",
            "Iteration 19, loss = 0.56781168\n",
            "Iteration 20, loss = 0.57023454\n",
            "Iteration 21, loss = 0.57546402\n",
            "Iteration 22, loss = 0.56876732\n",
            "Iteration 23, loss = 0.56690518\n",
            "Iteration 24, loss = 0.56805610\n",
            "Iteration 25, loss = 0.56754311\n",
            "Iteration 26, loss = 0.56690482\n",
            "Iteration 27, loss = 0.56358310\n",
            "Iteration 28, loss = 0.56690149\n",
            "Iteration 29, loss = 0.56423185\n",
            "Iteration 30, loss = 0.56406000\n",
            "Iteration 31, loss = 0.56441919\n",
            "Iteration 32, loss = 0.56467315\n",
            "Iteration 33, loss = 0.56104596\n",
            "Iteration 34, loss = 0.56248104\n",
            "Iteration 35, loss = 0.56417240\n",
            "Iteration 36, loss = 0.56128574\n",
            "Iteration 37, loss = 0.56944737\n",
            "Iteration 38, loss = 0.56383144\n",
            "Iteration 39, loss = 0.56150317\n",
            "Iteration 40, loss = 0.56047860\n",
            "Iteration 41, loss = 0.56333236\n",
            "Iteration 42, loss = 0.56406971\n",
            "Iteration 43, loss = 0.56208153\n",
            "Iteration 44, loss = 0.56457399\n",
            "Iteration 45, loss = 0.56096651\n",
            "Iteration 46, loss = 0.56225749\n",
            "Iteration 47, loss = 0.56055579\n",
            "Iteration 48, loss = 0.56436539\n",
            "Iteration 49, loss = 0.55903284\n",
            "Iteration 50, loss = 0.56158528\n",
            "Iteration 51, loss = 0.56325968\n",
            "Iteration 52, loss = 0.56019144\n",
            "Iteration 53, loss = 0.56439613\n",
            "Iteration 54, loss = 0.56498908\n",
            "Iteration 55, loss = 0.56088571\n",
            "Iteration 56, loss = 0.56192581\n",
            "Iteration 57, loss = 0.56106721\n",
            "Iteration 58, loss = 0.56509623\n",
            "Iteration 59, loss = 0.56021881\n",
            "Iteration 60, loss = 0.55871451\n",
            "Iteration 61, loss = 0.56492412\n",
            "Iteration 62, loss = 0.56226580\n",
            "Iteration 63, loss = 0.56228344\n",
            "Iteration 64, loss = 0.56135071\n",
            "Iteration 65, loss = 0.56300423\n",
            "Iteration 66, loss = 0.56569270\n",
            "Iteration 67, loss = 0.55930540\n",
            "Iteration 68, loss = 0.57076560\n",
            "Iteration 69, loss = 0.57154037\n",
            "Iteration 70, loss = 0.56628222\n",
            "Iteration 71, loss = 0.56802910\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 72, loss = 0.52104948\n",
            "Iteration 73, loss = 0.50887708\n",
            "Iteration 74, loss = 0.49875351\n",
            "Iteration 75, loss = 0.49058604\n",
            "Iteration 76, loss = 0.48302715\n",
            "Iteration 77, loss = 0.47801213\n",
            "Iteration 78, loss = 0.47306699\n",
            "Iteration 79, loss = 0.47765947\n",
            "Iteration 80, loss = 0.51234205\n",
            "Iteration 81, loss = 0.51552736\n",
            "Iteration 82, loss = 0.50386099\n",
            "Iteration 83, loss = 0.49445998\n",
            "Iteration 84, loss = 0.52582008\n",
            "Iteration 85, loss = 0.51789654\n",
            "Iteration 86, loss = 0.49430763\n",
            "Iteration 87, loss = 0.51513787\n",
            "Iteration 88, loss = 0.50407743\n",
            "Iteration 89, loss = 0.52583452\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.43994539\n",
            "Iteration 91, loss = 0.43181189\n",
            "Iteration 92, loss = 0.42911032\n",
            "Iteration 93, loss = 0.42579323\n",
            "Iteration 94, loss = 0.42267038\n",
            "Iteration 95, loss = 0.42019327\n",
            "Iteration 96, loss = 0.41738072\n",
            "Iteration 97, loss = 0.41465079\n",
            "Iteration 98, loss = 0.41169546\n",
            "Iteration 99, loss = 0.40897199\n",
            "Iteration 100, loss = 0.40621779\n",
            "Iteration 101, loss = 0.40290173\n",
            "Iteration 102, loss = 0.40048131\n",
            "Iteration 103, loss = 0.39830234\n",
            "Iteration 104, loss = 0.39503233\n",
            "Iteration 105, loss = 0.39278213\n",
            "Iteration 106, loss = 0.38991370\n",
            "Iteration 107, loss = 0.38760177\n",
            "Iteration 108, loss = 0.38557462\n",
            "Iteration 109, loss = 0.38262281\n",
            "Iteration 110, loss = 0.38070807\n",
            "Iteration 111, loss = 0.37814680\n",
            "Iteration 112, loss = 0.37556636\n",
            "Iteration 113, loss = 0.37321186\n",
            "Iteration 114, loss = 0.37091022\n",
            "Iteration 115, loss = 0.36859577\n",
            "Iteration 116, loss = 0.36715427\n",
            "Iteration 117, loss = 0.36451688\n",
            "Iteration 118, loss = 0.36263938\n",
            "Iteration 119, loss = 0.36115077\n",
            "Iteration 120, loss = 0.35872427\n",
            "Iteration 121, loss = 0.35654731\n",
            "Iteration 122, loss = 0.35485261\n",
            "Iteration 123, loss = 0.35306602\n",
            "Iteration 124, loss = 0.35061206\n",
            "Iteration 125, loss = 0.34872455\n",
            "Iteration 126, loss = 0.34761540\n",
            "Iteration 127, loss = 0.34566889\n",
            "Iteration 128, loss = 0.34366770\n",
            "Iteration 129, loss = 0.34257621\n",
            "Iteration 130, loss = 0.34000648\n",
            "Iteration 131, loss = 0.33884715\n",
            "Iteration 132, loss = 0.33759826\n",
            "Iteration 133, loss = 0.33573210\n",
            "Iteration 134, loss = 0.33398730\n",
            "Iteration 135, loss = 0.33205068\n",
            "Iteration 136, loss = 0.33109474\n",
            "Iteration 137, loss = 0.32991348\n",
            "Iteration 138, loss = 0.32868875\n",
            "Iteration 139, loss = 0.32634744\n",
            "Iteration 140, loss = 0.32522496\n",
            "Iteration 141, loss = 0.32401485\n",
            "Iteration 142, loss = 0.32257129\n",
            "Iteration 143, loss = 0.32181074\n",
            "Iteration 144, loss = 0.32016847\n",
            "Iteration 145, loss = 0.31932391\n",
            "Iteration 146, loss = 0.31780657\n",
            "Iteration 147, loss = 0.31715413\n",
            "Iteration 148, loss = 0.31524068\n",
            "Iteration 149, loss = 0.31517056\n",
            "Iteration 150, loss = 0.31356988\n",
            "Iteration 151, loss = 0.31166296\n",
            "Iteration 152, loss = 0.31116077\n",
            "Iteration 153, loss = 0.31171705\n",
            "Iteration 154, loss = 0.30998715\n",
            "Iteration 155, loss = 0.30768256\n",
            "Iteration 156, loss = 0.30744087\n",
            "Iteration 157, loss = 0.30828945\n",
            "Iteration 158, loss = 0.30591427\n",
            "Iteration 159, loss = 0.30525767\n",
            "Iteration 160, loss = 0.30390531\n",
            "Iteration 161, loss = 0.30146460\n",
            "Iteration 162, loss = 0.30236932\n",
            "Iteration 163, loss = 0.30125187\n",
            "Iteration 164, loss = 0.30155665\n",
            "Iteration 165, loss = 0.30044601\n",
            "Iteration 166, loss = 0.29783164\n",
            "Iteration 167, loss = 0.29659709\n",
            "Iteration 168, loss = 0.29713380\n",
            "Iteration 169, loss = 0.29615757\n",
            "Iteration 170, loss = 0.29674041\n",
            "Iteration 171, loss = 0.29453850\n",
            "Iteration 172, loss = 0.29449099\n",
            "Iteration 173, loss = 0.29459802\n",
            "Iteration 174, loss = 0.29277184\n",
            "Iteration 175, loss = 0.29585601\n",
            "Iteration 176, loss = 0.29093355\n",
            "Iteration 177, loss = 0.29303738\n",
            "Iteration 178, loss = 0.29163225\n",
            "Iteration 179, loss = 0.29093731\n",
            "Iteration 180, loss = 0.29032137\n",
            "Iteration 181, loss = 0.28826819\n",
            "Iteration 182, loss = 0.28874919\n",
            "Iteration 183, loss = 0.28830734\n",
            "Iteration 184, loss = 0.28723831\n",
            "Iteration 185, loss = 0.28750427\n",
            "Iteration 186, loss = 0.28697055\n",
            "Iteration 187, loss = 0.28509864\n",
            "Iteration 188, loss = 0.28674348\n",
            "Iteration 189, loss = 0.28510529\n",
            "Iteration 190, loss = 0.28411696\n",
            "Iteration 191, loss = 0.28482783\n",
            "Iteration 192, loss = 0.28291478\n",
            "Iteration 193, loss = 0.28313463\n",
            "Iteration 194, loss = 0.28413224\n",
            "Iteration 195, loss = 0.29118730\n",
            "Iteration 196, loss = 0.28259688\n",
            "Iteration 197, loss = 0.28428254\n",
            "Iteration 198, loss = 0.28325430\n",
            "Iteration 199, loss = 0.28115484\n",
            "Iteration 200, loss = 0.28277877\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 17.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[33312   415]\n",
            " [ 1209 15710]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     33727\n",
            "           1       0.97      0.93      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.96     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.733% (4.846%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.986% (9.994%)\n",
            "Iteration 1, loss = 0.87225576\n",
            "Iteration 2, loss = 0.63617011\n",
            "Iteration 3, loss = 0.61204943\n",
            "Iteration 4, loss = 0.59884399\n",
            "Iteration 5, loss = 0.59879565\n",
            "Iteration 6, loss = 0.59014907\n",
            "Iteration 7, loss = 0.58612943\n",
            "Iteration 8, loss = 0.58372541\n",
            "Iteration 9, loss = 0.58178804\n",
            "Iteration 10, loss = 0.58276042\n",
            "Iteration 11, loss = 0.58129257\n",
            "Iteration 12, loss = 0.58138527\n",
            "Iteration 13, loss = 0.58071514\n",
            "Iteration 14, loss = 0.58123599\n",
            "Iteration 15, loss = 0.59220981\n",
            "Iteration 16, loss = 0.63643750\n",
            "Iteration 17, loss = 0.61826009\n",
            "Iteration 18, loss = 0.60181423\n",
            "Iteration 19, loss = 0.59864560\n",
            "Iteration 20, loss = 0.60417709\n",
            "Iteration 21, loss = 0.59263295\n",
            "Iteration 22, loss = 0.58800281\n",
            "Iteration 23, loss = 0.58600077\n",
            "Iteration 24, loss = 0.58721136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.56290205\n",
            "Iteration 26, loss = 0.55258555\n",
            "Iteration 27, loss = 0.54533058\n",
            "Iteration 28, loss = 0.54791233\n",
            "Iteration 29, loss = 0.55179800\n",
            "Iteration 30, loss = 0.55222545\n",
            "Iteration 31, loss = 0.55127038\n",
            "Iteration 32, loss = 0.55251487\n",
            "Iteration 33, loss = 0.54863500\n",
            "Iteration 34, loss = 0.54807952\n",
            "Iteration 35, loss = 0.54668418\n",
            "Iteration 36, loss = 0.54875832\n",
            "Iteration 37, loss = 0.54724370\n",
            "Iteration 38, loss = 0.54677959\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.48168792\n",
            "Iteration 40, loss = 0.47146413\n",
            "Iteration 41, loss = 0.46216950\n",
            "Iteration 42, loss = 0.45347241\n",
            "Iteration 43, loss = 0.44534713\n",
            "Iteration 44, loss = 0.43800287\n",
            "Iteration 45, loss = 0.43164413\n",
            "Iteration 46, loss = 0.42560744\n",
            "Iteration 47, loss = 0.42119390\n",
            "Iteration 48, loss = 0.41646872\n",
            "Iteration 49, loss = 0.41270900\n",
            "Iteration 50, loss = 0.40968823\n",
            "Iteration 51, loss = 0.40669765\n",
            "Iteration 52, loss = 0.40399255\n",
            "Iteration 53, loss = 0.40364770\n",
            "Iteration 54, loss = 0.40211727\n",
            "Iteration 55, loss = 0.39843017\n",
            "Iteration 56, loss = 0.40153217\n",
            "Iteration 57, loss = 0.40120521\n",
            "Iteration 58, loss = 0.39703437\n",
            "Iteration 59, loss = 0.40008834\n",
            "Iteration 60, loss = 0.40002755\n",
            "Iteration 61, loss = 0.40007700\n",
            "Iteration 62, loss = 0.39866296\n",
            "Iteration 63, loss = 0.39703902\n",
            "Iteration 64, loss = 0.40134321\n",
            "Iteration 65, loss = 0.39736237\n",
            "Iteration 66, loss = 0.39958782\n",
            "Iteration 67, loss = 0.39774461\n",
            "Iteration 68, loss = 0.39698066\n",
            "Iteration 69, loss = 0.39657647\n",
            "Iteration 70, loss = 0.39840438\n",
            "Iteration 71, loss = 0.39780712\n",
            "Iteration 72, loss = 0.39542814\n",
            "Iteration 73, loss = 0.39404387\n",
            "Iteration 74, loss = 0.39599935\n",
            "Iteration 75, loss = 0.39805367\n",
            "Iteration 76, loss = 0.39835285\n",
            "Iteration 77, loss = 0.39497797\n",
            "Iteration 78, loss = 0.39473776\n",
            "Iteration 79, loss = 0.39355812\n",
            "Iteration 80, loss = 0.39499173\n",
            "Iteration 81, loss = 0.39250655\n",
            "Iteration 82, loss = 0.39469149\n",
            "Iteration 83, loss = 0.39234492\n",
            "Iteration 84, loss = 0.39149023\n",
            "Iteration 85, loss = 0.39136660\n",
            "Iteration 86, loss = 0.39374228\n",
            "Iteration 87, loss = 0.39585961\n",
            "Iteration 88, loss = 0.39130978\n",
            "Iteration 89, loss = 0.38970601\n",
            "Iteration 90, loss = 0.38700569\n",
            "Iteration 91, loss = 0.38814816\n",
            "Iteration 92, loss = 0.39220552\n",
            "Iteration 93, loss = 0.38938917\n",
            "Iteration 94, loss = 0.38941862\n",
            "Iteration 95, loss = 0.38840562\n",
            "Iteration 96, loss = 0.38414644\n",
            "Iteration 97, loss = 0.39149744\n",
            "Iteration 98, loss = 0.38978405\n",
            "Iteration 99, loss = 0.38725946\n",
            "Iteration 100, loss = 0.38156637\n",
            "Iteration 101, loss = 0.38737872\n",
            "Iteration 102, loss = 0.38679960\n",
            "Iteration 103, loss = 0.38528964\n",
            "Iteration 104, loss = 0.38294530\n",
            "Iteration 105, loss = 0.38707709\n",
            "Iteration 106, loss = 0.38137889\n",
            "Iteration 107, loss = 0.38071452\n",
            "Iteration 108, loss = 0.38063797\n",
            "Iteration 109, loss = 0.38504553\n",
            "Iteration 110, loss = 0.38614438\n",
            "Iteration 111, loss = 0.38462938\n",
            "Iteration 112, loss = 0.38153110\n",
            "Iteration 113, loss = 0.37755336\n",
            "Iteration 114, loss = 0.38334808\n",
            "Iteration 115, loss = 0.38297843\n",
            "Iteration 116, loss = 0.37936704\n",
            "Iteration 117, loss = 0.37602075\n",
            "Iteration 118, loss = 0.37968341\n",
            "Iteration 119, loss = 0.37839878\n",
            "Iteration 120, loss = 0.38395866\n",
            "Iteration 121, loss = 0.38015448\n",
            "Iteration 122, loss = 0.37666619\n",
            "Iteration 123, loss = 0.37495518\n",
            "Iteration 124, loss = 0.38157795\n",
            "Iteration 125, loss = 0.38365237\n",
            "Iteration 126, loss = 0.37809135\n",
            "Iteration 127, loss = 0.37832486\n",
            "Iteration 128, loss = 0.38026111\n",
            "Iteration 129, loss = 0.37311347\n",
            "Iteration 130, loss = 0.38105542\n",
            "Iteration 131, loss = 0.37458147\n",
            "Iteration 132, loss = 0.38272279\n",
            "Iteration 133, loss = 0.38363688\n",
            "Iteration 134, loss = 0.37187010\n",
            "Iteration 135, loss = 0.37706318\n",
            "Iteration 136, loss = 0.37619823\n",
            "Iteration 137, loss = 0.37763522\n",
            "Iteration 138, loss = 0.37675353\n",
            "Iteration 139, loss = 0.37458625\n",
            "Iteration 140, loss = 0.37817561\n",
            "Iteration 141, loss = 0.37538368\n",
            "Iteration 142, loss = 0.37696238\n",
            "Iteration 143, loss = 0.37844699\n",
            "Iteration 144, loss = 0.37415053\n",
            "Iteration 145, loss = 0.37596035\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 146, loss = 0.33778087\n",
            "Iteration 147, loss = 0.33669406\n",
            "Iteration 148, loss = 0.33664696\n",
            "Iteration 149, loss = 0.33646231\n",
            "Iteration 150, loss = 0.33637003\n",
            "Iteration 151, loss = 0.33622849\n",
            "Iteration 152, loss = 0.33620145\n",
            "Iteration 153, loss = 0.33619217\n",
            "Iteration 154, loss = 0.33611853\n",
            "Iteration 155, loss = 0.33600550\n",
            "Iteration 156, loss = 0.33594546\n",
            "Iteration 157, loss = 0.33583814\n",
            "Iteration 158, loss = 0.33569124\n",
            "Iteration 159, loss = 0.33567874\n",
            "Iteration 160, loss = 0.33567438\n",
            "Iteration 161, loss = 0.33548592\n",
            "Iteration 162, loss = 0.33544514\n",
            "Iteration 163, loss = 0.33546281\n",
            "Iteration 164, loss = 0.33538154\n",
            "Iteration 165, loss = 0.33524675\n",
            "Iteration 166, loss = 0.33517603\n",
            "Iteration 167, loss = 0.33509347\n",
            "Iteration 168, loss = 0.33506115\n",
            "Iteration 169, loss = 0.33496233\n",
            "Iteration 170, loss = 0.33497104\n",
            "Iteration 171, loss = 0.33488762\n",
            "Iteration 172, loss = 0.33481994\n",
            "Iteration 173, loss = 0.33471357\n",
            "Iteration 174, loss = 0.33466894\n",
            "Iteration 175, loss = 0.33453988\n",
            "Iteration 176, loss = 0.33458498\n",
            "Iteration 177, loss = 0.33448073\n",
            "Iteration 178, loss = 0.33434432\n",
            "Iteration 179, loss = 0.33426115\n",
            "Iteration 180, loss = 0.33420174\n",
            "Iteration 181, loss = 0.33414563\n",
            "Iteration 182, loss = 0.33417574\n",
            "Iteration 183, loss = 0.33405366\n",
            "Iteration 184, loss = 0.33396542\n",
            "Iteration 185, loss = 0.33393795\n",
            "Iteration 186, loss = 0.33390868\n",
            "Iteration 187, loss = 0.33378451\n",
            "Iteration 188, loss = 0.33369279\n",
            "Iteration 189, loss = 0.33362705\n",
            "Iteration 190, loss = 0.33358398\n",
            "Iteration 191, loss = 0.33367640\n",
            "Iteration 192, loss = 0.33361371\n",
            "Iteration 193, loss = 0.33348595\n",
            "Iteration 194, loss = 0.33345122\n",
            "Iteration 195, loss = 0.33333618\n",
            "Iteration 196, loss = 0.33335796\n",
            "Iteration 197, loss = 0.33321986\n",
            "Iteration 198, loss = 0.33319049\n",
            "Iteration 199, loss = 0.33319620\n",
            "Iteration 200, loss = 0.33308717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.61921571\n",
            "Iteration 3, loss = 0.59661277\n",
            "Iteration 4, loss = 0.58233253\n",
            "Iteration 5, loss = 0.57492069\n",
            "Iteration 6, loss = 0.56833984\n",
            "Iteration 7, loss = 0.56361426\n",
            "Iteration 8, loss = 0.56019395\n",
            "Iteration 9, loss = 0.55943939\n",
            "Iteration 10, loss = 0.56248393\n",
            "Iteration 11, loss = 0.56167138\n",
            "Iteration 12, loss = 0.55875065\n",
            "Iteration 13, loss = 0.56111247\n",
            "Iteration 14, loss = 0.56072674\n",
            "Iteration 15, loss = 0.56142731\n",
            "Iteration 16, loss = 0.56129525\n",
            "Iteration 17, loss = 0.56267099\n",
            "Iteration 18, loss = 0.56268812\n",
            "Iteration 19, loss = 0.56434022\n",
            "Iteration 20, loss = 0.56359995\n",
            "Iteration 21, loss = 0.56322001\n",
            "Iteration 22, loss = 0.56313409\n",
            "Iteration 23, loss = 0.56187155\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.49644706\n",
            "Iteration 25, loss = 0.47967754\n",
            "Iteration 26, loss = 0.50316137\n",
            "Iteration 27, loss = 0.50577099\n",
            "Iteration 28, loss = 0.50898479\n",
            "Iteration 29, loss = 0.50568155\n",
            "Iteration 30, loss = 0.50749181\n",
            "Iteration 31, loss = 0.50342389\n",
            "Iteration 32, loss = 0.51012635\n",
            "Iteration 33, loss = 0.50376641\n",
            "Iteration 34, loss = 0.50722353\n",
            "Iteration 35, loss = 0.50534088\n",
            "Iteration 36, loss = 0.50427830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.40504596\n",
            "Iteration 38, loss = 0.39137908\n",
            "Iteration 39, loss = 0.37887529\n",
            "Iteration 40, loss = 0.36757452\n",
            "Iteration 41, loss = 0.35737116\n",
            "Iteration 42, loss = 0.34815413\n",
            "Iteration 43, loss = 0.33984577\n",
            "Iteration 44, loss = 0.33242748\n",
            "Iteration 45, loss = 0.32534413\n",
            "Iteration 46, loss = 0.31935539\n",
            "Iteration 47, loss = 0.31405915\n",
            "Iteration 48, loss = 0.30898012\n",
            "Iteration 49, loss = 0.30464677\n",
            "Iteration 50, loss = 0.30030036\n",
            "Iteration 51, loss = 0.29693207\n",
            "Iteration 52, loss = 0.29300698\n",
            "Iteration 53, loss = 0.29023356\n",
            "Iteration 54, loss = 0.28755774\n",
            "Iteration 55, loss = 0.28520243\n",
            "Iteration 56, loss = 0.28229656\n",
            "Iteration 57, loss = 0.28055223\n",
            "Iteration 58, loss = 0.27857539\n",
            "Iteration 59, loss = 0.27693323\n",
            "Iteration 60, loss = 0.27617666\n",
            "Iteration 61, loss = 0.27434456\n",
            "Iteration 62, loss = 0.27314031\n",
            "Iteration 63, loss = 0.27217637\n",
            "Iteration 64, loss = 0.27050054\n",
            "Iteration 65, loss = 0.27028930\n",
            "Iteration 66, loss = 0.26858396\n",
            "Iteration 67, loss = 0.26826329\n",
            "Iteration 68, loss = 0.26757761\n",
            "Iteration 69, loss = 0.26703237\n",
            "Iteration 70, loss = 0.26641480\n",
            "Iteration 71, loss = 0.26642487\n",
            "Iteration 72, loss = 0.26773149\n",
            "Iteration 73, loss = 0.26578017\n",
            "Iteration 74, loss = 0.26596052\n",
            "Iteration 75, loss = 0.26517117\n",
            "Iteration 76, loss = 0.26465621\n",
            "Iteration 77, loss = 0.26491530\n",
            "Iteration 78, loss = 0.26500823\n",
            "Iteration 79, loss = 0.26573632\n",
            "Iteration 80, loss = 0.26390320\n",
            "Iteration 81, loss = 0.26529283\n",
            "Iteration 82, loss = 0.26355855\n",
            "Iteration 83, loss = 0.26552087\n",
            "Iteration 84, loss = 0.26610466\n",
            "Iteration 85, loss = 0.26217325\n",
            "Iteration 86, loss = 0.26569424\n",
            "Iteration 87, loss = 0.27225465\n",
            "Iteration 88, loss = 0.26644916\n",
            "Iteration 89, loss = 0.26487327\n",
            "Iteration 90, loss = 0.26855159\n",
            "Iteration 91, loss = 0.26377868\n",
            "Iteration 92, loss = 0.26218008\n",
            "Iteration 93, loss = 0.26983799\n",
            "Iteration 94, loss = 0.27320688\n",
            "Iteration 95, loss = 0.26264951\n",
            "Iteration 96, loss = 0.26760073\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.25159479\n",
            "Iteration 98, loss = 0.25140806\n",
            "Iteration 99, loss = 0.25147723\n",
            "Iteration 100, loss = 0.25132169\n",
            "Iteration 101, loss = 0.25133717\n",
            "Iteration 102, loss = 0.25130088\n",
            "Iteration 103, loss = 0.25129794\n",
            "Iteration 104, loss = 0.25124976\n",
            "Iteration 105, loss = 0.25117801\n",
            "Iteration 106, loss = 0.25118938\n",
            "Iteration 107, loss = 0.25108973\n",
            "Iteration 108, loss = 0.25110064\n",
            "Iteration 109, loss = 0.25100860\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.25073456\n",
            "Iteration 111, loss = 0.25072961\n",
            "Iteration 112, loss = 0.25073630\n",
            "Iteration 113, loss = 0.25071852\n",
            "Iteration 114, loss = 0.25071539\n",
            "Iteration 115, loss = 0.25071255\n",
            "Iteration 116, loss = 0.25069761\n",
            "Iteration 117, loss = 0.25068723\n",
            "Iteration 118, loss = 0.25070149\n",
            "Iteration 119, loss = 0.25068514\n",
            "Iteration 120, loss = 0.25068777\n",
            "Iteration 121, loss = 0.25065568\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.25058340\n",
            "Iteration 123, loss = 0.25057261\n",
            "Iteration 124, loss = 0.25057204\n",
            "Iteration 125, loss = 0.25056373\n",
            "Iteration 126, loss = 0.25057197\n",
            "Iteration 127, loss = 0.25057100\n",
            "Iteration 128, loss = 0.25055756\n",
            "Iteration 129, loss = 0.25056508\n",
            "Iteration 130, loss = 0.25056986\n",
            "Iteration 131, loss = 0.25054480\n",
            "Iteration 132, loss = 0.25055961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.89229659\n",
            "Iteration 2, loss = 0.63268133\n",
            "Iteration 3, loss = 0.60502330\n",
            "Iteration 4, loss = 0.58957166\n",
            "Iteration 5, loss = 0.58029346\n",
            "Iteration 6, loss = 0.57393187\n",
            "Iteration 7, loss = 0.57109535\n",
            "Iteration 8, loss = 0.56733897\n",
            "Iteration 9, loss = 0.56624215\n",
            "Iteration 10, loss = 0.56613997\n",
            "Iteration 11, loss = 0.59696452\n",
            "Iteration 12, loss = 0.58845405\n",
            "Iteration 13, loss = 0.58821749\n",
            "Iteration 14, loss = 0.58035039\n",
            "Iteration 15, loss = 0.56951481\n",
            "Iteration 16, loss = 0.56698540\n",
            "Iteration 17, loss = 0.56500362\n",
            "Iteration 18, loss = 0.56464360\n",
            "Iteration 19, loss = 0.56368855\n",
            "Iteration 20, loss = 0.56322481\n",
            "Iteration 21, loss = 0.56235791\n",
            "Iteration 22, loss = 0.56407752\n",
            "Iteration 23, loss = 0.56632998\n",
            "Iteration 24, loss = 0.56489107\n",
            "Iteration 25, loss = 0.56385076\n",
            "Iteration 26, loss = 0.61228277\n",
            "Iteration 27, loss = 0.58924696\n",
            "Iteration 28, loss = 0.57779388\n",
            "Iteration 29, loss = 0.57255109\n",
            "Iteration 30, loss = 0.56865759\n",
            "Iteration 31, loss = 0.56782869\n",
            "Iteration 32, loss = 0.56660583\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 33, loss = 0.52806890\n",
            "Iteration 34, loss = 0.51121645\n",
            "Iteration 35, loss = 0.50742548\n",
            "Iteration 36, loss = 0.51847486\n",
            "Iteration 37, loss = 0.52198246\n",
            "Iteration 38, loss = 0.52160940\n",
            "Iteration 39, loss = 0.52036845\n",
            "Iteration 40, loss = 0.51829609\n",
            "Iteration 41, loss = 0.51840933\n",
            "Iteration 42, loss = 0.51541821\n",
            "Iteration 43, loss = 0.52093795\n",
            "Iteration 44, loss = 0.51671134\n",
            "Iteration 45, loss = 0.51737225\n",
            "Iteration 46, loss = 0.51426129\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.43735736\n",
            "Iteration 48, loss = 0.42613983\n",
            "Iteration 49, loss = 0.41575009\n",
            "Iteration 50, loss = 0.40643953\n",
            "Iteration 51, loss = 0.39744856\n",
            "Iteration 52, loss = 0.38941023\n",
            "Iteration 53, loss = 0.38194707\n",
            "Iteration 54, loss = 0.37506104\n",
            "Iteration 55, loss = 0.36889867\n",
            "Iteration 56, loss = 0.36354465\n",
            "Iteration 57, loss = 0.35875485\n",
            "Iteration 58, loss = 0.35396672\n",
            "Iteration 59, loss = 0.34985506\n",
            "Iteration 60, loss = 0.34645203\n",
            "Iteration 61, loss = 0.34336142\n",
            "Iteration 62, loss = 0.34243971\n",
            "Iteration 63, loss = 0.34003534\n",
            "Iteration 64, loss = 0.33614227\n",
            "Iteration 65, loss = 0.33608970\n",
            "Iteration 66, loss = 0.33422617\n",
            "Iteration 67, loss = 0.33332750\n",
            "Iteration 68, loss = 0.33380835\n",
            "Iteration 69, loss = 0.33410732\n",
            "Iteration 70, loss = 0.33148054\n",
            "Iteration 71, loss = 0.32957098\n",
            "Iteration 72, loss = 0.33167260\n",
            "Iteration 73, loss = 0.32964498\n",
            "Iteration 74, loss = 0.32798196\n",
            "Iteration 75, loss = 0.32904781\n",
            "Iteration 76, loss = 0.33194042\n",
            "Iteration 77, loss = 0.33036123\n",
            "Iteration 78, loss = 0.33130593\n",
            "Iteration 79, loss = 0.33239931\n",
            "Iteration 80, loss = 0.32740329\n",
            "Iteration 81, loss = 0.33438490\n",
            "Iteration 82, loss = 0.33128852\n",
            "Iteration 83, loss = 0.32645516\n",
            "Iteration 84, loss = 0.33512480\n",
            "Iteration 85, loss = 0.33149371\n",
            "Iteration 86, loss = 0.33122480\n",
            "Iteration 87, loss = 0.32991731\n",
            "Iteration 88, loss = 0.33255812\n",
            "Iteration 89, loss = 0.33203715\n",
            "Iteration 90, loss = 0.32693542\n",
            "Iteration 91, loss = 0.33345546\n",
            "Iteration 92, loss = 0.32971664\n",
            "Iteration 93, loss = 0.32959737\n",
            "Iteration 94, loss = 0.32792930\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.28992769\n",
            "Iteration 96, loss = 0.28949335\n",
            "Iteration 97, loss = 0.28926618\n",
            "Iteration 98, loss = 0.28900703\n",
            "Iteration 99, loss = 0.28878833\n",
            "Iteration 100, loss = 0.28860535\n",
            "Iteration 101, loss = 0.28845714\n",
            "Iteration 102, loss = 0.28836609\n",
            "Iteration 103, loss = 0.28819665\n",
            "Iteration 104, loss = 0.28800461\n",
            "Iteration 105, loss = 0.28784562\n",
            "Iteration 106, loss = 0.28773822\n",
            "Iteration 107, loss = 0.28757873\n",
            "Iteration 108, loss = 0.28751021\n",
            "Iteration 109, loss = 0.28724406\n",
            "Iteration 110, loss = 0.28707640\n",
            "Iteration 111, loss = 0.28697344\n",
            "Iteration 112, loss = 0.28681846\n",
            "Iteration 113, loss = 0.28666345\n",
            "Iteration 114, loss = 0.28656244\n",
            "Iteration 115, loss = 0.28635039\n",
            "Iteration 116, loss = 0.28624326\n",
            "Iteration 117, loss = 0.28614416\n",
            "Iteration 118, loss = 0.28599909\n",
            "Iteration 119, loss = 0.28581260\n",
            "Iteration 120, loss = 0.28566548\n",
            "Iteration 121, loss = 0.28553832\n",
            "Iteration 122, loss = 0.28543964\n",
            "Iteration 123, loss = 0.28523447\n",
            "Iteration 124, loss = 0.28512042\n",
            "Iteration 125, loss = 0.28504193\n",
            "Iteration 126, loss = 0.28483496\n",
            "Iteration 127, loss = 0.28468527\n",
            "Iteration 128, loss = 0.28461507\n",
            "Iteration 129, loss = 0.28445408\n",
            "Iteration 130, loss = 0.28431677\n",
            "Iteration 131, loss = 0.28416806\n",
            "Iteration 132, loss = 0.28410115\n",
            "Iteration 133, loss = 0.28401484\n",
            "Iteration 134, loss = 0.28381754\n",
            "Iteration 135, loss = 0.28375289\n",
            "Iteration 136, loss = 0.28354514\n",
            "Iteration 137, loss = 0.28348551\n",
            "Iteration 138, loss = 0.28337620\n",
            "Iteration 139, loss = 0.28323944\n",
            "Iteration 140, loss = 0.28316886\n",
            "Iteration 141, loss = 0.28298562\n",
            "Iteration 142, loss = 0.28290406\n",
            "Iteration 143, loss = 0.28280221\n",
            "Iteration 144, loss = 0.28265810\n",
            "Iteration 145, loss = 0.28257426\n",
            "Iteration 146, loss = 0.28241670\n",
            "Iteration 147, loss = 0.28230567\n",
            "Iteration 148, loss = 0.28224717\n",
            "Iteration 149, loss = 0.28210074\n",
            "Iteration 150, loss = 0.28203578\n",
            "Iteration 151, loss = 0.28187755\n",
            "Iteration 152, loss = 0.28173780\n",
            "Iteration 153, loss = 0.28167037\n",
            "Iteration 154, loss = 0.28149866\n",
            "Iteration 155, loss = 0.28141265\n",
            "Iteration 156, loss = 0.28139951\n",
            "Iteration 157, loss = 0.28124284\n",
            "Iteration 158, loss = 0.28110278\n",
            "Iteration 159, loss = 0.28102060\n",
            "Iteration 160, loss = 0.28093002\n",
            "Iteration 161, loss = 0.28081333\n",
            "Iteration 162, loss = 0.28074815\n",
            "Iteration 163, loss = 0.28066178\n",
            "Iteration 164, loss = 0.28050224\n",
            "Iteration 165, loss = 0.28041377\n",
            "Iteration 166, loss = 0.28029001\n",
            "Iteration 167, loss = 0.28022975\n",
            "Iteration 168, loss = 0.28013862\n",
            "Iteration 169, loss = 0.28005005\n",
            "Iteration 170, loss = 0.27996419\n",
            "Iteration 171, loss = 0.27976962\n",
            "Iteration 172, loss = 0.27971227\n",
            "Iteration 173, loss = 0.27965586\n",
            "Iteration 174, loss = 0.27956594\n",
            "Iteration 175, loss = 0.27945111\n",
            "Iteration 176, loss = 0.27933719\n",
            "Iteration 177, loss = 0.27929271\n",
            "Iteration 178, loss = 0.27918307\n",
            "Iteration 179, loss = 0.27908031\n",
            "Iteration 180, loss = 0.27895155\n",
            "Iteration 181, loss = 0.27885933\n",
            "Iteration 182, loss = 0.27877702\n",
            "Iteration 183, loss = 0.27868257\n",
            "Iteration 184, loss = 0.27854836\n",
            "Iteration 185, loss = 0.27855558\n",
            "Iteration 186, loss = 0.27843035\n",
            "Iteration 187, loss = 0.27834333\n",
            "Iteration 188, loss = 0.27825267\n",
            "Iteration 189, loss = 0.27817720\n",
            "Iteration 190, loss = 0.27810686\n",
            "Iteration 191, loss = 0.27800270\n",
            "Iteration 192, loss = 0.27790852\n",
            "Iteration 193, loss = 0.27789684\n",
            "Iteration 194, loss = 0.27774102\n",
            "Iteration 195, loss = 0.27765449\n",
            "Iteration 196, loss = 0.27761388\n",
            "Iteration 197, loss = 0.27748182\n",
            "Iteration 198, loss = 0.27745860\n",
            "Iteration 199, loss = 0.27737392\n",
            "Iteration 200, loss = 0.27728062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63424396\n",
            "Iteration 3, loss = 0.60568558\n",
            "Iteration 4, loss = 0.59137575\n",
            "Iteration 5, loss = 0.58060790\n",
            "Iteration 6, loss = 0.57471509\n",
            "Iteration 7, loss = 0.56975971\n",
            "Iteration 8, loss = 0.56856321\n",
            "Iteration 9, loss = 0.56634441\n",
            "Iteration 10, loss = 0.56610966\n",
            "Iteration 11, loss = 0.56538981\n",
            "Iteration 12, loss = 0.56519804\n",
            "Iteration 13, loss = 0.56531944\n",
            "Iteration 14, loss = 0.56556662\n",
            "Iteration 15, loss = 0.56585339\n",
            "Iteration 16, loss = 0.56509942\n",
            "Iteration 17, loss = 0.56590282\n",
            "Iteration 18, loss = 0.56458685\n",
            "Iteration 19, loss = 0.56576520\n",
            "Iteration 20, loss = 0.56550473\n",
            "Iteration 21, loss = 0.56684710\n",
            "Iteration 22, loss = 0.56627869\n",
            "Iteration 23, loss = 0.56656322\n",
            "Iteration 24, loss = 0.56507596\n",
            "Iteration 25, loss = 0.56649846\n",
            "Iteration 26, loss = 0.56643835\n",
            "Iteration 27, loss = 0.56806720\n",
            "Iteration 28, loss = 0.56664666\n",
            "Iteration 29, loss = 0.56739037\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.51345360\n",
            "Iteration 31, loss = 0.49149139\n",
            "Iteration 32, loss = 0.50767518\n",
            "Iteration 33, loss = 0.51359197\n",
            "Iteration 34, loss = 0.51602942\n",
            "Iteration 35, loss = 0.51413276\n",
            "Iteration 36, loss = 0.51085167\n",
            "Iteration 37, loss = 0.51406796\n",
            "Iteration 38, loss = 0.51274586\n",
            "Iteration 39, loss = 0.51032230\n",
            "Iteration 40, loss = 0.51025192\n",
            "Iteration 41, loss = 0.50942063\n",
            "Iteration 42, loss = 0.51129972\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 43, loss = 0.42503673\n",
            "Iteration 44, loss = 0.41272103\n",
            "Iteration 45, loss = 0.40189348\n",
            "Iteration 46, loss = 0.39208790\n",
            "Iteration 47, loss = 0.38286183\n",
            "Iteration 48, loss = 0.37483535\n",
            "Iteration 49, loss = 0.36724626\n",
            "Iteration 50, loss = 0.36068443\n",
            "Iteration 51, loss = 0.35472313\n",
            "Iteration 52, loss = 0.34946790\n",
            "Iteration 53, loss = 0.34420975\n",
            "Iteration 54, loss = 0.33979771\n",
            "Iteration 55, loss = 0.33622714\n",
            "Iteration 56, loss = 0.33279420\n",
            "Iteration 57, loss = 0.33011687\n",
            "Iteration 58, loss = 0.32725086\n",
            "Iteration 59, loss = 0.32566851\n",
            "Iteration 60, loss = 0.32186809\n",
            "Iteration 61, loss = 0.32054334\n",
            "Iteration 62, loss = 0.31901557\n",
            "Iteration 63, loss = 0.31771091\n",
            "Iteration 64, loss = 0.31641315\n",
            "Iteration 65, loss = 0.31544164\n",
            "Iteration 66, loss = 0.31470323\n",
            "Iteration 67, loss = 0.31223453\n",
            "Iteration 68, loss = 0.31262624\n",
            "Iteration 69, loss = 0.31128667\n",
            "Iteration 70, loss = 0.31162863\n",
            "Iteration 71, loss = 0.31285046\n",
            "Iteration 72, loss = 0.31317703\n",
            "Iteration 73, loss = 0.31058514\n",
            "Iteration 74, loss = 0.31003522\n",
            "Iteration 75, loss = 0.31056794\n",
            "Iteration 76, loss = 0.31075342\n",
            "Iteration 77, loss = 0.31269160\n",
            "Iteration 78, loss = 0.31259589\n",
            "Iteration 79, loss = 0.31427745\n",
            "Iteration 80, loss = 0.31022813\n",
            "Iteration 81, loss = 0.31562341\n",
            "Iteration 82, loss = 0.31149261\n",
            "Iteration 83, loss = 0.30944456\n",
            "Iteration 84, loss = 0.31355874\n",
            "Iteration 85, loss = 0.31181441\n",
            "Iteration 86, loss = 0.31604800\n",
            "Iteration 87, loss = 0.31350244\n",
            "Iteration 88, loss = 0.31562807\n",
            "Iteration 89, loss = 0.31105311\n",
            "Iteration 90, loss = 0.31073965\n",
            "Iteration 91, loss = 0.31157465\n",
            "Iteration 92, loss = 0.31541395\n",
            "Iteration 93, loss = 0.31055654\n",
            "Iteration 94, loss = 0.31020698\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.28144218\n",
            "Iteration 96, loss = 0.28119663\n",
            "Iteration 97, loss = 0.28094847\n",
            "Iteration 98, loss = 0.28084473\n",
            "Iteration 99, loss = 0.28074500\n",
            "Iteration 100, loss = 0.28060064\n",
            "Iteration 101, loss = 0.28045791\n",
            "Iteration 102, loss = 0.28040357\n",
            "Iteration 103, loss = 0.28023433\n",
            "Iteration 104, loss = 0.28018187\n",
            "Iteration 105, loss = 0.28008799\n",
            "Iteration 106, loss = 0.27989562\n",
            "Iteration 107, loss = 0.27983498\n",
            "Iteration 108, loss = 0.27973208\n",
            "Iteration 109, loss = 0.27969573\n",
            "Iteration 110, loss = 0.27952546\n",
            "Iteration 111, loss = 0.27946196\n",
            "Iteration 112, loss = 0.27934229\n",
            "Iteration 113, loss = 0.27920300\n",
            "Iteration 114, loss = 0.27905318\n",
            "Iteration 115, loss = 0.27909798\n",
            "Iteration 116, loss = 0.27889573\n",
            "Iteration 117, loss = 0.27879701\n",
            "Iteration 118, loss = 0.27868556\n",
            "Iteration 119, loss = 0.27862526\n",
            "Iteration 120, loss = 0.27852725\n",
            "Iteration 121, loss = 0.27839787\n",
            "Iteration 122, loss = 0.27831084\n",
            "Iteration 123, loss = 0.27821455\n",
            "Iteration 124, loss = 0.27813222\n",
            "Iteration 125, loss = 0.27800145\n",
            "Iteration 126, loss = 0.27791705\n",
            "Iteration 127, loss = 0.27782309\n",
            "Iteration 128, loss = 0.27776248\n",
            "Iteration 129, loss = 0.27767140\n",
            "Iteration 130, loss = 0.27760029\n",
            "Iteration 131, loss = 0.27760223\n",
            "Iteration 132, loss = 0.27739212\n",
            "Iteration 133, loss = 0.27731037\n",
            "Iteration 134, loss = 0.27720746\n",
            "Iteration 135, loss = 0.27716646\n",
            "Iteration 136, loss = 0.27699492\n",
            "Iteration 137, loss = 0.27692968\n",
            "Iteration 138, loss = 0.27690557\n",
            "Iteration 139, loss = 0.27679108\n",
            "Iteration 140, loss = 0.27667182\n",
            "Iteration 141, loss = 0.27665147\n",
            "Iteration 142, loss = 0.27652444\n",
            "Iteration 143, loss = 0.27641317\n",
            "Iteration 144, loss = 0.27638310\n",
            "Iteration 145, loss = 0.27624363\n",
            "Iteration 146, loss = 0.27620051\n",
            "Iteration 147, loss = 0.27614186\n",
            "Iteration 148, loss = 0.27600780\n",
            "Iteration 149, loss = 0.27598101\n",
            "Iteration 150, loss = 0.27585849\n",
            "Iteration 151, loss = 0.27577729\n",
            "Iteration 152, loss = 0.27570273\n",
            "Iteration 153, loss = 0.27568671\n",
            "Iteration 154, loss = 0.27556519\n",
            "Iteration 155, loss = 0.27550902\n",
            "Iteration 156, loss = 0.27540897\n",
            "Iteration 157, loss = 0.27530285\n",
            "Iteration 158, loss = 0.27522820\n",
            "Iteration 159, loss = 0.27515079\n",
            "Iteration 160, loss = 0.27509966\n",
            "Iteration 161, loss = 0.27502310\n",
            "Iteration 162, loss = 0.27495531\n",
            "Iteration 163, loss = 0.27487055\n",
            "Iteration 164, loss = 0.27480996\n",
            "Iteration 165, loss = 0.27466726\n",
            "Iteration 166, loss = 0.27462213\n",
            "Iteration 167, loss = 0.27458753\n",
            "Iteration 168, loss = 0.27454793\n",
            "Iteration 169, loss = 0.27441387\n",
            "Iteration 170, loss = 0.27441480\n",
            "Iteration 171, loss = 0.27422536\n",
            "Iteration 172, loss = 0.27416654\n",
            "Iteration 173, loss = 0.27419583\n",
            "Iteration 174, loss = 0.27408586\n",
            "Iteration 175, loss = 0.27403781\n",
            "Iteration 176, loss = 0.27390016\n",
            "Iteration 177, loss = 0.27385757\n",
            "Iteration 178, loss = 0.27378660\n",
            "Iteration 179, loss = 0.27368122\n",
            "Iteration 180, loss = 0.27367869\n",
            "Iteration 181, loss = 0.27362292\n",
            "Iteration 182, loss = 0.27349153\n",
            "Iteration 183, loss = 0.27344583\n",
            "Iteration 184, loss = 0.27337177\n",
            "Iteration 185, loss = 0.27338799\n",
            "Iteration 186, loss = 0.27329353\n",
            "Iteration 187, loss = 0.27321058\n",
            "Iteration 188, loss = 0.27313065\n",
            "Iteration 189, loss = 0.27307851\n",
            "Iteration 190, loss = 0.27303276\n",
            "Iteration 191, loss = 0.27294128\n",
            "Iteration 192, loss = 0.27287959\n",
            "Iteration 193, loss = 0.27286599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 194, loss = 0.27244558\n",
            "Iteration 195, loss = 0.27243863\n",
            "Iteration 196, loss = 0.27242107\n",
            "Iteration 197, loss = 0.27241289\n",
            "Iteration 198, loss = 0.27242354\n",
            "Iteration 199, loss = 0.27240027\n",
            "Iteration 200, loss = 0.27237902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63519134\n",
            "Iteration 3, loss = 0.60646327\n",
            "Iteration 4, loss = 0.58715630\n",
            "Iteration 5, loss = 0.57807730\n",
            "Iteration 6, loss = 0.58283131\n",
            "Iteration 7, loss = 0.56999984\n",
            "Iteration 8, loss = 0.56359905\n",
            "Iteration 9, loss = 0.56088353\n",
            "Iteration 10, loss = 0.55903212\n",
            "Iteration 11, loss = 0.55995078\n",
            "Iteration 12, loss = 0.56153557\n",
            "Iteration 13, loss = 0.56060213\n",
            "Iteration 14, loss = 0.56125266\n",
            "Iteration 15, loss = 0.56077960\n",
            "Iteration 16, loss = 0.56008370\n",
            "Iteration 17, loss = 0.56066947\n",
            "Iteration 18, loss = 0.56056934\n",
            "Iteration 19, loss = 0.56173849\n",
            "Iteration 20, loss = 0.60989884\n",
            "Iteration 21, loss = 0.58718305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 22, loss = 0.55868924\n",
            "Iteration 23, loss = 0.55078026\n",
            "Iteration 24, loss = 0.54267081\n",
            "Iteration 25, loss = 0.53320887\n",
            "Iteration 26, loss = 0.52335831\n",
            "Iteration 27, loss = 0.52578303\n",
            "Iteration 28, loss = 0.52562747\n",
            "Iteration 29, loss = 0.52306189\n",
            "Iteration 30, loss = 0.51947636\n",
            "Iteration 31, loss = 0.51827269\n",
            "Iteration 32, loss = 0.51329475\n",
            "Iteration 33, loss = 0.51627833\n",
            "Iteration 34, loss = 0.51119523\n",
            "Iteration 35, loss = 0.51247500\n",
            "Iteration 36, loss = 0.50986820\n",
            "Iteration 37, loss = 0.50856087\n",
            "Iteration 38, loss = 0.51199370\n",
            "Iteration 39, loss = 0.50708721\n",
            "Iteration 40, loss = 0.50850290\n",
            "Iteration 41, loss = 0.50520163\n",
            "Iteration 42, loss = 0.50813882\n",
            "Iteration 43, loss = 0.51081903\n",
            "Iteration 44, loss = 0.50663117\n",
            "Iteration 45, loss = 0.50828668\n",
            "Iteration 46, loss = 0.50550312\n",
            "Iteration 47, loss = 0.50656930\n",
            "Iteration 48, loss = 0.50883191\n",
            "Iteration 49, loss = 0.50428574\n",
            "Iteration 50, loss = 0.50885683\n",
            "Iteration 51, loss = 0.50686313\n",
            "Iteration 52, loss = 0.50756433\n",
            "Iteration 53, loss = 0.50693590\n",
            "Iteration 54, loss = 0.50879537\n",
            "Iteration 55, loss = 0.50430242\n",
            "Iteration 56, loss = 0.51118508\n",
            "Iteration 57, loss = 0.51232118\n",
            "Iteration 58, loss = 0.50697996\n",
            "Iteration 59, loss = 0.50908001\n",
            "Iteration 60, loss = 0.50867059\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 61, loss = 0.39645431\n",
            "Iteration 62, loss = 0.38075472\n",
            "Iteration 63, loss = 0.36765913\n",
            "Iteration 64, loss = 0.35602446\n",
            "Iteration 65, loss = 0.34587853\n",
            "Iteration 66, loss = 0.33681023\n",
            "Iteration 67, loss = 0.32894151\n",
            "Iteration 68, loss = 0.32208711\n",
            "Iteration 69, loss = 0.31592558\n",
            "Iteration 70, loss = 0.31038449\n",
            "Iteration 71, loss = 0.30525701\n",
            "Iteration 72, loss = 0.30103260\n",
            "Iteration 73, loss = 0.29716181\n",
            "Iteration 74, loss = 0.29348702\n",
            "Iteration 75, loss = 0.29029589\n",
            "Iteration 76, loss = 0.28742419\n",
            "Iteration 77, loss = 0.28502255\n",
            "Iteration 78, loss = 0.28248781\n",
            "Iteration 79, loss = 0.28077280\n",
            "Iteration 80, loss = 0.27853524\n",
            "Iteration 81, loss = 0.27686634\n",
            "Iteration 82, loss = 0.27563285\n",
            "Iteration 83, loss = 0.27376593\n",
            "Iteration 84, loss = 0.27282461\n",
            "Iteration 85, loss = 0.27116865\n",
            "Iteration 86, loss = 0.27009872\n",
            "Iteration 87, loss = 0.26875693\n",
            "Iteration 88, loss = 0.26788207\n",
            "Iteration 89, loss = 0.26756341\n",
            "Iteration 90, loss = 0.26647575\n",
            "Iteration 91, loss = 0.26597946\n",
            "Iteration 92, loss = 0.26524688\n",
            "Iteration 93, loss = 0.26469290\n",
            "Iteration 94, loss = 0.26413974\n",
            "Iteration 95, loss = 0.26354209\n",
            "Iteration 96, loss = 0.26258540\n",
            "Iteration 97, loss = 0.26222284\n",
            "Iteration 98, loss = 0.26115284\n",
            "Iteration 99, loss = 0.26154806\n",
            "Iteration 100, loss = 0.26103862\n",
            "Iteration 101, loss = 0.26112479\n",
            "Iteration 102, loss = 0.26140081\n",
            "Iteration 103, loss = 0.26034451\n",
            "Iteration 104, loss = 0.25994062\n",
            "Iteration 105, loss = 0.25984921\n",
            "Iteration 106, loss = 0.26052616\n",
            "Iteration 107, loss = 0.25962632\n",
            "Iteration 108, loss = 0.25953704\n",
            "Iteration 109, loss = 0.25906396\n",
            "Iteration 110, loss = 0.25863277\n",
            "Iteration 111, loss = 0.25913614\n",
            "Iteration 112, loss = 0.25847637\n",
            "Iteration 113, loss = 0.25916536\n",
            "Iteration 114, loss = 0.25897186\n",
            "Iteration 115, loss = 0.25897189\n",
            "Iteration 116, loss = 0.25833772\n",
            "Iteration 117, loss = 0.25817511\n",
            "Iteration 118, loss = 0.25886430\n",
            "Iteration 119, loss = 0.25786507\n",
            "Iteration 120, loss = 0.25759190\n",
            "Iteration 121, loss = 0.25880822\n",
            "Iteration 122, loss = 0.25765816\n",
            "Iteration 123, loss = 0.26884092\n",
            "Iteration 124, loss = 0.25846992\n",
            "Iteration 125, loss = 0.26256522\n",
            "Iteration 126, loss = 0.26109175\n",
            "Iteration 127, loss = 0.26026466\n",
            "Iteration 128, loss = 0.26289429\n",
            "Iteration 129, loss = 0.26404745\n",
            "Iteration 130, loss = 0.26516573\n",
            "Iteration 131, loss = 0.25937909\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 132, loss = 0.25029458\n",
            "Iteration 133, loss = 0.25019221\n",
            "Iteration 134, loss = 0.25013042\n",
            "Iteration 135, loss = 0.25014239\n",
            "Iteration 136, loss = 0.25012962\n",
            "Iteration 137, loss = 0.25011095\n",
            "Iteration 138, loss = 0.25002133\n",
            "Iteration 139, loss = 0.25005011\n",
            "Iteration 140, loss = 0.24994774\n",
            "Iteration 141, loss = 0.25001850\n",
            "Iteration 142, loss = 0.25001521\n",
            "Iteration 143, loss = 0.24994614\n",
            "Iteration 144, loss = 0.24984309\n",
            "Iteration 145, loss = 0.24987696\n",
            "Iteration 146, loss = 0.24982205\n",
            "Iteration 147, loss = 0.24978223\n",
            "Iteration 148, loss = 0.24977068\n",
            "Iteration 149, loss = 0.24978881\n",
            "Iteration 150, loss = 0.24978949\n",
            "Iteration 151, loss = 0.24971347\n",
            "Iteration 152, loss = 0.24964953\n",
            "Iteration 153, loss = 0.24964159\n",
            "Iteration 154, loss = 0.24966125\n",
            "Iteration 155, loss = 0.24965576\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 156, loss = 0.24935657\n",
            "Iteration 157, loss = 0.24933505\n",
            "Iteration 158, loss = 0.24933992\n",
            "Iteration 159, loss = 0.24932215\n",
            "Iteration 160, loss = 0.24931640\n",
            "Iteration 161, loss = 0.24930083\n",
            "Iteration 162, loss = 0.24932795\n",
            "Iteration 163, loss = 0.24931020\n",
            "Iteration 164, loss = 0.24930959\n",
            "Iteration 165, loss = 0.24931301\n",
            "Iteration 166, loss = 0.24930519\n",
            "Iteration 167, loss = 0.24930917\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 168, loss = 0.24920467\n",
            "Iteration 169, loss = 0.24921356\n",
            "Iteration 170, loss = 0.24920310\n",
            "Iteration 171, loss = 0.24920333\n",
            "Iteration 172, loss = 0.24920632\n",
            "Iteration 173, loss = 0.24920550\n",
            "Iteration 174, loss = 0.24919960\n",
            "Iteration 175, loss = 0.24920426\n",
            "Iteration 176, loss = 0.24920279\n",
            "Iteration 177, loss = 0.24919717\n",
            "Iteration 178, loss = 0.24918781\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64503734\n",
            "Iteration 3, loss = 0.61029795\n",
            "Iteration 4, loss = 0.59273380\n",
            "Iteration 5, loss = 0.58159341\n",
            "Iteration 6, loss = 0.57438397\n",
            "Iteration 7, loss = 0.57126438\n",
            "Iteration 8, loss = 0.56885182\n",
            "Iteration 9, loss = 0.56561087\n",
            "Iteration 10, loss = 0.56577375\n",
            "Iteration 11, loss = 0.56606389\n",
            "Iteration 12, loss = 0.56495202\n",
            "Iteration 13, loss = 0.56630602\n",
            "Iteration 14, loss = 0.56542988\n",
            "Iteration 15, loss = 0.56657211\n",
            "Iteration 16, loss = 0.56515573\n",
            "Iteration 17, loss = 0.56645272\n",
            "Iteration 18, loss = 0.56529935\n",
            "Iteration 19, loss = 0.56608550\n",
            "Iteration 20, loss = 0.56578544\n",
            "Iteration 21, loss = 0.56682315\n",
            "Iteration 22, loss = 0.56610688\n",
            "Iteration 23, loss = 0.56620714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.51254801\n",
            "Iteration 25, loss = 0.49457986\n",
            "Iteration 26, loss = 0.51074949\n",
            "Iteration 27, loss = 0.51421921\n",
            "Iteration 28, loss = 0.51937939\n",
            "Iteration 29, loss = 0.51930174\n",
            "Iteration 30, loss = 0.51573459\n",
            "Iteration 31, loss = 0.51755378\n",
            "Iteration 32, loss = 0.51592979\n",
            "Iteration 33, loss = 0.51449980\n",
            "Iteration 34, loss = 0.51543350\n",
            "Iteration 35, loss = 0.51794242\n",
            "Iteration 36, loss = 0.51532267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.43319939\n",
            "Iteration 38, loss = 0.41949974\n",
            "Iteration 39, loss = 0.40831611\n",
            "Iteration 40, loss = 0.39797998\n",
            "Iteration 41, loss = 0.38878124\n",
            "Iteration 42, loss = 0.38027645\n",
            "Iteration 43, loss = 0.37229827\n",
            "Iteration 44, loss = 0.36597526\n",
            "Iteration 45, loss = 0.35956198\n",
            "Iteration 46, loss = 0.35422492\n",
            "Iteration 47, loss = 0.34876704\n",
            "Iteration 48, loss = 0.34483846\n",
            "Iteration 49, loss = 0.34059711\n",
            "Iteration 50, loss = 0.33759852\n",
            "Iteration 51, loss = 0.33515670\n",
            "Iteration 52, loss = 0.33179117\n",
            "Iteration 53, loss = 0.33064877\n",
            "Iteration 54, loss = 0.32824609\n",
            "Iteration 55, loss = 0.32691838\n",
            "Iteration 56, loss = 0.32560578\n",
            "Iteration 57, loss = 0.32377510\n",
            "Iteration 58, loss = 0.32333726\n",
            "Iteration 59, loss = 0.32188555\n",
            "Iteration 60, loss = 0.32266635\n",
            "Iteration 61, loss = 0.32090265\n",
            "Iteration 62, loss = 0.32437065\n",
            "Iteration 63, loss = 0.31758317\n",
            "Iteration 64, loss = 0.32208999\n",
            "Iteration 65, loss = 0.32256729\n",
            "Iteration 66, loss = 0.32277788\n",
            "Iteration 67, loss = 0.32028419\n",
            "Iteration 68, loss = 0.32092832\n",
            "Iteration 69, loss = 0.32065981\n",
            "Iteration 70, loss = 0.32025112\n",
            "Iteration 71, loss = 0.32424905\n",
            "Iteration 72, loss = 0.31996447\n",
            "Iteration 73, loss = 0.32542471\n",
            "Iteration 74, loss = 0.32446028\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 75, loss = 0.29316156\n",
            "Iteration 76, loss = 0.29287049\n",
            "Iteration 77, loss = 0.29253965\n",
            "Iteration 78, loss = 0.29232906\n",
            "Iteration 79, loss = 0.29216593\n",
            "Iteration 80, loss = 0.29190356\n",
            "Iteration 81, loss = 0.29175195\n",
            "Iteration 82, loss = 0.29158068\n",
            "Iteration 83, loss = 0.29132902\n",
            "Iteration 84, loss = 0.29124493\n",
            "Iteration 85, loss = 0.29101518\n",
            "Iteration 86, loss = 0.29088595\n",
            "Iteration 87, loss = 0.29069589\n",
            "Iteration 88, loss = 0.29054796\n",
            "Iteration 89, loss = 0.29032087\n",
            "Iteration 90, loss = 0.29019002\n",
            "Iteration 91, loss = 0.28998713\n",
            "Iteration 92, loss = 0.28980753\n",
            "Iteration 93, loss = 0.28965768\n",
            "Iteration 94, loss = 0.28946989\n",
            "Iteration 95, loss = 0.28933337\n",
            "Iteration 96, loss = 0.28923487\n",
            "Iteration 97, loss = 0.28905744\n",
            "Iteration 98, loss = 0.28895082\n",
            "Iteration 99, loss = 0.28880597\n",
            "Iteration 100, loss = 0.28857825\n",
            "Iteration 101, loss = 0.28840814\n",
            "Iteration 102, loss = 0.28823571\n",
            "Iteration 103, loss = 0.28814309\n",
            "Iteration 104, loss = 0.28798635\n",
            "Iteration 105, loss = 0.28791071\n",
            "Iteration 106, loss = 0.28769127\n",
            "Iteration 107, loss = 0.28757545\n",
            "Iteration 108, loss = 0.28742138\n",
            "Iteration 109, loss = 0.28728590\n",
            "Iteration 110, loss = 0.28714016\n",
            "Iteration 111, loss = 0.28697976\n",
            "Iteration 112, loss = 0.28687216\n",
            "Iteration 113, loss = 0.28669313\n",
            "Iteration 114, loss = 0.28659977\n",
            "Iteration 115, loss = 0.28644264\n",
            "Iteration 116, loss = 0.28637952\n",
            "Iteration 117, loss = 0.28615687\n",
            "Iteration 118, loss = 0.28603594\n",
            "Iteration 119, loss = 0.28590029\n",
            "Iteration 120, loss = 0.28579695\n",
            "Iteration 121, loss = 0.28562865\n",
            "Iteration 122, loss = 0.28555581\n",
            "Iteration 123, loss = 0.28546946\n",
            "Iteration 124, loss = 0.28530111\n",
            "Iteration 125, loss = 0.28517620\n",
            "Iteration 126, loss = 0.28505223\n",
            "Iteration 127, loss = 0.28492051\n",
            "Iteration 128, loss = 0.28481124\n",
            "Iteration 129, loss = 0.28465881\n",
            "Iteration 130, loss = 0.28452646\n",
            "Iteration 131, loss = 0.28431820\n",
            "Iteration 132, loss = 0.28432811\n",
            "Iteration 133, loss = 0.28423227\n",
            "Iteration 134, loss = 0.28405582\n",
            "Iteration 135, loss = 0.28395427\n",
            "Iteration 136, loss = 0.28378866\n",
            "Iteration 137, loss = 0.28371747\n",
            "Iteration 138, loss = 0.28361162\n",
            "Iteration 139, loss = 0.28345209\n",
            "Iteration 140, loss = 0.28335866\n",
            "Iteration 141, loss = 0.28327170\n",
            "Iteration 142, loss = 0.28315642\n",
            "Iteration 143, loss = 0.28300379\n",
            "Iteration 144, loss = 0.28293156\n",
            "Iteration 145, loss = 0.28284107\n",
            "Iteration 146, loss = 0.28270338\n",
            "Iteration 147, loss = 0.28261701\n",
            "Iteration 148, loss = 0.28252251\n",
            "Iteration 149, loss = 0.28239063\n",
            "Iteration 150, loss = 0.28231939\n",
            "Iteration 151, loss = 0.28213593\n",
            "Iteration 152, loss = 0.28211302\n",
            "Iteration 153, loss = 0.28199686\n",
            "Iteration 154, loss = 0.28185493\n",
            "Iteration 155, loss = 0.28176074\n",
            "Iteration 156, loss = 0.28168119\n",
            "Iteration 157, loss = 0.28159122\n",
            "Iteration 158, loss = 0.28150070\n",
            "Iteration 159, loss = 0.28141401\n",
            "Iteration 160, loss = 0.28127347\n",
            "Iteration 161, loss = 0.28120523\n",
            "Iteration 162, loss = 0.28105855\n",
            "Iteration 163, loss = 0.28098994\n",
            "Iteration 164, loss = 0.28085198\n",
            "Iteration 165, loss = 0.28081812\n",
            "Iteration 166, loss = 0.28072142\n",
            "Iteration 167, loss = 0.28059418\n",
            "Iteration 168, loss = 0.28051626\n",
            "Iteration 169, loss = 0.28042068\n",
            "Iteration 170, loss = 0.28034945\n",
            "Iteration 171, loss = 0.28017246\n",
            "Iteration 172, loss = 0.28015546\n",
            "Iteration 173, loss = 0.28008271\n",
            "Iteration 174, loss = 0.27996441\n",
            "Iteration 175, loss = 0.27986761\n",
            "Iteration 176, loss = 0.27978972\n",
            "Iteration 177, loss = 0.27968051\n",
            "Iteration 178, loss = 0.27964468\n",
            "Iteration 179, loss = 0.27952945\n",
            "Iteration 180, loss = 0.27939359\n",
            "Iteration 181, loss = 0.27935858\n",
            "Iteration 182, loss = 0.27923357\n",
            "Iteration 183, loss = 0.27919116\n",
            "Iteration 184, loss = 0.27906838\n",
            "Iteration 185, loss = 0.27900125\n",
            "Iteration 186, loss = 0.27893292\n",
            "Iteration 187, loss = 0.27883442\n",
            "Iteration 188, loss = 0.27871073\n",
            "Iteration 189, loss = 0.27867263\n",
            "Iteration 190, loss = 0.27863998\n",
            "Iteration 191, loss = 0.27846623\n",
            "Iteration 192, loss = 0.27851773\n",
            "Iteration 193, loss = 0.27833438\n",
            "Iteration 194, loss = 0.27826211\n",
            "Iteration 195, loss = 0.27824408\n",
            "Iteration 196, loss = 0.27809394\n",
            "Iteration 197, loss = 0.27809752\n",
            "Iteration 198, loss = 0.27799136\n",
            "Iteration 199, loss = 0.27784779\n",
            "Iteration 200, loss = 0.27782373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64551395\n",
            "Iteration 3, loss = 0.61146748\n",
            "Iteration 4, loss = 0.59282451\n",
            "Iteration 5, loss = 0.58023868\n",
            "Iteration 6, loss = 0.57489325\n",
            "Iteration 7, loss = 0.57114137\n",
            "Iteration 8, loss = 0.56735302\n",
            "Iteration 9, loss = 0.56610156\n",
            "Iteration 10, loss = 0.56629194\n",
            "Iteration 11, loss = 0.59380338\n",
            "Iteration 12, loss = 0.59126226\n",
            "Iteration 13, loss = 0.57486035\n",
            "Iteration 14, loss = 0.57020415\n",
            "Iteration 15, loss = 0.56569214\n",
            "Iteration 16, loss = 0.56317009\n",
            "Iteration 17, loss = 0.56518557\n",
            "Iteration 18, loss = 0.56419652\n",
            "Iteration 19, loss = 0.56355783\n",
            "Iteration 20, loss = 0.56385649\n",
            "Iteration 21, loss = 0.56458318\n",
            "Iteration 22, loss = 0.56411610\n",
            "Iteration 23, loss = 0.56465027\n",
            "Iteration 24, loss = 0.56418525\n",
            "Iteration 25, loss = 0.56518718\n",
            "Iteration 26, loss = 0.56567235\n",
            "Iteration 27, loss = 0.56497970\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 28, loss = 0.51561553\n",
            "Iteration 29, loss = 0.49417398\n",
            "Iteration 30, loss = 0.50774561\n",
            "Iteration 31, loss = 0.51871020\n",
            "Iteration 32, loss = 0.51408527\n",
            "Iteration 33, loss = 0.51683987\n",
            "Iteration 34, loss = 0.51990875\n",
            "Iteration 35, loss = 0.51359976\n",
            "Iteration 36, loss = 0.51887614\n",
            "Iteration 37, loss = 0.51476460\n",
            "Iteration 38, loss = 0.51400447\n",
            "Iteration 39, loss = 0.51562872\n",
            "Iteration 40, loss = 0.51358753\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 41, loss = 0.43125024\n",
            "Iteration 42, loss = 0.41946574\n",
            "Iteration 43, loss = 0.40864422\n",
            "Iteration 44, loss = 0.39864688\n",
            "Iteration 45, loss = 0.38936343\n",
            "Iteration 46, loss = 0.38097660\n",
            "Iteration 47, loss = 0.37379470\n",
            "Iteration 48, loss = 0.36654567\n",
            "Iteration 49, loss = 0.36066327\n",
            "Iteration 50, loss = 0.35548469\n",
            "Iteration 51, loss = 0.35033696\n",
            "Iteration 52, loss = 0.34614314\n",
            "Iteration 53, loss = 0.34266378\n",
            "Iteration 54, loss = 0.33973789\n",
            "Iteration 55, loss = 0.33621458\n",
            "Iteration 56, loss = 0.33316070\n",
            "Iteration 57, loss = 0.33160684\n",
            "Iteration 58, loss = 0.33049868\n",
            "Iteration 59, loss = 0.32850592\n",
            "Iteration 60, loss = 0.32831134\n",
            "Iteration 61, loss = 0.32600615\n",
            "Iteration 62, loss = 0.32607879\n",
            "Iteration 63, loss = 0.32513518\n",
            "Iteration 64, loss = 0.32406093\n",
            "Iteration 65, loss = 0.32618411\n",
            "Iteration 66, loss = 0.32208787\n",
            "Iteration 67, loss = 0.32662264\n",
            "Iteration 68, loss = 0.32094795\n",
            "Iteration 69, loss = 0.32263319\n",
            "Iteration 70, loss = 0.32265820\n",
            "Iteration 71, loss = 0.32552557\n",
            "Iteration 72, loss = 0.32103337\n",
            "Iteration 73, loss = 0.32061688\n",
            "Iteration 74, loss = 0.32561685\n",
            "Iteration 75, loss = 0.32283973\n",
            "Iteration 76, loss = 0.32334956\n",
            "Iteration 77, loss = 0.32329833\n",
            "Iteration 78, loss = 0.32307590\n",
            "Iteration 79, loss = 0.32424392\n",
            "Iteration 80, loss = 0.32608769\n",
            "Iteration 81, loss = 0.32468411\n",
            "Iteration 82, loss = 0.32260302\n",
            "Iteration 83, loss = 0.31995502\n",
            "Iteration 84, loss = 0.32375464\n",
            "Iteration 85, loss = 0.32823519\n",
            "Iteration 86, loss = 0.31768292\n",
            "Iteration 87, loss = 0.32098777\n",
            "Iteration 88, loss = 0.31882745\n",
            "Iteration 89, loss = 0.32056781\n",
            "Iteration 90, loss = 0.32078186\n",
            "Iteration 91, loss = 0.32571913\n",
            "Iteration 92, loss = 0.32279099\n",
            "Iteration 93, loss = 0.32252330\n",
            "Iteration 94, loss = 0.32257915\n",
            "Iteration 95, loss = 0.32218015\n",
            "Iteration 96, loss = 0.32035153\n",
            "Iteration 97, loss = 0.32016706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.28330558\n",
            "Iteration 99, loss = 0.28304762\n",
            "Iteration 100, loss = 0.28285217\n",
            "Iteration 101, loss = 0.28278376\n",
            "Iteration 102, loss = 0.28260399\n",
            "Iteration 103, loss = 0.28244825\n",
            "Iteration 104, loss = 0.28236464\n",
            "Iteration 105, loss = 0.28221914\n",
            "Iteration 106, loss = 0.28207584\n",
            "Iteration 107, loss = 0.28202470\n",
            "Iteration 108, loss = 0.28195654\n",
            "Iteration 109, loss = 0.28173239\n",
            "Iteration 110, loss = 0.28166649\n",
            "Iteration 111, loss = 0.28161675\n",
            "Iteration 112, loss = 0.28141733\n",
            "Iteration 113, loss = 0.28129752\n",
            "Iteration 114, loss = 0.28119805\n",
            "Iteration 115, loss = 0.28115432\n",
            "Iteration 116, loss = 0.28099728\n",
            "Iteration 117, loss = 0.28090685\n",
            "Iteration 118, loss = 0.28082759\n",
            "Iteration 119, loss = 0.28079544\n",
            "Iteration 120, loss = 0.28063866\n",
            "Iteration 121, loss = 0.28051468\n",
            "Iteration 122, loss = 0.28045431\n",
            "Iteration 123, loss = 0.28034198\n",
            "Iteration 124, loss = 0.28029027\n",
            "Iteration 125, loss = 0.28014072\n",
            "Iteration 126, loss = 0.28003308\n",
            "Iteration 127, loss = 0.27990960\n",
            "Iteration 128, loss = 0.27980411\n",
            "Iteration 129, loss = 0.27973880\n",
            "Iteration 130, loss = 0.27962624\n",
            "Iteration 131, loss = 0.27954522\n",
            "Iteration 132, loss = 0.27945791\n",
            "Iteration 133, loss = 0.27937443\n",
            "Iteration 134, loss = 0.27926717\n",
            "Iteration 135, loss = 0.27918562\n",
            "Iteration 136, loss = 0.27907592\n",
            "Iteration 137, loss = 0.27896506\n",
            "Iteration 138, loss = 0.27892478\n",
            "Iteration 139, loss = 0.27884557\n",
            "Iteration 140, loss = 0.27873674\n",
            "Iteration 141, loss = 0.27866951\n",
            "Iteration 142, loss = 0.27856341\n",
            "Iteration 143, loss = 0.27850192\n",
            "Iteration 144, loss = 0.27840683\n",
            "Iteration 145, loss = 0.27834153\n",
            "Iteration 146, loss = 0.27818931\n",
            "Iteration 147, loss = 0.27814146\n",
            "Iteration 148, loss = 0.27800436\n",
            "Iteration 149, loss = 0.27794423\n",
            "Iteration 150, loss = 0.27787288\n",
            "Iteration 151, loss = 0.27777275\n",
            "Iteration 152, loss = 0.27771766\n",
            "Iteration 153, loss = 0.27762094\n",
            "Iteration 154, loss = 0.27760905\n",
            "Iteration 155, loss = 0.27741446\n",
            "Iteration 156, loss = 0.27736881\n",
            "Iteration 157, loss = 0.27735449\n",
            "Iteration 158, loss = 0.27727949\n",
            "Iteration 159, loss = 0.27712239\n",
            "Iteration 160, loss = 0.27706044\n",
            "Iteration 161, loss = 0.27697965\n",
            "Iteration 162, loss = 0.27695247\n",
            "Iteration 163, loss = 0.27687303\n",
            "Iteration 164, loss = 0.27675613\n",
            "Iteration 165, loss = 0.27670020\n",
            "Iteration 166, loss = 0.27660953\n",
            "Iteration 167, loss = 0.27648839\n",
            "Iteration 168, loss = 0.27643723\n",
            "Iteration 169, loss = 0.27633534\n",
            "Iteration 170, loss = 0.27639072\n",
            "Iteration 171, loss = 0.27622831\n",
            "Iteration 172, loss = 0.27617620\n",
            "Iteration 173, loss = 0.27608366\n",
            "Iteration 174, loss = 0.27598129\n",
            "Iteration 175, loss = 0.27594874\n",
            "Iteration 176, loss = 0.27591162\n",
            "Iteration 177, loss = 0.27579098\n",
            "Iteration 178, loss = 0.27572611\n",
            "Iteration 179, loss = 0.27572289\n",
            "Iteration 180, loss = 0.27567482\n",
            "Iteration 181, loss = 0.27552255\n",
            "Iteration 182, loss = 0.27543489\n",
            "Iteration 183, loss = 0.27543696\n",
            "Iteration 184, loss = 0.27531794\n",
            "Iteration 185, loss = 0.27528276\n",
            "Iteration 186, loss = 0.27519958\n",
            "Iteration 187, loss = 0.27510182\n",
            "Iteration 188, loss = 0.27505376\n",
            "Iteration 189, loss = 0.27498772\n",
            "Iteration 190, loss = 0.27490188\n",
            "Iteration 191, loss = 0.27487101\n",
            "Iteration 192, loss = 0.27477157\n",
            "Iteration 193, loss = 0.27474862\n",
            "Iteration 194, loss = 0.27463158\n",
            "Iteration 195, loss = 0.27462028\n",
            "Iteration 196, loss = 0.27449332\n",
            "Iteration 197, loss = 0.27447169\n",
            "Iteration 198, loss = 0.27445638\n",
            "Iteration 199, loss = 0.27433105\n",
            "Iteration 200, loss = 0.27425820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62712090\n",
            "Iteration 3, loss = 0.60073630\n",
            "Iteration 4, loss = 0.58285498\n",
            "Iteration 5, loss = 0.57829137\n",
            "Iteration 6, loss = 0.56909532\n",
            "Iteration 7, loss = 0.56546860\n",
            "Iteration 8, loss = 0.56196741\n",
            "Iteration 9, loss = 0.55954180\n",
            "Iteration 10, loss = 0.56053418\n",
            "Iteration 11, loss = 0.55846076\n",
            "Iteration 12, loss = 0.55792605\n",
            "Iteration 13, loss = 0.55881994\n",
            "Iteration 14, loss = 0.55774306\n",
            "Iteration 15, loss = 0.55833422\n",
            "Iteration 16, loss = 0.55776182\n",
            "Iteration 17, loss = 0.55790763\n",
            "Iteration 18, loss = 0.55863023\n",
            "Iteration 19, loss = 0.55898453\n",
            "Iteration 20, loss = 0.55865334\n",
            "Iteration 21, loss = 0.55777743\n",
            "Iteration 22, loss = 0.55746233\n",
            "Iteration 23, loss = 0.55986052\n",
            "Iteration 24, loss = 0.55910501\n",
            "Iteration 25, loss = 0.55704755\n",
            "Iteration 26, loss = 0.55832737\n",
            "Iteration 27, loss = 0.56157534\n",
            "Iteration 28, loss = 0.59749940\n",
            "Iteration 29, loss = 0.57181396\n",
            "Iteration 30, loss = 0.56244162\n",
            "Iteration 31, loss = 0.55979445\n",
            "Iteration 32, loss = 0.55550518\n",
            "Iteration 33, loss = 0.55649807\n",
            "Iteration 34, loss = 0.55345037\n",
            "Iteration 35, loss = 0.55447389\n",
            "Iteration 36, loss = 0.55468572\n",
            "Iteration 37, loss = 0.55437878\n",
            "Iteration 38, loss = 0.55347425\n",
            "Iteration 39, loss = 0.55460918\n",
            "Iteration 40, loss = 0.55528381\n",
            "Iteration 41, loss = 0.55460180\n",
            "Iteration 42, loss = 0.56260042\n",
            "Iteration 43, loss = 0.56070832\n",
            "Iteration 44, loss = 0.55621409\n",
            "Iteration 45, loss = 0.55465030\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 46, loss = 0.50419194\n",
            "Iteration 47, loss = 0.47879978\n",
            "Iteration 48, loss = 0.48973824\n",
            "Iteration 49, loss = 0.50067698\n",
            "Iteration 50, loss = 0.49902613\n",
            "Iteration 51, loss = 0.49786206\n",
            "Iteration 52, loss = 0.49660471\n",
            "Iteration 53, loss = 0.49449008\n",
            "Iteration 54, loss = 0.49944545\n",
            "Iteration 55, loss = 0.49475611\n",
            "Iteration 56, loss = 0.49905949\n",
            "Iteration 57, loss = 0.49398297\n",
            "Iteration 58, loss = 0.49485249\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 59, loss = 0.40188916\n",
            "Iteration 60, loss = 0.38950652\n",
            "Iteration 61, loss = 0.37835398\n",
            "Iteration 62, loss = 0.36785810\n",
            "Iteration 63, loss = 0.35839660\n",
            "Iteration 64, loss = 0.34954231\n",
            "Iteration 65, loss = 0.34153344\n",
            "Iteration 66, loss = 0.33394389\n",
            "Iteration 67, loss = 0.32754748\n",
            "Iteration 68, loss = 0.32139404\n",
            "Iteration 69, loss = 0.31588612\n",
            "Iteration 70, loss = 0.31093250\n",
            "Iteration 71, loss = 0.30611220\n",
            "Iteration 72, loss = 0.30195496\n",
            "Iteration 73, loss = 0.29789043\n",
            "Iteration 74, loss = 0.29510598\n",
            "Iteration 75, loss = 0.29143885\n",
            "Iteration 76, loss = 0.28819114\n",
            "Iteration 77, loss = 0.28549061\n",
            "Iteration 78, loss = 0.28263381\n",
            "Iteration 79, loss = 0.28111375\n",
            "Iteration 80, loss = 0.27842389\n",
            "Iteration 81, loss = 0.27672734\n",
            "Iteration 82, loss = 0.27566209\n",
            "Iteration 83, loss = 0.27411223\n",
            "Iteration 84, loss = 0.27188615\n",
            "Iteration 85, loss = 0.27089334\n",
            "Iteration 86, loss = 0.26898865\n",
            "Iteration 87, loss = 0.26885192\n",
            "Iteration 88, loss = 0.26700530\n",
            "Iteration 89, loss = 0.26662084\n",
            "Iteration 90, loss = 0.26511256\n",
            "Iteration 91, loss = 0.26459359\n",
            "Iteration 92, loss = 0.26440718\n",
            "Iteration 93, loss = 0.26510701\n",
            "Iteration 94, loss = 0.26313552\n",
            "Iteration 95, loss = 0.26302238\n",
            "Iteration 96, loss = 0.26266932\n",
            "Iteration 97, loss = 0.26172789\n",
            "Iteration 98, loss = 0.26052577\n",
            "Iteration 99, loss = 0.26174137\n",
            "Iteration 100, loss = 0.25942226\n",
            "Iteration 101, loss = 0.26167561\n",
            "Iteration 102, loss = 0.25999889\n",
            "Iteration 103, loss = 0.26072526\n",
            "Iteration 104, loss = 0.25991569\n",
            "Iteration 105, loss = 0.25993357\n",
            "Iteration 106, loss = 0.26228201\n",
            "Iteration 107, loss = 0.25962227\n",
            "Iteration 108, loss = 0.25863942\n",
            "Iteration 109, loss = 0.25950107\n",
            "Iteration 110, loss = 0.26384125\n",
            "Iteration 111, loss = 0.26021603\n",
            "Iteration 112, loss = 0.25765822\n",
            "Iteration 113, loss = 0.26156664\n",
            "Iteration 114, loss = 0.26230583\n",
            "Iteration 115, loss = 0.26203521\n",
            "Iteration 116, loss = 0.25741023\n",
            "Iteration 117, loss = 0.26485077\n",
            "Iteration 118, loss = 0.26909528\n",
            "Iteration 119, loss = 0.26284742\n",
            "Iteration 120, loss = 0.25706288\n",
            "Iteration 121, loss = 0.26315574\n",
            "Iteration 122, loss = 0.26044308\n",
            "Iteration 123, loss = 0.26475814\n",
            "Iteration 124, loss = 0.25822239\n",
            "Iteration 125, loss = 0.26364368\n",
            "Iteration 126, loss = 0.27506142\n",
            "Iteration 127, loss = 0.25993201\n",
            "Iteration 128, loss = 0.25914527\n",
            "Iteration 129, loss = 0.27197135\n",
            "Iteration 130, loss = 0.26312664\n",
            "Iteration 131, loss = 0.26336016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 132, loss = 0.24709445\n",
            "Iteration 133, loss = 0.24706581\n",
            "Iteration 134, loss = 0.24702684\n",
            "Iteration 135, loss = 0.24701254\n",
            "Iteration 136, loss = 0.24698164\n",
            "Iteration 137, loss = 0.24697589\n",
            "Iteration 138, loss = 0.24693469\n",
            "Iteration 139, loss = 0.24693298\n",
            "Iteration 140, loss = 0.24687181\n",
            "Iteration 141, loss = 0.24690638\n",
            "Iteration 142, loss = 0.24682844\n",
            "Iteration 143, loss = 0.24681688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 144, loss = 0.24656279\n",
            "Iteration 145, loss = 0.24653932\n",
            "Iteration 146, loss = 0.24651508\n",
            "Iteration 147, loss = 0.24652452\n",
            "Iteration 148, loss = 0.24656118\n",
            "Iteration 149, loss = 0.24656840\n",
            "Iteration 150, loss = 0.24651462\n",
            "Iteration 151, loss = 0.24653886\n",
            "Iteration 152, loss = 0.24652076\n",
            "Iteration 153, loss = 0.24651068\n",
            "Iteration 154, loss = 0.24649438\n",
            "Iteration 155, loss = 0.24651075\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 156, loss = 0.24641842\n",
            "Iteration 157, loss = 0.24641646\n",
            "Iteration 158, loss = 0.24641909\n",
            "Iteration 159, loss = 0.24641666\n",
            "Iteration 160, loss = 0.24642522\n",
            "Iteration 161, loss = 0.24641316\n",
            "Iteration 162, loss = 0.24641462\n",
            "Iteration 163, loss = 0.24642352\n",
            "Iteration 164, loss = 0.24641561\n",
            "Iteration 165, loss = 0.24640118\n",
            "Iteration 166, loss = 0.24640569\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62263238\n",
            "Iteration 3, loss = 0.59964837\n",
            "Iteration 4, loss = 0.58473391\n",
            "Iteration 5, loss = 0.57560203\n",
            "Iteration 6, loss = 0.57026813\n",
            "Iteration 7, loss = 0.57153212\n",
            "Iteration 8, loss = 0.56800648\n",
            "Iteration 9, loss = 0.56579140\n",
            "Iteration 10, loss = 0.56441845\n",
            "Iteration 11, loss = 0.56274439\n",
            "Iteration 12, loss = 0.56073672\n",
            "Iteration 13, loss = 0.56215163\n",
            "Iteration 14, loss = 0.56226490\n",
            "Iteration 15, loss = 0.56219666\n",
            "Iteration 16, loss = 0.56504052\n",
            "Iteration 17, loss = 0.56081617\n",
            "Iteration 18, loss = 0.56245958\n",
            "Iteration 19, loss = 0.56192723\n",
            "Iteration 20, loss = 0.56101202\n",
            "Iteration 21, loss = 0.56042756\n",
            "Iteration 22, loss = 0.56001551\n",
            "Iteration 23, loss = 0.55903092\n",
            "Iteration 24, loss = 0.56031989\n",
            "Iteration 25, loss = 0.55937889\n",
            "Iteration 26, loss = 0.55996592\n",
            "Iteration 27, loss = 0.55990256\n",
            "Iteration 28, loss = 0.55831434\n",
            "Iteration 29, loss = 0.55783951\n",
            "Iteration 30, loss = 0.55806889\n",
            "Iteration 31, loss = 0.55897606\n",
            "Iteration 32, loss = 0.55998939\n",
            "Iteration 33, loss = 0.55768402\n",
            "Iteration 34, loss = 0.55876994\n",
            "Iteration 35, loss = 0.55695595\n",
            "Iteration 36, loss = 0.55815573\n",
            "Iteration 37, loss = 0.55810818\n",
            "Iteration 38, loss = 0.55608411\n",
            "Iteration 39, loss = 0.55848390\n",
            "Iteration 40, loss = 0.55833338\n",
            "Iteration 41, loss = 0.55606770\n",
            "Iteration 42, loss = 0.55555044\n",
            "Iteration 43, loss = 0.55745511\n",
            "Iteration 44, loss = 0.55691339\n",
            "Iteration 45, loss = 0.55675112\n",
            "Iteration 46, loss = 0.55591485\n",
            "Iteration 47, loss = 0.55555655\n",
            "Iteration 48, loss = 0.55608904\n",
            "Iteration 49, loss = 0.55504629\n",
            "Iteration 50, loss = 0.55451186\n",
            "Iteration 51, loss = 0.55472400\n",
            "Iteration 52, loss = 0.55495434\n",
            "Iteration 53, loss = 0.55361079\n",
            "Iteration 54, loss = 0.55485121\n",
            "Iteration 55, loss = 0.55429962\n",
            "Iteration 56, loss = 0.55425247\n",
            "Iteration 57, loss = 0.55392025\n",
            "Iteration 58, loss = 0.58066296\n",
            "Iteration 59, loss = 0.56776620\n",
            "Iteration 60, loss = 0.56030040\n",
            "Iteration 61, loss = 0.55572626\n",
            "Iteration 62, loss = 0.55290327\n",
            "Iteration 63, loss = 0.55185528\n",
            "Iteration 64, loss = 0.55151795\n",
            "Iteration 65, loss = 0.54924017\n",
            "Iteration 66, loss = 0.55603679\n",
            "Iteration 67, loss = 0.67517353\n",
            "Iteration 68, loss = 0.57396330\n",
            "Iteration 69, loss = 0.56255943\n",
            "Iteration 70, loss = 0.55699633\n",
            "Iteration 71, loss = 0.55394336\n",
            "Iteration 72, loss = 0.55219873\n",
            "Iteration 73, loss = 0.55184158\n",
            "Iteration 74, loss = 0.55149342\n",
            "Iteration 75, loss = 0.55214216\n",
            "Iteration 76, loss = 0.55140693\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 77, loss = 0.49748568\n",
            "Iteration 78, loss = 0.47375672\n",
            "Iteration 79, loss = 0.49396214\n",
            "Iteration 80, loss = 0.49678417\n",
            "Iteration 81, loss = 0.49418094\n",
            "Iteration 82, loss = 0.50174374\n",
            "Iteration 83, loss = 0.49747600\n",
            "Iteration 84, loss = 0.49973652\n",
            "Iteration 85, loss = 0.49640177\n",
            "Iteration 86, loss = 0.49701296\n",
            "Iteration 87, loss = 0.49856320\n",
            "Iteration 88, loss = 0.49728566\n",
            "Iteration 89, loss = 0.49601226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 90, loss = 0.40095672\n",
            "Iteration 91, loss = 0.38754373\n",
            "Iteration 92, loss = 0.37546877\n",
            "Iteration 93, loss = 0.36429384\n",
            "Iteration 94, loss = 0.35410874\n",
            "Iteration 95, loss = 0.34476239\n",
            "Iteration 96, loss = 0.33636353\n",
            "Iteration 97, loss = 0.32910312\n",
            "Iteration 98, loss = 0.32231201\n",
            "Iteration 99, loss = 0.31629994\n",
            "Iteration 100, loss = 0.31056690\n",
            "Iteration 101, loss = 0.30540542\n",
            "Iteration 102, loss = 0.30076738\n",
            "Iteration 103, loss = 0.29663365\n",
            "Iteration 104, loss = 0.29317290\n",
            "Iteration 105, loss = 0.28945328\n",
            "Iteration 106, loss = 0.28594116\n",
            "Iteration 107, loss = 0.28369700\n",
            "Iteration 108, loss = 0.28106162\n",
            "Iteration 109, loss = 0.27861936\n",
            "Iteration 110, loss = 0.27626726\n",
            "Iteration 111, loss = 0.27432564\n",
            "Iteration 112, loss = 0.27310651\n",
            "Iteration 113, loss = 0.27093579\n",
            "Iteration 114, loss = 0.27012244\n",
            "Iteration 115, loss = 0.26877866\n",
            "Iteration 116, loss = 0.26767737\n",
            "Iteration 117, loss = 0.26646683\n",
            "Iteration 118, loss = 0.26564199\n",
            "Iteration 119, loss = 0.26417976\n",
            "Iteration 120, loss = 0.26386302\n",
            "Iteration 121, loss = 0.26349805\n",
            "Iteration 122, loss = 0.26217999\n",
            "Iteration 123, loss = 0.26084847\n",
            "Iteration 124, loss = 0.26161478\n",
            "Iteration 125, loss = 0.26129524\n",
            "Iteration 126, loss = 0.25952746\n",
            "Iteration 127, loss = 0.26058792\n",
            "Iteration 128, loss = 0.25903651\n",
            "Iteration 129, loss = 0.25927159\n",
            "Iteration 130, loss = 0.25825064\n",
            "Iteration 131, loss = 0.26014844\n",
            "Iteration 132, loss = 0.25818103\n",
            "Iteration 133, loss = 0.25737184\n",
            "Iteration 134, loss = 0.25776665\n",
            "Iteration 135, loss = 0.26089931\n",
            "Iteration 136, loss = 0.26201718\n",
            "Iteration 137, loss = 0.25801201\n",
            "Iteration 138, loss = 0.25880965\n",
            "Iteration 139, loss = 0.26144807\n",
            "Iteration 140, loss = 0.25992157\n",
            "Iteration 141, loss = 0.25934370\n",
            "Iteration 142, loss = 0.25957539\n",
            "Iteration 143, loss = 0.25871569\n",
            "Iteration 144, loss = 0.26077942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 145, loss = 0.24754458\n",
            "Iteration 146, loss = 0.24743991\n",
            "Iteration 147, loss = 0.24741270\n",
            "Iteration 148, loss = 0.24731541\n",
            "Iteration 149, loss = 0.24729651\n",
            "Iteration 150, loss = 0.24731452\n",
            "Iteration 151, loss = 0.24726029\n",
            "Iteration 152, loss = 0.24716899\n",
            "Iteration 153, loss = 0.24720627\n",
            "Iteration 154, loss = 0.24716287\n",
            "Iteration 155, loss = 0.24709140\n",
            "Iteration 156, loss = 0.24700990\n",
            "Iteration 157, loss = 0.24698703\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 158, loss = 0.24672174\n",
            "Iteration 159, loss = 0.24669886\n",
            "Iteration 160, loss = 0.24668298\n",
            "Iteration 161, loss = 0.24668650\n",
            "Iteration 162, loss = 0.24668826\n",
            "Iteration 163, loss = 0.24663807\n",
            "Iteration 164, loss = 0.24666517\n",
            "Iteration 165, loss = 0.24664338\n",
            "Iteration 166, loss = 0.24663054\n",
            "Iteration 167, loss = 0.24663785\n",
            "Iteration 168, loss = 0.24663654\n",
            "Iteration 169, loss = 0.24662942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 170, loss = 0.24653333\n",
            "Iteration 171, loss = 0.24652576\n",
            "Iteration 172, loss = 0.24653050\n",
            "Iteration 173, loss = 0.24652214\n",
            "Iteration 174, loss = 0.24652744\n",
            "Iteration 175, loss = 0.24651801\n",
            "Iteration 176, loss = 0.24651104\n",
            "Iteration 177, loss = 0.24651449\n",
            "Iteration 178, loss = 0.24651413\n",
            "Iteration 179, loss = 0.24651942\n",
            "Iteration 180, loss = 0.24651682\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.90026088\n",
            "Iteration 2, loss = 0.63264227\n",
            "Iteration 3, loss = 0.60582498\n",
            "Iteration 4, loss = 0.59133826\n",
            "Iteration 5, loss = 0.58143357\n",
            "Iteration 6, loss = 0.57512201\n",
            "Iteration 7, loss = 0.57109753\n",
            "Iteration 8, loss = 0.56875382\n",
            "Iteration 9, loss = 0.56609179\n",
            "Iteration 10, loss = 0.56665586\n",
            "Iteration 11, loss = 0.56458261\n",
            "Iteration 12, loss = 0.56648360\n",
            "Iteration 13, loss = 0.56398168\n",
            "Iteration 14, loss = 0.56394608\n",
            "Iteration 15, loss = 0.56727504\n",
            "Iteration 16, loss = 0.56425951\n",
            "Iteration 17, loss = 0.56467881\n",
            "Iteration 18, loss = 0.56510872\n",
            "Iteration 19, loss = 0.56523750\n",
            "Iteration 20, loss = 0.56619452\n",
            "Iteration 21, loss = 0.56702285\n",
            "Iteration 22, loss = 0.56608495\n",
            "Iteration 23, loss = 0.56686225\n",
            "Iteration 24, loss = 0.56670217\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.51529474\n",
            "Iteration 26, loss = 0.49364719\n",
            "Iteration 27, loss = 0.50819032\n",
            "Iteration 28, loss = 0.52094800\n",
            "Iteration 29, loss = 0.51699722\n",
            "Iteration 30, loss = 0.51557416\n",
            "Iteration 31, loss = 0.51992413\n",
            "Iteration 32, loss = 0.51478441\n",
            "Iteration 33, loss = 0.51914126\n",
            "Iteration 34, loss = 0.51560649\n",
            "Iteration 35, loss = 0.51563607\n",
            "Iteration 36, loss = 0.51352456\n",
            "Iteration 37, loss = 0.51683836\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.43250749\n",
            "Iteration 39, loss = 0.42034646\n",
            "Iteration 40, loss = 0.40943750\n",
            "Iteration 41, loss = 0.39943406\n",
            "Iteration 42, loss = 0.39016172\n",
            "Iteration 43, loss = 0.38152362\n",
            "Iteration 44, loss = 0.37424287\n",
            "Iteration 45, loss = 0.36713840\n",
            "Iteration 46, loss = 0.36088819\n",
            "Iteration 47, loss = 0.35544173\n",
            "Iteration 48, loss = 0.35115740\n",
            "Iteration 49, loss = 0.34626674\n",
            "Iteration 50, loss = 0.34255435\n",
            "Iteration 51, loss = 0.33943175\n",
            "Iteration 52, loss = 0.33593990\n",
            "Iteration 53, loss = 0.33438340\n",
            "Iteration 54, loss = 0.33123430\n",
            "Iteration 55, loss = 0.33093742\n",
            "Iteration 56, loss = 0.32926613\n",
            "Iteration 57, loss = 0.32596453\n",
            "Iteration 58, loss = 0.32528398\n",
            "Iteration 59, loss = 0.32494923\n",
            "Iteration 60, loss = 0.32312799\n",
            "Iteration 61, loss = 0.32369771\n",
            "Iteration 62, loss = 0.32496480\n",
            "Iteration 63, loss = 0.32215676\n",
            "Iteration 64, loss = 0.32129993\n",
            "Iteration 65, loss = 0.32044645\n",
            "Iteration 66, loss = 0.32100397\n",
            "Iteration 67, loss = 0.32356836\n",
            "Iteration 68, loss = 0.32146326\n",
            "Iteration 69, loss = 0.32211783\n",
            "Iteration 70, loss = 0.32087247\n",
            "Iteration 71, loss = 0.32568674\n",
            "Iteration 72, loss = 0.32499935\n",
            "Iteration 73, loss = 0.32147894\n",
            "Iteration 74, loss = 0.32159993\n",
            "Iteration 75, loss = 0.32254528\n",
            "Iteration 76, loss = 0.32334023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 77, loss = 0.29282708\n",
            "Iteration 78, loss = 0.29253254\n",
            "Iteration 79, loss = 0.29229748\n",
            "Iteration 80, loss = 0.29209889\n",
            "Iteration 81, loss = 0.29192978\n",
            "Iteration 82, loss = 0.29164947\n",
            "Iteration 83, loss = 0.29140741\n",
            "Iteration 84, loss = 0.29125307\n",
            "Iteration 85, loss = 0.29112403\n",
            "Iteration 86, loss = 0.29095960\n",
            "Iteration 87, loss = 0.29077108\n",
            "Iteration 88, loss = 0.29055486\n",
            "Iteration 89, loss = 0.29041910\n",
            "Iteration 90, loss = 0.29026690\n",
            "Iteration 91, loss = 0.29006809\n",
            "Iteration 92, loss = 0.28994834\n",
            "Iteration 93, loss = 0.28972691\n",
            "Iteration 94, loss = 0.28952225\n",
            "Iteration 95, loss = 0.28936798\n",
            "Iteration 96, loss = 0.28919752\n",
            "Iteration 97, loss = 0.28907894\n",
            "Iteration 98, loss = 0.28895202\n",
            "Iteration 99, loss = 0.28879640\n",
            "Iteration 100, loss = 0.28860653\n",
            "Iteration 101, loss = 0.28844557\n",
            "Iteration 102, loss = 0.28828709\n",
            "Iteration 103, loss = 0.28813886\n",
            "Iteration 104, loss = 0.28799250\n",
            "Iteration 105, loss = 0.28781233\n",
            "Iteration 106, loss = 0.28764796\n",
            "Iteration 107, loss = 0.28757370\n",
            "Iteration 108, loss = 0.28739931\n",
            "Iteration 109, loss = 0.28720684\n",
            "Iteration 110, loss = 0.28709431\n",
            "Iteration 111, loss = 0.28701559\n",
            "Iteration 112, loss = 0.28683665\n",
            "Iteration 113, loss = 0.28670011\n",
            "Iteration 114, loss = 0.28655104\n",
            "Iteration 115, loss = 0.28634323\n",
            "Iteration 116, loss = 0.28616280\n",
            "Iteration 117, loss = 0.28606832\n",
            "Iteration 118, loss = 0.28598355\n",
            "Iteration 119, loss = 0.28579310\n",
            "Iteration 120, loss = 0.28570723\n",
            "Iteration 121, loss = 0.28559968\n",
            "Iteration 122, loss = 0.28544964\n",
            "Iteration 123, loss = 0.28529797\n",
            "Iteration 124, loss = 0.28521882\n",
            "Iteration 125, loss = 0.28504976\n",
            "Iteration 126, loss = 0.28492375\n",
            "Iteration 127, loss = 0.28481823\n",
            "Iteration 128, loss = 0.28468589\n",
            "Iteration 129, loss = 0.28456687\n",
            "Iteration 130, loss = 0.28441105\n",
            "Iteration 131, loss = 0.28431760\n",
            "Iteration 132, loss = 0.28418148\n",
            "Iteration 133, loss = 0.28404768\n",
            "Iteration 134, loss = 0.28388400\n",
            "Iteration 135, loss = 0.28376646\n",
            "Iteration 136, loss = 0.28363943\n",
            "Iteration 137, loss = 0.28354706\n",
            "Iteration 138, loss = 0.28344850\n",
            "Iteration 139, loss = 0.28326686\n",
            "Iteration 140, loss = 0.28320655\n",
            "Iteration 141, loss = 0.28305077\n",
            "Iteration 142, loss = 0.28296479\n",
            "Iteration 143, loss = 0.28288168\n",
            "Iteration 144, loss = 0.28277963\n",
            "Iteration 145, loss = 0.28264476\n",
            "Iteration 146, loss = 0.28250121\n",
            "Iteration 147, loss = 0.28241937\n",
            "Iteration 148, loss = 0.28229518\n",
            "Iteration 149, loss = 0.28219546\n",
            "Iteration 150, loss = 0.28207679\n",
            "Iteration 151, loss = 0.28199510\n",
            "Iteration 152, loss = 0.28186578\n",
            "Iteration 153, loss = 0.28174662\n",
            "Iteration 154, loss = 0.28164805\n",
            "Iteration 155, loss = 0.28155536\n",
            "Iteration 156, loss = 0.28139795\n",
            "Iteration 157, loss = 0.28133263\n",
            "Iteration 158, loss = 0.28126435\n",
            "Iteration 159, loss = 0.28113374\n",
            "Iteration 160, loss = 0.28097763\n",
            "Iteration 161, loss = 0.28098446\n",
            "Iteration 162, loss = 0.28085530\n",
            "Iteration 163, loss = 0.28070650\n",
            "Iteration 164, loss = 0.28061452\n",
            "Iteration 165, loss = 0.28051652\n",
            "Iteration 166, loss = 0.28038186\n",
            "Iteration 167, loss = 0.28031634\n",
            "Iteration 168, loss = 0.28018842\n",
            "Iteration 169, loss = 0.28017237\n",
            "Iteration 170, loss = 0.28001446\n",
            "Iteration 171, loss = 0.28000025\n",
            "Iteration 172, loss = 0.27985178\n",
            "Iteration 173, loss = 0.27977526\n",
            "Iteration 174, loss = 0.27966479\n",
            "Iteration 175, loss = 0.27961782\n",
            "Iteration 176, loss = 0.27948679\n",
            "Iteration 177, loss = 0.27940666\n",
            "Iteration 178, loss = 0.27933939\n",
            "Iteration 179, loss = 0.27928198\n",
            "Iteration 180, loss = 0.27912872\n",
            "Iteration 181, loss = 0.27904091\n",
            "Iteration 182, loss = 0.27892025\n",
            "Iteration 183, loss = 0.27885188\n",
            "Iteration 184, loss = 0.27877389\n",
            "Iteration 185, loss = 0.27862683\n",
            "Iteration 186, loss = 0.27862831\n",
            "Iteration 187, loss = 0.27852063\n",
            "Iteration 188, loss = 0.27843556\n",
            "Iteration 189, loss = 0.27832231\n",
            "Iteration 190, loss = 0.27831047\n",
            "Iteration 191, loss = 0.27820426\n",
            "Iteration 192, loss = 0.27813821\n",
            "Iteration 193, loss = 0.27795073\n",
            "Iteration 194, loss = 0.27793010\n",
            "Iteration 195, loss = 0.27783932\n",
            "Iteration 196, loss = 0.27772663\n",
            "Iteration 197, loss = 0.27771354\n",
            "Iteration 198, loss = 0.27761489\n",
            "Iteration 199, loss = 0.27744317\n",
            "Iteration 200, loss = 0.27742741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.92103963\n",
            "Iteration 2, loss = 0.64133073\n",
            "Iteration 3, loss = 0.61132577\n",
            "Iteration 4, loss = 0.59195574\n",
            "Iteration 5, loss = 0.58266273\n",
            "Iteration 6, loss = 0.57587343\n",
            "Iteration 7, loss = 0.59215152\n",
            "Iteration 8, loss = 0.59818258\n",
            "Iteration 9, loss = 0.57841691\n",
            "Iteration 10, loss = 0.57217610\n",
            "Iteration 11, loss = 0.56918028\n",
            "Iteration 12, loss = 0.56505217\n",
            "Iteration 13, loss = 0.56542166\n",
            "Iteration 14, loss = 0.56412366\n",
            "Iteration 15, loss = 0.60453631\n",
            "Iteration 16, loss = 0.58038503\n",
            "Iteration 17, loss = 0.57125790\n",
            "Iteration 18, loss = 0.56722048\n",
            "Iteration 19, loss = 0.56564416\n",
            "Iteration 20, loss = 0.56459177\n",
            "Iteration 21, loss = 0.56359204\n",
            "Iteration 22, loss = 0.56524622\n",
            "Iteration 23, loss = 0.56360349\n",
            "Iteration 24, loss = 0.56468222\n",
            "Iteration 25, loss = 0.60280972\n",
            "Iteration 26, loss = 0.59119333\n",
            "Iteration 27, loss = 0.57856374\n",
            "Iteration 28, loss = 0.57081178\n",
            "Iteration 29, loss = 0.56643843\n",
            "Iteration 30, loss = 0.56511428\n",
            "Iteration 31, loss = 0.56495978\n",
            "Iteration 32, loss = 0.56312084\n",
            "Iteration 33, loss = 0.56502016\n",
            "Iteration 34, loss = 0.56369547\n",
            "Iteration 35, loss = 0.56502967\n",
            "Iteration 36, loss = 0.60530908\n",
            "Iteration 37, loss = 0.58042756\n",
            "Iteration 38, loss = 0.57160811\n",
            "Iteration 39, loss = 0.56760164\n",
            "Iteration 40, loss = 0.56704694\n",
            "Iteration 41, loss = 0.56464961\n",
            "Iteration 42, loss = 0.56450951\n",
            "Iteration 43, loss = 0.56364774\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.52207736\n",
            "Iteration 45, loss = 0.50688455\n",
            "Iteration 46, loss = 0.50671490\n",
            "Iteration 47, loss = 0.51920915\n",
            "Iteration 48, loss = 0.52225173\n",
            "Iteration 49, loss = 0.51956433\n",
            "Iteration 50, loss = 0.52052602\n",
            "Iteration 51, loss = 0.51745871\n",
            "Iteration 52, loss = 0.51750230\n",
            "Iteration 53, loss = 0.52065696\n",
            "Iteration 54, loss = 0.51730160\n",
            "Iteration 55, loss = 0.51946315\n",
            "Iteration 56, loss = 0.51413324\n",
            "Iteration 57, loss = 0.51416679\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 58, loss = 0.43483676\n",
            "Iteration 59, loss = 0.42360625\n",
            "Iteration 60, loss = 0.41343762\n",
            "Iteration 61, loss = 0.40378358\n",
            "Iteration 62, loss = 0.39475517\n",
            "Iteration 63, loss = 0.38663584\n",
            "Iteration 64, loss = 0.37904394\n",
            "Iteration 65, loss = 0.37257792\n",
            "Iteration 66, loss = 0.36628570\n",
            "Iteration 67, loss = 0.36095281\n",
            "Iteration 68, loss = 0.35593411\n",
            "Iteration 69, loss = 0.35166284\n",
            "Iteration 70, loss = 0.34759374\n",
            "Iteration 71, loss = 0.34511854\n",
            "Iteration 72, loss = 0.34150670\n",
            "Iteration 73, loss = 0.34023499\n",
            "Iteration 74, loss = 0.33751436\n",
            "Iteration 75, loss = 0.33504397\n",
            "Iteration 76, loss = 0.33445228\n",
            "Iteration 77, loss = 0.33316823\n",
            "Iteration 78, loss = 0.33407208\n",
            "Iteration 79, loss = 0.33104803\n",
            "Iteration 80, loss = 0.32964375\n",
            "Iteration 81, loss = 0.33016779\n",
            "Iteration 82, loss = 0.33145718\n",
            "Iteration 83, loss = 0.33181074\n",
            "Iteration 84, loss = 0.32933039\n",
            "Iteration 85, loss = 0.32715930\n",
            "Iteration 86, loss = 0.32892113\n",
            "Iteration 87, loss = 0.32957602\n",
            "Iteration 88, loss = 0.33043940\n",
            "Iteration 89, loss = 0.32791077\n",
            "Iteration 90, loss = 0.32846953\n",
            "Iteration 91, loss = 0.33055540\n",
            "Iteration 92, loss = 0.33245650\n",
            "Iteration 93, loss = 0.32943306\n",
            "Iteration 94, loss = 0.33244715\n",
            "Iteration 95, loss = 0.33000799\n",
            "Iteration 96, loss = 0.33157125\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.29495236\n",
            "Iteration 98, loss = 0.29461385\n",
            "Iteration 99, loss = 0.29437409\n",
            "Iteration 100, loss = 0.29407493\n",
            "Iteration 101, loss = 0.29383980\n",
            "Iteration 102, loss = 0.29364070\n",
            "Iteration 103, loss = 0.29343420\n",
            "Iteration 104, loss = 0.29324286\n",
            "Iteration 105, loss = 0.29310348\n",
            "Iteration 106, loss = 0.29285642\n",
            "Iteration 107, loss = 0.29269804\n",
            "Iteration 108, loss = 0.29243685\n",
            "Iteration 109, loss = 0.29222963\n",
            "Iteration 110, loss = 0.29208194\n",
            "Iteration 111, loss = 0.29190217\n",
            "Iteration 112, loss = 0.29168794\n",
            "Iteration 113, loss = 0.29148950\n",
            "Iteration 114, loss = 0.29128142\n",
            "Iteration 115, loss = 0.29115195\n",
            "Iteration 116, loss = 0.29096025\n",
            "Iteration 117, loss = 0.29076541\n",
            "Iteration 118, loss = 0.29059709\n",
            "Iteration 119, loss = 0.29042049\n",
            "Iteration 120, loss = 0.29028349\n",
            "Iteration 121, loss = 0.29009734\n",
            "Iteration 122, loss = 0.28992101\n",
            "Iteration 123, loss = 0.28975706\n",
            "Iteration 124, loss = 0.28960156\n",
            "Iteration 125, loss = 0.28939330\n",
            "Iteration 126, loss = 0.28921902\n",
            "Iteration 127, loss = 0.28912711\n",
            "Iteration 128, loss = 0.28888487\n",
            "Iteration 129, loss = 0.28880359\n",
            "Iteration 130, loss = 0.28860184\n",
            "Iteration 131, loss = 0.28840037\n",
            "Iteration 132, loss = 0.28832759\n",
            "Iteration 133, loss = 0.28816246\n",
            "Iteration 134, loss = 0.28798314\n",
            "Iteration 135, loss = 0.28775660\n",
            "Iteration 136, loss = 0.28771301\n",
            "Iteration 137, loss = 0.28757975\n",
            "Iteration 138, loss = 0.28736048\n",
            "Iteration 139, loss = 0.28720474\n",
            "Iteration 140, loss = 0.28705632\n",
            "Iteration 141, loss = 0.28694376\n",
            "Iteration 142, loss = 0.28683444\n",
            "Iteration 143, loss = 0.28661824\n",
            "Iteration 144, loss = 0.28648989\n",
            "Iteration 145, loss = 0.28637769\n",
            "Iteration 146, loss = 0.28625086\n",
            "Iteration 147, loss = 0.28605102\n",
            "Iteration 148, loss = 0.28599914\n",
            "Iteration 149, loss = 0.28579501\n",
            "Iteration 150, loss = 0.28560875\n",
            "Iteration 151, loss = 0.28552693\n",
            "Iteration 152, loss = 0.28534616\n",
            "Iteration 153, loss = 0.28526037\n",
            "Iteration 154, loss = 0.28506732\n",
            "Iteration 155, loss = 0.28501297\n",
            "Iteration 156, loss = 0.28484214\n",
            "Iteration 157, loss = 0.28472374\n",
            "Iteration 158, loss = 0.28458747\n",
            "Iteration 159, loss = 0.28455163\n",
            "Iteration 160, loss = 0.28428183\n",
            "Iteration 161, loss = 0.28415850\n",
            "Iteration 162, loss = 0.28399385\n",
            "Iteration 163, loss = 0.28395888\n",
            "Iteration 164, loss = 0.28384003\n",
            "Iteration 165, loss = 0.28366925\n",
            "Iteration 166, loss = 0.28358453\n",
            "Iteration 167, loss = 0.28350473\n",
            "Iteration 168, loss = 0.28334623\n",
            "Iteration 169, loss = 0.28316692\n",
            "Iteration 170, loss = 0.28313328\n",
            "Iteration 171, loss = 0.28297369\n",
            "Iteration 172, loss = 0.28282582\n",
            "Iteration 173, loss = 0.28278998\n",
            "Iteration 174, loss = 0.28266215\n",
            "Iteration 175, loss = 0.28253256\n",
            "Iteration 176, loss = 0.28239902\n",
            "Iteration 177, loss = 0.28230938\n",
            "Iteration 178, loss = 0.28213433\n",
            "Iteration 179, loss = 0.28210743\n",
            "Iteration 180, loss = 0.28190970\n",
            "Iteration 181, loss = 0.28183197\n",
            "Iteration 182, loss = 0.28174935\n",
            "Iteration 183, loss = 0.28161611\n",
            "Iteration 184, loss = 0.28153083\n",
            "Iteration 185, loss = 0.28143541\n",
            "Iteration 186, loss = 0.28132294\n",
            "Iteration 187, loss = 0.28119415\n",
            "Iteration 188, loss = 0.28109185\n",
            "Iteration 189, loss = 0.28102980\n",
            "Iteration 190, loss = 0.28085584\n",
            "Iteration 191, loss = 0.28078598\n",
            "Iteration 192, loss = 0.28073184\n",
            "Iteration 193, loss = 0.28066859\n",
            "Iteration 194, loss = 0.28043388\n",
            "Iteration 195, loss = 0.28039799\n",
            "Iteration 196, loss = 0.28027489\n",
            "Iteration 197, loss = 0.28019980\n",
            "Iteration 198, loss = 0.28003279\n",
            "Iteration 199, loss = 0.27996898\n",
            "Iteration 200, loss = 0.27982401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 63.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67686932\n",
            "Iteration 3, loss = 0.66987616\n",
            "Iteration 4, loss = 0.65501985\n",
            "Iteration 5, loss = 0.64157073\n",
            "Iteration 6, loss = 0.63127841\n",
            "Iteration 7, loss = 0.62305248\n",
            "Iteration 8, loss = 0.61287749\n",
            "Iteration 9, loss = 0.60662251\n",
            "Iteration 10, loss = 0.60222417\n",
            "Iteration 11, loss = 0.59580806\n",
            "Iteration 12, loss = 0.59106770\n",
            "Iteration 13, loss = 0.58765201\n",
            "Iteration 14, loss = 0.58374255\n",
            "Iteration 15, loss = 0.58037253\n",
            "Iteration 16, loss = 0.57883852\n",
            "Iteration 17, loss = 0.57572014\n",
            "Iteration 18, loss = 0.57523158\n",
            "Iteration 19, loss = 0.57056933\n",
            "Iteration 20, loss = 0.57372891\n",
            "Iteration 21, loss = 0.56986594\n",
            "Iteration 22, loss = 0.56432829\n",
            "Iteration 23, loss = 0.56753920\n",
            "Iteration 24, loss = 0.56508658\n",
            "Iteration 25, loss = 0.56369779\n",
            "Iteration 26, loss = 0.56516485\n",
            "Iteration 27, loss = 0.56282165\n",
            "Iteration 28, loss = 0.56345689\n",
            "Iteration 29, loss = 0.56218396\n",
            "Iteration 30, loss = 0.56202397\n",
            "Iteration 31, loss = 0.56262472\n",
            "Iteration 32, loss = 0.56408555\n",
            "Iteration 33, loss = 0.55985305\n",
            "Iteration 34, loss = 0.56378403\n",
            "Iteration 35, loss = 0.56011602\n",
            "Iteration 36, loss = 0.56017443\n",
            "Iteration 37, loss = 0.56368529\n",
            "Iteration 38, loss = 0.56031256\n",
            "Iteration 39, loss = 0.57171577\n",
            "Iteration 40, loss = 0.58073127\n",
            "Iteration 41, loss = 0.57670058\n",
            "Iteration 42, loss = 0.57169342\n",
            "Iteration 43, loss = 0.60942185\n",
            "Iteration 44, loss = 0.62364001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.61500713\n",
            "Iteration 46, loss = 0.60915702\n",
            "Iteration 47, loss = 0.60343359\n",
            "Iteration 48, loss = 0.59735266\n",
            "Iteration 49, loss = 0.59189998\n",
            "Iteration 50, loss = 0.58717131\n",
            "Iteration 51, loss = 0.58401193\n",
            "Iteration 52, loss = 0.58086224\n",
            "Iteration 53, loss = 0.57813129\n",
            "Iteration 54, loss = 0.57649685\n",
            "Iteration 55, loss = 0.57491310\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 56, loss = 0.57766686\n",
            "Iteration 57, loss = 0.57083183\n",
            "Iteration 58, loss = 0.57047128\n",
            "Iteration 59, loss = 0.56990548\n",
            "Iteration 60, loss = 0.56948470\n",
            "Iteration 61, loss = 0.56932505\n",
            "Iteration 62, loss = 0.56866294\n",
            "Iteration 63, loss = 0.56854763\n",
            "Iteration 64, loss = 0.56802445\n",
            "Iteration 65, loss = 0.56782902\n",
            "Iteration 66, loss = 0.56720503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 67, loss = 0.56649497\n",
            "Iteration 68, loss = 0.56632652\n",
            "Iteration 69, loss = 0.56621444\n",
            "Iteration 70, loss = 0.56612383\n",
            "Iteration 71, loss = 0.56607879\n",
            "Iteration 72, loss = 0.56591055\n",
            "Iteration 73, loss = 0.56581370\n",
            "Iteration 74, loss = 0.56577889\n",
            "Iteration 75, loss = 0.56571178\n",
            "Iteration 76, loss = 0.56564580\n",
            "Iteration 77, loss = 0.56550510\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 78, loss = 0.56551810\n",
            "Iteration 79, loss = 0.56534265\n",
            "Iteration 80, loss = 0.56529445\n",
            "Iteration 81, loss = 0.56527557\n",
            "Iteration 82, loss = 0.56526338\n",
            "Iteration 83, loss = 0.56521375\n",
            "Iteration 84, loss = 0.56525982\n",
            "Iteration 85, loss = 0.56518882\n",
            "Iteration 86, loss = 0.56519428\n",
            "Iteration 87, loss = 0.56515527\n",
            "Iteration 88, loss = 0.56513965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 89, loss = 0.56510307\n",
            "Iteration 90, loss = 0.56510591\n",
            "Iteration 91, loss = 0.56509346\n",
            "Iteration 92, loss = 0.56508790\n",
            "Iteration 93, loss = 0.56508646\n",
            "Iteration 94, loss = 0.56508644\n",
            "Iteration 95, loss = 0.56508091\n",
            "Iteration 96, loss = 0.56508335\n",
            "Iteration 97, loss = 0.56507834\n",
            "Iteration 98, loss = 0.56506709\n",
            "Iteration 99, loss = 0.56506718\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69593234\n",
            "Iteration 3, loss = 0.67363882\n",
            "Iteration 4, loss = 0.65991852\n",
            "Iteration 5, loss = 0.64342884\n",
            "Iteration 6, loss = 0.63331498\n",
            "Iteration 7, loss = 0.62233959\n",
            "Iteration 8, loss = 0.62043842\n",
            "Iteration 9, loss = 0.61066459\n",
            "Iteration 10, loss = 0.60663686\n",
            "Iteration 11, loss = 0.60070297\n",
            "Iteration 12, loss = 0.59285621\n",
            "Iteration 13, loss = 0.59053952\n",
            "Iteration 14, loss = 0.58478978\n",
            "Iteration 15, loss = 0.58253535\n",
            "Iteration 16, loss = 0.57922134\n",
            "Iteration 17, loss = 0.57786073\n",
            "Iteration 18, loss = 0.57536955\n",
            "Iteration 19, loss = 0.57099101\n",
            "Iteration 20, loss = 0.57205704\n",
            "Iteration 21, loss = 0.56839138\n",
            "Iteration 22, loss = 0.57009405\n",
            "Iteration 23, loss = 0.56476365\n",
            "Iteration 24, loss = 0.56666069\n",
            "Iteration 25, loss = 0.56276675\n",
            "Iteration 26, loss = 0.56285778\n",
            "Iteration 27, loss = 0.56199398\n",
            "Iteration 28, loss = 0.56141260\n",
            "Iteration 29, loss = 0.56179180\n",
            "Iteration 30, loss = 0.56319566\n",
            "Iteration 31, loss = 0.56358801\n",
            "Iteration 32, loss = 0.56070616\n",
            "Iteration 33, loss = 0.56014870\n",
            "Iteration 34, loss = 0.55959557\n",
            "Iteration 35, loss = 0.55793337\n",
            "Iteration 36, loss = 0.55967381\n",
            "Iteration 37, loss = 0.55902093\n",
            "Iteration 38, loss = 0.56175183\n",
            "Iteration 39, loss = 0.55688135\n",
            "Iteration 40, loss = 0.55695006\n",
            "Iteration 41, loss = 0.56046917\n",
            "Iteration 42, loss = 0.55367866\n",
            "Iteration 43, loss = 0.55975685\n",
            "Iteration 44, loss = 0.55769274\n",
            "Iteration 45, loss = 0.55752156\n",
            "Iteration 46, loss = 0.56057767\n",
            "Iteration 47, loss = 0.56160369\n",
            "Iteration 48, loss = 0.55761526\n",
            "Iteration 49, loss = 0.55881146\n",
            "Iteration 50, loss = 0.55878595\n",
            "Iteration 51, loss = 0.55871732\n",
            "Iteration 52, loss = 0.55869122\n",
            "Iteration 53, loss = 0.55659508\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.51884900\n",
            "Iteration 55, loss = 0.50520926\n",
            "Iteration 56, loss = 0.49898697\n",
            "Iteration 57, loss = 0.49376561\n",
            "Iteration 58, loss = 0.48718145\n",
            "Iteration 59, loss = 0.48248667\n",
            "Iteration 60, loss = 0.47849949\n",
            "Iteration 61, loss = 0.47702150\n",
            "Iteration 62, loss = 0.47311600\n",
            "Iteration 63, loss = 0.48190743\n",
            "Iteration 64, loss = 0.49605234\n",
            "Iteration 65, loss = 0.49034346\n",
            "Iteration 66, loss = 0.50385028\n",
            "Iteration 67, loss = 0.51200364\n",
            "Iteration 68, loss = 0.50611090\n",
            "Iteration 69, loss = 0.50416131\n",
            "Iteration 70, loss = 0.50021435\n",
            "Iteration 71, loss = 0.50470033\n",
            "Iteration 72, loss = 0.50270604\n",
            "Iteration 73, loss = 0.51006261\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.43528556\n",
            "Iteration 75, loss = 0.43279195\n",
            "Iteration 76, loss = 0.43058549\n",
            "Iteration 77, loss = 0.42859263\n",
            "Iteration 78, loss = 0.42680300\n",
            "Iteration 79, loss = 0.42485924\n",
            "Iteration 80, loss = 0.42248399\n",
            "Iteration 81, loss = 0.42091660\n",
            "Iteration 82, loss = 0.41889313\n",
            "Iteration 83, loss = 0.41689228\n",
            "Iteration 84, loss = 0.41468665\n",
            "Iteration 85, loss = 0.41331711\n",
            "Iteration 86, loss = 0.41113052\n",
            "Iteration 87, loss = 0.40884132\n",
            "Iteration 88, loss = 0.40735969\n",
            "Iteration 89, loss = 0.40544344\n",
            "Iteration 90, loss = 0.40346633\n",
            "Iteration 91, loss = 0.40171323\n",
            "Iteration 92, loss = 0.39994807\n",
            "Iteration 93, loss = 0.39837671\n",
            "Iteration 94, loss = 0.39629331\n",
            "Iteration 95, loss = 0.39429263\n",
            "Iteration 96, loss = 0.39277899\n",
            "Iteration 97, loss = 0.39094574\n",
            "Iteration 98, loss = 0.38911345\n",
            "Iteration 99, loss = 0.38765990\n",
            "Iteration 100, loss = 0.38628745\n",
            "Iteration 101, loss = 0.38432147\n",
            "Iteration 102, loss = 0.38278870\n",
            "Iteration 103, loss = 0.38187464\n",
            "Iteration 104, loss = 0.37980770\n",
            "Iteration 105, loss = 0.37777687\n",
            "Iteration 106, loss = 0.37638733\n",
            "Iteration 107, loss = 0.37470643\n",
            "Iteration 108, loss = 0.37322743\n",
            "Iteration 109, loss = 0.37169919\n",
            "Iteration 110, loss = 0.37036592\n",
            "Iteration 111, loss = 0.36870301\n",
            "Iteration 112, loss = 0.36771743\n",
            "Iteration 113, loss = 0.36648584\n",
            "Iteration 114, loss = 0.36421080\n",
            "Iteration 115, loss = 0.36375668\n",
            "Iteration 116, loss = 0.36141384\n",
            "Iteration 117, loss = 0.36030173\n",
            "Iteration 118, loss = 0.35865064\n",
            "Iteration 119, loss = 0.35752853\n",
            "Iteration 120, loss = 0.35627308\n",
            "Iteration 121, loss = 0.35529927\n",
            "Iteration 122, loss = 0.35443030\n",
            "Iteration 123, loss = 0.35267702\n",
            "Iteration 124, loss = 0.35212467\n",
            "Iteration 125, loss = 0.35076452\n",
            "Iteration 126, loss = 0.34161705\n",
            "Iteration 127, loss = 0.33662277\n",
            "Iteration 128, loss = 0.33595958\n",
            "Iteration 129, loss = 0.33366189\n",
            "Iteration 130, loss = 0.33310441\n",
            "Iteration 131, loss = 0.33098154\n",
            "Iteration 132, loss = 0.33084280\n",
            "Iteration 133, loss = 0.32828643\n",
            "Iteration 134, loss = 0.32825789\n",
            "Iteration 135, loss = 0.32677101\n",
            "Iteration 136, loss = 0.32479987\n",
            "Iteration 137, loss = 0.32287430\n",
            "Iteration 138, loss = 0.32342959\n",
            "Iteration 139, loss = 0.32209758\n",
            "Iteration 140, loss = 0.32051973\n",
            "Iteration 141, loss = 0.32016858\n",
            "Iteration 142, loss = 0.31742062\n",
            "Iteration 143, loss = 0.31786284\n",
            "Iteration 144, loss = 0.31633708\n",
            "Iteration 145, loss = 0.31556993\n",
            "Iteration 146, loss = 0.31428047\n",
            "Iteration 147, loss = 0.31419814\n",
            "Iteration 148, loss = 0.31274522\n",
            "Iteration 149, loss = 0.31219300\n",
            "Iteration 150, loss = 0.31028200\n",
            "Iteration 151, loss = 0.31059588\n",
            "Iteration 152, loss = 0.30893795\n",
            "Iteration 153, loss = 0.30805343\n",
            "Iteration 154, loss = 0.30785030\n",
            "Iteration 155, loss = 0.30825597\n",
            "Iteration 156, loss = 0.30618054\n",
            "Iteration 157, loss = 0.30562119\n",
            "Iteration 158, loss = 0.30358198\n",
            "Iteration 159, loss = 0.30255764\n",
            "Iteration 160, loss = 0.30286518\n",
            "Iteration 161, loss = 0.30350011\n",
            "Iteration 162, loss = 0.30188247\n",
            "Iteration 163, loss = 0.30415370\n",
            "Iteration 164, loss = 0.30036720\n",
            "Iteration 165, loss = 0.29915114\n",
            "Iteration 166, loss = 0.29965905\n",
            "Iteration 167, loss = 0.29932233\n",
            "Iteration 168, loss = 0.30157724\n",
            "Iteration 169, loss = 0.29711920\n",
            "Iteration 170, loss = 0.29935499\n",
            "Iteration 171, loss = 0.29942443\n",
            "Iteration 172, loss = 0.29526874\n",
            "Iteration 173, loss = 0.29529365\n",
            "Iteration 174, loss = 0.29628860\n",
            "Iteration 175, loss = 0.29534518\n",
            "Iteration 176, loss = 0.29327744\n",
            "Iteration 177, loss = 0.29370506\n",
            "Iteration 178, loss = 0.29267459\n",
            "Iteration 179, loss = 0.29601930\n",
            "Iteration 180, loss = 0.29439892\n",
            "Iteration 181, loss = 0.28955723\n",
            "Iteration 182, loss = 0.29195508\n",
            "Iteration 183, loss = 0.29038784\n",
            "Iteration 184, loss = 0.29039586\n",
            "Iteration 185, loss = 0.29002215\n",
            "Iteration 186, loss = 0.28910948\n",
            "Iteration 187, loss = 0.29008081\n",
            "Iteration 188, loss = 0.28789330\n",
            "Iteration 189, loss = 0.29054741\n",
            "Iteration 190, loss = 0.28833687\n",
            "Iteration 191, loss = 0.29348254\n",
            "Iteration 192, loss = 0.28847935\n",
            "Iteration 193, loss = 0.28778593\n",
            "Iteration 194, loss = 0.28790504\n",
            "Iteration 195, loss = 0.29049393\n",
            "Iteration 196, loss = 0.28786258\n",
            "Iteration 197, loss = 0.28895091\n",
            "Iteration 198, loss = 0.28812204\n",
            "Iteration 199, loss = 0.28807252\n",
            "Iteration 200, loss = 0.28968840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69512876\n",
            "Iteration 3, loss = 0.67702406\n",
            "Iteration 4, loss = 0.66099061\n",
            "Iteration 5, loss = 0.64859122\n",
            "Iteration 6, loss = 0.63758566\n",
            "Iteration 7, loss = 0.62690562\n",
            "Iteration 8, loss = 0.61793574\n",
            "Iteration 9, loss = 0.61130862\n",
            "Iteration 10, loss = 0.60363105\n",
            "Iteration 11, loss = 0.59927760\n",
            "Iteration 12, loss = 0.59602892\n",
            "Iteration 13, loss = 0.59065723\n",
            "Iteration 14, loss = 0.58655099\n",
            "Iteration 15, loss = 0.58298227\n",
            "Iteration 16, loss = 0.57935572\n",
            "Iteration 17, loss = 0.57470628\n",
            "Iteration 18, loss = 0.57624032\n",
            "Iteration 19, loss = 0.57337612\n",
            "Iteration 20, loss = 0.57180075\n",
            "Iteration 21, loss = 0.57038256\n",
            "Iteration 22, loss = 0.57042682\n",
            "Iteration 23, loss = 0.56497117\n",
            "Iteration 24, loss = 0.56714258\n",
            "Iteration 25, loss = 0.56504271\n",
            "Iteration 26, loss = 0.56461313\n",
            "Iteration 27, loss = 0.56119323\n",
            "Iteration 28, loss = 0.56288226\n",
            "Iteration 29, loss = 0.55816440\n",
            "Iteration 30, loss = 0.56263147\n",
            "Iteration 31, loss = 0.56420386\n",
            "Iteration 32, loss = 0.55839857\n",
            "Iteration 33, loss = 0.56068067\n",
            "Iteration 34, loss = 0.55957184\n",
            "Iteration 35, loss = 0.55710386\n",
            "Iteration 36, loss = 0.55941263\n",
            "Iteration 37, loss = 0.56168340\n",
            "Iteration 38, loss = 0.56065027\n",
            "Iteration 39, loss = 0.55846118\n",
            "Iteration 40, loss = 0.55641509\n",
            "Iteration 41, loss = 0.55941447\n",
            "Iteration 42, loss = 0.56064623\n",
            "Iteration 43, loss = 0.55632807\n",
            "Iteration 44, loss = 0.56151093\n",
            "Iteration 45, loss = 0.55777365\n",
            "Iteration 46, loss = 0.55597400\n",
            "Iteration 47, loss = 0.56245554\n",
            "Iteration 48, loss = 0.55703083\n",
            "Iteration 49, loss = 0.55928426\n",
            "Iteration 50, loss = 0.55806435\n",
            "Iteration 51, loss = 0.55880519\n",
            "Iteration 52, loss = 0.56037061\n",
            "Iteration 53, loss = 0.55588262\n",
            "Iteration 54, loss = 0.55938176\n",
            "Iteration 55, loss = 0.55895587\n",
            "Iteration 56, loss = 0.55558579\n",
            "Iteration 57, loss = 0.56107422\n",
            "Iteration 58, loss = 0.55787457\n",
            "Iteration 59, loss = 0.56046011\n",
            "Iteration 60, loss = 0.55956160\n",
            "Iteration 61, loss = 0.55674809\n",
            "Iteration 62, loss = 0.55978111\n",
            "Iteration 63, loss = 0.56013276\n",
            "Iteration 64, loss = 0.56044478\n",
            "Iteration 65, loss = 0.55801436\n",
            "Iteration 66, loss = 0.55927296\n",
            "Iteration 67, loss = 0.55766489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.51069541\n",
            "Iteration 69, loss = 0.49806663\n",
            "Iteration 70, loss = 0.48968095\n",
            "Iteration 71, loss = 0.48177373\n",
            "Iteration 72, loss = 0.47415803\n",
            "Iteration 73, loss = 0.46885587\n",
            "Iteration 74, loss = 0.46149884\n",
            "Iteration 75, loss = 0.47259002\n",
            "Iteration 76, loss = 0.49023552\n",
            "Iteration 77, loss = 0.48326735\n",
            "Iteration 78, loss = 0.49364546\n",
            "Iteration 79, loss = 0.50641912\n",
            "Iteration 80, loss = 0.50825828\n",
            "Iteration 81, loss = 0.49744369\n",
            "Iteration 82, loss = 0.49911076\n",
            "Iteration 83, loss = 0.49515526\n",
            "Iteration 84, loss = 0.50121383\n",
            "Iteration 85, loss = 0.49628445\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 86, loss = 0.42692146\n",
            "Iteration 87, loss = 0.42049965\n",
            "Iteration 88, loss = 0.41777913\n",
            "Iteration 89, loss = 0.41504655\n",
            "Iteration 90, loss = 0.41250168\n",
            "Iteration 91, loss = 0.40980446\n",
            "Iteration 92, loss = 0.40725440\n",
            "Iteration 93, loss = 0.40490058\n",
            "Iteration 94, loss = 0.40236563\n",
            "Iteration 95, loss = 0.40018562\n",
            "Iteration 96, loss = 0.39750462\n",
            "Iteration 97, loss = 0.39516150\n",
            "Iteration 98, loss = 0.39348246\n",
            "Iteration 99, loss = 0.39110859\n",
            "Iteration 100, loss = 0.38844898\n",
            "Iteration 101, loss = 0.38588571\n",
            "Iteration 102, loss = 0.38414404\n",
            "Iteration 103, loss = 0.38147559\n",
            "Iteration 104, loss = 0.37995514\n",
            "Iteration 105, loss = 0.37744645\n",
            "Iteration 106, loss = 0.37501154\n",
            "Iteration 107, loss = 0.37266019\n",
            "Iteration 108, loss = 0.37107682\n",
            "Iteration 109, loss = 0.36915441\n",
            "Iteration 110, loss = 0.36749124\n",
            "Iteration 111, loss = 0.36553552\n",
            "Iteration 112, loss = 0.36356508\n",
            "Iteration 113, loss = 0.36118172\n",
            "Iteration 114, loss = 0.35939925\n",
            "Iteration 115, loss = 0.35790933\n",
            "Iteration 116, loss = 0.35594304\n",
            "Iteration 117, loss = 0.35394715\n",
            "Iteration 118, loss = 0.35247691\n",
            "Iteration 119, loss = 0.35035620\n",
            "Iteration 120, loss = 0.34920622\n",
            "Iteration 121, loss = 0.34722657\n",
            "Iteration 122, loss = 0.34564591\n",
            "Iteration 123, loss = 0.34412345\n",
            "Iteration 124, loss = 0.34271306\n",
            "Iteration 125, loss = 0.34077566\n",
            "Iteration 126, loss = 0.33974708\n",
            "Iteration 127, loss = 0.33804716\n",
            "Iteration 128, loss = 0.33701387\n",
            "Iteration 129, loss = 0.33569959\n",
            "Iteration 130, loss = 0.33379229\n",
            "Iteration 131, loss = 0.33216295\n",
            "Iteration 132, loss = 0.33122013\n",
            "Iteration 133, loss = 0.33043667\n",
            "Iteration 134, loss = 0.32862695\n",
            "Iteration 135, loss = 0.32731758\n",
            "Iteration 136, loss = 0.32547349\n",
            "Iteration 137, loss = 0.32443320\n",
            "Iteration 138, loss = 0.32399996\n",
            "Iteration 139, loss = 0.32196886\n",
            "Iteration 140, loss = 0.32117023\n",
            "Iteration 141, loss = 0.31984492\n",
            "Iteration 142, loss = 0.31924458\n",
            "Iteration 143, loss = 0.31718533\n",
            "Iteration 144, loss = 0.31711076\n",
            "Iteration 145, loss = 0.31467352\n",
            "Iteration 146, loss = 0.31417654\n",
            "Iteration 147, loss = 0.31314199\n",
            "Iteration 148, loss = 0.31252827\n",
            "Iteration 149, loss = 0.31129134\n",
            "Iteration 150, loss = 0.31030065\n",
            "Iteration 151, loss = 0.30992179\n",
            "Iteration 152, loss = 0.30890722\n",
            "Iteration 153, loss = 0.30716712\n",
            "Iteration 154, loss = 0.30638287\n",
            "Iteration 155, loss = 0.30596987\n",
            "Iteration 156, loss = 0.30526787\n",
            "Iteration 157, loss = 0.30411042\n",
            "Iteration 158, loss = 0.30285656\n",
            "Iteration 159, loss = 0.30329681\n",
            "Iteration 160, loss = 0.30135958\n",
            "Iteration 161, loss = 0.30086447\n",
            "Iteration 162, loss = 0.29984348\n",
            "Iteration 163, loss = 0.29937581\n",
            "Iteration 164, loss = 0.29815990\n",
            "Iteration 165, loss = 0.29861470\n",
            "Iteration 166, loss = 0.29685847\n",
            "Iteration 167, loss = 0.29577682\n",
            "Iteration 168, loss = 0.29505644\n",
            "Iteration 169, loss = 0.29431649\n",
            "Iteration 170, loss = 0.29642099\n",
            "Iteration 171, loss = 0.29564524\n",
            "Iteration 172, loss = 0.29179574\n",
            "Iteration 173, loss = 0.29267624\n",
            "Iteration 174, loss = 0.29272208\n",
            "Iteration 175, loss = 0.29139003\n",
            "Iteration 176, loss = 0.29154294\n",
            "Iteration 177, loss = 0.28952760\n",
            "Iteration 178, loss = 0.28943420\n",
            "Iteration 179, loss = 0.29228218\n",
            "Iteration 180, loss = 0.29004044\n",
            "Iteration 181, loss = 0.28868006\n",
            "Iteration 182, loss = 0.28888092\n",
            "Iteration 183, loss = 0.28824313\n",
            "Iteration 184, loss = 0.28620716\n",
            "Iteration 185, loss = 0.28513444\n",
            "Iteration 186, loss = 0.28569559\n",
            "Iteration 187, loss = 0.28546012\n",
            "Iteration 188, loss = 0.28532442\n",
            "Iteration 189, loss = 0.28501361\n",
            "Iteration 190, loss = 0.28516579\n",
            "Iteration 191, loss = 0.28246668\n",
            "Iteration 192, loss = 0.28268309\n",
            "Iteration 193, loss = 0.28010785\n",
            "Iteration 194, loss = 0.28198800\n",
            "Iteration 195, loss = 0.28325446\n",
            "Iteration 196, loss = 0.28129900\n",
            "Iteration 197, loss = 0.28330301\n",
            "Iteration 198, loss = 0.28061372\n",
            "Iteration 199, loss = 0.28117213\n",
            "Iteration 200, loss = 0.27932211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67518552\n",
            "Iteration 3, loss = 0.66622760\n",
            "Iteration 4, loss = 0.65149058\n",
            "Iteration 5, loss = 0.64019719\n",
            "Iteration 6, loss = 0.63256033\n",
            "Iteration 7, loss = 0.62835600\n",
            "Iteration 8, loss = 0.62007951\n",
            "Iteration 9, loss = 0.60959242\n",
            "Iteration 10, loss = 0.60386837\n",
            "Iteration 11, loss = 0.60021631\n",
            "Iteration 12, loss = 0.59401370\n",
            "Iteration 13, loss = 0.59286379\n",
            "Iteration 14, loss = 0.58672496\n",
            "Iteration 15, loss = 0.58385560\n",
            "Iteration 16, loss = 0.58231535\n",
            "Iteration 17, loss = 0.57523584\n",
            "Iteration 18, loss = 0.58615805\n",
            "Iteration 19, loss = 0.57950924\n",
            "Iteration 20, loss = 0.57483157\n",
            "Iteration 21, loss = 0.57434669\n",
            "Iteration 22, loss = 0.57626462\n",
            "Iteration 23, loss = 0.57241864\n",
            "Iteration 24, loss = 0.57652807\n",
            "Iteration 25, loss = 0.57071428\n",
            "Iteration 26, loss = 0.56920412\n",
            "Iteration 27, loss = 0.56852067\n",
            "Iteration 28, loss = 0.56681851\n",
            "Iteration 29, loss = 0.56826762\n",
            "Iteration 30, loss = 0.56815696\n",
            "Iteration 31, loss = 0.56259377\n",
            "Iteration 32, loss = 0.56232367\n",
            "Iteration 33, loss = 0.56542959\n",
            "Iteration 34, loss = 0.55899177\n",
            "Iteration 35, loss = 0.56090346\n",
            "Iteration 36, loss = 0.56137484\n",
            "Iteration 37, loss = 0.55792409\n",
            "Iteration 38, loss = 0.56197193\n",
            "Iteration 39, loss = 0.55783905\n",
            "Iteration 40, loss = 0.55860597\n",
            "Iteration 41, loss = 0.56144694\n",
            "Iteration 42, loss = 0.55796881\n",
            "Iteration 43, loss = 0.55954466\n",
            "Iteration 44, loss = 0.55671990\n",
            "Iteration 45, loss = 0.55960086\n",
            "Iteration 46, loss = 0.56170731\n",
            "Iteration 47, loss = 0.55630711\n",
            "Iteration 48, loss = 0.56164877\n",
            "Iteration 49, loss = 0.55680524\n",
            "Iteration 50, loss = 0.55972014\n",
            "Iteration 51, loss = 0.55638011\n",
            "Iteration 52, loss = 0.55822367\n",
            "Iteration 53, loss = 0.56135797\n",
            "Iteration 54, loss = 0.55677453\n",
            "Iteration 55, loss = 0.55913375\n",
            "Iteration 56, loss = 0.55989269\n",
            "Iteration 57, loss = 0.55335980\n",
            "Iteration 58, loss = 0.56120985\n",
            "Iteration 59, loss = 0.55782858\n",
            "Iteration 60, loss = 0.55999075\n",
            "Iteration 61, loss = 0.55806985\n",
            "Iteration 62, loss = 0.55582551\n",
            "Iteration 63, loss = 0.56022972\n",
            "Iteration 64, loss = 0.56164980\n",
            "Iteration 65, loss = 0.56088390\n",
            "Iteration 66, loss = 0.56375495\n",
            "Iteration 67, loss = 0.55983010\n",
            "Iteration 68, loss = 0.56130658\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.51695998\n",
            "Iteration 70, loss = 0.50189662\n",
            "Iteration 71, loss = 0.49282956\n",
            "Iteration 72, loss = 0.48537700\n",
            "Iteration 73, loss = 0.47802319\n",
            "Iteration 74, loss = 0.47148213\n",
            "Iteration 75, loss = 0.47152419\n",
            "Iteration 76, loss = 0.46423777\n",
            "Iteration 77, loss = 0.47574980\n",
            "Iteration 78, loss = 0.49738636\n",
            "Iteration 79, loss = 0.50369532\n",
            "Iteration 80, loss = 0.48916089\n",
            "Iteration 81, loss = 0.51045728\n",
            "Iteration 82, loss = 0.49256795\n",
            "Iteration 83, loss = 0.50426257\n",
            "Iteration 84, loss = 0.49093155\n",
            "Iteration 85, loss = 0.50068912\n",
            "Iteration 86, loss = 0.50188416\n",
            "Iteration 87, loss = 0.49506994\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.42314415\n",
            "Iteration 89, loss = 0.41955586\n",
            "Iteration 90, loss = 0.41663685\n",
            "Iteration 91, loss = 0.41463905\n",
            "Iteration 92, loss = 0.41142590\n",
            "Iteration 93, loss = 0.40882499\n",
            "Iteration 94, loss = 0.40661997\n",
            "Iteration 95, loss = 0.40417632\n",
            "Iteration 96, loss = 0.40148618\n",
            "Iteration 97, loss = 0.39980233\n",
            "Iteration 98, loss = 0.39718447\n",
            "Iteration 99, loss = 0.39471466\n",
            "Iteration 100, loss = 0.39252507\n",
            "Iteration 101, loss = 0.39018215\n",
            "Iteration 102, loss = 0.38810372\n",
            "Iteration 103, loss = 0.38552838\n",
            "Iteration 104, loss = 0.38401947\n",
            "Iteration 105, loss = 0.38145054\n",
            "Iteration 106, loss = 0.37963721\n",
            "Iteration 107, loss = 0.37762168\n",
            "Iteration 108, loss = 0.37519173\n",
            "Iteration 109, loss = 0.37345843\n",
            "Iteration 110, loss = 0.37115607\n",
            "Iteration 111, loss = 0.36944104\n",
            "Iteration 112, loss = 0.36711872\n",
            "Iteration 113, loss = 0.36553646\n",
            "Iteration 114, loss = 0.36383489\n",
            "Iteration 115, loss = 0.36183901\n",
            "Iteration 116, loss = 0.35964459\n",
            "Iteration 117, loss = 0.35830893\n",
            "Iteration 118, loss = 0.35615972\n",
            "Iteration 119, loss = 0.35444976\n",
            "Iteration 120, loss = 0.35254159\n",
            "Iteration 121, loss = 0.35119535\n",
            "Iteration 122, loss = 0.34952082\n",
            "Iteration 123, loss = 0.34793181\n",
            "Iteration 124, loss = 0.34705882\n",
            "Iteration 125, loss = 0.34419738\n",
            "Iteration 126, loss = 0.34282667\n",
            "Iteration 127, loss = 0.34163840\n",
            "Iteration 128, loss = 0.34019574\n",
            "Iteration 129, loss = 0.33865230\n",
            "Iteration 130, loss = 0.33708685\n",
            "Iteration 131, loss = 0.33626920\n",
            "Iteration 132, loss = 0.33428015\n",
            "Iteration 133, loss = 0.33374207\n",
            "Iteration 134, loss = 0.33144954\n",
            "Iteration 135, loss = 0.33051940\n",
            "Iteration 136, loss = 0.32882555\n",
            "Iteration 137, loss = 0.32775275\n",
            "Iteration 138, loss = 0.32615771\n",
            "Iteration 139, loss = 0.32606301\n",
            "Iteration 140, loss = 0.32383199\n",
            "Iteration 141, loss = 0.32266962\n",
            "Iteration 142, loss = 0.32143683\n",
            "Iteration 143, loss = 0.32059926\n",
            "Iteration 144, loss = 0.31947232\n",
            "Iteration 145, loss = 0.31819031\n",
            "Iteration 146, loss = 0.31703227\n",
            "Iteration 147, loss = 0.31706549\n",
            "Iteration 148, loss = 0.31454853\n",
            "Iteration 149, loss = 0.31355382\n",
            "Iteration 150, loss = 0.31357538\n",
            "Iteration 151, loss = 0.31247415\n",
            "Iteration 152, loss = 0.31113920\n",
            "Iteration 153, loss = 0.30983607\n",
            "Iteration 154, loss = 0.30890701\n",
            "Iteration 155, loss = 0.30795614\n",
            "Iteration 156, loss = 0.30706901\n",
            "Iteration 157, loss = 0.30644988\n",
            "Iteration 158, loss = 0.30585982\n",
            "Iteration 159, loss = 0.30505733\n",
            "Iteration 160, loss = 0.30403120\n",
            "Iteration 161, loss = 0.30259281\n",
            "Iteration 162, loss = 0.30287709\n",
            "Iteration 163, loss = 0.30118867\n",
            "Iteration 164, loss = 0.30063257\n",
            "Iteration 165, loss = 0.30011354\n",
            "Iteration 166, loss = 0.30087095\n",
            "Iteration 167, loss = 0.30011994\n",
            "Iteration 168, loss = 0.29627746\n",
            "Iteration 169, loss = 0.29757301\n",
            "Iteration 170, loss = 0.29633989\n",
            "Iteration 171, loss = 0.29608592\n",
            "Iteration 172, loss = 0.29447725\n",
            "Iteration 173, loss = 0.29473202\n",
            "Iteration 174, loss = 0.29519817\n",
            "Iteration 175, loss = 0.29243314\n",
            "Iteration 176, loss = 0.29483703\n",
            "Iteration 177, loss = 0.29332930\n",
            "Iteration 178, loss = 0.29244598\n",
            "Iteration 179, loss = 0.29043286\n",
            "Iteration 180, loss = 0.29036069\n",
            "Iteration 181, loss = 0.28969388\n",
            "Iteration 182, loss = 0.28854360\n",
            "Iteration 183, loss = 0.28931766\n",
            "Iteration 184, loss = 0.28941326\n",
            "Iteration 185, loss = 0.28806088\n",
            "Iteration 186, loss = 0.28685245\n",
            "Iteration 187, loss = 0.28547135\n",
            "Iteration 188, loss = 0.28846326\n",
            "Iteration 189, loss = 0.28914782\n",
            "Iteration 190, loss = 0.28565105\n",
            "Iteration 191, loss = 0.28492015\n",
            "Iteration 192, loss = 0.28483061\n",
            "Iteration 193, loss = 0.28444044\n",
            "Iteration 194, loss = 0.28350229\n",
            "Iteration 195, loss = 0.28316789\n",
            "Iteration 196, loss = 0.28133982\n",
            "Iteration 197, loss = 0.28239291\n",
            "Iteration 198, loss = 0.28418221\n",
            "Iteration 199, loss = 0.28116152\n",
            "Iteration 200, loss = 0.28144675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66417631\n",
            "Iteration 3, loss = 0.65368677\n",
            "Iteration 4, loss = 0.64342002\n",
            "Iteration 5, loss = 0.63208717\n",
            "Iteration 6, loss = 0.62476509\n",
            "Iteration 7, loss = 0.61611010\n",
            "Iteration 8, loss = 0.61318159\n",
            "Iteration 9, loss = 0.60257728\n",
            "Iteration 10, loss = 0.60025526\n",
            "Iteration 11, loss = 0.59527374\n",
            "Iteration 12, loss = 0.59308071\n",
            "Iteration 13, loss = 0.58718164\n",
            "Iteration 14, loss = 0.58999943\n",
            "Iteration 15, loss = 0.58253598\n",
            "Iteration 16, loss = 0.58214549\n",
            "Iteration 17, loss = 0.57863273\n",
            "Iteration 18, loss = 0.57505837\n",
            "Iteration 19, loss = 0.57537664\n",
            "Iteration 20, loss = 0.57187662\n",
            "Iteration 21, loss = 0.57135900\n",
            "Iteration 22, loss = 0.57112240\n",
            "Iteration 23, loss = 0.56649095\n",
            "Iteration 24, loss = 0.57036288\n",
            "Iteration 25, loss = 0.57047366\n",
            "Iteration 26, loss = 0.56588292\n",
            "Iteration 27, loss = 0.56629097\n",
            "Iteration 28, loss = 0.56397192\n",
            "Iteration 29, loss = 0.56701197\n",
            "Iteration 30, loss = 0.56397427\n",
            "Iteration 31, loss = 0.56275005\n",
            "Iteration 32, loss = 0.56348476\n",
            "Iteration 33, loss = 0.56244563\n",
            "Iteration 34, loss = 0.56458794\n",
            "Iteration 35, loss = 0.56280081\n",
            "Iteration 36, loss = 0.56097776\n",
            "Iteration 37, loss = 0.56033751\n",
            "Iteration 38, loss = 0.56101923\n",
            "Iteration 39, loss = 0.56280109\n",
            "Iteration 40, loss = 0.56172789\n",
            "Iteration 41, loss = 0.56074830\n",
            "Iteration 42, loss = 0.56356352\n",
            "Iteration 43, loss = 0.55853861\n",
            "Iteration 44, loss = 0.56156506\n",
            "Iteration 45, loss = 0.56249952\n",
            "Iteration 46, loss = 0.55860604\n",
            "Iteration 47, loss = 0.56298902\n",
            "Iteration 48, loss = 0.56339105\n",
            "Iteration 49, loss = 0.56021806\n",
            "Iteration 50, loss = 0.56605166\n",
            "Iteration 51, loss = 0.55921522\n",
            "Iteration 52, loss = 0.55972021\n",
            "Iteration 53, loss = 0.56335214\n",
            "Iteration 54, loss = 0.56060939\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 55, loss = 0.51409741\n",
            "Iteration 56, loss = 0.50515867\n",
            "Iteration 57, loss = 0.49794740\n",
            "Iteration 58, loss = 0.49133859\n",
            "Iteration 59, loss = 0.48619141\n",
            "Iteration 60, loss = 0.48070660\n",
            "Iteration 61, loss = 0.47792784\n",
            "Iteration 62, loss = 0.47402318\n",
            "Iteration 63, loss = 0.47914593\n",
            "Iteration 64, loss = 0.48577594\n",
            "Iteration 65, loss = 0.49186179\n",
            "Iteration 66, loss = 0.49771875\n",
            "Iteration 67, loss = 0.50703861\n",
            "Iteration 68, loss = 0.50345093\n",
            "Iteration 69, loss = 0.51109772\n",
            "Iteration 70, loss = 0.49731137\n",
            "Iteration 71, loss = 0.51148456\n",
            "Iteration 72, loss = 0.48752472\n",
            "Iteration 73, loss = 0.51210594\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.43477341\n",
            "Iteration 75, loss = 0.43179424\n",
            "Iteration 76, loss = 0.42999661\n",
            "Iteration 77, loss = 0.42749945\n",
            "Iteration 78, loss = 0.42564028\n",
            "Iteration 79, loss = 0.42347808\n",
            "Iteration 80, loss = 0.42170910\n",
            "Iteration 81, loss = 0.41989736\n",
            "Iteration 82, loss = 0.41757171\n",
            "Iteration 83, loss = 0.41587829\n",
            "Iteration 84, loss = 0.41384064\n",
            "Iteration 85, loss = 0.41161325\n",
            "Iteration 86, loss = 0.40934163\n",
            "Iteration 87, loss = 0.40733151\n",
            "Iteration 88, loss = 0.40590662\n",
            "Iteration 89, loss = 0.40376573\n",
            "Iteration 90, loss = 0.40157985\n",
            "Iteration 91, loss = 0.39991192\n",
            "Iteration 92, loss = 0.39824015\n",
            "Iteration 93, loss = 0.39649617\n",
            "Iteration 94, loss = 0.39437122\n",
            "Iteration 95, loss = 0.39282574\n",
            "Iteration 96, loss = 0.39176907\n",
            "Iteration 97, loss = 0.38954714\n",
            "Iteration 98, loss = 0.38582342\n",
            "Iteration 99, loss = 0.37500162\n",
            "Iteration 100, loss = 0.37320169\n",
            "Iteration 101, loss = 0.37115773\n",
            "Iteration 102, loss = 0.36924118\n",
            "Iteration 103, loss = 0.36744286\n",
            "Iteration 104, loss = 0.36496416\n",
            "Iteration 105, loss = 0.36333037\n",
            "Iteration 106, loss = 0.36138051\n",
            "Iteration 107, loss = 0.35963183\n",
            "Iteration 108, loss = 0.35770263\n",
            "Iteration 109, loss = 0.35601101\n",
            "Iteration 110, loss = 0.35428665\n",
            "Iteration 111, loss = 0.35292517\n",
            "Iteration 112, loss = 0.35111732\n",
            "Iteration 113, loss = 0.34965931\n",
            "Iteration 114, loss = 0.34699945\n",
            "Iteration 115, loss = 0.34576372\n",
            "Iteration 116, loss = 0.34472137\n",
            "Iteration 117, loss = 0.34322975\n",
            "Iteration 118, loss = 0.34126338\n",
            "Iteration 119, loss = 0.33990128\n",
            "Iteration 120, loss = 0.33831487\n",
            "Iteration 121, loss = 0.33722490\n",
            "Iteration 122, loss = 0.33542885\n",
            "Iteration 123, loss = 0.33423156\n",
            "Iteration 124, loss = 0.33279054\n",
            "Iteration 125, loss = 0.33160808\n",
            "Iteration 126, loss = 0.33019726\n",
            "Iteration 127, loss = 0.32907320\n",
            "Iteration 128, loss = 0.32779172\n",
            "Iteration 129, loss = 0.32742972\n",
            "Iteration 130, loss = 0.32560703\n",
            "Iteration 131, loss = 0.32393722\n",
            "Iteration 132, loss = 0.32303225\n",
            "Iteration 133, loss = 0.32130961\n",
            "Iteration 134, loss = 0.32088195\n",
            "Iteration 135, loss = 0.31937601\n",
            "Iteration 136, loss = 0.31848344\n",
            "Iteration 137, loss = 0.31739219\n",
            "Iteration 138, loss = 0.31707196\n",
            "Iteration 139, loss = 0.31616727\n",
            "Iteration 140, loss = 0.31523562\n",
            "Iteration 141, loss = 0.31312783\n",
            "Iteration 142, loss = 0.31249959\n",
            "Iteration 143, loss = 0.31120597\n",
            "Iteration 144, loss = 0.31122282\n",
            "Iteration 145, loss = 0.30866617\n",
            "Iteration 146, loss = 0.30953337\n",
            "Iteration 147, loss = 0.30782651\n",
            "Iteration 148, loss = 0.30678131\n",
            "Iteration 149, loss = 0.30642526\n",
            "Iteration 150, loss = 0.30710086\n",
            "Iteration 151, loss = 0.30593062\n",
            "Iteration 152, loss = 0.30479632\n",
            "Iteration 153, loss = 0.30215201\n",
            "Iteration 154, loss = 0.30155958\n",
            "Iteration 155, loss = 0.30062404\n",
            "Iteration 156, loss = 0.30163881\n",
            "Iteration 157, loss = 0.30031543\n",
            "Iteration 158, loss = 0.29921130\n",
            "Iteration 159, loss = 0.30271906\n",
            "Iteration 160, loss = 0.29752429\n",
            "Iteration 161, loss = 0.29759064\n",
            "Iteration 162, loss = 0.29723816\n",
            "Iteration 163, loss = 0.29771607\n",
            "Iteration 164, loss = 0.29548055\n",
            "Iteration 165, loss = 0.29451635\n",
            "Iteration 166, loss = 0.29563887\n",
            "Iteration 167, loss = 0.29395868\n",
            "Iteration 168, loss = 0.29442919\n",
            "Iteration 169, loss = 0.29302707\n",
            "Iteration 170, loss = 0.29606884\n",
            "Iteration 171, loss = 0.29146584\n",
            "Iteration 172, loss = 0.28965401\n",
            "Iteration 173, loss = 0.29118622\n",
            "Iteration 174, loss = 0.29020581\n",
            "Iteration 175, loss = 0.28962875\n",
            "Iteration 176, loss = 0.29189097\n",
            "Iteration 177, loss = 0.28734922\n",
            "Iteration 178, loss = 0.28723017\n",
            "Iteration 179, loss = 0.28800152\n",
            "Iteration 180, loss = 0.28767715\n",
            "Iteration 181, loss = 0.28608905\n",
            "Iteration 182, loss = 0.28664736\n",
            "Iteration 183, loss = 0.28823060\n",
            "Iteration 184, loss = 0.28521584\n",
            "Iteration 185, loss = 0.28747835\n",
            "Iteration 186, loss = 0.28552105\n",
            "Iteration 187, loss = 0.28754321\n",
            "Iteration 188, loss = 0.28598792\n",
            "Iteration 189, loss = 0.28328565\n",
            "Iteration 190, loss = 0.28345726\n",
            "Iteration 191, loss = 0.28446855\n",
            "Iteration 192, loss = 0.28177561\n",
            "Iteration 193, loss = 0.28540599\n",
            "Iteration 194, loss = 0.28423562\n",
            "Iteration 195, loss = 0.27960013\n",
            "Iteration 196, loss = 0.28268612\n",
            "Iteration 197, loss = 0.28010361\n",
            "Iteration 198, loss = 0.28134783\n",
            "Iteration 199, loss = 0.28092305\n",
            "Iteration 200, loss = 0.28096927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66858269\n",
            "Iteration 3, loss = 0.65531807\n",
            "Iteration 4, loss = 0.64660230\n",
            "Iteration 5, loss = 0.63283213\n",
            "Iteration 6, loss = 0.62393410\n",
            "Iteration 7, loss = 0.61586300\n",
            "Iteration 8, loss = 0.60824812\n",
            "Iteration 9, loss = 0.60260618\n",
            "Iteration 10, loss = 0.60061993\n",
            "Iteration 11, loss = 0.59703910\n",
            "Iteration 12, loss = 0.59168774\n",
            "Iteration 13, loss = 0.58707849\n",
            "Iteration 14, loss = 0.58637120\n",
            "Iteration 15, loss = 0.58006630\n",
            "Iteration 16, loss = 0.58274297\n",
            "Iteration 17, loss = 0.57762454\n",
            "Iteration 18, loss = 0.57572911\n",
            "Iteration 19, loss = 0.57397982\n",
            "Iteration 20, loss = 0.57197956\n",
            "Iteration 21, loss = 0.57265544\n",
            "Iteration 22, loss = 0.56672345\n",
            "Iteration 23, loss = 0.56977268\n",
            "Iteration 24, loss = 0.56552696\n",
            "Iteration 25, loss = 0.56943409\n",
            "Iteration 26, loss = 0.56446831\n",
            "Iteration 27, loss = 0.56308487\n",
            "Iteration 28, loss = 0.56896908\n",
            "Iteration 29, loss = 0.56633428\n",
            "Iteration 30, loss = 0.56447661\n",
            "Iteration 31, loss = 0.56372467\n",
            "Iteration 32, loss = 0.56579155\n",
            "Iteration 33, loss = 0.56471131\n",
            "Iteration 34, loss = 0.56066889\n",
            "Iteration 35, loss = 0.56266970\n",
            "Iteration 36, loss = 0.56149228\n",
            "Iteration 37, loss = 0.55952732\n",
            "Iteration 38, loss = 0.56061236\n",
            "Iteration 39, loss = 0.56051891\n",
            "Iteration 40, loss = 0.56197083\n",
            "Iteration 41, loss = 0.56015874\n",
            "Iteration 42, loss = 0.55979393\n",
            "Iteration 43, loss = 0.55824608\n",
            "Iteration 44, loss = 0.56012890\n",
            "Iteration 45, loss = 0.56059168\n",
            "Iteration 46, loss = 0.56083756\n",
            "Iteration 47, loss = 0.55825857\n",
            "Iteration 48, loss = 0.56214967\n",
            "Iteration 49, loss = 0.56024890\n",
            "Iteration 50, loss = 0.55858664\n",
            "Iteration 51, loss = 0.56479807\n",
            "Iteration 52, loss = 0.55596913\n",
            "Iteration 53, loss = 0.55879593\n",
            "Iteration 54, loss = 0.56021777\n",
            "Iteration 55, loss = 0.55700857\n",
            "Iteration 56, loss = 0.55848570\n",
            "Iteration 57, loss = 0.55977874\n",
            "Iteration 58, loss = 0.55853875\n",
            "Iteration 59, loss = 0.55979848\n",
            "Iteration 60, loss = 0.55948790\n",
            "Iteration 61, loss = 0.55790072\n",
            "Iteration 62, loss = 0.55990576\n",
            "Iteration 63, loss = 0.55995422\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 64, loss = 0.51480988\n",
            "Iteration 65, loss = 0.50162441\n",
            "Iteration 66, loss = 0.49358529\n",
            "Iteration 67, loss = 0.48426895\n",
            "Iteration 68, loss = 0.47589254\n",
            "Iteration 69, loss = 0.47058020\n",
            "Iteration 70, loss = 0.46299346\n",
            "Iteration 71, loss = 0.47061942\n",
            "Iteration 72, loss = 0.47106829\n",
            "Iteration 73, loss = 0.49133968\n",
            "Iteration 74, loss = 0.50365503\n",
            "Iteration 75, loss = 0.49562558\n",
            "Iteration 76, loss = 0.50219576\n",
            "Iteration 77, loss = 0.49406710\n",
            "Iteration 78, loss = 0.50943760\n",
            "Iteration 79, loss = 0.49019091\n",
            "Iteration 80, loss = 0.50401730\n",
            "Iteration 81, loss = 0.49755119\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.42418962\n",
            "Iteration 83, loss = 0.41977315\n",
            "Iteration 84, loss = 0.41710144\n",
            "Iteration 85, loss = 0.41452038\n",
            "Iteration 86, loss = 0.41241655\n",
            "Iteration 87, loss = 0.40956644\n",
            "Iteration 88, loss = 0.40719798\n",
            "Iteration 89, loss = 0.40487015\n",
            "Iteration 90, loss = 0.40245288\n",
            "Iteration 91, loss = 0.40012970\n",
            "Iteration 92, loss = 0.39778738\n",
            "Iteration 93, loss = 0.39540725\n",
            "Iteration 94, loss = 0.39318009\n",
            "Iteration 95, loss = 0.39115029\n",
            "Iteration 96, loss = 0.38851091\n",
            "Iteration 97, loss = 0.38630802\n",
            "Iteration 98, loss = 0.38454531\n",
            "Iteration 99, loss = 0.38254418\n",
            "Iteration 100, loss = 0.38005841\n",
            "Iteration 101, loss = 0.37825024\n",
            "Iteration 102, loss = 0.37620382\n",
            "Iteration 103, loss = 0.37394806\n",
            "Iteration 104, loss = 0.37205733\n",
            "Iteration 105, loss = 0.36968570\n",
            "Iteration 106, loss = 0.36826486\n",
            "Iteration 107, loss = 0.36674224\n",
            "Iteration 108, loss = 0.36464693\n",
            "Iteration 109, loss = 0.36277899\n",
            "Iteration 110, loss = 0.36090959\n",
            "Iteration 111, loss = 0.35869059\n",
            "Iteration 112, loss = 0.35723915\n",
            "Iteration 113, loss = 0.35574179\n",
            "Iteration 114, loss = 0.35391943\n",
            "Iteration 115, loss = 0.35181854\n",
            "Iteration 116, loss = 0.35101098\n",
            "Iteration 117, loss = 0.34908580\n",
            "Iteration 118, loss = 0.34713454\n",
            "Iteration 119, loss = 0.34582464\n",
            "Iteration 120, loss = 0.34463130\n",
            "Iteration 121, loss = 0.34230611\n",
            "Iteration 122, loss = 0.34171102\n",
            "Iteration 123, loss = 0.33954741\n",
            "Iteration 124, loss = 0.33880351\n",
            "Iteration 125, loss = 0.33674082\n",
            "Iteration 126, loss = 0.33532147\n",
            "Iteration 127, loss = 0.33481516\n",
            "Iteration 128, loss = 0.33285002\n",
            "Iteration 129, loss = 0.33158619\n",
            "Iteration 130, loss = 0.32995370\n",
            "Iteration 131, loss = 0.32865030\n",
            "Iteration 132, loss = 0.32740491\n",
            "Iteration 133, loss = 0.32696103\n",
            "Iteration 134, loss = 0.32520096\n",
            "Iteration 135, loss = 0.32428057\n",
            "Iteration 136, loss = 0.32326887\n",
            "Iteration 137, loss = 0.32234647\n",
            "Iteration 138, loss = 0.32110542\n",
            "Iteration 139, loss = 0.31978314\n",
            "Iteration 140, loss = 0.31851128\n",
            "Iteration 141, loss = 0.31797340\n",
            "Iteration 142, loss = 0.31697851\n",
            "Iteration 143, loss = 0.31585187\n",
            "Iteration 144, loss = 0.31386669\n",
            "Iteration 145, loss = 0.31277812\n",
            "Iteration 146, loss = 0.31150562\n",
            "Iteration 147, loss = 0.31134167\n",
            "Iteration 148, loss = 0.31093546\n",
            "Iteration 149, loss = 0.30805672\n",
            "Iteration 150, loss = 0.30809020\n",
            "Iteration 151, loss = 0.30769723\n",
            "Iteration 152, loss = 0.30662096\n",
            "Iteration 153, loss = 0.30490078\n",
            "Iteration 154, loss = 0.30469680\n",
            "Iteration 155, loss = 0.30559760\n",
            "Iteration 156, loss = 0.30399701\n",
            "Iteration 157, loss = 0.30354993\n",
            "Iteration 158, loss = 0.30282653\n",
            "Iteration 159, loss = 0.30145946\n",
            "Iteration 160, loss = 0.30117672\n",
            "Iteration 161, loss = 0.30112804\n",
            "Iteration 162, loss = 0.29801730\n",
            "Iteration 163, loss = 0.29828369\n",
            "Iteration 164, loss = 0.29769322\n",
            "Iteration 165, loss = 0.29704963\n",
            "Iteration 166, loss = 0.29584129\n",
            "Iteration 167, loss = 0.29509495\n",
            "Iteration 168, loss = 0.29424526\n",
            "Iteration 169, loss = 0.29511026\n",
            "Iteration 170, loss = 0.29441146\n",
            "Iteration 171, loss = 0.29323555\n",
            "Iteration 172, loss = 0.29146374\n",
            "Iteration 173, loss = 0.29279899\n",
            "Iteration 174, loss = 0.29184263\n",
            "Iteration 175, loss = 0.29219903\n",
            "Iteration 176, loss = 0.29071153\n",
            "Iteration 177, loss = 0.28840720\n",
            "Iteration 178, loss = 0.28885427\n",
            "Iteration 179, loss = 0.29016421\n",
            "Iteration 180, loss = 0.28780268\n",
            "Iteration 181, loss = 0.28821976\n",
            "Iteration 182, loss = 0.28714161\n",
            "Iteration 183, loss = 0.28695840\n",
            "Iteration 184, loss = 0.28496750\n",
            "Iteration 185, loss = 0.28638077\n",
            "Iteration 186, loss = 0.28446922\n",
            "Iteration 187, loss = 0.28483927\n",
            "Iteration 188, loss = 0.28559238\n",
            "Iteration 189, loss = 0.28453583\n",
            "Iteration 190, loss = 0.28238994\n",
            "Iteration 191, loss = 0.28299952\n",
            "Iteration 192, loss = 0.28330920\n",
            "Iteration 193, loss = 0.28362181\n",
            "Iteration 194, loss = 0.28383728\n",
            "Iteration 195, loss = 0.28230611\n",
            "Iteration 196, loss = 0.28208499\n",
            "Iteration 197, loss = 0.27955926\n",
            "Iteration 198, loss = 0.28098144\n",
            "Iteration 199, loss = 0.28064357\n",
            "Iteration 200, loss = 0.27983577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68496301\n",
            "Iteration 3, loss = 0.66811132\n",
            "Iteration 4, loss = 0.65802209\n",
            "Iteration 5, loss = 0.64464277\n",
            "Iteration 6, loss = 0.63580329\n",
            "Iteration 7, loss = 0.62767169\n",
            "Iteration 8, loss = 0.61734330\n",
            "Iteration 9, loss = 0.61503315\n",
            "Iteration 10, loss = 0.60885466\n",
            "Iteration 11, loss = 0.60420749\n",
            "Iteration 12, loss = 0.59899843\n",
            "Iteration 13, loss = 0.59571055\n",
            "Iteration 14, loss = 0.59177367\n",
            "Iteration 15, loss = 0.59047504\n",
            "Iteration 16, loss = 0.58571652\n",
            "Iteration 17, loss = 0.58411487\n",
            "Iteration 18, loss = 0.57894674\n",
            "Iteration 19, loss = 0.58007064\n",
            "Iteration 20, loss = 0.57972174\n",
            "Iteration 21, loss = 0.57712814\n",
            "Iteration 22, loss = 0.57746369\n",
            "Iteration 23, loss = 0.57092308\n",
            "Iteration 24, loss = 0.57489236\n",
            "Iteration 25, loss = 0.57397077\n",
            "Iteration 26, loss = 0.57277798\n",
            "Iteration 27, loss = 0.56870217\n",
            "Iteration 28, loss = 0.56964131\n",
            "Iteration 29, loss = 0.56854938\n",
            "Iteration 30, loss = 0.62896018\n",
            "Iteration 31, loss = 0.60664291\n",
            "Iteration 32, loss = 0.59638983\n",
            "Iteration 33, loss = 0.59057857\n",
            "Iteration 34, loss = 0.58583979\n",
            "Iteration 35, loss = 0.58302543\n",
            "Iteration 36, loss = 0.57735749\n",
            "Iteration 37, loss = 0.57508174\n",
            "Iteration 38, loss = 0.57391707\n",
            "Iteration 39, loss = 0.57123852\n",
            "Iteration 40, loss = 0.57077588\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 41, loss = 0.55468519\n",
            "Iteration 42, loss = 0.55143796\n",
            "Iteration 43, loss = 0.54985817\n",
            "Iteration 44, loss = 0.54856214\n",
            "Iteration 45, loss = 0.54632702\n",
            "Iteration 46, loss = 0.54457650\n",
            "Iteration 47, loss = 0.54306601\n",
            "Iteration 48, loss = 0.54100761\n",
            "Iteration 49, loss = 0.53862957\n",
            "Iteration 50, loss = 0.53714165\n",
            "Iteration 51, loss = 0.53364165\n",
            "Iteration 52, loss = 0.53187769\n",
            "Iteration 53, loss = 0.53012694\n",
            "Iteration 54, loss = 0.52681536\n",
            "Iteration 55, loss = 0.52475920\n",
            "Iteration 56, loss = 0.52586860\n",
            "Iteration 57, loss = 0.52377666\n",
            "Iteration 58, loss = 0.52664270\n",
            "Iteration 59, loss = 0.52394037\n",
            "Iteration 60, loss = 0.52490804\n",
            "Iteration 61, loss = 0.52584000\n",
            "Iteration 62, loss = 0.53366157\n",
            "Iteration 63, loss = 0.52907766\n",
            "Iteration 64, loss = 0.52382575\n",
            "Iteration 65, loss = 0.53458400\n",
            "Iteration 66, loss = 0.53315129\n",
            "Iteration 67, loss = 0.52772015\n",
            "Iteration 68, loss = 0.52776715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 69, loss = 0.48598943\n",
            "Iteration 70, loss = 0.48399165\n",
            "Iteration 71, loss = 0.48244974\n",
            "Iteration 72, loss = 0.48106253\n",
            "Iteration 73, loss = 0.47920638\n",
            "Iteration 74, loss = 0.47796830\n",
            "Iteration 75, loss = 0.47649046\n",
            "Iteration 76, loss = 0.47536365\n",
            "Iteration 77, loss = 0.47333403\n",
            "Iteration 78, loss = 0.47210859\n",
            "Iteration 79, loss = 0.47073124\n",
            "Iteration 80, loss = 0.46914745\n",
            "Iteration 81, loss = 0.46764451\n",
            "Iteration 82, loss = 0.46649554\n",
            "Iteration 83, loss = 0.46489764\n",
            "Iteration 84, loss = 0.46320592\n",
            "Iteration 85, loss = 0.46176979\n",
            "Iteration 86, loss = 0.46053701\n",
            "Iteration 87, loss = 0.45867580\n",
            "Iteration 88, loss = 0.45727053\n",
            "Iteration 89, loss = 0.45590180\n",
            "Iteration 90, loss = 0.45409664\n",
            "Iteration 91, loss = 0.45291474\n",
            "Iteration 92, loss = 0.45122699\n",
            "Iteration 93, loss = 0.44971179\n",
            "Iteration 94, loss = 0.44834208\n",
            "Iteration 95, loss = 0.44704328\n",
            "Iteration 96, loss = 0.44568783\n",
            "Iteration 97, loss = 0.44394885\n",
            "Iteration 98, loss = 0.44237706\n",
            "Iteration 99, loss = 0.44108207\n",
            "Iteration 100, loss = 0.44040671\n",
            "Iteration 101, loss = 0.43851078\n",
            "Iteration 102, loss = 0.43744559\n",
            "Iteration 103, loss = 0.43571706\n",
            "Iteration 104, loss = 0.43393873\n",
            "Iteration 105, loss = 0.43260425\n",
            "Iteration 106, loss = 0.43040681\n",
            "Iteration 107, loss = 0.42992915\n",
            "Iteration 108, loss = 0.42765045\n",
            "Iteration 109, loss = 0.42715203\n",
            "Iteration 110, loss = 0.42568039\n",
            "Iteration 111, loss = 0.42489925\n",
            "Iteration 112, loss = 0.42281121\n",
            "Iteration 113, loss = 0.42120798\n",
            "Iteration 114, loss = 0.42093857\n",
            "Iteration 115, loss = 0.42017220\n",
            "Iteration 116, loss = 0.41744398\n",
            "Iteration 117, loss = 0.41588136\n",
            "Iteration 118, loss = 0.41597784\n",
            "Iteration 119, loss = 0.41578604\n",
            "Iteration 120, loss = 0.41284838\n",
            "Iteration 121, loss = 0.41160095\n",
            "Iteration 122, loss = 0.41144218\n",
            "Iteration 123, loss = 0.40984521\n",
            "Iteration 124, loss = 0.40871263\n",
            "Iteration 125, loss = 0.40640973\n",
            "Iteration 126, loss = 0.40811853\n",
            "Iteration 127, loss = 0.40520725\n",
            "Iteration 128, loss = 0.40506158\n",
            "Iteration 129, loss = 0.40379605\n",
            "Iteration 130, loss = 0.40353409\n",
            "Iteration 131, loss = 0.40163767\n",
            "Iteration 132, loss = 0.40173809\n",
            "Iteration 133, loss = 0.40035832\n",
            "Iteration 134, loss = 0.39864411\n",
            "Iteration 135, loss = 0.39902628\n",
            "Iteration 136, loss = 0.39806256\n",
            "Iteration 137, loss = 0.39699191\n",
            "Iteration 138, loss = 0.39679394\n",
            "Iteration 139, loss = 0.39430046\n",
            "Iteration 140, loss = 0.39668118\n",
            "Iteration 141, loss = 0.39481892\n",
            "Iteration 142, loss = 0.39284844\n",
            "Iteration 143, loss = 0.39561052\n",
            "Iteration 144, loss = 0.39465452\n",
            "Iteration 145, loss = 0.39225701\n",
            "Iteration 146, loss = 0.38827187\n",
            "Iteration 147, loss = 0.39190231\n",
            "Iteration 148, loss = 0.38977715\n",
            "Iteration 149, loss = 0.38962740\n",
            "Iteration 150, loss = 0.39042554\n",
            "Iteration 151, loss = 0.39327983\n",
            "Iteration 152, loss = 0.39172126\n",
            "Iteration 153, loss = 0.38800493\n",
            "Iteration 154, loss = 0.39146048\n",
            "Iteration 155, loss = 0.38257944\n",
            "Iteration 156, loss = 0.39266105\n",
            "Iteration 157, loss = 0.38704743\n",
            "Iteration 158, loss = 0.38597405\n",
            "Iteration 159, loss = 0.38588668\n",
            "Iteration 160, loss = 0.38578480\n",
            "Iteration 161, loss = 0.39317286\n",
            "Iteration 162, loss = 0.38814817\n",
            "Iteration 163, loss = 0.38252674\n",
            "Iteration 164, loss = 0.38711487\n",
            "Iteration 165, loss = 0.38947683\n",
            "Iteration 166, loss = 0.39329454\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 167, loss = 0.36096102\n",
            "Iteration 168, loss = 0.36065911\n",
            "Iteration 169, loss = 0.36041914\n",
            "Iteration 170, loss = 0.36024748\n",
            "Iteration 171, loss = 0.36002946\n",
            "Iteration 172, loss = 0.35970487\n",
            "Iteration 173, loss = 0.35973503\n",
            "Iteration 174, loss = 0.35918690\n",
            "Iteration 175, loss = 0.35928957\n",
            "Iteration 176, loss = 0.35903258\n",
            "Iteration 177, loss = 0.35883405\n",
            "Iteration 178, loss = 0.35874345\n",
            "Iteration 179, loss = 0.35838728\n",
            "Iteration 180, loss = 0.35825695\n",
            "Iteration 181, loss = 0.35825731\n",
            "Iteration 182, loss = 0.35805868\n",
            "Iteration 183, loss = 0.35778994\n",
            "Iteration 184, loss = 0.35767313\n",
            "Iteration 185, loss = 0.35740389\n",
            "Iteration 186, loss = 0.35740099\n",
            "Iteration 187, loss = 0.35714967\n",
            "Iteration 188, loss = 0.35692417\n",
            "Iteration 189, loss = 0.35678797\n",
            "Iteration 190, loss = 0.35665002\n",
            "Iteration 191, loss = 0.35645819\n",
            "Iteration 192, loss = 0.35637025\n",
            "Iteration 193, loss = 0.35609177\n",
            "Iteration 194, loss = 0.35592990\n",
            "Iteration 195, loss = 0.35573004\n",
            "Iteration 196, loss = 0.35561311\n",
            "Iteration 197, loss = 0.35543225\n",
            "Iteration 198, loss = 0.35517234\n",
            "Iteration 199, loss = 0.35507286\n",
            "Iteration 200, loss = 0.35508114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71961951\n",
            "Iteration 3, loss = 0.70414805\n",
            "Iteration 4, loss = 0.68755132\n",
            "Iteration 5, loss = 0.66333475\n",
            "Iteration 6, loss = 0.64599033\n",
            "Iteration 7, loss = 0.63418133\n",
            "Iteration 8, loss = 0.62307018\n",
            "Iteration 9, loss = 0.61447421\n",
            "Iteration 10, loss = 0.60786297\n",
            "Iteration 11, loss = 0.60226148\n",
            "Iteration 12, loss = 0.59427911\n",
            "Iteration 13, loss = 0.59118953\n",
            "Iteration 14, loss = 0.58818004\n",
            "Iteration 15, loss = 0.58398175\n",
            "Iteration 16, loss = 0.57944966\n",
            "Iteration 17, loss = 0.58307533\n",
            "Iteration 18, loss = 0.57293848\n",
            "Iteration 19, loss = 0.57442983\n",
            "Iteration 20, loss = 0.57164182\n",
            "Iteration 21, loss = 0.57027955\n",
            "Iteration 22, loss = 0.56830107\n",
            "Iteration 23, loss = 0.56710075\n",
            "Iteration 24, loss = 0.56751148\n",
            "Iteration 25, loss = 0.56501046\n",
            "Iteration 26, loss = 0.56386599\n",
            "Iteration 27, loss = 0.56514780\n",
            "Iteration 28, loss = 0.56201734\n",
            "Iteration 29, loss = 0.56295800\n",
            "Iteration 30, loss = 0.56091853\n",
            "Iteration 31, loss = 0.56115012\n",
            "Iteration 32, loss = 0.55746733\n",
            "Iteration 33, loss = 0.56248115\n",
            "Iteration 34, loss = 0.55950056\n",
            "Iteration 35, loss = 0.55832471\n",
            "Iteration 36, loss = 0.56149519\n",
            "Iteration 37, loss = 0.55681462\n",
            "Iteration 38, loss = 0.56003282\n",
            "Iteration 39, loss = 0.56163845\n",
            "Iteration 40, loss = 0.56380868\n",
            "Iteration 41, loss = 0.55728122\n",
            "Iteration 42, loss = 0.55695776\n",
            "Iteration 43, loss = 0.55674681\n",
            "Iteration 44, loss = 0.55975901\n",
            "Iteration 45, loss = 0.56042418\n",
            "Iteration 46, loss = 0.55785759\n",
            "Iteration 47, loss = 0.56140516\n",
            "Iteration 48, loss = 0.55943850\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.52093412\n",
            "Iteration 50, loss = 0.50963539\n",
            "Iteration 51, loss = 0.50455134\n",
            "Iteration 52, loss = 0.49769735\n",
            "Iteration 53, loss = 0.49227540\n",
            "Iteration 54, loss = 0.48560998\n",
            "Iteration 55, loss = 0.48121409\n",
            "Iteration 56, loss = 0.47872430\n",
            "Iteration 57, loss = 0.47761334\n",
            "Iteration 58, loss = 0.50336808\n",
            "Iteration 59, loss = 0.50331381\n",
            "Iteration 60, loss = 0.50917559\n",
            "Iteration 61, loss = 0.50979256\n",
            "Iteration 62, loss = 0.51456992\n",
            "Iteration 63, loss = 0.50221905\n",
            "Iteration 64, loss = 0.51367147\n",
            "Iteration 65, loss = 0.51052074\n",
            "Iteration 66, loss = 0.50289991\n",
            "Iteration 67, loss = 0.50884087\n",
            "Iteration 68, loss = 0.51332134\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 69, loss = 0.43680703\n",
            "Iteration 70, loss = 0.43326781\n",
            "Iteration 71, loss = 0.43070395\n",
            "Iteration 72, loss = 0.42786219\n",
            "Iteration 73, loss = 0.42531595\n",
            "Iteration 74, loss = 0.42242015\n",
            "Iteration 75, loss = 0.41979496\n",
            "Iteration 76, loss = 0.41737966\n",
            "Iteration 77, loss = 0.41504038\n",
            "Iteration 78, loss = 0.41199194\n",
            "Iteration 79, loss = 0.40969168\n",
            "Iteration 80, loss = 0.40710917\n",
            "Iteration 81, loss = 0.40526214\n",
            "Iteration 82, loss = 0.40222647\n",
            "Iteration 83, loss = 0.40003025\n",
            "Iteration 84, loss = 0.39734592\n",
            "Iteration 85, loss = 0.39494041\n",
            "Iteration 86, loss = 0.39249663\n",
            "Iteration 87, loss = 0.39075041\n",
            "Iteration 88, loss = 0.38806752\n",
            "Iteration 89, loss = 0.38562853\n",
            "Iteration 90, loss = 0.38385876\n",
            "Iteration 91, loss = 0.38159727\n",
            "Iteration 92, loss = 0.37928171\n",
            "Iteration 93, loss = 0.37718747\n",
            "Iteration 94, loss = 0.37486115\n",
            "Iteration 95, loss = 0.37294093\n",
            "Iteration 96, loss = 0.37100200\n",
            "Iteration 97, loss = 0.36884228\n",
            "Iteration 98, loss = 0.36665806\n",
            "Iteration 99, loss = 0.36450725\n",
            "Iteration 100, loss = 0.36286092\n",
            "Iteration 101, loss = 0.36089096\n",
            "Iteration 102, loss = 0.35965414\n",
            "Iteration 103, loss = 0.35825277\n",
            "Iteration 104, loss = 0.35605925\n",
            "Iteration 105, loss = 0.35386726\n",
            "Iteration 106, loss = 0.35224587\n",
            "Iteration 107, loss = 0.35036119\n",
            "Iteration 108, loss = 0.34869823\n",
            "Iteration 109, loss = 0.34644209\n",
            "Iteration 110, loss = 0.34536821\n",
            "Iteration 111, loss = 0.34375099\n",
            "Iteration 112, loss = 0.34151547\n",
            "Iteration 113, loss = 0.34008430\n",
            "Iteration 114, loss = 0.33817810\n",
            "Iteration 115, loss = 0.33748191\n",
            "Iteration 116, loss = 0.33629687\n",
            "Iteration 117, loss = 0.33410792\n",
            "Iteration 118, loss = 0.33379488\n",
            "Iteration 119, loss = 0.33237487\n",
            "Iteration 120, loss = 0.33003574\n",
            "Iteration 121, loss = 0.32902692\n",
            "Iteration 122, loss = 0.32703756\n",
            "Iteration 123, loss = 0.32583089\n",
            "Iteration 124, loss = 0.32524215\n",
            "Iteration 125, loss = 0.32502576\n",
            "Iteration 126, loss = 0.32331152\n",
            "Iteration 127, loss = 0.32177731\n",
            "Iteration 128, loss = 0.32102827\n",
            "Iteration 129, loss = 0.31951261\n",
            "Iteration 130, loss = 0.31841675\n",
            "Iteration 131, loss = 0.31612916\n",
            "Iteration 132, loss = 0.31607948\n",
            "Iteration 133, loss = 0.31576175\n",
            "Iteration 134, loss = 0.31454448\n",
            "Iteration 135, loss = 0.31251814\n",
            "Iteration 136, loss = 0.31124524\n",
            "Iteration 137, loss = 0.31248308\n",
            "Iteration 138, loss = 0.31030104\n",
            "Iteration 139, loss = 0.30980397\n",
            "Iteration 140, loss = 0.31099691\n",
            "Iteration 141, loss = 0.30870902\n",
            "Iteration 142, loss = 0.30637837\n",
            "Iteration 143, loss = 0.30691655\n",
            "Iteration 144, loss = 0.30561417\n",
            "Iteration 145, loss = 0.30633535\n",
            "Iteration 146, loss = 0.30316904\n",
            "Iteration 147, loss = 0.30208266\n",
            "Iteration 148, loss = 0.30195181\n",
            "Iteration 149, loss = 0.30092561\n",
            "Iteration 150, loss = 0.29930067\n",
            "Iteration 151, loss = 0.29853623\n",
            "Iteration 152, loss = 0.29880951\n",
            "Iteration 153, loss = 0.29828914\n",
            "Iteration 154, loss = 0.29766624\n",
            "Iteration 155, loss = 0.29643700\n",
            "Iteration 156, loss = 0.29737267\n",
            "Iteration 157, loss = 0.29556887\n",
            "Iteration 158, loss = 0.29492228\n",
            "Iteration 159, loss = 0.29499107\n",
            "Iteration 160, loss = 0.29220185\n",
            "Iteration 161, loss = 0.29246763\n",
            "Iteration 162, loss = 0.29429505\n",
            "Iteration 163, loss = 0.29347041\n",
            "Iteration 164, loss = 0.29073789\n",
            "Iteration 165, loss = 0.29468886\n",
            "Iteration 166, loss = 0.28869673\n",
            "Iteration 167, loss = 0.29086379\n",
            "Iteration 168, loss = 0.29083451\n",
            "Iteration 169, loss = 0.29171203\n",
            "Iteration 170, loss = 0.28756308\n",
            "Iteration 171, loss = 0.29137785\n",
            "Iteration 172, loss = 0.28860481\n",
            "Iteration 173, loss = 0.28912279\n",
            "Iteration 174, loss = 0.29112739\n",
            "Iteration 175, loss = 0.28528510\n",
            "Iteration 176, loss = 0.29147729\n",
            "Iteration 177, loss = 0.28530564\n",
            "Iteration 178, loss = 0.28383343\n",
            "Iteration 179, loss = 0.28692372\n",
            "Iteration 180, loss = 0.29050580\n",
            "Iteration 181, loss = 0.28287071\n",
            "Iteration 182, loss = 0.28396929\n",
            "Iteration 183, loss = 0.28325509\n",
            "Iteration 184, loss = 0.28741347\n",
            "Iteration 185, loss = 0.28617387\n",
            "Iteration 186, loss = 0.28624969\n",
            "Iteration 187, loss = 0.28526917\n",
            "Iteration 188, loss = 0.28217270\n",
            "Iteration 189, loss = 0.28092530\n",
            "Iteration 190, loss = 0.28661496\n",
            "Iteration 191, loss = 0.28391174\n",
            "Iteration 192, loss = 0.28181540\n",
            "Iteration 193, loss = 0.28256611\n",
            "Iteration 194, loss = 0.27993609\n",
            "Iteration 195, loss = 0.28763083\n",
            "Iteration 196, loss = 0.28518998\n",
            "Iteration 197, loss = 0.28558999\n",
            "Iteration 198, loss = 0.28420777\n",
            "Iteration 199, loss = 0.27654705\n",
            "Iteration 200, loss = 0.28863119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66900176\n",
            "Iteration 3, loss = 0.65610256\n",
            "Iteration 4, loss = 0.64403004\n",
            "Iteration 5, loss = 0.63348196\n",
            "Iteration 6, loss = 0.62262908\n",
            "Iteration 7, loss = 0.61852409\n",
            "Iteration 8, loss = 0.61153816\n",
            "Iteration 9, loss = 0.60449835\n",
            "Iteration 10, loss = 0.59724713\n",
            "Iteration 11, loss = 0.59591890\n",
            "Iteration 12, loss = 0.59216306\n",
            "Iteration 13, loss = 0.58587362\n",
            "Iteration 14, loss = 0.58444611\n",
            "Iteration 15, loss = 0.58120203\n",
            "Iteration 16, loss = 0.57899905\n",
            "Iteration 17, loss = 0.57897701\n",
            "Iteration 18, loss = 0.57074935\n",
            "Iteration 19, loss = 0.57256248\n",
            "Iteration 20, loss = 0.57375879\n",
            "Iteration 21, loss = 0.57013306\n",
            "Iteration 22, loss = 0.57295952\n",
            "Iteration 23, loss = 0.56919091\n",
            "Iteration 24, loss = 0.56806615\n",
            "Iteration 25, loss = 0.56757733\n",
            "Iteration 26, loss = 0.56551522\n",
            "Iteration 27, loss = 0.56353088\n",
            "Iteration 28, loss = 0.56490586\n",
            "Iteration 29, loss = 0.56278749\n",
            "Iteration 30, loss = 0.56403570\n",
            "Iteration 31, loss = 0.56645701\n",
            "Iteration 32, loss = 0.56951152\n",
            "Iteration 33, loss = 0.57364153\n",
            "Iteration 34, loss = 0.57251605\n",
            "Iteration 35, loss = 0.56380405\n",
            "Iteration 36, loss = 0.56734074\n",
            "Iteration 37, loss = 0.56640202\n",
            "Iteration 38, loss = 0.56186067\n",
            "Iteration 39, loss = 0.56233941\n",
            "Iteration 40, loss = 0.56384844\n",
            "Iteration 41, loss = 0.56264045\n",
            "Iteration 42, loss = 0.56458282\n",
            "Iteration 43, loss = 0.56347039\n",
            "Iteration 44, loss = 0.56014896\n",
            "Iteration 45, loss = 0.56332688\n",
            "Iteration 46, loss = 0.56245376\n",
            "Iteration 47, loss = 0.56369040\n",
            "Iteration 48, loss = 0.56174647\n",
            "Iteration 49, loss = 0.55972552\n",
            "Iteration 50, loss = 0.56460547\n",
            "Iteration 51, loss = 0.56031315\n",
            "Iteration 52, loss = 0.55986454\n",
            "Iteration 53, loss = 0.55814285\n",
            "Iteration 54, loss = 0.55963261\n",
            "Iteration 55, loss = 0.55794938\n",
            "Iteration 56, loss = 0.55843412\n",
            "Iteration 57, loss = 0.56100602\n",
            "Iteration 58, loss = 0.55854382\n",
            "Iteration 59, loss = 0.56335568\n",
            "Iteration 60, loss = 0.55985090\n",
            "Iteration 61, loss = 0.55790970\n",
            "Iteration 62, loss = 0.55955384\n",
            "Iteration 63, loss = 0.55926767\n",
            "Iteration 64, loss = 0.55914228\n",
            "Iteration 65, loss = 0.55921658\n",
            "Iteration 66, loss = 0.55823579\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 67, loss = 0.51482074\n",
            "Iteration 68, loss = 0.50236574\n",
            "Iteration 69, loss = 0.49323811\n",
            "Iteration 70, loss = 0.48659529\n",
            "Iteration 71, loss = 0.47804957\n",
            "Iteration 72, loss = 0.47375144\n",
            "Iteration 73, loss = 0.46866888\n",
            "Iteration 74, loss = 0.47777996\n",
            "Iteration 75, loss = 0.47837175\n",
            "Iteration 76, loss = 0.49570406\n",
            "Iteration 77, loss = 0.50008190\n",
            "Iteration 78, loss = 0.49325294\n",
            "Iteration 79, loss = 0.49733749\n",
            "Iteration 80, loss = 0.50719840\n",
            "Iteration 81, loss = 0.49914612\n",
            "Iteration 82, loss = 0.51352502\n",
            "Iteration 83, loss = 0.50624015\n",
            "Iteration 84, loss = 0.49814191\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 85, loss = 0.42876308\n",
            "Iteration 86, loss = 0.42446014\n",
            "Iteration 87, loss = 0.42200709\n",
            "Iteration 88, loss = 0.41974994\n",
            "Iteration 89, loss = 0.41695307\n",
            "Iteration 90, loss = 0.41471213\n",
            "Iteration 91, loss = 0.41190140\n",
            "Iteration 92, loss = 0.40927984\n",
            "Iteration 93, loss = 0.40710072\n",
            "Iteration 94, loss = 0.40438317\n",
            "Iteration 95, loss = 0.40194031\n",
            "Iteration 96, loss = 0.39988382\n",
            "Iteration 97, loss = 0.39771063\n",
            "Iteration 98, loss = 0.39504659\n",
            "Iteration 99, loss = 0.39264553\n",
            "Iteration 100, loss = 0.39079129\n",
            "Iteration 101, loss = 0.38871674\n",
            "Iteration 102, loss = 0.38641140\n",
            "Iteration 103, loss = 0.38442926\n",
            "Iteration 104, loss = 0.38202121\n",
            "Iteration 105, loss = 0.38071916\n",
            "Iteration 106, loss = 0.37826443\n",
            "Iteration 107, loss = 0.37599529\n",
            "Iteration 108, loss = 0.37428319\n",
            "Iteration 109, loss = 0.37191295\n",
            "Iteration 110, loss = 0.36991457\n",
            "Iteration 111, loss = 0.36804571\n",
            "Iteration 112, loss = 0.36622533\n",
            "Iteration 113, loss = 0.36452675\n",
            "Iteration 114, loss = 0.36268736\n",
            "Iteration 115, loss = 0.36095801\n",
            "Iteration 116, loss = 0.35914598\n",
            "Iteration 117, loss = 0.35714971\n",
            "Iteration 118, loss = 0.35589125\n",
            "Iteration 119, loss = 0.35362244\n",
            "Iteration 120, loss = 0.35255357\n",
            "Iteration 121, loss = 0.35008258\n",
            "Iteration 122, loss = 0.34936284\n",
            "Iteration 123, loss = 0.34762727\n",
            "Iteration 124, loss = 0.34650743\n",
            "Iteration 125, loss = 0.34428807\n",
            "Iteration 126, loss = 0.34284675\n",
            "Iteration 127, loss = 0.34206373\n",
            "Iteration 128, loss = 0.34012259\n",
            "Iteration 129, loss = 0.33853849\n",
            "Iteration 130, loss = 0.33691762\n",
            "Iteration 131, loss = 0.33526725\n",
            "Iteration 132, loss = 0.33398617\n",
            "Iteration 133, loss = 0.33279927\n",
            "Iteration 134, loss = 0.33220187\n",
            "Iteration 135, loss = 0.33036652\n",
            "Iteration 136, loss = 0.32901710\n",
            "Iteration 137, loss = 0.32848139\n",
            "Iteration 138, loss = 0.32635275\n",
            "Iteration 139, loss = 0.32528032\n",
            "Iteration 140, loss = 0.32427430\n",
            "Iteration 141, loss = 0.32294014\n",
            "Iteration 142, loss = 0.32130080\n",
            "Iteration 143, loss = 0.32111579\n",
            "Iteration 144, loss = 0.31892974\n",
            "Iteration 145, loss = 0.31837656\n",
            "Iteration 146, loss = 0.31772021\n",
            "Iteration 147, loss = 0.31729140\n",
            "Iteration 148, loss = 0.31534442\n",
            "Iteration 149, loss = 0.31414673\n",
            "Iteration 150, loss = 0.31400016\n",
            "Iteration 151, loss = 0.31261791\n",
            "Iteration 152, loss = 0.31262926\n",
            "Iteration 153, loss = 0.30994993\n",
            "Iteration 154, loss = 0.31009078\n",
            "Iteration 155, loss = 0.30850383\n",
            "Iteration 156, loss = 0.30706360\n",
            "Iteration 157, loss = 0.30633996\n",
            "Iteration 158, loss = 0.30504731\n",
            "Iteration 159, loss = 0.30684467\n",
            "Iteration 160, loss = 0.30500769\n",
            "Iteration 161, loss = 0.30417387\n",
            "Iteration 162, loss = 0.30382228\n",
            "Iteration 163, loss = 0.30290276\n",
            "Iteration 164, loss = 0.30175870\n",
            "Iteration 165, loss = 0.30225511\n",
            "Iteration 166, loss = 0.29994717\n",
            "Iteration 167, loss = 0.29886188\n",
            "Iteration 168, loss = 0.29806925\n",
            "Iteration 169, loss = 0.29903146\n",
            "Iteration 170, loss = 0.29744287\n",
            "Iteration 171, loss = 0.29665970\n",
            "Iteration 172, loss = 0.29678507\n",
            "Iteration 173, loss = 0.29567201\n",
            "Iteration 174, loss = 0.29479891\n",
            "Iteration 175, loss = 0.29415661\n",
            "Iteration 176, loss = 0.29359983\n",
            "Iteration 177, loss = 0.29174025\n",
            "Iteration 178, loss = 0.29433143\n",
            "Iteration 179, loss = 0.29203420\n",
            "Iteration 180, loss = 0.29269710\n",
            "Iteration 181, loss = 0.29083899\n",
            "Iteration 182, loss = 0.29100447\n",
            "Iteration 183, loss = 0.28930396\n",
            "Iteration 184, loss = 0.28929551\n",
            "Iteration 185, loss = 0.29093359\n",
            "Iteration 186, loss = 0.28897360\n",
            "Iteration 187, loss = 0.28723080\n",
            "Iteration 188, loss = 0.28736715\n",
            "Iteration 189, loss = 0.28772260\n",
            "Iteration 190, loss = 0.28624604\n",
            "Iteration 191, loss = 0.28609056\n",
            "Iteration 192, loss = 0.28644032\n",
            "Iteration 193, loss = 0.28412000\n",
            "Iteration 194, loss = 0.28698599\n",
            "Iteration 195, loss = 0.28515590\n",
            "Iteration 196, loss = 0.28308745\n",
            "Iteration 197, loss = 0.28534075\n",
            "Iteration 198, loss = 0.28297812\n",
            "Iteration 199, loss = 0.28051669\n",
            "Iteration 200, loss = 0.28199159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69265927\n",
            "Iteration 3, loss = 0.67131847\n",
            "Iteration 4, loss = 0.65633185\n",
            "Iteration 5, loss = 0.64454322\n",
            "Iteration 6, loss = 0.63207841\n",
            "Iteration 7, loss = 0.62272174\n",
            "Iteration 8, loss = 0.61385187\n",
            "Iteration 9, loss = 0.60859030\n",
            "Iteration 10, loss = 0.60150950\n",
            "Iteration 11, loss = 0.59562522\n",
            "Iteration 12, loss = 0.59173518\n",
            "Iteration 13, loss = 0.59160930\n",
            "Iteration 14, loss = 0.58494162\n",
            "Iteration 15, loss = 0.58127448\n",
            "Iteration 16, loss = 0.57954125\n",
            "Iteration 17, loss = 0.57492117\n",
            "Iteration 18, loss = 0.57678205\n",
            "Iteration 19, loss = 0.57186484\n",
            "Iteration 20, loss = 0.57006197\n",
            "Iteration 21, loss = 0.56888544\n",
            "Iteration 22, loss = 0.56932782\n",
            "Iteration 23, loss = 0.56536086\n",
            "Iteration 24, loss = 0.56522545\n",
            "Iteration 25, loss = 0.56592633\n",
            "Iteration 26, loss = 0.56171225\n",
            "Iteration 27, loss = 0.56056817\n",
            "Iteration 28, loss = 0.56771548\n",
            "Iteration 29, loss = 0.56384186\n",
            "Iteration 30, loss = 0.56559135\n",
            "Iteration 31, loss = 0.57042359\n",
            "Iteration 32, loss = 0.56797722\n",
            "Iteration 33, loss = 0.56617832\n",
            "Iteration 34, loss = 0.56579915\n",
            "Iteration 35, loss = 0.56390942\n",
            "Iteration 36, loss = 0.56591100\n",
            "Iteration 37, loss = 0.56156878\n",
            "Iteration 38, loss = 0.56163965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.52865317\n",
            "Iteration 40, loss = 0.52156139\n",
            "Iteration 41, loss = 0.51645262\n",
            "Iteration 42, loss = 0.51271716\n",
            "Iteration 43, loss = 0.50817280\n",
            "Iteration 44, loss = 0.50155940\n",
            "Iteration 45, loss = 0.49842211\n",
            "Iteration 46, loss = 0.49597660\n",
            "Iteration 47, loss = 0.49530357\n",
            "Iteration 48, loss = 0.49442619\n",
            "Iteration 49, loss = 0.49866510\n",
            "Iteration 50, loss = 0.50491604\n",
            "Iteration 51, loss = 0.50840681\n",
            "Iteration 52, loss = 0.51063003\n",
            "Iteration 53, loss = 0.51443419\n",
            "Iteration 54, loss = 0.51069649\n",
            "Iteration 55, loss = 0.51855000\n",
            "Iteration 56, loss = 0.51475854\n",
            "Iteration 57, loss = 0.51067939\n",
            "Iteration 58, loss = 0.53978534\n",
            "Iteration 59, loss = 0.52008340\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 60, loss = 0.46436927\n",
            "Iteration 61, loss = 0.45706908\n",
            "Iteration 62, loss = 0.45501004\n",
            "Iteration 63, loss = 0.45301077\n",
            "Iteration 64, loss = 0.45107499\n",
            "Iteration 65, loss = 0.44866605\n",
            "Iteration 66, loss = 0.44673755\n",
            "Iteration 67, loss = 0.44492252\n",
            "Iteration 68, loss = 0.44305085\n",
            "Iteration 69, loss = 0.44107195\n",
            "Iteration 70, loss = 0.43901185\n",
            "Iteration 71, loss = 0.43705294\n",
            "Iteration 72, loss = 0.43482831\n",
            "Iteration 73, loss = 0.43284438\n",
            "Iteration 74, loss = 0.43141862\n",
            "Iteration 75, loss = 0.42933077\n",
            "Iteration 76, loss = 0.42745470\n",
            "Iteration 77, loss = 0.42556067\n",
            "Iteration 78, loss = 0.42367829\n",
            "Iteration 79, loss = 0.42204339\n",
            "Iteration 80, loss = 0.42000869\n",
            "Iteration 81, loss = 0.41834281\n",
            "Iteration 82, loss = 0.41642530\n",
            "Iteration 83, loss = 0.41449155\n",
            "Iteration 84, loss = 0.41283785\n",
            "Iteration 85, loss = 0.41131896\n",
            "Iteration 86, loss = 0.40934373\n",
            "Iteration 87, loss = 0.40826726\n",
            "Iteration 88, loss = 0.40627379\n",
            "Iteration 89, loss = 0.40530901\n",
            "Iteration 90, loss = 0.40333925\n",
            "Iteration 91, loss = 0.40141044\n",
            "Iteration 92, loss = 0.40014731\n",
            "Iteration 93, loss = 0.39875097\n",
            "Iteration 94, loss = 0.39663038\n",
            "Iteration 95, loss = 0.39597428\n",
            "Iteration 96, loss = 0.39425277\n",
            "Iteration 97, loss = 0.39261346\n",
            "Iteration 98, loss = 0.39217758\n",
            "Iteration 99, loss = 0.39061979\n",
            "Iteration 100, loss = 0.38918002\n",
            "Iteration 101, loss = 0.38754436\n",
            "Iteration 102, loss = 0.38723254\n",
            "Iteration 103, loss = 0.38458287\n",
            "Iteration 104, loss = 0.38380773\n",
            "Iteration 105, loss = 0.38221722\n",
            "Iteration 106, loss = 0.38150854\n",
            "Iteration 107, loss = 0.38070725\n",
            "Iteration 108, loss = 0.38015288\n",
            "Iteration 109, loss = 0.37892409\n",
            "Iteration 110, loss = 0.37644970\n",
            "Iteration 111, loss = 0.37705921\n",
            "Iteration 112, loss = 0.37457368\n",
            "Iteration 113, loss = 0.37481411\n",
            "Iteration 114, loss = 0.37311003\n",
            "Iteration 115, loss = 0.37311255\n",
            "Iteration 116, loss = 0.37255374\n",
            "Iteration 117, loss = 0.37190803\n",
            "Iteration 118, loss = 0.37041489\n",
            "Iteration 119, loss = 0.37085952\n",
            "Iteration 120, loss = 0.36860236\n",
            "Iteration 121, loss = 0.36836855\n",
            "Iteration 122, loss = 0.36734656\n",
            "Iteration 123, loss = 0.36754240\n",
            "Iteration 124, loss = 0.36825484\n",
            "Iteration 125, loss = 0.36581659\n",
            "Iteration 126, loss = 0.36373581\n",
            "Iteration 127, loss = 0.36458110\n",
            "Iteration 128, loss = 0.36549499\n",
            "Iteration 129, loss = 0.36517563\n",
            "Iteration 130, loss = 0.36297049\n",
            "Iteration 131, loss = 0.36541268\n",
            "Iteration 132, loss = 0.36321598\n",
            "Iteration 133, loss = 0.36318268\n",
            "Iteration 134, loss = 0.36327124\n",
            "Iteration 135, loss = 0.36346290\n",
            "Iteration 136, loss = 0.36249998\n",
            "Iteration 137, loss = 0.36115248\n",
            "Iteration 138, loss = 0.35986596\n",
            "Iteration 139, loss = 0.35815212\n",
            "Iteration 140, loss = 0.36035600\n",
            "Iteration 141, loss = 0.35847281\n",
            "Iteration 142, loss = 0.36104450\n",
            "Iteration 143, loss = 0.35846701\n",
            "Iteration 144, loss = 0.35971793\n",
            "Iteration 145, loss = 0.35916210\n",
            "Iteration 146, loss = 0.35486518\n",
            "Iteration 147, loss = 0.35672074\n",
            "Iteration 148, loss = 0.35648161\n",
            "Iteration 149, loss = 0.36314696\n",
            "Iteration 150, loss = 0.35875299\n",
            "Iteration 151, loss = 0.35840040\n",
            "Iteration 152, loss = 0.35790487\n",
            "Iteration 153, loss = 0.35599761\n",
            "Iteration 154, loss = 0.36113210\n",
            "Iteration 155, loss = 0.35837259\n",
            "Iteration 156, loss = 0.35810718\n",
            "Iteration 157, loss = 0.36298577\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 158, loss = 0.33327136\n",
            "Iteration 159, loss = 0.33294353\n",
            "Iteration 160, loss = 0.33285389\n",
            "Iteration 161, loss = 0.33256328\n",
            "Iteration 162, loss = 0.33256017\n",
            "Iteration 163, loss = 0.33234713\n",
            "Iteration 164, loss = 0.33218525\n",
            "Iteration 165, loss = 0.33223452\n",
            "Iteration 166, loss = 0.33197444\n",
            "Iteration 167, loss = 0.33170986\n",
            "Iteration 168, loss = 0.33167992\n",
            "Iteration 169, loss = 0.33152872\n",
            "Iteration 170, loss = 0.33144391\n",
            "Iteration 171, loss = 0.33120972\n",
            "Iteration 172, loss = 0.33101619\n",
            "Iteration 173, loss = 0.33091073\n",
            "Iteration 174, loss = 0.33092528\n",
            "Iteration 175, loss = 0.33075610\n",
            "Iteration 176, loss = 0.33056872\n",
            "Iteration 177, loss = 0.33051938\n",
            "Iteration 178, loss = 0.33030728\n",
            "Iteration 179, loss = 0.33012485\n",
            "Iteration 180, loss = 0.33002906\n",
            "Iteration 181, loss = 0.33002288\n",
            "Iteration 182, loss = 0.32968454\n",
            "Iteration 183, loss = 0.32973670\n",
            "Iteration 184, loss = 0.32959231\n",
            "Iteration 185, loss = 0.32949631\n",
            "Iteration 186, loss = 0.32934935\n",
            "Iteration 187, loss = 0.32918277\n",
            "Iteration 188, loss = 0.32896161\n",
            "Iteration 189, loss = 0.32907198\n",
            "Iteration 190, loss = 0.32875969\n",
            "Iteration 191, loss = 0.32869131\n",
            "Iteration 192, loss = 0.32852407\n",
            "Iteration 193, loss = 0.32841399\n",
            "Iteration 194, loss = 0.32837898\n",
            "Iteration 195, loss = 0.32819338\n",
            "Iteration 196, loss = 0.32813568\n",
            "Iteration 197, loss = 0.32812177\n",
            "Iteration 198, loss = 0.32779203\n",
            "Iteration 199, loss = 0.32782452\n",
            "Iteration 200, loss = 0.32765229\n",
            "----------------------------------\n",
            "[[33451   276]\n",
            " [ 1353 15566]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 13.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     33727\n",
            "           1       0.98      0.92      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.96     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.671% (0.172%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.947% (8.914%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63402444\n",
            "Iteration 3, loss = 0.61386608\n",
            "Iteration 4, loss = 0.59972359\n",
            "Iteration 5, loss = 0.59517296\n",
            "Iteration 6, loss = 0.58890545\n",
            "Iteration 7, loss = 0.58351569\n",
            "Iteration 8, loss = 0.58292578\n",
            "Iteration 9, loss = 0.57981998\n",
            "Iteration 10, loss = 0.57986454\n",
            "Iteration 11, loss = 0.58178305\n",
            "Iteration 12, loss = 0.58432010\n",
            "Iteration 13, loss = 0.58198690\n",
            "Iteration 14, loss = 0.58069600\n",
            "Iteration 15, loss = 0.58271623\n",
            "Iteration 16, loss = 0.58405394\n",
            "Iteration 17, loss = 0.58167585\n",
            "Iteration 18, loss = 0.58278905\n",
            "Iteration 19, loss = 0.58236091\n",
            "Iteration 20, loss = 0.58288181\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 21, loss = 0.52915103\n",
            "Iteration 22, loss = 0.51245863\n",
            "Iteration 23, loss = 0.53451507\n",
            "Iteration 24, loss = 0.53217225\n",
            "Iteration 25, loss = 0.53424105\n",
            "Iteration 26, loss = 0.53385976\n",
            "Iteration 27, loss = 0.53245450\n",
            "Iteration 28, loss = 0.52969332\n",
            "Iteration 29, loss = 0.53283764\n",
            "Iteration 30, loss = 0.53178237\n",
            "Iteration 31, loss = 0.53111252\n",
            "Iteration 32, loss = 0.53224078\n",
            "Iteration 33, loss = 0.52987369\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 34, loss = 0.44260754\n",
            "Iteration 35, loss = 0.42851733\n",
            "Iteration 36, loss = 0.41610039\n",
            "Iteration 37, loss = 0.40505972\n",
            "Iteration 38, loss = 0.39519265\n",
            "Iteration 39, loss = 0.38657581\n",
            "Iteration 40, loss = 0.37898884\n",
            "Iteration 41, loss = 0.37213925\n",
            "Iteration 42, loss = 0.36617861\n",
            "Iteration 43, loss = 0.36119150\n",
            "Iteration 44, loss = 0.35630828\n",
            "Iteration 45, loss = 0.35253020\n",
            "Iteration 46, loss = 0.34862348\n",
            "Iteration 47, loss = 0.34564064\n",
            "Iteration 48, loss = 0.34329859\n",
            "Iteration 49, loss = 0.34054229\n",
            "Iteration 50, loss = 0.33887572\n",
            "Iteration 51, loss = 0.33704823\n",
            "Iteration 52, loss = 0.33539359\n",
            "Iteration 53, loss = 0.33407505\n",
            "Iteration 54, loss = 0.33292680\n",
            "Iteration 55, loss = 0.33092896\n",
            "Iteration 56, loss = 0.33018869\n",
            "Iteration 57, loss = 0.32958923\n",
            "Iteration 58, loss = 0.32993104\n",
            "Iteration 59, loss = 0.32822855\n",
            "Iteration 60, loss = 0.32825750\n",
            "Iteration 61, loss = 0.32794604\n",
            "Iteration 62, loss = 0.33005261\n",
            "Iteration 63, loss = 0.33046314\n",
            "Iteration 64, loss = 0.32981649\n",
            "Iteration 65, loss = 0.32881811\n",
            "Iteration 66, loss = 0.32877246\n",
            "Iteration 67, loss = 0.32961435\n",
            "Iteration 68, loss = 0.33036477\n",
            "Iteration 69, loss = 0.32969661\n",
            "Iteration 70, loss = 0.33097583\n",
            "Iteration 71, loss = 0.33428127\n",
            "Iteration 72, loss = 0.33071779\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 73, loss = 0.31375298\n",
            "Iteration 74, loss = 0.31354200\n",
            "Iteration 75, loss = 0.31352633\n",
            "Iteration 76, loss = 0.31339876\n",
            "Iteration 77, loss = 0.31332587\n",
            "Iteration 78, loss = 0.31323196\n",
            "Iteration 79, loss = 0.31306188\n",
            "Iteration 80, loss = 0.31308043\n",
            "Iteration 81, loss = 0.31309924\n",
            "Iteration 82, loss = 0.31295992\n",
            "Iteration 83, loss = 0.31284618\n",
            "Iteration 84, loss = 0.31282336\n",
            "Iteration 85, loss = 0.31272548\n",
            "Iteration 86, loss = 0.31269813\n",
            "Iteration 87, loss = 0.31262135\n",
            "Iteration 88, loss = 0.31255669\n",
            "Iteration 89, loss = 0.31248149\n",
            "Iteration 90, loss = 0.31249466\n",
            "Iteration 91, loss = 0.31243785\n",
            "Iteration 92, loss = 0.31229558\n",
            "Iteration 93, loss = 0.31221950\n",
            "Iteration 94, loss = 0.31217208\n",
            "Iteration 95, loss = 0.31213166\n",
            "Iteration 96, loss = 0.31203124\n",
            "Iteration 97, loss = 0.31198711\n",
            "Iteration 98, loss = 0.31199047\n",
            "Iteration 99, loss = 0.31184943\n",
            "Iteration 100, loss = 0.31180045\n",
            "Iteration 101, loss = 0.31181478\n",
            "Iteration 102, loss = 0.31178196\n",
            "Iteration 103, loss = 0.31167267\n",
            "Iteration 104, loss = 0.31156261\n",
            "Iteration 105, loss = 0.31158902\n",
            "Iteration 106, loss = 0.31155320\n",
            "Iteration 107, loss = 0.31147403\n",
            "Iteration 108, loss = 0.31139087\n",
            "Iteration 109, loss = 0.31131812\n",
            "Iteration 110, loss = 0.31136664\n",
            "Iteration 111, loss = 0.31131424\n",
            "Iteration 112, loss = 0.31123469\n",
            "Iteration 113, loss = 0.31122227\n",
            "Iteration 114, loss = 0.31122025\n",
            "Iteration 115, loss = 0.31111179\n",
            "Iteration 116, loss = 0.31107571\n",
            "Iteration 117, loss = 0.31102611\n",
            "Iteration 118, loss = 0.31095508\n",
            "Iteration 119, loss = 0.31087829\n",
            "Iteration 120, loss = 0.31089110\n",
            "Iteration 121, loss = 0.31086147\n",
            "Iteration 122, loss = 0.31079896\n",
            "Iteration 123, loss = 0.31079138\n",
            "Iteration 124, loss = 0.31077623\n",
            "Iteration 125, loss = 0.31072766\n",
            "Iteration 126, loss = 0.31060871\n",
            "Iteration 127, loss = 0.31056398\n",
            "Iteration 128, loss = 0.31062019\n",
            "Iteration 129, loss = 0.31055412\n",
            "Iteration 130, loss = 0.31042028\n",
            "Iteration 131, loss = 0.31041644\n",
            "Iteration 132, loss = 0.31039693\n",
            "Iteration 133, loss = 0.31040135\n",
            "Iteration 134, loss = 0.31031539\n",
            "Iteration 135, loss = 0.31028770\n",
            "Iteration 136, loss = 0.31019546\n",
            "Iteration 137, loss = 0.31024588\n",
            "Iteration 138, loss = 0.31016003\n",
            "Iteration 139, loss = 0.31027170\n",
            "Iteration 140, loss = 0.31014623\n",
            "Iteration 141, loss = 0.31000846\n",
            "Iteration 142, loss = 0.31007064\n",
            "Iteration 143, loss = 0.31006681\n",
            "Iteration 144, loss = 0.31004857\n",
            "Iteration 145, loss = 0.30996045\n",
            "Iteration 146, loss = 0.30988751\n",
            "Iteration 147, loss = 0.30990793\n",
            "Iteration 148, loss = 0.30994712\n",
            "Iteration 149, loss = 0.30977813\n",
            "Iteration 150, loss = 0.30979647\n",
            "Iteration 151, loss = 0.30976825\n",
            "Iteration 152, loss = 0.30965089\n",
            "Iteration 153, loss = 0.30975658\n",
            "Iteration 154, loss = 0.30969761\n",
            "Iteration 155, loss = 0.30961042\n",
            "Iteration 156, loss = 0.30961482\n",
            "Iteration 157, loss = 0.30955252\n",
            "Iteration 158, loss = 0.30950053\n",
            "Iteration 159, loss = 0.30951280\n",
            "Iteration 160, loss = 0.30949100\n",
            "Iteration 161, loss = 0.30943454\n",
            "Iteration 162, loss = 0.30946435\n",
            "Iteration 163, loss = 0.30946459\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 164, loss = 0.30891324\n",
            "Iteration 165, loss = 0.30889706\n",
            "Iteration 166, loss = 0.30891924\n",
            "Iteration 167, loss = 0.30888443\n",
            "Iteration 168, loss = 0.30888728\n",
            "Iteration 169, loss = 0.30884733\n",
            "Iteration 170, loss = 0.30883981\n",
            "Iteration 171, loss = 0.30886070\n",
            "Iteration 172, loss = 0.30886857\n",
            "Iteration 173, loss = 0.30883529\n",
            "Iteration 174, loss = 0.30883817\n",
            "Iteration 175, loss = 0.30879152\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 176, loss = 0.30867212\n",
            "Iteration 177, loss = 0.30866299\n",
            "Iteration 178, loss = 0.30866908\n",
            "Iteration 179, loss = 0.30866348\n",
            "Iteration 180, loss = 0.30866664\n",
            "Iteration 181, loss = 0.30866752\n",
            "Iteration 182, loss = 0.30865695\n",
            "Iteration 183, loss = 0.30866934\n",
            "Iteration 184, loss = 0.30866352\n",
            "Iteration 185, loss = 0.30865201\n",
            "Iteration 186, loss = 0.30864378\n",
            "Iteration 187, loss = 0.30866013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63211894\n",
            "Iteration 3, loss = 0.60728848\n",
            "Iteration 4, loss = 0.59266289\n",
            "Iteration 5, loss = 0.58201628\n",
            "Iteration 6, loss = 0.58382537\n",
            "Iteration 7, loss = 0.57870093\n",
            "Iteration 8, loss = 0.57476271\n",
            "Iteration 9, loss = 0.57294443\n",
            "Iteration 10, loss = 0.57060186\n",
            "Iteration 11, loss = 0.57006795\n",
            "Iteration 12, loss = 0.56819618\n",
            "Iteration 13, loss = 0.56932154\n",
            "Iteration 14, loss = 0.56922017\n",
            "Iteration 15, loss = 0.56910534\n",
            "Iteration 16, loss = 0.56843053\n",
            "Iteration 17, loss = 0.56895836\n",
            "Iteration 18, loss = 0.56901524\n",
            "Iteration 19, loss = 0.56772749\n",
            "Iteration 20, loss = 0.56953029\n",
            "Iteration 21, loss = 0.57073163\n",
            "Iteration 22, loss = 0.56928570\n",
            "Iteration 23, loss = 0.56906253\n",
            "Iteration 24, loss = 0.56932265\n",
            "Iteration 25, loss = 0.56942960\n",
            "Iteration 26, loss = 0.57107255\n",
            "Iteration 27, loss = 0.57232197\n",
            "Iteration 28, loss = 0.57105131\n",
            "Iteration 29, loss = 0.57032938\n",
            "Iteration 30, loss = 0.57103413\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 31, loss = 0.50809941\n",
            "Iteration 32, loss = 0.49562546\n",
            "Iteration 33, loss = 0.51635510\n",
            "Iteration 34, loss = 0.51832289\n",
            "Iteration 35, loss = 0.51594664\n",
            "Iteration 36, loss = 0.51814569\n",
            "Iteration 37, loss = 0.51857562\n",
            "Iteration 38, loss = 0.51285437\n",
            "Iteration 39, loss = 0.52032546\n",
            "Iteration 40, loss = 0.51415877\n",
            "Iteration 41, loss = 0.51802934\n",
            "Iteration 42, loss = 0.51466000\n",
            "Iteration 43, loss = 0.51660790\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.41415434\n",
            "Iteration 45, loss = 0.39934507\n",
            "Iteration 46, loss = 0.38611926\n",
            "Iteration 47, loss = 0.37386600\n",
            "Iteration 48, loss = 0.36299783\n",
            "Iteration 49, loss = 0.35309550\n",
            "Iteration 50, loss = 0.34427234\n",
            "Iteration 51, loss = 0.33620038\n",
            "Iteration 52, loss = 0.32886917\n",
            "Iteration 53, loss = 0.32253269\n",
            "Iteration 54, loss = 0.31670070\n",
            "Iteration 55, loss = 0.31172302\n",
            "Iteration 56, loss = 0.30674954\n",
            "Iteration 57, loss = 0.30245576\n",
            "Iteration 58, loss = 0.29825869\n",
            "Iteration 59, loss = 0.29509354\n",
            "Iteration 60, loss = 0.29189663\n",
            "Iteration 61, loss = 0.28899416\n",
            "Iteration 62, loss = 0.28594820\n",
            "Iteration 63, loss = 0.28431072\n",
            "Iteration 64, loss = 0.28183948\n",
            "Iteration 65, loss = 0.27977521\n",
            "Iteration 66, loss = 0.27839341\n",
            "Iteration 67, loss = 0.27701225\n",
            "Iteration 68, loss = 0.27525418\n",
            "Iteration 69, loss = 0.27406782\n",
            "Iteration 70, loss = 0.27264233\n",
            "Iteration 71, loss = 0.27167273\n",
            "Iteration 72, loss = 0.26981213\n",
            "Iteration 73, loss = 0.27091770\n",
            "Iteration 74, loss = 0.26932394\n",
            "Iteration 75, loss = 0.26903691\n",
            "Iteration 76, loss = 0.26616168\n",
            "Iteration 77, loss = 0.26788804\n",
            "Iteration 78, loss = 0.26787033\n",
            "Iteration 79, loss = 0.26606617\n",
            "Iteration 80, loss = 0.26735527\n",
            "Iteration 81, loss = 0.26511773\n",
            "Iteration 82, loss = 0.26707788\n",
            "Iteration 83, loss = 0.26441674\n",
            "Iteration 84, loss = 0.26425037\n",
            "Iteration 85, loss = 0.26724826\n",
            "Iteration 86, loss = 0.26617374\n",
            "Iteration 87, loss = 0.26421828\n",
            "Iteration 88, loss = 0.27077909\n",
            "Iteration 89, loss = 0.26682150\n",
            "Iteration 90, loss = 0.26731589\n",
            "Iteration 91, loss = 0.26873167\n",
            "Iteration 92, loss = 0.26831526\n",
            "Iteration 93, loss = 0.26526332\n",
            "Iteration 94, loss = 0.26749005\n",
            "Iteration 95, loss = 0.26454965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 96, loss = 0.25241351\n",
            "Iteration 97, loss = 0.25236988\n",
            "Iteration 98, loss = 0.25221144\n",
            "Iteration 99, loss = 0.25214708\n",
            "Iteration 100, loss = 0.25217500\n",
            "Iteration 101, loss = 0.25212232\n",
            "Iteration 102, loss = 0.25203552\n",
            "Iteration 103, loss = 0.25198968\n",
            "Iteration 104, loss = 0.25196716\n",
            "Iteration 105, loss = 0.25191014\n",
            "Iteration 106, loss = 0.25183033\n",
            "Iteration 107, loss = 0.25181977\n",
            "Iteration 108, loss = 0.25169916\n",
            "Iteration 109, loss = 0.25169164\n",
            "Iteration 110, loss = 0.25162761\n",
            "Iteration 111, loss = 0.25155041\n",
            "Iteration 112, loss = 0.25152487\n",
            "Iteration 113, loss = 0.25151696\n",
            "Iteration 114, loss = 0.25139884\n",
            "Iteration 115, loss = 0.25139375\n",
            "Iteration 116, loss = 0.25135328\n",
            "Iteration 117, loss = 0.25126480\n",
            "Iteration 118, loss = 0.25129770\n",
            "Iteration 119, loss = 0.25120258\n",
            "Iteration 120, loss = 0.25115307\n",
            "Iteration 121, loss = 0.25112601\n",
            "Iteration 122, loss = 0.25109505\n",
            "Iteration 123, loss = 0.25106527\n",
            "Iteration 124, loss = 0.25103386\n",
            "Iteration 125, loss = 0.25099623\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 126, loss = 0.25066188\n",
            "Iteration 127, loss = 0.25064095\n",
            "Iteration 128, loss = 0.25065516\n",
            "Iteration 129, loss = 0.25063815\n",
            "Iteration 130, loss = 0.25065345\n",
            "Iteration 131, loss = 0.25063335\n",
            "Iteration 132, loss = 0.25060462\n",
            "Iteration 133, loss = 0.25062804\n",
            "Iteration 134, loss = 0.25061260\n",
            "Iteration 135, loss = 0.25060225\n",
            "Iteration 136, loss = 0.25058986\n",
            "Iteration 137, loss = 0.25059772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 138, loss = 0.25050482\n",
            "Iteration 139, loss = 0.25048566\n",
            "Iteration 140, loss = 0.25048233\n",
            "Iteration 141, loss = 0.25049044\n",
            "Iteration 142, loss = 0.25049159\n",
            "Iteration 143, loss = 0.25048376\n",
            "Iteration 144, loss = 0.25048326\n",
            "Iteration 145, loss = 0.25048176\n",
            "Iteration 146, loss = 0.25047282\n",
            "Iteration 147, loss = 0.25048802\n",
            "Iteration 148, loss = 0.25048865\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64177468\n",
            "Iteration 3, loss = 0.61509796\n",
            "Iteration 4, loss = 0.59893408\n",
            "Iteration 5, loss = 0.58943692\n",
            "Iteration 6, loss = 0.58254904\n",
            "Iteration 7, loss = 0.57861539\n",
            "Iteration 8, loss = 0.59643855\n",
            "Iteration 9, loss = 0.59735397\n",
            "Iteration 10, loss = 0.58340747\n",
            "Iteration 11, loss = 0.57816771\n",
            "Iteration 12, loss = 0.57516564\n",
            "Iteration 13, loss = 0.57266947\n",
            "Iteration 14, loss = 0.60622565\n",
            "Iteration 15, loss = 0.58920984\n",
            "Iteration 16, loss = 0.57894740\n",
            "Iteration 17, loss = 0.57618598\n",
            "Iteration 18, loss = 0.57229301\n",
            "Iteration 19, loss = 0.57256081\n",
            "Iteration 20, loss = 0.57297687\n",
            "Iteration 21, loss = 0.57890279\n",
            "Iteration 22, loss = 0.60716761\n",
            "Iteration 23, loss = 0.58647397\n",
            "Iteration 24, loss = 0.57725515\n",
            "Iteration 25, loss = 0.57538285\n",
            "Iteration 26, loss = 0.57373175\n",
            "Iteration 27, loss = 0.57232678\n",
            "Iteration 28, loss = 0.57192273\n",
            "Iteration 29, loss = 0.57086085\n",
            "Iteration 30, loss = 0.57208861\n",
            "Iteration 31, loss = 0.57191798\n",
            "Iteration 32, loss = 0.57261098\n",
            "Iteration 33, loss = 0.57295783\n",
            "Iteration 34, loss = 0.57361324\n",
            "Iteration 35, loss = 0.57316521\n",
            "Iteration 36, loss = 0.57217332\n",
            "Iteration 37, loss = 0.57336134\n",
            "Iteration 38, loss = 0.57909358\n",
            "Iteration 39, loss = 0.61965394\n",
            "Iteration 40, loss = 0.59792243\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 41, loss = 0.57045415\n",
            "Iteration 42, loss = 0.56266094\n",
            "Iteration 43, loss = 0.55426719\n",
            "Iteration 44, loss = 0.54342934\n",
            "Iteration 45, loss = 0.53603975\n",
            "Iteration 46, loss = 0.54079099\n",
            "Iteration 47, loss = 0.53773760\n",
            "Iteration 48, loss = 0.53612982\n",
            "Iteration 49, loss = 0.53159530\n",
            "Iteration 50, loss = 0.52900889\n",
            "Iteration 51, loss = 0.52759655\n",
            "Iteration 52, loss = 0.52608968\n",
            "Iteration 53, loss = 0.52594871\n",
            "Iteration 54, loss = 0.52025970\n",
            "Iteration 55, loss = 0.52314470\n",
            "Iteration 56, loss = 0.52212302\n",
            "Iteration 57, loss = 0.52016687\n",
            "Iteration 58, loss = 0.52065210\n",
            "Iteration 59, loss = 0.51957754\n",
            "Iteration 60, loss = 0.52207396\n",
            "Iteration 61, loss = 0.51551232\n",
            "Iteration 62, loss = 0.51681225\n",
            "Iteration 63, loss = 0.51864430\n",
            "Iteration 64, loss = 0.52156488\n",
            "Iteration 65, loss = 0.51927487\n",
            "Iteration 66, loss = 0.51780045\n",
            "Iteration 67, loss = 0.52024749\n",
            "Iteration 68, loss = 0.52112793\n",
            "Iteration 69, loss = 0.51999988\n",
            "Iteration 70, loss = 0.51905590\n",
            "Iteration 71, loss = 0.51748475\n",
            "Iteration 72, loss = 0.52352136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.40764826\n",
            "Iteration 74, loss = 0.39163613\n",
            "Iteration 75, loss = 0.37758270\n",
            "Iteration 76, loss = 0.36500991\n",
            "Iteration 77, loss = 0.35391390\n",
            "Iteration 78, loss = 0.34398659\n",
            "Iteration 79, loss = 0.33535800\n",
            "Iteration 80, loss = 0.32752745\n",
            "Iteration 81, loss = 0.32070666\n",
            "Iteration 82, loss = 0.31470731\n",
            "Iteration 83, loss = 0.30898561\n",
            "Iteration 84, loss = 0.30444762\n",
            "Iteration 85, loss = 0.30015750\n",
            "Iteration 86, loss = 0.29606154\n",
            "Iteration 87, loss = 0.29252699\n",
            "Iteration 88, loss = 0.28964107\n",
            "Iteration 89, loss = 0.28686410\n",
            "Iteration 90, loss = 0.28380656\n",
            "Iteration 91, loss = 0.28138731\n",
            "Iteration 92, loss = 0.27946505\n",
            "Iteration 93, loss = 0.27762183\n",
            "Iteration 94, loss = 0.27585350\n",
            "Iteration 95, loss = 0.27409026\n",
            "Iteration 96, loss = 0.27330710\n",
            "Iteration 97, loss = 0.27084973\n",
            "Iteration 98, loss = 0.26974598\n",
            "Iteration 99, loss = 0.26897936\n",
            "Iteration 100, loss = 0.26858166\n",
            "Iteration 101, loss = 0.26709110\n",
            "Iteration 102, loss = 0.26625173\n",
            "Iteration 103, loss = 0.26614769\n",
            "Iteration 104, loss = 0.26499922\n",
            "Iteration 105, loss = 0.26363643\n",
            "Iteration 106, loss = 0.26323629\n",
            "Iteration 107, loss = 0.26233730\n",
            "Iteration 108, loss = 0.26232735\n",
            "Iteration 109, loss = 0.26186124\n",
            "Iteration 110, loss = 0.26103674\n",
            "Iteration 111, loss = 0.26039467\n",
            "Iteration 112, loss = 0.25945800\n",
            "Iteration 113, loss = 0.26006419\n",
            "Iteration 114, loss = 0.25979742\n",
            "Iteration 115, loss = 0.25922999\n",
            "Iteration 116, loss = 0.25936517\n",
            "Iteration 117, loss = 0.26068495\n",
            "Iteration 118, loss = 0.25995759\n",
            "Iteration 119, loss = 0.25896263\n",
            "Iteration 120, loss = 0.25911668\n",
            "Iteration 121, loss = 0.25929016\n",
            "Iteration 122, loss = 0.25943780\n",
            "Iteration 123, loss = 0.25977496\n",
            "Iteration 124, loss = 0.25864119\n",
            "Iteration 125, loss = 0.25881050\n",
            "Iteration 126, loss = 0.25964016\n",
            "Iteration 127, loss = 0.25981035\n",
            "Iteration 128, loss = 0.26196000\n",
            "Iteration 129, loss = 0.26963325\n",
            "Iteration 130, loss = 0.25968999\n",
            "Iteration 131, loss = 0.25967306\n",
            "Iteration 132, loss = 0.26119284\n",
            "Iteration 133, loss = 0.26268995\n",
            "Iteration 134, loss = 0.25985093\n",
            "Iteration 135, loss = 0.25841228\n",
            "Iteration 136, loss = 0.26609655\n",
            "Iteration 137, loss = 0.27334144\n",
            "Iteration 138, loss = 0.26404341\n",
            "Iteration 139, loss = 0.26220126\n",
            "Iteration 140, loss = 0.26796327\n",
            "Iteration 141, loss = 0.26705978\n",
            "Iteration 142, loss = 0.26554735\n",
            "Iteration 143, loss = 0.27138374\n",
            "Iteration 144, loss = 0.26415843\n",
            "Iteration 145, loss = 0.27547528\n",
            "Iteration 146, loss = 0.27072859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 147, loss = 0.24854468\n",
            "Iteration 148, loss = 0.24848721\n",
            "Iteration 149, loss = 0.24846230\n",
            "Iteration 150, loss = 0.24843993\n",
            "Iteration 151, loss = 0.24837122\n",
            "Iteration 152, loss = 0.24834334\n",
            "Iteration 153, loss = 0.24834732\n",
            "Iteration 154, loss = 0.24828661\n",
            "Iteration 155, loss = 0.24827410\n",
            "Iteration 156, loss = 0.24824437\n",
            "Iteration 157, loss = 0.24821691\n",
            "Iteration 158, loss = 0.24820994\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 159, loss = 0.24795625\n",
            "Iteration 160, loss = 0.24792197\n",
            "Iteration 161, loss = 0.24791429\n",
            "Iteration 162, loss = 0.24793303\n",
            "Iteration 163, loss = 0.24791080\n",
            "Iteration 164, loss = 0.24790132\n",
            "Iteration 165, loss = 0.24788682\n",
            "Iteration 166, loss = 0.24787852\n",
            "Iteration 167, loss = 0.24788447\n",
            "Iteration 168, loss = 0.24788986\n",
            "Iteration 169, loss = 0.24787683\n",
            "Iteration 170, loss = 0.24789228\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 171, loss = 0.24779243\n",
            "Iteration 172, loss = 0.24779270\n",
            "Iteration 173, loss = 0.24779029\n",
            "Iteration 174, loss = 0.24779337\n",
            "Iteration 175, loss = 0.24779764\n",
            "Iteration 176, loss = 0.24778714\n",
            "Iteration 177, loss = 0.24778604\n",
            "Iteration 178, loss = 0.24778736\n",
            "Iteration 179, loss = 0.24779175\n",
            "Iteration 180, loss = 0.24778517\n",
            "Iteration 181, loss = 0.24778523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63935556\n",
            "Iteration 3, loss = 0.60950891\n",
            "Iteration 4, loss = 0.59453411\n",
            "Iteration 5, loss = 0.58320643\n",
            "Iteration 6, loss = 0.57709055\n",
            "Iteration 7, loss = 0.57361884\n",
            "Iteration 8, loss = 0.57174690\n",
            "Iteration 9, loss = 0.57125744\n",
            "Iteration 10, loss = 0.56989211\n",
            "Iteration 11, loss = 0.56955839\n",
            "Iteration 12, loss = 0.56996935\n",
            "Iteration 13, loss = 0.57140078\n",
            "Iteration 14, loss = 0.57011399\n",
            "Iteration 15, loss = 0.57466304\n",
            "Iteration 16, loss = 0.57282157\n",
            "Iteration 17, loss = 0.57289156\n",
            "Iteration 18, loss = 0.57188088\n",
            "Iteration 19, loss = 0.57386293\n",
            "Iteration 20, loss = 0.57206951\n",
            "Iteration 21, loss = 0.57228294\n",
            "Iteration 22, loss = 0.57296743\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.50827807\n",
            "Iteration 24, loss = 0.48739289\n",
            "Iteration 25, loss = 0.51380915\n",
            "Iteration 26, loss = 0.51718132\n",
            "Iteration 27, loss = 0.51339799\n",
            "Iteration 28, loss = 0.51248706\n",
            "Iteration 29, loss = 0.51584488\n",
            "Iteration 30, loss = 0.51620559\n",
            "Iteration 31, loss = 0.51210842\n",
            "Iteration 32, loss = 0.51495521\n",
            "Iteration 33, loss = 0.51857160\n",
            "Iteration 34, loss = 0.50785057\n",
            "Iteration 35, loss = 0.51839176\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.41263768\n",
            "Iteration 37, loss = 0.39739073\n",
            "Iteration 38, loss = 0.38378558\n",
            "Iteration 39, loss = 0.37138010\n",
            "Iteration 40, loss = 0.36035919\n",
            "Iteration 41, loss = 0.35042930\n",
            "Iteration 42, loss = 0.34156209\n",
            "Iteration 43, loss = 0.33340907\n",
            "Iteration 44, loss = 0.32626312\n",
            "Iteration 45, loss = 0.31994530\n",
            "Iteration 46, loss = 0.31389434\n",
            "Iteration 47, loss = 0.30866717\n",
            "Iteration 48, loss = 0.30436726\n",
            "Iteration 49, loss = 0.29961224\n",
            "Iteration 50, loss = 0.29660846\n",
            "Iteration 51, loss = 0.29297180\n",
            "Iteration 52, loss = 0.28957511\n",
            "Iteration 53, loss = 0.28646865\n",
            "Iteration 54, loss = 0.28388599\n",
            "Iteration 55, loss = 0.28210971\n",
            "Iteration 56, loss = 0.27922202\n",
            "Iteration 57, loss = 0.27830651\n",
            "Iteration 58, loss = 0.27627662\n",
            "Iteration 59, loss = 0.27429130\n",
            "Iteration 60, loss = 0.27307810\n",
            "Iteration 61, loss = 0.27168154\n",
            "Iteration 62, loss = 0.27112426\n",
            "Iteration 63, loss = 0.26932686\n",
            "Iteration 64, loss = 0.26827719\n",
            "Iteration 65, loss = 0.26840355\n",
            "Iteration 66, loss = 0.26661630\n",
            "Iteration 67, loss = 0.26566823\n",
            "Iteration 68, loss = 0.26480118\n",
            "Iteration 69, loss = 0.26520191\n",
            "Iteration 70, loss = 0.26388801\n",
            "Iteration 71, loss = 0.26564145\n",
            "Iteration 72, loss = 0.26421055\n",
            "Iteration 73, loss = 0.26520411\n",
            "Iteration 74, loss = 0.26395356\n",
            "Iteration 75, loss = 0.26187005\n",
            "Iteration 76, loss = 0.26432297\n",
            "Iteration 77, loss = 0.25980499\n",
            "Iteration 78, loss = 0.26131721\n",
            "Iteration 79, loss = 0.26592374\n",
            "Iteration 80, loss = 0.26132085\n",
            "Iteration 81, loss = 0.26391210\n",
            "Iteration 82, loss = 0.26460113\n",
            "Iteration 83, loss = 0.27006601\n",
            "Iteration 84, loss = 0.26626621\n",
            "Iteration 85, loss = 0.26443432\n",
            "Iteration 86, loss = 0.26541792\n",
            "Iteration 87, loss = 0.27127224\n",
            "Iteration 88, loss = 0.26961204\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 89, loss = 0.25080855\n",
            "Iteration 90, loss = 0.25070560\n",
            "Iteration 91, loss = 0.25062876\n",
            "Iteration 92, loss = 0.25057918\n",
            "Iteration 93, loss = 0.25056042\n",
            "Iteration 94, loss = 0.25048231\n",
            "Iteration 95, loss = 0.25040946\n",
            "Iteration 96, loss = 0.25033893\n",
            "Iteration 97, loss = 0.25031920\n",
            "Iteration 98, loss = 0.25025041\n",
            "Iteration 99, loss = 0.25023688\n",
            "Iteration 100, loss = 0.25016356\n",
            "Iteration 101, loss = 0.25009876\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 102, loss = 0.24983416\n",
            "Iteration 103, loss = 0.24984309\n",
            "Iteration 104, loss = 0.24980579\n",
            "Iteration 105, loss = 0.24980634\n",
            "Iteration 106, loss = 0.24980703\n",
            "Iteration 107, loss = 0.24980442\n",
            "Iteration 108, loss = 0.24977992\n",
            "Iteration 109, loss = 0.24976119\n",
            "Iteration 110, loss = 0.24977350\n",
            "Iteration 111, loss = 0.24976947\n",
            "Iteration 112, loss = 0.24974744\n",
            "Iteration 113, loss = 0.24976478\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 114, loss = 0.24966034\n",
            "Iteration 115, loss = 0.24965479\n",
            "Iteration 116, loss = 0.24965109\n",
            "Iteration 117, loss = 0.24964468\n",
            "Iteration 118, loss = 0.24963892\n",
            "Iteration 119, loss = 0.24964016\n",
            "Iteration 120, loss = 0.24965063\n",
            "Iteration 121, loss = 0.24964225\n",
            "Iteration 122, loss = 0.24964300\n",
            "Iteration 123, loss = 0.24963951\n",
            "Iteration 124, loss = 0.24962803\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64173093\n",
            "Iteration 3, loss = 0.61334836\n",
            "Iteration 4, loss = 0.59745486\n",
            "Iteration 5, loss = 0.58992170\n",
            "Iteration 6, loss = 0.58438825\n",
            "Iteration 7, loss = 0.57827740\n",
            "Iteration 8, loss = 0.57636305\n",
            "Iteration 9, loss = 0.57433975\n",
            "Iteration 10, loss = 0.57552469\n",
            "Iteration 11, loss = 0.57252467\n",
            "Iteration 12, loss = 0.57384018\n",
            "Iteration 13, loss = 0.57332003\n",
            "Iteration 14, loss = 0.57355825\n",
            "Iteration 15, loss = 0.57446204\n",
            "Iteration 16, loss = 0.57368963\n",
            "Iteration 17, loss = 0.57588042\n",
            "Iteration 18, loss = 0.57419328\n",
            "Iteration 19, loss = 0.57350870\n",
            "Iteration 20, loss = 0.57566543\n",
            "Iteration 21, loss = 0.57451146\n",
            "Iteration 22, loss = 0.57617332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.52340762\n",
            "Iteration 24, loss = 0.50508425\n",
            "Iteration 25, loss = 0.51959247\n",
            "Iteration 26, loss = 0.52944005\n",
            "Iteration 27, loss = 0.52589067\n",
            "Iteration 28, loss = 0.52521780\n",
            "Iteration 29, loss = 0.52808238\n",
            "Iteration 30, loss = 0.52604743\n",
            "Iteration 31, loss = 0.52459351\n",
            "Iteration 32, loss = 0.52532396\n",
            "Iteration 33, loss = 0.52526221\n",
            "Iteration 34, loss = 0.52637083\n",
            "Iteration 35, loss = 0.52111286\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.43916696\n",
            "Iteration 37, loss = 0.42620860\n",
            "Iteration 38, loss = 0.41487318\n",
            "Iteration 39, loss = 0.40437337\n",
            "Iteration 40, loss = 0.39487077\n",
            "Iteration 41, loss = 0.38594211\n",
            "Iteration 42, loss = 0.37801808\n",
            "Iteration 43, loss = 0.37132717\n",
            "Iteration 44, loss = 0.36502172\n",
            "Iteration 45, loss = 0.36021963\n",
            "Iteration 46, loss = 0.35479847\n",
            "Iteration 47, loss = 0.35132915\n",
            "Iteration 48, loss = 0.34761723\n",
            "Iteration 49, loss = 0.34490667\n",
            "Iteration 50, loss = 0.34138995\n",
            "Iteration 51, loss = 0.33868809\n",
            "Iteration 52, loss = 0.33719350\n",
            "Iteration 53, loss = 0.33540594\n",
            "Iteration 54, loss = 0.33406360\n",
            "Iteration 55, loss = 0.33188403\n",
            "Iteration 56, loss = 0.33369571\n",
            "Iteration 57, loss = 0.33037992\n",
            "Iteration 58, loss = 0.33268279\n",
            "Iteration 59, loss = 0.33163815\n",
            "Iteration 60, loss = 0.33149612\n",
            "Iteration 61, loss = 0.33042585\n",
            "Iteration 62, loss = 0.33304540\n",
            "Iteration 63, loss = 0.33071684\n",
            "Iteration 64, loss = 0.33126338\n",
            "Iteration 65, loss = 0.33045759\n",
            "Iteration 66, loss = 0.33210886\n",
            "Iteration 67, loss = 0.33278211\n",
            "Iteration 68, loss = 0.33197653\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 69, loss = 0.30193323\n",
            "Iteration 70, loss = 0.30157943\n",
            "Iteration 71, loss = 0.30133584\n",
            "Iteration 72, loss = 0.30101215\n",
            "Iteration 73, loss = 0.30073538\n",
            "Iteration 74, loss = 0.30048617\n",
            "Iteration 75, loss = 0.30037277\n",
            "Iteration 76, loss = 0.30002718\n",
            "Iteration 77, loss = 0.29980712\n",
            "Iteration 78, loss = 0.29952717\n",
            "Iteration 79, loss = 0.29942782\n",
            "Iteration 80, loss = 0.29905600\n",
            "Iteration 81, loss = 0.29889875\n",
            "Iteration 82, loss = 0.29867813\n",
            "Iteration 83, loss = 0.29845380\n",
            "Iteration 84, loss = 0.29822878\n",
            "Iteration 85, loss = 0.29804582\n",
            "Iteration 86, loss = 0.29787347\n",
            "Iteration 87, loss = 0.29765742\n",
            "Iteration 88, loss = 0.29739594\n",
            "Iteration 89, loss = 0.29717891\n",
            "Iteration 90, loss = 0.29706334\n",
            "Iteration 91, loss = 0.29681957\n",
            "Iteration 92, loss = 0.29657236\n",
            "Iteration 93, loss = 0.29645902\n",
            "Iteration 94, loss = 0.29619769\n",
            "Iteration 95, loss = 0.29605413\n",
            "Iteration 96, loss = 0.29581832\n",
            "Iteration 97, loss = 0.29564469\n",
            "Iteration 98, loss = 0.29546423\n",
            "Iteration 99, loss = 0.29521218\n",
            "Iteration 100, loss = 0.29511633\n",
            "Iteration 101, loss = 0.29488941\n",
            "Iteration 102, loss = 0.29475151\n",
            "Iteration 103, loss = 0.29457294\n",
            "Iteration 104, loss = 0.29441389\n",
            "Iteration 105, loss = 0.29417143\n",
            "Iteration 106, loss = 0.29401206\n",
            "Iteration 107, loss = 0.29386253\n",
            "Iteration 108, loss = 0.29364074\n",
            "Iteration 109, loss = 0.29353264\n",
            "Iteration 110, loss = 0.29331549\n",
            "Iteration 111, loss = 0.29317025\n",
            "Iteration 112, loss = 0.29298440\n",
            "Iteration 113, loss = 0.29281157\n",
            "Iteration 114, loss = 0.29267071\n",
            "Iteration 115, loss = 0.29244602\n",
            "Iteration 116, loss = 0.29232022\n",
            "Iteration 117, loss = 0.29213228\n",
            "Iteration 118, loss = 0.29204566\n",
            "Iteration 119, loss = 0.29183575\n",
            "Iteration 120, loss = 0.29173103\n",
            "Iteration 121, loss = 0.29160035\n",
            "Iteration 122, loss = 0.29138570\n",
            "Iteration 123, loss = 0.29121298\n",
            "Iteration 124, loss = 0.29104183\n",
            "Iteration 125, loss = 0.29096090\n",
            "Iteration 126, loss = 0.29085494\n",
            "Iteration 127, loss = 0.29062141\n",
            "Iteration 128, loss = 0.29046742\n",
            "Iteration 129, loss = 0.29032481\n",
            "Iteration 130, loss = 0.29016390\n",
            "Iteration 131, loss = 0.29001926\n",
            "Iteration 132, loss = 0.28988737\n",
            "Iteration 133, loss = 0.28978211\n",
            "Iteration 134, loss = 0.28966966\n",
            "Iteration 135, loss = 0.28948663\n",
            "Iteration 136, loss = 0.28936736\n",
            "Iteration 137, loss = 0.28925707\n",
            "Iteration 138, loss = 0.28907469\n",
            "Iteration 139, loss = 0.28894673\n",
            "Iteration 140, loss = 0.28890496\n",
            "Iteration 141, loss = 0.28868245\n",
            "Iteration 142, loss = 0.28852458\n",
            "Iteration 143, loss = 0.28844781\n",
            "Iteration 144, loss = 0.28827180\n",
            "Iteration 145, loss = 0.28819702\n",
            "Iteration 146, loss = 0.28799024\n",
            "Iteration 147, loss = 0.28788051\n",
            "Iteration 148, loss = 0.28784953\n",
            "Iteration 149, loss = 0.28763016\n",
            "Iteration 150, loss = 0.28758026\n",
            "Iteration 151, loss = 0.28744526\n",
            "Iteration 152, loss = 0.28730191\n",
            "Iteration 153, loss = 0.28719178\n",
            "Iteration 154, loss = 0.28706018\n",
            "Iteration 155, loss = 0.28690914\n",
            "Iteration 156, loss = 0.28682555\n",
            "Iteration 157, loss = 0.28672137\n",
            "Iteration 158, loss = 0.28657150\n",
            "Iteration 159, loss = 0.28642802\n",
            "Iteration 160, loss = 0.28632189\n",
            "Iteration 161, loss = 0.28623898\n",
            "Iteration 162, loss = 0.28614804\n",
            "Iteration 163, loss = 0.28597668\n",
            "Iteration 164, loss = 0.28582964\n",
            "Iteration 165, loss = 0.28575357\n",
            "Iteration 166, loss = 0.28569113\n",
            "Iteration 167, loss = 0.28552882\n",
            "Iteration 168, loss = 0.28546437\n",
            "Iteration 169, loss = 0.28531352\n",
            "Iteration 170, loss = 0.28515285\n",
            "Iteration 171, loss = 0.28507257\n",
            "Iteration 172, loss = 0.28496376\n",
            "Iteration 173, loss = 0.28491348\n",
            "Iteration 174, loss = 0.28479532\n",
            "Iteration 175, loss = 0.28469781\n",
            "Iteration 176, loss = 0.28455559\n",
            "Iteration 177, loss = 0.28449472\n",
            "Iteration 178, loss = 0.28438014\n",
            "Iteration 179, loss = 0.28426004\n",
            "Iteration 180, loss = 0.28415154\n",
            "Iteration 181, loss = 0.28405910\n",
            "Iteration 182, loss = 0.28398574\n",
            "Iteration 183, loss = 0.28382656\n",
            "Iteration 184, loss = 0.28371749\n",
            "Iteration 185, loss = 0.28364868\n",
            "Iteration 186, loss = 0.28353160\n",
            "Iteration 187, loss = 0.28346121\n",
            "Iteration 188, loss = 0.28336140\n",
            "Iteration 189, loss = 0.28322483\n",
            "Iteration 190, loss = 0.28316396\n",
            "Iteration 191, loss = 0.28304313\n",
            "Iteration 192, loss = 0.28294918\n",
            "Iteration 193, loss = 0.28281998\n",
            "Iteration 194, loss = 0.28268463\n",
            "Iteration 195, loss = 0.28261042\n",
            "Iteration 196, loss = 0.28256879\n",
            "Iteration 197, loss = 0.28246211\n",
            "Iteration 198, loss = 0.28235073\n",
            "Iteration 199, loss = 0.28228795\n",
            "Iteration 200, loss = 0.28219276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63682813\n",
            "Iteration 3, loss = 0.60863019\n",
            "Iteration 4, loss = 0.59249194\n",
            "Iteration 5, loss = 0.58276898\n",
            "Iteration 6, loss = 0.57834735\n",
            "Iteration 7, loss = 0.57416698\n",
            "Iteration 8, loss = 0.57282583\n",
            "Iteration 9, loss = 0.57099067\n",
            "Iteration 10, loss = 0.57042075\n",
            "Iteration 11, loss = 0.57096358\n",
            "Iteration 12, loss = 0.57147884\n",
            "Iteration 13, loss = 0.57133387\n",
            "Iteration 14, loss = 0.56988298\n",
            "Iteration 15, loss = 0.56865552\n",
            "Iteration 16, loss = 0.56932441\n",
            "Iteration 17, loss = 0.58741508\n",
            "Iteration 18, loss = 0.63420377\n",
            "Iteration 19, loss = 0.61547011\n",
            "Iteration 20, loss = 0.60500899\n",
            "Iteration 21, loss = 0.60424202\n",
            "Iteration 22, loss = 0.60204246\n",
            "Iteration 23, loss = 0.59964134\n",
            "Iteration 24, loss = 0.58812363\n",
            "Iteration 25, loss = 0.57939756\n",
            "Iteration 26, loss = 0.57550712\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.55622356\n",
            "Iteration 28, loss = 0.54904646\n",
            "Iteration 29, loss = 0.54000246\n",
            "Iteration 30, loss = 0.53336387\n",
            "Iteration 31, loss = 0.53668691\n",
            "Iteration 32, loss = 0.53982514\n",
            "Iteration 33, loss = 0.53757622\n",
            "Iteration 34, loss = 0.53560290\n",
            "Iteration 35, loss = 0.53748596\n",
            "Iteration 36, loss = 0.53511688\n",
            "Iteration 37, loss = 0.53442077\n",
            "Iteration 38, loss = 0.53214366\n",
            "Iteration 39, loss = 0.53107910\n",
            "Iteration 40, loss = 0.52864532\n",
            "Iteration 41, loss = 0.52885522\n",
            "Iteration 42, loss = 0.52991161\n",
            "Iteration 43, loss = 0.52728307\n",
            "Iteration 44, loss = 0.52810758\n",
            "Iteration 45, loss = 0.52615868\n",
            "Iteration 46, loss = 0.52980946\n",
            "Iteration 47, loss = 0.52359597\n",
            "Iteration 48, loss = 0.52524759\n",
            "Iteration 49, loss = 0.52712106\n",
            "Iteration 50, loss = 0.52488359\n",
            "Iteration 51, loss = 0.52436453\n",
            "Iteration 52, loss = 0.52529452\n",
            "Iteration 53, loss = 0.52820649\n",
            "Iteration 54, loss = 0.52732567\n",
            "Iteration 55, loss = 0.52168368\n",
            "Iteration 56, loss = 0.52737916\n",
            "Iteration 57, loss = 0.52808674\n",
            "Iteration 58, loss = 0.52681256\n",
            "Iteration 59, loss = 0.52180549\n",
            "Iteration 60, loss = 0.53035041\n",
            "Iteration 61, loss = 0.52289654\n",
            "Iteration 62, loss = 0.53210547\n",
            "Iteration 63, loss = 0.52586323\n",
            "Iteration 64, loss = 0.52745714\n",
            "Iteration 65, loss = 0.52501377\n",
            "Iteration 66, loss = 0.53286433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.42850093\n",
            "Iteration 68, loss = 0.41285227\n",
            "Iteration 69, loss = 0.39925892\n",
            "Iteration 70, loss = 0.38719645\n",
            "Iteration 71, loss = 0.37672617\n",
            "Iteration 72, loss = 0.36739447\n",
            "Iteration 73, loss = 0.35948979\n",
            "Iteration 74, loss = 0.35260434\n",
            "Iteration 75, loss = 0.34641379\n",
            "Iteration 76, loss = 0.34144387\n",
            "Iteration 77, loss = 0.33658075\n",
            "Iteration 78, loss = 0.33256640\n",
            "Iteration 79, loss = 0.32924567\n",
            "Iteration 80, loss = 0.32644412\n",
            "Iteration 81, loss = 0.32298432\n",
            "Iteration 82, loss = 0.32079487\n",
            "Iteration 83, loss = 0.31886008\n",
            "Iteration 84, loss = 0.31671232\n",
            "Iteration 85, loss = 0.31499529\n",
            "Iteration 86, loss = 0.31343357\n",
            "Iteration 87, loss = 0.31229524\n",
            "Iteration 88, loss = 0.31097049\n",
            "Iteration 89, loss = 0.31119848\n",
            "Iteration 90, loss = 0.31010381\n",
            "Iteration 91, loss = 0.30901349\n",
            "Iteration 92, loss = 0.30841353\n",
            "Iteration 93, loss = 0.30798917\n",
            "Iteration 94, loss = 0.30712258\n",
            "Iteration 95, loss = 0.30710904\n",
            "Iteration 96, loss = 0.30786143\n",
            "Iteration 97, loss = 0.30992350\n",
            "Iteration 98, loss = 0.30871450\n",
            "Iteration 99, loss = 0.30682567\n",
            "Iteration 100, loss = 0.30978724\n",
            "Iteration 101, loss = 0.30544710\n",
            "Iteration 102, loss = 0.30808378\n",
            "Iteration 103, loss = 0.30894646\n",
            "Iteration 104, loss = 0.30692499\n",
            "Iteration 105, loss = 0.30818323\n",
            "Iteration 106, loss = 0.31050853\n",
            "Iteration 107, loss = 0.30789829\n",
            "Iteration 108, loss = 0.30942550\n",
            "Iteration 109, loss = 0.30570747\n",
            "Iteration 110, loss = 0.31360944\n",
            "Iteration 111, loss = 0.31010901\n",
            "Iteration 112, loss = 0.31128394\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 113, loss = 0.28433553\n",
            "Iteration 114, loss = 0.28401819\n",
            "Iteration 115, loss = 0.28388967\n",
            "Iteration 116, loss = 0.28370457\n",
            "Iteration 117, loss = 0.28360437\n",
            "Iteration 118, loss = 0.28346756\n",
            "Iteration 119, loss = 0.28335432\n",
            "Iteration 120, loss = 0.28322384\n",
            "Iteration 121, loss = 0.28312534\n",
            "Iteration 122, loss = 0.28304781\n",
            "Iteration 123, loss = 0.28290323\n",
            "Iteration 124, loss = 0.28275727\n",
            "Iteration 125, loss = 0.28270371\n",
            "Iteration 126, loss = 0.28260388\n",
            "Iteration 127, loss = 0.28247479\n",
            "Iteration 128, loss = 0.28238655\n",
            "Iteration 129, loss = 0.28231054\n",
            "Iteration 130, loss = 0.28218521\n",
            "Iteration 131, loss = 0.28206810\n",
            "Iteration 132, loss = 0.28197796\n",
            "Iteration 133, loss = 0.28195480\n",
            "Iteration 134, loss = 0.28180201\n",
            "Iteration 135, loss = 0.28162912\n",
            "Iteration 136, loss = 0.28159779\n",
            "Iteration 137, loss = 0.28146037\n",
            "Iteration 138, loss = 0.28134420\n",
            "Iteration 139, loss = 0.28131889\n",
            "Iteration 140, loss = 0.28121117\n",
            "Iteration 141, loss = 0.28110408\n",
            "Iteration 142, loss = 0.28096663\n",
            "Iteration 143, loss = 0.28092452\n",
            "Iteration 144, loss = 0.28079073\n",
            "Iteration 145, loss = 0.28073697\n",
            "Iteration 146, loss = 0.28056311\n",
            "Iteration 147, loss = 0.28049466\n",
            "Iteration 148, loss = 0.28044216\n",
            "Iteration 149, loss = 0.28031747\n",
            "Iteration 150, loss = 0.28025997\n",
            "Iteration 151, loss = 0.28018204\n",
            "Iteration 152, loss = 0.28008242\n",
            "Iteration 153, loss = 0.27999534\n",
            "Iteration 154, loss = 0.27995535\n",
            "Iteration 155, loss = 0.27986574\n",
            "Iteration 156, loss = 0.27975037\n",
            "Iteration 157, loss = 0.27961802\n",
            "Iteration 158, loss = 0.27953304\n",
            "Iteration 159, loss = 0.27947284\n",
            "Iteration 160, loss = 0.27943034\n",
            "Iteration 161, loss = 0.27931496\n",
            "Iteration 162, loss = 0.27922603\n",
            "Iteration 163, loss = 0.27916045\n",
            "Iteration 164, loss = 0.27903148\n",
            "Iteration 165, loss = 0.27901223\n",
            "Iteration 166, loss = 0.27892488\n",
            "Iteration 167, loss = 0.27875395\n",
            "Iteration 168, loss = 0.27872659\n",
            "Iteration 169, loss = 0.27863165\n",
            "Iteration 170, loss = 0.27861805\n",
            "Iteration 171, loss = 0.27855348\n",
            "Iteration 172, loss = 0.27843715\n",
            "Iteration 173, loss = 0.27836686\n",
            "Iteration 174, loss = 0.27827498\n",
            "Iteration 175, loss = 0.27821820\n",
            "Iteration 176, loss = 0.27812188\n",
            "Iteration 177, loss = 0.27803527\n",
            "Iteration 178, loss = 0.27800762\n",
            "Iteration 179, loss = 0.27788446\n",
            "Iteration 180, loss = 0.27781104\n",
            "Iteration 181, loss = 0.27775324\n",
            "Iteration 182, loss = 0.27760780\n",
            "Iteration 183, loss = 0.27756498\n",
            "Iteration 184, loss = 0.27749498\n",
            "Iteration 185, loss = 0.27741429\n",
            "Iteration 186, loss = 0.27748436\n",
            "Iteration 187, loss = 0.27728415\n",
            "Iteration 188, loss = 0.27721550\n",
            "Iteration 189, loss = 0.27715109\n",
            "Iteration 190, loss = 0.27710027\n",
            "Iteration 191, loss = 0.27704340\n",
            "Iteration 192, loss = 0.27695147\n",
            "Iteration 193, loss = 0.27682396\n",
            "Iteration 194, loss = 0.27679812\n",
            "Iteration 195, loss = 0.27670779\n",
            "Iteration 196, loss = 0.27663323\n",
            "Iteration 197, loss = 0.27656202\n",
            "Iteration 198, loss = 0.27659278\n",
            "Iteration 199, loss = 0.27647673\n",
            "Iteration 200, loss = 0.27641060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63602623\n",
            "Iteration 3, loss = 0.60906463\n",
            "Iteration 4, loss = 0.59222764\n",
            "Iteration 5, loss = 0.58174345\n",
            "Iteration 6, loss = 0.57943264\n",
            "Iteration 7, loss = 0.58347288\n",
            "Iteration 8, loss = 0.57686031\n",
            "Iteration 9, loss = 0.57374511\n",
            "Iteration 10, loss = 0.57245149\n",
            "Iteration 11, loss = 0.57361128\n",
            "Iteration 12, loss = 0.57078228\n",
            "Iteration 13, loss = 0.57099389\n",
            "Iteration 14, loss = 0.57057428\n",
            "Iteration 15, loss = 0.57028987\n",
            "Iteration 16, loss = 0.57004259\n",
            "Iteration 17, loss = 0.56954570\n",
            "Iteration 18, loss = 0.56943076\n",
            "Iteration 19, loss = 0.62422380\n",
            "Iteration 20, loss = 0.61883538\n",
            "Iteration 21, loss = 0.61110002\n",
            "Iteration 22, loss = 0.60395936\n",
            "Iteration 23, loss = 0.59939692\n",
            "Iteration 24, loss = 0.58720991\n",
            "Iteration 25, loss = 0.57926575\n",
            "Iteration 26, loss = 0.57627095\n",
            "Iteration 27, loss = 0.57374134\n",
            "Iteration 28, loss = 0.57256004\n",
            "Iteration 29, loss = 0.57135998\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.54166599\n",
            "Iteration 31, loss = 0.52746259\n",
            "Iteration 32, loss = 0.52230238\n",
            "Iteration 33, loss = 0.53070990\n",
            "Iteration 34, loss = 0.53183807\n",
            "Iteration 35, loss = 0.53281691\n",
            "Iteration 36, loss = 0.53219826\n",
            "Iteration 37, loss = 0.52891097\n",
            "Iteration 38, loss = 0.52948296\n",
            "Iteration 39, loss = 0.53341641\n",
            "Iteration 40, loss = 0.52697161\n",
            "Iteration 41, loss = 0.52974329\n",
            "Iteration 42, loss = 0.52690217\n",
            "Iteration 43, loss = 0.52738020\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.45292657\n",
            "Iteration 45, loss = 0.44209408\n",
            "Iteration 46, loss = 0.43212060\n",
            "Iteration 47, loss = 0.42242275\n",
            "Iteration 48, loss = 0.41382120\n",
            "Iteration 49, loss = 0.40567189\n",
            "Iteration 50, loss = 0.39822644\n",
            "Iteration 51, loss = 0.39119323\n",
            "Iteration 52, loss = 0.38544427\n",
            "Iteration 53, loss = 0.37925602\n",
            "Iteration 54, loss = 0.37486938\n",
            "Iteration 55, loss = 0.37088399\n",
            "Iteration 56, loss = 0.36716038\n",
            "Iteration 57, loss = 0.36356576\n",
            "Iteration 58, loss = 0.36086712\n",
            "Iteration 59, loss = 0.35970428\n",
            "Iteration 60, loss = 0.35613921\n",
            "Iteration 61, loss = 0.35413318\n",
            "Iteration 62, loss = 0.35410798\n",
            "Iteration 63, loss = 0.35128798\n",
            "Iteration 64, loss = 0.35249710\n",
            "Iteration 65, loss = 0.35144032\n",
            "Iteration 66, loss = 0.35239794\n",
            "Iteration 67, loss = 0.35126483\n",
            "Iteration 68, loss = 0.35217532\n",
            "Iteration 69, loss = 0.35098275\n",
            "Iteration 70, loss = 0.34862555\n",
            "Iteration 71, loss = 0.35051535\n",
            "Iteration 72, loss = 0.34987887\n",
            "Iteration 73, loss = 0.34795591\n",
            "Iteration 74, loss = 0.34975162\n",
            "Iteration 75, loss = 0.35112840\n",
            "Iteration 76, loss = 0.34942761\n",
            "Iteration 77, loss = 0.34988202\n",
            "Iteration 78, loss = 0.35098960\n",
            "Iteration 79, loss = 0.34529988\n",
            "Iteration 80, loss = 0.34780414\n",
            "Iteration 81, loss = 0.34811513\n",
            "Iteration 82, loss = 0.34875520\n",
            "Iteration 83, loss = 0.34512554\n",
            "Iteration 84, loss = 0.34972216\n",
            "Iteration 85, loss = 0.34448997\n",
            "Iteration 86, loss = 0.34558226\n",
            "Iteration 87, loss = 0.34590227\n",
            "Iteration 88, loss = 0.34322508\n",
            "Iteration 89, loss = 0.34656764\n",
            "Iteration 90, loss = 0.34582240\n",
            "Iteration 91, loss = 0.34355166\n",
            "Iteration 92, loss = 0.34310569\n",
            "Iteration 93, loss = 0.34287368\n",
            "Iteration 94, loss = 0.34084600\n",
            "Iteration 95, loss = 0.34241308\n",
            "Iteration 96, loss = 0.33698291\n",
            "Iteration 97, loss = 0.33886261\n",
            "Iteration 98, loss = 0.34296179\n",
            "Iteration 99, loss = 0.33858214\n",
            "Iteration 100, loss = 0.33668695\n",
            "Iteration 101, loss = 0.33530925\n",
            "Iteration 102, loss = 0.33458764\n",
            "Iteration 103, loss = 0.33649910\n",
            "Iteration 104, loss = 0.32867022\n",
            "Iteration 105, loss = 0.33260157\n",
            "Iteration 106, loss = 0.34155873\n",
            "Iteration 107, loss = 0.33046483\n",
            "Iteration 108, loss = 0.33107859\n",
            "Iteration 109, loss = 0.33758026\n",
            "Iteration 110, loss = 0.33640760\n",
            "Iteration 111, loss = 0.33030059\n",
            "Iteration 112, loss = 0.33103145\n",
            "Iteration 113, loss = 0.33113804\n",
            "Iteration 114, loss = 0.32928367\n",
            "Iteration 115, loss = 0.32782689\n",
            "Iteration 116, loss = 0.33370911\n",
            "Iteration 117, loss = 0.33250140\n",
            "Iteration 118, loss = 0.32818848\n",
            "Iteration 119, loss = 0.32864317\n",
            "Iteration 120, loss = 0.32591770\n",
            "Iteration 121, loss = 0.32278095\n",
            "Iteration 122, loss = 0.32976421\n",
            "Iteration 123, loss = 0.32813103\n",
            "Iteration 124, loss = 0.31667113\n",
            "Iteration 125, loss = 0.32306548\n",
            "Iteration 126, loss = 0.32538256\n",
            "Iteration 127, loss = 0.32496637\n",
            "Iteration 128, loss = 0.32189783\n",
            "Iteration 129, loss = 0.31870197\n",
            "Iteration 130, loss = 0.32166991\n",
            "Iteration 131, loss = 0.32447357\n",
            "Iteration 132, loss = 0.32426875\n",
            "Iteration 133, loss = 0.32013483\n",
            "Iteration 134, loss = 0.32530175\n",
            "Iteration 135, loss = 0.32477830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 136, loss = 0.28057883\n",
            "Iteration 137, loss = 0.28030512\n",
            "Iteration 138, loss = 0.28014912\n",
            "Iteration 139, loss = 0.28003132\n",
            "Iteration 140, loss = 0.27991774\n",
            "Iteration 141, loss = 0.27979931\n",
            "Iteration 142, loss = 0.27974685\n",
            "Iteration 143, loss = 0.27962037\n",
            "Iteration 144, loss = 0.27953407\n",
            "Iteration 145, loss = 0.27952422\n",
            "Iteration 146, loss = 0.27938264\n",
            "Iteration 147, loss = 0.27930723\n",
            "Iteration 148, loss = 0.27922617\n",
            "Iteration 149, loss = 0.27916050\n",
            "Iteration 150, loss = 0.27903660\n",
            "Iteration 151, loss = 0.27900448\n",
            "Iteration 152, loss = 0.27885890\n",
            "Iteration 153, loss = 0.27884359\n",
            "Iteration 154, loss = 0.27877129\n",
            "Iteration 155, loss = 0.27868222\n",
            "Iteration 156, loss = 0.27863530\n",
            "Iteration 157, loss = 0.27847960\n",
            "Iteration 158, loss = 0.27844514\n",
            "Iteration 159, loss = 0.27828339\n",
            "Iteration 160, loss = 0.27830732\n",
            "Iteration 161, loss = 0.27820958\n",
            "Iteration 162, loss = 0.27814697\n",
            "Iteration 163, loss = 0.27814249\n",
            "Iteration 164, loss = 0.27799053\n",
            "Iteration 165, loss = 0.27789691\n",
            "Iteration 166, loss = 0.27780703\n",
            "Iteration 167, loss = 0.27776294\n",
            "Iteration 168, loss = 0.27769087\n",
            "Iteration 169, loss = 0.27765703\n",
            "Iteration 170, loss = 0.27755999\n",
            "Iteration 171, loss = 0.27750906\n",
            "Iteration 172, loss = 0.27749832\n",
            "Iteration 173, loss = 0.27733417\n",
            "Iteration 174, loss = 0.27729434\n",
            "Iteration 175, loss = 0.27718798\n",
            "Iteration 176, loss = 0.27714858\n",
            "Iteration 177, loss = 0.27710056\n",
            "Iteration 178, loss = 0.27703620\n",
            "Iteration 179, loss = 0.27693442\n",
            "Iteration 180, loss = 0.27687041\n",
            "Iteration 181, loss = 0.27674971\n",
            "Iteration 182, loss = 0.27674128\n",
            "Iteration 183, loss = 0.27668352\n",
            "Iteration 184, loss = 0.27660941\n",
            "Iteration 185, loss = 0.27657659\n",
            "Iteration 186, loss = 0.27646771\n",
            "Iteration 187, loss = 0.27640875\n",
            "Iteration 188, loss = 0.27633732\n",
            "Iteration 189, loss = 0.27630726\n",
            "Iteration 190, loss = 0.27621898\n",
            "Iteration 191, loss = 0.27613101\n",
            "Iteration 192, loss = 0.27607939\n",
            "Iteration 193, loss = 0.27599707\n",
            "Iteration 194, loss = 0.27593788\n",
            "Iteration 195, loss = 0.27587912\n",
            "Iteration 196, loss = 0.27589629\n",
            "Iteration 197, loss = 0.27577688\n",
            "Iteration 198, loss = 0.27568407\n",
            "Iteration 199, loss = 0.27563548\n",
            "Iteration 200, loss = 0.27555060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65219564\n",
            "Iteration 3, loss = 0.64860510\n",
            "Iteration 4, loss = 0.62279989\n",
            "Iteration 5, loss = 0.60206625\n",
            "Iteration 6, loss = 0.58988739\n",
            "Iteration 7, loss = 0.58528505\n",
            "Iteration 8, loss = 0.64024932\n",
            "Iteration 9, loss = 0.60201453\n",
            "Iteration 10, loss = 0.58878361\n",
            "Iteration 11, loss = 0.58178471\n",
            "Iteration 12, loss = 0.57867304\n",
            "Iteration 13, loss = 0.57674836\n",
            "Iteration 14, loss = 0.57788355\n",
            "Iteration 15, loss = 0.57529426\n",
            "Iteration 16, loss = 0.57689265\n",
            "Iteration 17, loss = 0.57767738\n",
            "Iteration 18, loss = 0.57424514\n",
            "Iteration 19, loss = 0.57442018\n",
            "Iteration 20, loss = 0.57398918\n",
            "Iteration 21, loss = 0.57391089\n",
            "Iteration 22, loss = 0.57297349\n",
            "Iteration 23, loss = 0.57384514\n",
            "Iteration 24, loss = 0.57405299\n",
            "Iteration 25, loss = 0.57416808\n",
            "Iteration 26, loss = 0.57547541\n",
            "Iteration 27, loss = 0.57483388\n",
            "Iteration 28, loss = 0.57516743\n",
            "Iteration 29, loss = 0.57526191\n",
            "Iteration 30, loss = 0.57484571\n",
            "Iteration 31, loss = 0.59345153\n",
            "Iteration 32, loss = 0.60937502\n",
            "Iteration 33, loss = 0.59076520\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.56635090\n",
            "Iteration 35, loss = 0.55877034\n",
            "Iteration 36, loss = 0.55080826\n",
            "Iteration 37, loss = 0.54231916\n",
            "Iteration 38, loss = 0.53965207\n",
            "Iteration 39, loss = 0.54380942\n",
            "Iteration 40, loss = 0.54337709\n",
            "Iteration 41, loss = 0.54737249\n",
            "Iteration 42, loss = 0.54737645\n",
            "Iteration 43, loss = 0.54187604\n",
            "Iteration 44, loss = 0.54311448\n",
            "Iteration 45, loss = 0.54165859\n",
            "Iteration 46, loss = 0.53662174\n",
            "Iteration 47, loss = 0.53618200\n",
            "Iteration 48, loss = 0.53369903\n",
            "Iteration 49, loss = 0.53322782\n",
            "Iteration 50, loss = 0.53284341\n",
            "Iteration 51, loss = 0.52958468\n",
            "Iteration 52, loss = 0.53136998\n",
            "Iteration 53, loss = 0.53194972\n",
            "Iteration 54, loss = 0.52778114\n",
            "Iteration 55, loss = 0.52941912\n",
            "Iteration 56, loss = 0.52451613\n",
            "Iteration 57, loss = 0.52933717\n",
            "Iteration 58, loss = 0.52786297\n",
            "Iteration 59, loss = 0.52601140\n",
            "Iteration 60, loss = 0.52618576\n",
            "Iteration 61, loss = 0.52822380\n",
            "Iteration 62, loss = 0.52677859\n",
            "Iteration 63, loss = 0.52314585\n",
            "Iteration 64, loss = 0.52543898\n",
            "Iteration 65, loss = 0.52491460\n",
            "Iteration 66, loss = 0.53003869\n",
            "Iteration 67, loss = 0.52387532\n",
            "Iteration 68, loss = 0.53064473\n",
            "Iteration 69, loss = 0.52404655\n",
            "Iteration 70, loss = 0.52813482\n",
            "Iteration 71, loss = 0.52909110\n",
            "Iteration 72, loss = 0.52410838\n",
            "Iteration 73, loss = 0.52775400\n",
            "Iteration 74, loss = 0.53094076\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 75, loss = 0.42775823\n",
            "Iteration 76, loss = 0.41197727\n",
            "Iteration 77, loss = 0.39850969\n",
            "Iteration 78, loss = 0.38645280\n",
            "Iteration 79, loss = 0.37554998\n",
            "Iteration 80, loss = 0.36632890\n",
            "Iteration 81, loss = 0.35818029\n",
            "Iteration 82, loss = 0.35138042\n",
            "Iteration 83, loss = 0.34533552\n",
            "Iteration 84, loss = 0.33982411\n",
            "Iteration 85, loss = 0.33534872\n",
            "Iteration 86, loss = 0.33109797\n",
            "Iteration 87, loss = 0.32723914\n",
            "Iteration 88, loss = 0.32421270\n",
            "Iteration 89, loss = 0.32170671\n",
            "Iteration 90, loss = 0.31925946\n",
            "Iteration 91, loss = 0.31669226\n",
            "Iteration 92, loss = 0.31450053\n",
            "Iteration 93, loss = 0.31369845\n",
            "Iteration 94, loss = 0.31181008\n",
            "Iteration 95, loss = 0.31040587\n",
            "Iteration 96, loss = 0.30930247\n",
            "Iteration 97, loss = 0.30806727\n",
            "Iteration 98, loss = 0.30751598\n",
            "Iteration 99, loss = 0.30651921\n",
            "Iteration 100, loss = 0.30802309\n",
            "Iteration 101, loss = 0.30601339\n",
            "Iteration 102, loss = 0.30540608\n",
            "Iteration 103, loss = 0.30463091\n",
            "Iteration 104, loss = 0.30654790\n",
            "Iteration 105, loss = 0.30831678\n",
            "Iteration 106, loss = 0.30572389\n",
            "Iteration 107, loss = 0.30531001\n",
            "Iteration 108, loss = 0.30360905\n",
            "Iteration 109, loss = 0.30600031\n",
            "Iteration 110, loss = 0.30682065\n",
            "Iteration 111, loss = 0.30595506\n",
            "Iteration 112, loss = 0.30568432\n",
            "Iteration 113, loss = 0.30976194\n",
            "Iteration 114, loss = 0.30708993\n",
            "Iteration 115, loss = 0.30981252\n",
            "Iteration 116, loss = 0.30823110\n",
            "Iteration 117, loss = 0.30848413\n",
            "Iteration 118, loss = 0.31251683\n",
            "Iteration 119, loss = 0.30882420\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 120, loss = 0.28314977\n",
            "Iteration 121, loss = 0.28285683\n",
            "Iteration 122, loss = 0.28268637\n",
            "Iteration 123, loss = 0.28254411\n",
            "Iteration 124, loss = 0.28242119\n",
            "Iteration 125, loss = 0.28229846\n",
            "Iteration 126, loss = 0.28216296\n",
            "Iteration 127, loss = 0.28208838\n",
            "Iteration 128, loss = 0.28193427\n",
            "Iteration 129, loss = 0.28184640\n",
            "Iteration 130, loss = 0.28174440\n",
            "Iteration 131, loss = 0.28167909\n",
            "Iteration 132, loss = 0.28157677\n",
            "Iteration 133, loss = 0.28144627\n",
            "Iteration 134, loss = 0.28131505\n",
            "Iteration 135, loss = 0.28122591\n",
            "Iteration 136, loss = 0.28113826\n",
            "Iteration 137, loss = 0.28096129\n",
            "Iteration 138, loss = 0.28090852\n",
            "Iteration 139, loss = 0.28077739\n",
            "Iteration 140, loss = 0.28066883\n",
            "Iteration 141, loss = 0.28062268\n",
            "Iteration 142, loss = 0.28047491\n",
            "Iteration 143, loss = 0.28037236\n",
            "Iteration 144, loss = 0.28025838\n",
            "Iteration 145, loss = 0.28019678\n",
            "Iteration 146, loss = 0.28005450\n",
            "Iteration 147, loss = 0.27997487\n",
            "Iteration 148, loss = 0.27990290\n",
            "Iteration 149, loss = 0.27982784\n",
            "Iteration 150, loss = 0.27968293\n",
            "Iteration 151, loss = 0.27963701\n",
            "Iteration 152, loss = 0.27950019\n",
            "Iteration 153, loss = 0.27941452\n",
            "Iteration 154, loss = 0.27921743\n",
            "Iteration 155, loss = 0.27912455\n",
            "Iteration 156, loss = 0.27899892\n",
            "Iteration 157, loss = 0.27892921\n",
            "Iteration 158, loss = 0.27885374\n",
            "Iteration 159, loss = 0.27877896\n",
            "Iteration 160, loss = 0.27869043\n",
            "Iteration 161, loss = 0.27858103\n",
            "Iteration 162, loss = 0.27849206\n",
            "Iteration 163, loss = 0.27842869\n",
            "Iteration 164, loss = 0.27831872\n",
            "Iteration 165, loss = 0.27823579\n",
            "Iteration 166, loss = 0.27815685\n",
            "Iteration 167, loss = 0.27804892\n",
            "Iteration 168, loss = 0.27800205\n",
            "Iteration 169, loss = 0.27791184\n",
            "Iteration 170, loss = 0.27781783\n",
            "Iteration 171, loss = 0.27773441\n",
            "Iteration 172, loss = 0.27763017\n",
            "Iteration 173, loss = 0.27758245\n",
            "Iteration 174, loss = 0.27752176\n",
            "Iteration 175, loss = 0.27738584\n",
            "Iteration 176, loss = 0.27736650\n",
            "Iteration 177, loss = 0.27725644\n",
            "Iteration 178, loss = 0.27718680\n",
            "Iteration 179, loss = 0.27710245\n",
            "Iteration 180, loss = 0.27702719\n",
            "Iteration 181, loss = 0.27692573\n",
            "Iteration 182, loss = 0.27688665\n",
            "Iteration 183, loss = 0.27677640\n",
            "Iteration 184, loss = 0.27666294\n",
            "Iteration 185, loss = 0.27666634\n",
            "Iteration 186, loss = 0.27653268\n",
            "Iteration 187, loss = 0.27645580\n",
            "Iteration 188, loss = 0.27643287\n",
            "Iteration 189, loss = 0.27630703\n",
            "Iteration 190, loss = 0.27624731\n",
            "Iteration 191, loss = 0.27618035\n",
            "Iteration 192, loss = 0.27610967\n",
            "Iteration 193, loss = 0.27601589\n",
            "Iteration 194, loss = 0.27593367\n",
            "Iteration 195, loss = 0.27586129\n",
            "Iteration 196, loss = 0.27586349\n",
            "Iteration 197, loss = 0.27570195\n",
            "Iteration 198, loss = 0.27567538\n",
            "Iteration 199, loss = 0.27560723\n",
            "Iteration 200, loss = 0.27552092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64515369\n",
            "Iteration 3, loss = 0.62009897\n",
            "Iteration 4, loss = 0.60239182\n",
            "Iteration 5, loss = 0.59086982\n",
            "Iteration 6, loss = 0.58349076\n",
            "Iteration 7, loss = 0.58102377\n",
            "Iteration 8, loss = 0.57736089\n",
            "Iteration 9, loss = 0.57542004\n",
            "Iteration 10, loss = 0.57519553\n",
            "Iteration 11, loss = 0.57348024\n",
            "Iteration 12, loss = 0.57513480\n",
            "Iteration 13, loss = 0.57398458\n",
            "Iteration 14, loss = 0.57330318\n",
            "Iteration 15, loss = 0.57411727\n",
            "Iteration 16, loss = 0.57427821\n",
            "Iteration 17, loss = 0.57330177\n",
            "Iteration 18, loss = 0.57381219\n",
            "Iteration 19, loss = 0.57557890\n",
            "Iteration 20, loss = 0.57428923\n",
            "Iteration 21, loss = 0.57537117\n",
            "Iteration 22, loss = 0.57538453\n",
            "Iteration 23, loss = 0.57554259\n",
            "Iteration 24, loss = 0.58677576\n",
            "Iteration 25, loss = 0.61283232\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.58172814\n",
            "Iteration 27, loss = 0.57462003\n",
            "Iteration 28, loss = 0.56757608\n",
            "Iteration 29, loss = 0.55957965\n",
            "Iteration 30, loss = 0.55049298\n",
            "Iteration 31, loss = 0.54765547\n",
            "Iteration 32, loss = 0.55038731\n",
            "Iteration 33, loss = 0.54889696\n",
            "Iteration 34, loss = 0.54370364\n",
            "Iteration 35, loss = 0.54669833\n",
            "Iteration 36, loss = 0.54161430\n",
            "Iteration 37, loss = 0.53829968\n",
            "Iteration 38, loss = 0.53877957\n",
            "Iteration 39, loss = 0.53630883\n",
            "Iteration 40, loss = 0.53295490\n",
            "Iteration 41, loss = 0.53203585\n",
            "Iteration 42, loss = 0.53388399\n",
            "Iteration 43, loss = 0.52870352\n",
            "Iteration 44, loss = 0.53218551\n",
            "Iteration 45, loss = 0.52753509\n",
            "Iteration 46, loss = 0.53096811\n",
            "Iteration 47, loss = 0.52696325\n",
            "Iteration 48, loss = 0.53167340\n",
            "Iteration 49, loss = 0.52402945\n",
            "Iteration 50, loss = 0.52762535\n",
            "Iteration 51, loss = 0.52564383\n",
            "Iteration 52, loss = 0.52663536\n",
            "Iteration 53, loss = 0.52737907\n",
            "Iteration 54, loss = 0.52869947\n",
            "Iteration 55, loss = 0.52406961\n",
            "Iteration 56, loss = 0.52748061\n",
            "Iteration 57, loss = 0.52670300\n",
            "Iteration 58, loss = 0.52755590\n",
            "Iteration 59, loss = 0.52860647\n",
            "Iteration 60, loss = 0.52628498\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 61, loss = 0.43168469\n",
            "Iteration 62, loss = 0.41651105\n",
            "Iteration 63, loss = 0.40371944\n",
            "Iteration 64, loss = 0.39230346\n",
            "Iteration 65, loss = 0.38217122\n",
            "Iteration 66, loss = 0.37321313\n",
            "Iteration 67, loss = 0.36529136\n",
            "Iteration 68, loss = 0.35797345\n",
            "Iteration 69, loss = 0.35218215\n",
            "Iteration 70, loss = 0.34711177\n",
            "Iteration 71, loss = 0.34221561\n",
            "Iteration 72, loss = 0.33792935\n",
            "Iteration 73, loss = 0.33478908\n",
            "Iteration 74, loss = 0.33146604\n",
            "Iteration 75, loss = 0.32863338\n",
            "Iteration 76, loss = 0.32558496\n",
            "Iteration 77, loss = 0.32398532\n",
            "Iteration 78, loss = 0.32306578\n",
            "Iteration 79, loss = 0.32045196\n",
            "Iteration 80, loss = 0.31875649\n",
            "Iteration 81, loss = 0.31846488\n",
            "Iteration 82, loss = 0.31836785\n",
            "Iteration 83, loss = 0.31648716\n",
            "Iteration 84, loss = 0.31530996\n",
            "Iteration 85, loss = 0.31473821\n",
            "Iteration 86, loss = 0.31471169\n",
            "Iteration 87, loss = 0.31360596\n",
            "Iteration 88, loss = 0.31396603\n",
            "Iteration 89, loss = 0.31423692\n",
            "Iteration 90, loss = 0.31477130\n",
            "Iteration 91, loss = 0.31414601\n",
            "Iteration 92, loss = 0.31532059\n",
            "Iteration 93, loss = 0.31545062\n",
            "Iteration 94, loss = 0.31746216\n",
            "Iteration 95, loss = 0.31510738\n",
            "Iteration 96, loss = 0.31511661\n",
            "Iteration 97, loss = 0.31666735\n",
            "Iteration 98, loss = 0.31643248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 99, loss = 0.29181525\n",
            "Iteration 100, loss = 0.29157212\n",
            "Iteration 101, loss = 0.29140146\n",
            "Iteration 102, loss = 0.29122590\n",
            "Iteration 103, loss = 0.29101979\n",
            "Iteration 104, loss = 0.29092089\n",
            "Iteration 105, loss = 0.29078001\n",
            "Iteration 106, loss = 0.29064780\n",
            "Iteration 107, loss = 0.29045387\n",
            "Iteration 108, loss = 0.29027294\n",
            "Iteration 109, loss = 0.29018165\n",
            "Iteration 110, loss = 0.29003946\n",
            "Iteration 111, loss = 0.28987607\n",
            "Iteration 112, loss = 0.28970076\n",
            "Iteration 113, loss = 0.28955302\n",
            "Iteration 114, loss = 0.28946137\n",
            "Iteration 115, loss = 0.28934476\n",
            "Iteration 116, loss = 0.28915263\n",
            "Iteration 117, loss = 0.28900080\n",
            "Iteration 118, loss = 0.28884555\n",
            "Iteration 119, loss = 0.28877250\n",
            "Iteration 120, loss = 0.28863297\n",
            "Iteration 121, loss = 0.28847780\n",
            "Iteration 122, loss = 0.28838692\n",
            "Iteration 123, loss = 0.28820991\n",
            "Iteration 124, loss = 0.28806952\n",
            "Iteration 125, loss = 0.28797574\n",
            "Iteration 126, loss = 0.28787137\n",
            "Iteration 127, loss = 0.28767999\n",
            "Iteration 128, loss = 0.28752852\n",
            "Iteration 129, loss = 0.28747936\n",
            "Iteration 130, loss = 0.28729965\n",
            "Iteration 131, loss = 0.28717790\n",
            "Iteration 132, loss = 0.28708144\n",
            "Iteration 133, loss = 0.28692263\n",
            "Iteration 134, loss = 0.28686069\n",
            "Iteration 135, loss = 0.28674855\n",
            "Iteration 136, loss = 0.28661018\n",
            "Iteration 137, loss = 0.28650475\n",
            "Iteration 138, loss = 0.28644262\n",
            "Iteration 139, loss = 0.28623807\n",
            "Iteration 140, loss = 0.28614861\n",
            "Iteration 141, loss = 0.28598106\n",
            "Iteration 142, loss = 0.28592079\n",
            "Iteration 143, loss = 0.28576723\n",
            "Iteration 144, loss = 0.28569589\n",
            "Iteration 145, loss = 0.28559167\n",
            "Iteration 146, loss = 0.28548042\n",
            "Iteration 147, loss = 0.28533330\n",
            "Iteration 148, loss = 0.28524707\n",
            "Iteration 149, loss = 0.28517370\n",
            "Iteration 150, loss = 0.28497253\n",
            "Iteration 151, loss = 0.28491027\n",
            "Iteration 152, loss = 0.28479865\n",
            "Iteration 153, loss = 0.28467248\n",
            "Iteration 154, loss = 0.28458224\n",
            "Iteration 155, loss = 0.28450044\n",
            "Iteration 156, loss = 0.28438007\n",
            "Iteration 157, loss = 0.28427959\n",
            "Iteration 158, loss = 0.28413551\n",
            "Iteration 159, loss = 0.28409201\n",
            "Iteration 160, loss = 0.28400312\n",
            "Iteration 161, loss = 0.28393979\n",
            "Iteration 162, loss = 0.28376018\n",
            "Iteration 163, loss = 0.28364559\n",
            "Iteration 164, loss = 0.28363115\n",
            "Iteration 165, loss = 0.28351797\n",
            "Iteration 166, loss = 0.28339032\n",
            "Iteration 167, loss = 0.28333184\n",
            "Iteration 168, loss = 0.28316746\n",
            "Iteration 169, loss = 0.28306349\n",
            "Iteration 170, loss = 0.28303138\n",
            "Iteration 171, loss = 0.28289747\n",
            "Iteration 172, loss = 0.28286973\n",
            "Iteration 173, loss = 0.28277538\n",
            "Iteration 174, loss = 0.28264097\n",
            "Iteration 175, loss = 0.28256776\n",
            "Iteration 176, loss = 0.28243174\n",
            "Iteration 177, loss = 0.28246371\n",
            "Iteration 178, loss = 0.28233003\n",
            "Iteration 179, loss = 0.28218780\n",
            "Iteration 180, loss = 0.28207945\n",
            "Iteration 181, loss = 0.28208527\n",
            "Iteration 182, loss = 0.28194169\n",
            "Iteration 183, loss = 0.28190250\n",
            "Iteration 184, loss = 0.28172900\n",
            "Iteration 185, loss = 0.28173607\n",
            "Iteration 186, loss = 0.28159635\n",
            "Iteration 187, loss = 0.28154432\n",
            "Iteration 188, loss = 0.28141834\n",
            "Iteration 189, loss = 0.28140333\n",
            "Iteration 190, loss = 0.28127183\n",
            "Iteration 191, loss = 0.28119781\n",
            "Iteration 192, loss = 0.28109601\n",
            "Iteration 193, loss = 0.28105589\n",
            "Iteration 194, loss = 0.28095892\n",
            "Iteration 195, loss = 0.28085458\n",
            "Iteration 196, loss = 0.28072038\n",
            "Iteration 197, loss = 0.28071809\n",
            "Iteration 198, loss = 0.28066851\n",
            "Iteration 199, loss = 0.28053292\n",
            "Iteration 200, loss = 0.28042699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63675526\n",
            "Iteration 3, loss = 0.60919094\n",
            "Iteration 4, loss = 0.59321967\n",
            "Iteration 5, loss = 0.58382797\n",
            "Iteration 6, loss = 0.57777735\n",
            "Iteration 7, loss = 0.57233583\n",
            "Iteration 8, loss = 0.57189116\n",
            "Iteration 9, loss = 0.56907490\n",
            "Iteration 10, loss = 0.57021854\n",
            "Iteration 11, loss = 0.56920199\n",
            "Iteration 12, loss = 0.56973440\n",
            "Iteration 13, loss = 0.56908948\n",
            "Iteration 14, loss = 0.57046845\n",
            "Iteration 15, loss = 0.56767828\n",
            "Iteration 16, loss = 0.56954050\n",
            "Iteration 17, loss = 0.57003870\n",
            "Iteration 18, loss = 0.56964590\n",
            "Iteration 19, loss = 0.57268891\n",
            "Iteration 20, loss = 0.57304770\n",
            "Iteration 21, loss = 0.57266898\n",
            "Iteration 22, loss = 0.57352919\n",
            "Iteration 23, loss = 0.57149668\n",
            "Iteration 24, loss = 0.57308690\n",
            "Iteration 25, loss = 0.57331378\n",
            "Iteration 26, loss = 0.57168521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.50571615\n",
            "Iteration 28, loss = 0.48648345\n",
            "Iteration 29, loss = 0.51152391\n",
            "Iteration 30, loss = 0.51996263\n",
            "Iteration 31, loss = 0.51133696\n",
            "Iteration 32, loss = 0.51151281\n",
            "Iteration 33, loss = 0.51771172\n",
            "Iteration 34, loss = 0.51704140\n",
            "Iteration 35, loss = 0.51401732\n",
            "Iteration 36, loss = 0.51059762\n",
            "Iteration 37, loss = 0.51668326\n",
            "Iteration 38, loss = 0.51167630\n",
            "Iteration 39, loss = 0.51577365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 40, loss = 0.41153148\n",
            "Iteration 41, loss = 0.39664212\n",
            "Iteration 42, loss = 0.38323758\n",
            "Iteration 43, loss = 0.37112750\n",
            "Iteration 44, loss = 0.36025620\n",
            "Iteration 45, loss = 0.35030817\n",
            "Iteration 46, loss = 0.34143930\n",
            "Iteration 47, loss = 0.33355493\n",
            "Iteration 48, loss = 0.32650373\n",
            "Iteration 49, loss = 0.32041902\n",
            "Iteration 50, loss = 0.31447149\n",
            "Iteration 51, loss = 0.30911047\n",
            "Iteration 52, loss = 0.30448319\n",
            "Iteration 53, loss = 0.29987120\n",
            "Iteration 54, loss = 0.29628320\n",
            "Iteration 55, loss = 0.29296056\n",
            "Iteration 56, loss = 0.28976833\n",
            "Iteration 57, loss = 0.28689288\n",
            "Iteration 58, loss = 0.28438521\n",
            "Iteration 59, loss = 0.28180400\n",
            "Iteration 60, loss = 0.27943762\n",
            "Iteration 61, loss = 0.27762563\n",
            "Iteration 62, loss = 0.27610094\n",
            "Iteration 63, loss = 0.27379092\n",
            "Iteration 64, loss = 0.27347192\n",
            "Iteration 65, loss = 0.27149011\n",
            "Iteration 66, loss = 0.27077667\n",
            "Iteration 67, loss = 0.26901280\n",
            "Iteration 68, loss = 0.26880275\n",
            "Iteration 69, loss = 0.26803914\n",
            "Iteration 70, loss = 0.26780570\n",
            "Iteration 71, loss = 0.26687131\n",
            "Iteration 72, loss = 0.26510963\n",
            "Iteration 73, loss = 0.26565586\n",
            "Iteration 74, loss = 0.26584001\n",
            "Iteration 75, loss = 0.26362700\n",
            "Iteration 76, loss = 0.26358737\n",
            "Iteration 77, loss = 0.26345562\n",
            "Iteration 78, loss = 0.26272549\n",
            "Iteration 79, loss = 0.26318479\n",
            "Iteration 80, loss = 0.26417351\n",
            "Iteration 81, loss = 0.26133441\n",
            "Iteration 82, loss = 0.26269708\n",
            "Iteration 83, loss = 0.26458484\n",
            "Iteration 84, loss = 0.26871768\n",
            "Iteration 85, loss = 0.26226023\n",
            "Iteration 86, loss = 0.26218504\n",
            "Iteration 87, loss = 0.26466139\n",
            "Iteration 88, loss = 0.26657690\n",
            "Iteration 89, loss = 0.26337547\n",
            "Iteration 90, loss = 0.26045973\n",
            "Iteration 91, loss = 0.26306281\n",
            "Iteration 92, loss = 0.26437703\n",
            "Iteration 93, loss = 0.26958144\n",
            "Iteration 94, loss = 0.27382994\n",
            "Iteration 95, loss = 0.25867497\n",
            "Iteration 96, loss = 0.26855017\n",
            "Iteration 97, loss = 0.26556899\n",
            "Iteration 98, loss = 0.26286036\n",
            "Iteration 99, loss = 0.26768283\n",
            "Iteration 100, loss = 0.27178976\n",
            "Iteration 101, loss = 0.26372417\n",
            "Iteration 102, loss = 0.27894832\n",
            "Iteration 103, loss = 0.27141069\n",
            "Iteration 104, loss = 0.26830821\n",
            "Iteration 105, loss = 0.28776121\n",
            "Iteration 106, loss = 0.28724594\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 107, loss = 0.24932919\n",
            "Iteration 108, loss = 0.24915335\n",
            "Iteration 109, loss = 0.24908273\n",
            "Iteration 110, loss = 0.24906272\n",
            "Iteration 111, loss = 0.24900478\n",
            "Iteration 112, loss = 0.24896828\n",
            "Iteration 113, loss = 0.24896555\n",
            "Iteration 114, loss = 0.24894015\n",
            "Iteration 115, loss = 0.24892225\n",
            "Iteration 116, loss = 0.24883332\n",
            "Iteration 117, loss = 0.24881514\n",
            "Iteration 118, loss = 0.24879611\n",
            "Iteration 119, loss = 0.24873935\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.24848364\n",
            "Iteration 121, loss = 0.24847562\n",
            "Iteration 122, loss = 0.24846125\n",
            "Iteration 123, loss = 0.24847486\n",
            "Iteration 124, loss = 0.24846982\n",
            "Iteration 125, loss = 0.24846260\n",
            "Iteration 126, loss = 0.24846726\n",
            "Iteration 127, loss = 0.24845781\n",
            "Iteration 128, loss = 0.24842374\n",
            "Iteration 129, loss = 0.24843881\n",
            "Iteration 130, loss = 0.24843735\n",
            "Iteration 131, loss = 0.24840942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 132, loss = 0.24834744\n",
            "Iteration 133, loss = 0.24833266\n",
            "Iteration 134, loss = 0.24832866\n",
            "Iteration 135, loss = 0.24833900\n",
            "Iteration 136, loss = 0.24833111\n",
            "Iteration 137, loss = 0.24834761\n",
            "Iteration 138, loss = 0.24833877\n",
            "Iteration 139, loss = 0.24833765\n",
            "Iteration 140, loss = 0.24832973\n",
            "Iteration 141, loss = 0.24833180\n",
            "Iteration 142, loss = 0.24832636\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63546639\n",
            "Iteration 3, loss = 0.60945181\n",
            "Iteration 4, loss = 0.59408249\n",
            "Iteration 5, loss = 0.58443102\n",
            "Iteration 6, loss = 0.57804583\n",
            "Iteration 7, loss = 0.57364418\n",
            "Iteration 8, loss = 0.57079274\n",
            "Iteration 9, loss = 0.57129151\n",
            "Iteration 10, loss = 0.57067156\n",
            "Iteration 11, loss = 0.56856879\n",
            "Iteration 12, loss = 0.56971787\n",
            "Iteration 13, loss = 0.57038478\n",
            "Iteration 14, loss = 0.56665019\n",
            "Iteration 15, loss = 0.57256785\n",
            "Iteration 16, loss = 0.57165028\n",
            "Iteration 17, loss = 0.57170390\n",
            "Iteration 18, loss = 0.57442242\n",
            "Iteration 19, loss = 0.57214574\n",
            "Iteration 20, loss = 0.57366679\n",
            "Iteration 21, loss = 0.57228868\n",
            "Iteration 22, loss = 0.57152547\n",
            "Iteration 23, loss = 0.57145232\n",
            "Iteration 24, loss = 0.57122035\n",
            "Iteration 25, loss = 0.57032019\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.50658926\n",
            "Iteration 27, loss = 0.48753302\n",
            "Iteration 28, loss = 0.51369736\n",
            "Iteration 29, loss = 0.51635568\n",
            "Iteration 30, loss = 0.51283909\n",
            "Iteration 31, loss = 0.51552093\n",
            "Iteration 32, loss = 0.51440252\n",
            "Iteration 33, loss = 0.51370343\n",
            "Iteration 34, loss = 0.51481608\n",
            "Iteration 35, loss = 0.51149715\n",
            "Iteration 36, loss = 0.51807512\n",
            "Iteration 37, loss = 0.50865397\n",
            "Iteration 38, loss = 0.51862125\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.41317058\n",
            "Iteration 40, loss = 0.39788594\n",
            "Iteration 41, loss = 0.38433074\n",
            "Iteration 42, loss = 0.37205811\n",
            "Iteration 43, loss = 0.36129158\n",
            "Iteration 44, loss = 0.35154125\n",
            "Iteration 45, loss = 0.34280835\n",
            "Iteration 46, loss = 0.33464053\n",
            "Iteration 47, loss = 0.32745616\n",
            "Iteration 48, loss = 0.32123050\n",
            "Iteration 49, loss = 0.31548411\n",
            "Iteration 50, loss = 0.31017126\n",
            "Iteration 51, loss = 0.30577654\n",
            "Iteration 52, loss = 0.30165831\n",
            "Iteration 53, loss = 0.29740358\n",
            "Iteration 54, loss = 0.29369073\n",
            "Iteration 55, loss = 0.29110670\n",
            "Iteration 56, loss = 0.28836831\n",
            "Iteration 57, loss = 0.28543701\n",
            "Iteration 58, loss = 0.28329632\n",
            "Iteration 59, loss = 0.28102848\n",
            "Iteration 60, loss = 0.27880603\n",
            "Iteration 61, loss = 0.27711389\n",
            "Iteration 62, loss = 0.27528585\n",
            "Iteration 63, loss = 0.27443627\n",
            "Iteration 64, loss = 0.27234582\n",
            "Iteration 65, loss = 0.27206557\n",
            "Iteration 66, loss = 0.27057644\n",
            "Iteration 67, loss = 0.26927019\n",
            "Iteration 68, loss = 0.26851076\n",
            "Iteration 69, loss = 0.26862444\n",
            "Iteration 70, loss = 0.26794288\n",
            "Iteration 71, loss = 0.26693377\n",
            "Iteration 72, loss = 0.26585967\n",
            "Iteration 73, loss = 0.26573656\n",
            "Iteration 74, loss = 0.26636710\n",
            "Iteration 75, loss = 0.26382911\n",
            "Iteration 76, loss = 0.26677625\n",
            "Iteration 77, loss = 0.26319272\n",
            "Iteration 78, loss = 0.26355278\n",
            "Iteration 79, loss = 0.26570796\n",
            "Iteration 80, loss = 0.26368765\n",
            "Iteration 81, loss = 0.26506021\n",
            "Iteration 82, loss = 0.26878498\n",
            "Iteration 83, loss = 0.26587973\n",
            "Iteration 84, loss = 0.26158404\n",
            "Iteration 85, loss = 0.26579415\n",
            "Iteration 86, loss = 0.26627677\n",
            "Iteration 87, loss = 0.26919250\n",
            "Iteration 88, loss = 0.27131931\n",
            "Iteration 89, loss = 0.26842919\n",
            "Iteration 90, loss = 0.26152170\n",
            "Iteration 91, loss = 0.26628331\n",
            "Iteration 92, loss = 0.27034148\n",
            "Iteration 93, loss = 0.26272364\n",
            "Iteration 94, loss = 0.26617051\n",
            "Iteration 95, loss = 0.26700602\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 96, loss = 0.25093404\n",
            "Iteration 97, loss = 0.25085751\n",
            "Iteration 98, loss = 0.25077633\n",
            "Iteration 99, loss = 0.25076575\n",
            "Iteration 100, loss = 0.25074550\n",
            "Iteration 101, loss = 0.25063887\n",
            "Iteration 102, loss = 0.25064947\n",
            "Iteration 103, loss = 0.25058025\n",
            "Iteration 104, loss = 0.25054486\n",
            "Iteration 105, loss = 0.25052455\n",
            "Iteration 106, loss = 0.25048741\n",
            "Iteration 107, loss = 0.25046431\n",
            "Iteration 108, loss = 0.25041103\n",
            "Iteration 109, loss = 0.25032851\n",
            "Iteration 110, loss = 0.25030236\n",
            "Iteration 111, loss = 0.25024795\n",
            "Iteration 112, loss = 0.25027642\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.24996682\n",
            "Iteration 114, loss = 0.24993641\n",
            "Iteration 115, loss = 0.24993996\n",
            "Iteration 116, loss = 0.24991847\n",
            "Iteration 117, loss = 0.24991022\n",
            "Iteration 118, loss = 0.24992644\n",
            "Iteration 119, loss = 0.24990269\n",
            "Iteration 120, loss = 0.24991175\n",
            "Iteration 121, loss = 0.24987140\n",
            "Iteration 122, loss = 0.24985400\n",
            "Iteration 123, loss = 0.24989869\n",
            "Iteration 124, loss = 0.24987370\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 125, loss = 0.24977942\n",
            "Iteration 126, loss = 0.24977838\n",
            "Iteration 127, loss = 0.24976194\n",
            "Iteration 128, loss = 0.24978427\n",
            "Iteration 129, loss = 0.24977540\n",
            "Iteration 130, loss = 0.24977023\n",
            "Iteration 131, loss = 0.24976882\n",
            "Iteration 132, loss = 0.24976981\n",
            "Iteration 133, loss = 0.24976854\n",
            "Iteration 134, loss = 0.24976024\n",
            "Iteration 135, loss = 0.24976943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 40.7min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68853407\n",
            "Iteration 3, loss = 0.67509929\n",
            "Iteration 4, loss = 0.67137977\n",
            "Iteration 5, loss = 0.66314035\n",
            "Iteration 6, loss = 0.65096588\n",
            "Iteration 7, loss = 0.64085303\n",
            "Iteration 8, loss = 0.63249338\n",
            "Iteration 9, loss = 0.62455792\n",
            "Iteration 10, loss = 0.61932428\n",
            "Iteration 11, loss = 0.61396321\n",
            "Iteration 12, loss = 0.60854612\n",
            "Iteration 13, loss = 0.60327906\n",
            "Iteration 14, loss = 0.60175193\n",
            "Iteration 15, loss = 0.59682882\n",
            "Iteration 16, loss = 0.59527512\n",
            "Iteration 17, loss = 0.59302549\n",
            "Iteration 18, loss = 0.59241052\n",
            "Iteration 19, loss = 0.58820225\n",
            "Iteration 20, loss = 0.58757275\n",
            "Iteration 21, loss = 0.58659844\n",
            "Iteration 22, loss = 0.58021135\n",
            "Iteration 23, loss = 0.58050408\n",
            "Iteration 24, loss = 0.58119865\n",
            "Iteration 25, loss = 0.57932160\n",
            "Iteration 26, loss = 0.58066260\n",
            "Iteration 27, loss = 0.58372670\n",
            "Iteration 28, loss = 0.57606464\n",
            "Iteration 29, loss = 0.57765786\n",
            "Iteration 30, loss = 0.57543036\n",
            "Iteration 31, loss = 0.57905725\n",
            "Iteration 32, loss = 0.57648643\n",
            "Iteration 33, loss = 0.57586925\n",
            "Iteration 34, loss = 0.57296390\n",
            "Iteration 35, loss = 0.57639625\n",
            "Iteration 36, loss = 0.57636246\n",
            "Iteration 37, loss = 0.57643969\n",
            "Iteration 38, loss = 0.57485969\n",
            "Iteration 39, loss = 0.57643489\n",
            "Iteration 40, loss = 0.57205150\n",
            "Iteration 41, loss = 0.57787799\n",
            "Iteration 42, loss = 0.57456086\n",
            "Iteration 43, loss = 0.57585023\n",
            "Iteration 44, loss = 0.57297614\n",
            "Iteration 45, loss = 0.57441418\n",
            "Iteration 46, loss = 0.58006420\n",
            "Iteration 47, loss = 0.57379920\n",
            "Iteration 48, loss = 0.57254354\n",
            "Iteration 49, loss = 0.57392196\n",
            "Iteration 50, loss = 0.57584595\n",
            "Iteration 51, loss = 0.57406367\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.53927033\n",
            "Iteration 53, loss = 0.53015472\n",
            "Iteration 54, loss = 0.52418394\n",
            "Iteration 55, loss = 0.51879145\n",
            "Iteration 56, loss = 0.51526090\n",
            "Iteration 57, loss = 0.50932650\n",
            "Iteration 58, loss = 0.50395527\n",
            "Iteration 59, loss = 0.51069446\n",
            "Iteration 60, loss = 0.51454465\n",
            "Iteration 61, loss = 0.51562272\n",
            "Iteration 62, loss = 0.52482357\n",
            "Iteration 63, loss = 0.52761894\n",
            "Iteration 64, loss = 0.52115038\n",
            "Iteration 65, loss = 0.53007644\n",
            "Iteration 66, loss = 0.53313408\n",
            "Iteration 67, loss = 0.53232769\n",
            "Iteration 68, loss = 0.52862850\n",
            "Iteration 69, loss = 0.52485977\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 70, loss = 0.47220596\n",
            "Iteration 71, loss = 0.46433596\n",
            "Iteration 72, loss = 0.46194147\n",
            "Iteration 73, loss = 0.45958995\n",
            "Iteration 74, loss = 0.45719399\n",
            "Iteration 75, loss = 0.45479774\n",
            "Iteration 76, loss = 0.45251789\n",
            "Iteration 77, loss = 0.45038435\n",
            "Iteration 78, loss = 0.44842908\n",
            "Iteration 79, loss = 0.44659245\n",
            "Iteration 80, loss = 0.44400958\n",
            "Iteration 81, loss = 0.44203009\n",
            "Iteration 82, loss = 0.43968446\n",
            "Iteration 83, loss = 0.43764677\n",
            "Iteration 84, loss = 0.43581842\n",
            "Iteration 85, loss = 0.43388349\n",
            "Iteration 86, loss = 0.43165992\n",
            "Iteration 87, loss = 0.42960538\n",
            "Iteration 88, loss = 0.42781526\n",
            "Iteration 89, loss = 0.42602684\n",
            "Iteration 90, loss = 0.42415176\n",
            "Iteration 91, loss = 0.42252140\n",
            "Iteration 92, loss = 0.42055684\n",
            "Iteration 93, loss = 0.41843007\n",
            "Iteration 94, loss = 0.41638589\n",
            "Iteration 95, loss = 0.41518788\n",
            "Iteration 96, loss = 0.41259787\n",
            "Iteration 97, loss = 0.41083398\n",
            "Iteration 98, loss = 0.40918686\n",
            "Iteration 99, loss = 0.40799145\n",
            "Iteration 100, loss = 0.40635701\n",
            "Iteration 101, loss = 0.40473342\n",
            "Iteration 102, loss = 0.40282598\n",
            "Iteration 103, loss = 0.40192222\n",
            "Iteration 104, loss = 0.39978435\n",
            "Iteration 105, loss = 0.39886850\n",
            "Iteration 106, loss = 0.39664658\n",
            "Iteration 107, loss = 0.39594832\n",
            "Iteration 108, loss = 0.39482629\n",
            "Iteration 109, loss = 0.39400633\n",
            "Iteration 110, loss = 0.39178594\n",
            "Iteration 111, loss = 0.39112011\n",
            "Iteration 112, loss = 0.38962453\n",
            "Iteration 113, loss = 0.38768185\n",
            "Iteration 114, loss = 0.38755543\n",
            "Iteration 115, loss = 0.38664339\n",
            "Iteration 116, loss = 0.38560215\n",
            "Iteration 117, loss = 0.38411839\n",
            "Iteration 118, loss = 0.38350122\n",
            "Iteration 119, loss = 0.38354541\n",
            "Iteration 120, loss = 0.37980898\n",
            "Iteration 121, loss = 0.37989950\n",
            "Iteration 122, loss = 0.38042974\n",
            "Iteration 123, loss = 0.38034377\n",
            "Iteration 124, loss = 0.37914285\n",
            "Iteration 125, loss = 0.37794322\n",
            "Iteration 126, loss = 0.37655310\n",
            "Iteration 127, loss = 0.37704209\n",
            "Iteration 128, loss = 0.37536874\n",
            "Iteration 129, loss = 0.37416057\n",
            "Iteration 130, loss = 0.37414693\n",
            "Iteration 131, loss = 0.37373796\n",
            "Iteration 132, loss = 0.37345041\n",
            "Iteration 133, loss = 0.37675110\n",
            "Iteration 134, loss = 0.37342302\n",
            "Iteration 135, loss = 0.37059624\n",
            "Iteration 136, loss = 0.37289113\n",
            "Iteration 137, loss = 0.37113742\n",
            "Iteration 138, loss = 0.36877191\n",
            "Iteration 139, loss = 0.36880886\n",
            "Iteration 140, loss = 0.36947930\n",
            "Iteration 141, loss = 0.36917014\n",
            "Iteration 142, loss = 0.36993811\n",
            "Iteration 143, loss = 0.37063295\n",
            "Iteration 144, loss = 0.36782294\n",
            "Iteration 145, loss = 0.37246913\n",
            "Iteration 146, loss = 0.36700996\n",
            "Iteration 147, loss = 0.36936600\n",
            "Iteration 148, loss = 0.36873494\n",
            "Iteration 149, loss = 0.36605906\n",
            "Iteration 150, loss = 0.36996155\n",
            "Iteration 151, loss = 0.36424474\n",
            "Iteration 152, loss = 0.36783781\n",
            "Iteration 153, loss = 0.37424975\n",
            "Iteration 154, loss = 0.36658467\n",
            "Iteration 155, loss = 0.36751900\n",
            "Iteration 156, loss = 0.36723318\n",
            "Iteration 157, loss = 0.37246935\n",
            "Iteration 158, loss = 0.36890004\n",
            "Iteration 159, loss = 0.37004311\n",
            "Iteration 160, loss = 0.36628079\n",
            "Iteration 161, loss = 0.36343921\n",
            "Iteration 162, loss = 0.36664173\n",
            "Iteration 163, loss = 0.36288222\n",
            "Iteration 164, loss = 0.36640372\n",
            "Iteration 165, loss = 0.36320713\n",
            "Iteration 166, loss = 0.36950323\n",
            "Iteration 167, loss = 0.36654352\n",
            "Iteration 168, loss = 0.36941259\n",
            "Iteration 169, loss = 0.36550835\n",
            "Iteration 170, loss = 0.36852055\n",
            "Iteration 171, loss = 0.36135472\n",
            "Iteration 172, loss = 0.36562797\n",
            "Iteration 173, loss = 0.36259785\n",
            "Iteration 174, loss = 0.37009082\n",
            "Iteration 175, loss = 0.36050825\n",
            "Iteration 176, loss = 0.36477202\n",
            "Iteration 177, loss = 0.37079620\n",
            "Iteration 178, loss = 0.36281073\n",
            "Iteration 179, loss = 0.36616937\n",
            "Iteration 180, loss = 0.36484775\n",
            "Iteration 181, loss = 0.35564253\n",
            "Iteration 182, loss = 0.36397316\n",
            "Iteration 183, loss = 0.36439235\n",
            "Iteration 184, loss = 0.35964458\n",
            "Iteration 185, loss = 0.36755870\n",
            "Iteration 186, loss = 0.36517176\n",
            "Iteration 187, loss = 0.36968293\n",
            "Iteration 188, loss = 0.35815559\n",
            "Iteration 189, loss = 0.36278638\n",
            "Iteration 190, loss = 0.36405335\n",
            "Iteration 191, loss = 0.36745522\n",
            "Iteration 192, loss = 0.35973655\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 193, loss = 0.32680278\n",
            "Iteration 194, loss = 0.32533944\n",
            "Iteration 195, loss = 0.32508517\n",
            "Iteration 196, loss = 0.32502975\n",
            "Iteration 197, loss = 0.32471229\n",
            "Iteration 198, loss = 0.32475085\n",
            "Iteration 199, loss = 0.32446906\n",
            "Iteration 200, loss = 0.32435925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72733177\n",
            "Iteration 3, loss = 0.71245451\n",
            "Iteration 4, loss = 0.69872315\n",
            "Iteration 5, loss = 0.68376707\n",
            "Iteration 6, loss = 0.66620365\n",
            "Iteration 7, loss = 0.65384998\n",
            "Iteration 8, loss = 0.64529049\n",
            "Iteration 9, loss = 0.63601467\n",
            "Iteration 10, loss = 0.62746117\n",
            "Iteration 11, loss = 0.62185033\n",
            "Iteration 12, loss = 0.61632123\n",
            "Iteration 13, loss = 0.61356729\n",
            "Iteration 14, loss = 0.60856321\n",
            "Iteration 15, loss = 0.60302250\n",
            "Iteration 16, loss = 0.59977594\n",
            "Iteration 17, loss = 0.59902197\n",
            "Iteration 18, loss = 0.59049459\n",
            "Iteration 19, loss = 0.59282133\n",
            "Iteration 20, loss = 0.59095553\n",
            "Iteration 21, loss = 0.58784006\n",
            "Iteration 22, loss = 0.66694409\n",
            "Iteration 23, loss = 0.64293090\n",
            "Iteration 24, loss = 0.63242130\n",
            "Iteration 25, loss = 0.62703593\n",
            "Iteration 26, loss = 0.62334525\n",
            "Iteration 27, loss = 0.62101729\n",
            "Iteration 28, loss = 0.62011378\n",
            "Iteration 29, loss = 0.61814360\n",
            "Iteration 30, loss = 0.61647862\n",
            "Iteration 31, loss = 0.61507021\n",
            "Iteration 32, loss = 0.61469305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 33, loss = 0.61298920\n",
            "Iteration 34, loss = 0.61278042\n",
            "Iteration 35, loss = 0.61259210\n",
            "Iteration 36, loss = 0.61233291\n",
            "Iteration 37, loss = 0.61216689\n",
            "Iteration 38, loss = 0.61198338\n",
            "Iteration 39, loss = 0.61206607\n",
            "Iteration 40, loss = 0.61152181\n",
            "Iteration 41, loss = 0.61159841\n",
            "Iteration 42, loss = 0.61131773\n",
            "Iteration 43, loss = 0.61102248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.61091819\n",
            "Iteration 45, loss = 0.61087248\n",
            "Iteration 46, loss = 0.61081803\n",
            "Iteration 47, loss = 0.61066128\n",
            "Iteration 48, loss = 0.61070907\n",
            "Iteration 49, loss = 0.61070380\n",
            "Iteration 50, loss = 0.61066421\n",
            "Iteration 51, loss = 0.61062246\n",
            "Iteration 52, loss = 0.61056323\n",
            "Iteration 53, loss = 0.61056285\n",
            "Iteration 54, loss = 0.61058389\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 55, loss = 0.61064365\n",
            "Iteration 56, loss = 0.61043852\n",
            "Iteration 57, loss = 0.61040188\n",
            "Iteration 58, loss = 0.61041421\n",
            "Iteration 59, loss = 0.61041168\n",
            "Iteration 60, loss = 0.61040562\n",
            "Iteration 61, loss = 0.61039826\n",
            "Iteration 62, loss = 0.61040313\n",
            "Iteration 63, loss = 0.61038435\n",
            "Iteration 64, loss = 0.61038772\n",
            "Iteration 65, loss = 0.61037059\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 66, loss = 0.61033616\n",
            "Iteration 67, loss = 0.61033377\n",
            "Iteration 68, loss = 0.61033476\n",
            "Iteration 69, loss = 0.61032937\n",
            "Iteration 70, loss = 0.61033095\n",
            "Iteration 71, loss = 0.61032912\n",
            "Iteration 72, loss = 0.61032994\n",
            "Iteration 73, loss = 0.61032483\n",
            "Iteration 74, loss = 0.61032579\n",
            "Iteration 75, loss = 0.61032311\n",
            "Iteration 76, loss = 0.61032426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 77, loss = 0.61031713\n",
            "Iteration 78, loss = 0.61031721\n",
            "Iteration 79, loss = 0.61031563\n",
            "Iteration 80, loss = 0.61031559\n",
            "Iteration 81, loss = 0.61031545\n",
            "Iteration 82, loss = 0.61031538\n",
            "Iteration 83, loss = 0.61031564\n",
            "Iteration 84, loss = 0.61031436\n",
            "Iteration 85, loss = 0.61031435\n",
            "Iteration 86, loss = 0.61031386\n",
            "Iteration 87, loss = 0.61031331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71810060\n",
            "Iteration 3, loss = 0.70432119\n",
            "Iteration 4, loss = 0.69150592\n",
            "Iteration 5, loss = 0.67838203\n",
            "Iteration 6, loss = 0.66146678\n",
            "Iteration 7, loss = 0.64813086\n",
            "Iteration 8, loss = 0.64031020\n",
            "Iteration 9, loss = 0.63024694\n",
            "Iteration 10, loss = 0.62525448\n",
            "Iteration 11, loss = 0.61650457\n",
            "Iteration 12, loss = 0.60943634\n",
            "Iteration 13, loss = 0.60865067\n",
            "Iteration 14, loss = 0.60604127\n",
            "Iteration 15, loss = 0.59776278\n",
            "Iteration 16, loss = 0.59763319\n",
            "Iteration 17, loss = 0.59385102\n",
            "Iteration 18, loss = 0.59321055\n",
            "Iteration 19, loss = 0.58843751\n",
            "Iteration 20, loss = 0.58624320\n",
            "Iteration 21, loss = 0.58543043\n",
            "Iteration 22, loss = 0.58431788\n",
            "Iteration 23, loss = 0.58311146\n",
            "Iteration 24, loss = 0.58081858\n",
            "Iteration 25, loss = 0.62801117\n",
            "Iteration 26, loss = 0.60987544\n",
            "Iteration 27, loss = 0.60214818\n",
            "Iteration 28, loss = 0.59694340\n",
            "Iteration 29, loss = 0.59352556\n",
            "Iteration 30, loss = 0.58855406\n",
            "Iteration 31, loss = 0.58671565\n",
            "Iteration 32, loss = 0.58364591\n",
            "Iteration 33, loss = 0.58453394\n",
            "Iteration 34, loss = 0.57981797\n",
            "Iteration 35, loss = 0.57897360\n",
            "Iteration 36, loss = 0.57939532\n",
            "Iteration 37, loss = 0.58132719\n",
            "Iteration 38, loss = 0.57680947\n",
            "Iteration 39, loss = 0.57560352\n",
            "Iteration 40, loss = 0.57606377\n",
            "Iteration 41, loss = 0.57517466\n",
            "Iteration 42, loss = 0.57357863\n",
            "Iteration 43, loss = 0.57702047\n",
            "Iteration 44, loss = 0.57612034\n",
            "Iteration 45, loss = 0.57367938\n",
            "Iteration 46, loss = 0.57243353\n",
            "Iteration 47, loss = 0.57546256\n",
            "Iteration 48, loss = 0.57274807\n",
            "Iteration 49, loss = 0.57253117\n",
            "Iteration 50, loss = 0.57194432\n",
            "Iteration 51, loss = 0.57054662\n",
            "Iteration 52, loss = 0.57091836\n",
            "Iteration 53, loss = 0.57236409\n",
            "Iteration 54, loss = 0.57298937\n",
            "Iteration 55, loss = 0.57205152\n",
            "Iteration 56, loss = 0.57374590\n",
            "Iteration 57, loss = 0.57346586\n",
            "Iteration 58, loss = 0.57192392\n",
            "Iteration 59, loss = 0.57044357\n",
            "Iteration 60, loss = 0.57210835\n",
            "Iteration 61, loss = 0.57428572\n",
            "Iteration 62, loss = 0.57394075\n",
            "Iteration 63, loss = 0.65375902\n",
            "Iteration 64, loss = 0.62243462\n",
            "Iteration 65, loss = 0.60944569\n",
            "Iteration 66, loss = 0.60409505\n",
            "Iteration 67, loss = 0.59780402\n",
            "Iteration 68, loss = 0.59329198\n",
            "Iteration 69, loss = 0.58987201\n",
            "Iteration 70, loss = 0.63321098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 71, loss = 0.62040330\n",
            "Iteration 72, loss = 0.61829505\n",
            "Iteration 73, loss = 0.61538481\n",
            "Iteration 74, loss = 0.61220989\n",
            "Iteration 75, loss = 0.60834998\n",
            "Iteration 76, loss = 0.60399589\n",
            "Iteration 77, loss = 0.59921981\n",
            "Iteration 78, loss = 0.59458496\n",
            "Iteration 79, loss = 0.59058904\n",
            "Iteration 80, loss = 0.58745355\n",
            "Iteration 81, loss = 0.58493185\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.58225773\n",
            "Iteration 83, loss = 0.58167216\n",
            "Iteration 84, loss = 0.58114657\n",
            "Iteration 85, loss = 0.58067344\n",
            "Iteration 86, loss = 0.58017731\n",
            "Iteration 87, loss = 0.57976079\n",
            "Iteration 88, loss = 0.57956106\n",
            "Iteration 89, loss = 0.57903443\n",
            "Iteration 90, loss = 0.57869659\n",
            "Iteration 91, loss = 0.57822224\n",
            "Iteration 92, loss = 0.57809370\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 93, loss = 0.57754990\n",
            "Iteration 94, loss = 0.57718946\n",
            "Iteration 95, loss = 0.57703788\n",
            "Iteration 96, loss = 0.57710254\n",
            "Iteration 97, loss = 0.57701999\n",
            "Iteration 98, loss = 0.57694423\n",
            "Iteration 99, loss = 0.57682077\n",
            "Iteration 100, loss = 0.57674330\n",
            "Iteration 101, loss = 0.57684000\n",
            "Iteration 102, loss = 0.57652525\n",
            "Iteration 103, loss = 0.57649848\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 104, loss = 0.57636608\n",
            "Iteration 105, loss = 0.57635263\n",
            "Iteration 106, loss = 0.57628666\n",
            "Iteration 107, loss = 0.57629536\n",
            "Iteration 108, loss = 0.57632198\n",
            "Iteration 109, loss = 0.57630200\n",
            "Iteration 110, loss = 0.57628317\n",
            "Iteration 111, loss = 0.57624661\n",
            "Iteration 112, loss = 0.57625410\n",
            "Iteration 113, loss = 0.57622383\n",
            "Iteration 114, loss = 0.57624026\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 115, loss = 0.57618852\n",
            "Iteration 116, loss = 0.57617693\n",
            "Iteration 117, loss = 0.57617779\n",
            "Iteration 118, loss = 0.57617509\n",
            "Iteration 119, loss = 0.57616473\n",
            "Iteration 120, loss = 0.57616542\n",
            "Iteration 121, loss = 0.57616801\n",
            "Iteration 122, loss = 0.57616632\n",
            "Iteration 123, loss = 0.57616147\n",
            "Iteration 124, loss = 0.57615695\n",
            "Iteration 125, loss = 0.57615458\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70079523\n",
            "Iteration 3, loss = 0.68448757\n",
            "Iteration 4, loss = 0.67264980\n",
            "Iteration 5, loss = 0.65631822\n",
            "Iteration 6, loss = 0.64722037\n",
            "Iteration 7, loss = 0.63654348\n",
            "Iteration 8, loss = 0.62839645\n",
            "Iteration 9, loss = 0.62408505\n",
            "Iteration 10, loss = 0.61589571\n",
            "Iteration 11, loss = 0.60893737\n",
            "Iteration 12, loss = 0.60753507\n",
            "Iteration 13, loss = 0.59961999\n",
            "Iteration 14, loss = 0.59300445\n",
            "Iteration 15, loss = 0.59662733\n",
            "Iteration 16, loss = 0.59004903\n",
            "Iteration 17, loss = 0.58813111\n",
            "Iteration 18, loss = 0.58619346\n",
            "Iteration 19, loss = 0.58305745\n",
            "Iteration 20, loss = 0.58225875\n",
            "Iteration 21, loss = 0.58100454\n",
            "Iteration 22, loss = 0.57905847\n",
            "Iteration 23, loss = 0.57897900\n",
            "Iteration 24, loss = 0.57588924\n",
            "Iteration 25, loss = 0.57402424\n",
            "Iteration 26, loss = 0.57507995\n",
            "Iteration 27, loss = 0.57357203\n",
            "Iteration 28, loss = 0.57367246\n",
            "Iteration 29, loss = 0.57304906\n",
            "Iteration 30, loss = 0.57149852\n",
            "Iteration 31, loss = 0.57299100\n",
            "Iteration 32, loss = 0.57211922\n",
            "Iteration 33, loss = 0.57157694\n",
            "Iteration 34, loss = 0.57267595\n",
            "Iteration 35, loss = 0.57055451\n",
            "Iteration 36, loss = 0.57242255\n",
            "Iteration 37, loss = 0.57002313\n",
            "Iteration 38, loss = 0.56993944\n",
            "Iteration 39, loss = 0.57219263\n",
            "Iteration 40, loss = 0.57004882\n",
            "Iteration 41, loss = 0.56932860\n",
            "Iteration 42, loss = 0.56857127\n",
            "Iteration 43, loss = 0.57287927\n",
            "Iteration 44, loss = 0.56847935\n",
            "Iteration 45, loss = 0.56850640\n",
            "Iteration 46, loss = 0.57156808\n",
            "Iteration 47, loss = 0.57210168\n",
            "Iteration 48, loss = 0.57051936\n",
            "Iteration 49, loss = 0.57073838\n",
            "Iteration 50, loss = 0.57010848\n",
            "Iteration 51, loss = 0.57295990\n",
            "Iteration 52, loss = 0.57128012\n",
            "Iteration 53, loss = 0.57108351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.53022390\n",
            "Iteration 55, loss = 0.51643643\n",
            "Iteration 56, loss = 0.50699231\n",
            "Iteration 57, loss = 0.49732557\n",
            "Iteration 58, loss = 0.49124032\n",
            "Iteration 59, loss = 0.48570499\n",
            "Iteration 60, loss = 0.48432513\n",
            "Iteration 61, loss = 0.49448613\n",
            "Iteration 62, loss = 0.50954364\n",
            "Iteration 63, loss = 0.50187393\n",
            "Iteration 64, loss = 0.52237108\n",
            "Iteration 65, loss = 0.52278937\n",
            "Iteration 66, loss = 0.50570359\n",
            "Iteration 67, loss = 0.51260131\n",
            "Iteration 68, loss = 0.52281538\n",
            "Iteration 69, loss = 0.51612952\n",
            "Iteration 70, loss = 0.51086453\n",
            "Iteration 71, loss = 0.51187347\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.43873553\n",
            "Iteration 73, loss = 0.43599082\n",
            "Iteration 74, loss = 0.43239048\n",
            "Iteration 75, loss = 0.42964873\n",
            "Iteration 76, loss = 0.42686334\n",
            "Iteration 77, loss = 0.42343223\n",
            "Iteration 78, loss = 0.42100518\n",
            "Iteration 79, loss = 0.41816161\n",
            "Iteration 80, loss = 0.41496189\n",
            "Iteration 81, loss = 0.41264507\n",
            "Iteration 82, loss = 0.40982559\n",
            "Iteration 83, loss = 0.40729870\n",
            "Iteration 84, loss = 0.40434987\n",
            "Iteration 85, loss = 0.40205011\n",
            "Iteration 86, loss = 0.39878753\n",
            "Iteration 87, loss = 0.39622497\n",
            "Iteration 88, loss = 0.39400325\n",
            "Iteration 89, loss = 0.39156580\n",
            "Iteration 90, loss = 0.38900273\n",
            "Iteration 91, loss = 0.38711689\n",
            "Iteration 92, loss = 0.38451353\n",
            "Iteration 93, loss = 0.38190071\n",
            "Iteration 94, loss = 0.37931233\n",
            "Iteration 95, loss = 0.37765629\n",
            "Iteration 96, loss = 0.37489652\n",
            "Iteration 97, loss = 0.37259759\n",
            "Iteration 98, loss = 0.37088764\n",
            "Iteration 99, loss = 0.36877943\n",
            "Iteration 100, loss = 0.36654887\n",
            "Iteration 101, loss = 0.36475508\n",
            "Iteration 102, loss = 0.36322516\n",
            "Iteration 103, loss = 0.36082971\n",
            "Iteration 104, loss = 0.35870164\n",
            "Iteration 105, loss = 0.35674994\n",
            "Iteration 106, loss = 0.35444105\n",
            "Iteration 107, loss = 0.35360028\n",
            "Iteration 108, loss = 0.35209673\n",
            "Iteration 109, loss = 0.34958339\n",
            "Iteration 110, loss = 0.34734055\n",
            "Iteration 111, loss = 0.34573401\n",
            "Iteration 112, loss = 0.34461943\n",
            "Iteration 113, loss = 0.34341744\n",
            "Iteration 114, loss = 0.34181378\n",
            "Iteration 115, loss = 0.33957898\n",
            "Iteration 116, loss = 0.33847177\n",
            "Iteration 117, loss = 0.33771690\n",
            "Iteration 118, loss = 0.33503391\n",
            "Iteration 119, loss = 0.33387064\n",
            "Iteration 120, loss = 0.33285864\n",
            "Iteration 121, loss = 0.33169813\n",
            "Iteration 122, loss = 0.33025881\n",
            "Iteration 123, loss = 0.32872251\n",
            "Iteration 124, loss = 0.32678143\n",
            "Iteration 125, loss = 0.32552749\n",
            "Iteration 126, loss = 0.32411065\n",
            "Iteration 127, loss = 0.32352563\n",
            "Iteration 128, loss = 0.32212556\n",
            "Iteration 129, loss = 0.32161334\n",
            "Iteration 130, loss = 0.31982108\n",
            "Iteration 131, loss = 0.31782218\n",
            "Iteration 132, loss = 0.31795405\n",
            "Iteration 133, loss = 0.31675849\n",
            "Iteration 134, loss = 0.31431043\n",
            "Iteration 135, loss = 0.31414500\n",
            "Iteration 136, loss = 0.31255559\n",
            "Iteration 137, loss = 0.31207754\n",
            "Iteration 138, loss = 0.31188659\n",
            "Iteration 139, loss = 0.31084386\n",
            "Iteration 140, loss = 0.31139977\n",
            "Iteration 141, loss = 0.30903046\n",
            "Iteration 142, loss = 0.30811491\n",
            "Iteration 143, loss = 0.30701901\n",
            "Iteration 144, loss = 0.30578274\n",
            "Iteration 145, loss = 0.30496671\n",
            "Iteration 146, loss = 0.30441864\n",
            "Iteration 147, loss = 0.30393611\n",
            "Iteration 148, loss = 0.30437188\n",
            "Iteration 149, loss = 0.30379436\n",
            "Iteration 150, loss = 0.30089440\n",
            "Iteration 151, loss = 0.30102116\n",
            "Iteration 152, loss = 0.30025362\n",
            "Iteration 153, loss = 0.29992518\n",
            "Iteration 154, loss = 0.29959067\n",
            "Iteration 155, loss = 0.30049748\n",
            "Iteration 156, loss = 0.29642806\n",
            "Iteration 157, loss = 0.29721234\n",
            "Iteration 158, loss = 0.29703858\n",
            "Iteration 159, loss = 0.29455407\n",
            "Iteration 160, loss = 0.29734870\n",
            "Iteration 161, loss = 0.29204184\n",
            "Iteration 162, loss = 0.29398237\n",
            "Iteration 163, loss = 0.29255854\n",
            "Iteration 164, loss = 0.29301115\n",
            "Iteration 165, loss = 0.29133495\n",
            "Iteration 166, loss = 0.29320304\n",
            "Iteration 167, loss = 0.29241506\n",
            "Iteration 168, loss = 0.29291942\n",
            "Iteration 169, loss = 0.29318010\n",
            "Iteration 170, loss = 0.29080064\n",
            "Iteration 171, loss = 0.29155062\n",
            "Iteration 172, loss = 0.28941852\n",
            "Iteration 173, loss = 0.29018011\n",
            "Iteration 174, loss = 0.28700527\n",
            "Iteration 175, loss = 0.28838388\n",
            "Iteration 176, loss = 0.29205626\n",
            "Iteration 177, loss = 0.28981194\n",
            "Iteration 178, loss = 0.29195122\n",
            "Iteration 179, loss = 0.28735463\n",
            "Iteration 180, loss = 0.28921206\n",
            "Iteration 181, loss = 0.28359088\n",
            "Iteration 182, loss = 0.28576050\n",
            "Iteration 183, loss = 0.28643777\n",
            "Iteration 184, loss = 0.28557855\n",
            "Iteration 185, loss = 0.28467487\n",
            "Iteration 186, loss = 0.28839918\n",
            "Iteration 187, loss = 0.28607276\n",
            "Iteration 188, loss = 0.28504090\n",
            "Iteration 189, loss = 0.28446053\n",
            "Iteration 190, loss = 0.28692199\n",
            "Iteration 191, loss = 0.28358032\n",
            "Iteration 192, loss = 0.29005686\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 193, loss = 0.27255815\n",
            "Iteration 194, loss = 0.27193863\n",
            "Iteration 195, loss = 0.27196937\n",
            "Iteration 196, loss = 0.27179822\n",
            "Iteration 197, loss = 0.27175733\n",
            "Iteration 198, loss = 0.27165857\n",
            "Iteration 199, loss = 0.27176071\n",
            "Iteration 200, loss = 0.27142502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69655347\n",
            "Iteration 3, loss = 0.67463228\n",
            "Iteration 4, loss = 0.66087702\n",
            "Iteration 5, loss = 0.64786188\n",
            "Iteration 6, loss = 0.63853527\n",
            "Iteration 7, loss = 0.62860480\n",
            "Iteration 8, loss = 0.62296700\n",
            "Iteration 9, loss = 0.61494682\n",
            "Iteration 10, loss = 0.60724889\n",
            "Iteration 11, loss = 0.60511842\n",
            "Iteration 12, loss = 0.59649579\n",
            "Iteration 13, loss = 0.59708832\n",
            "Iteration 14, loss = 0.59544511\n",
            "Iteration 15, loss = 0.58820549\n",
            "Iteration 16, loss = 0.58426260\n",
            "Iteration 17, loss = 0.58524060\n",
            "Iteration 18, loss = 0.58209812\n",
            "Iteration 19, loss = 0.57995100\n",
            "Iteration 20, loss = 0.58034428\n",
            "Iteration 21, loss = 0.57713067\n",
            "Iteration 22, loss = 0.57763114\n",
            "Iteration 23, loss = 0.57643890\n",
            "Iteration 24, loss = 0.57253027\n",
            "Iteration 25, loss = 0.57296969\n",
            "Iteration 26, loss = 0.57272052\n",
            "Iteration 27, loss = 0.56880737\n",
            "Iteration 28, loss = 0.57433532\n",
            "Iteration 29, loss = 0.57302116\n",
            "Iteration 30, loss = 0.56959397\n",
            "Iteration 31, loss = 0.57520339\n",
            "Iteration 32, loss = 0.57691305\n",
            "Iteration 33, loss = 0.57457099\n",
            "Iteration 34, loss = 0.57331544\n",
            "Iteration 35, loss = 0.58009047\n",
            "Iteration 36, loss = 0.57157912\n",
            "Iteration 37, loss = 0.57185958\n",
            "Iteration 38, loss = 0.57049711\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.53087482\n",
            "Iteration 40, loss = 0.52043662\n",
            "Iteration 41, loss = 0.51396971\n",
            "Iteration 42, loss = 0.50626805\n",
            "Iteration 43, loss = 0.50055735\n",
            "Iteration 44, loss = 0.49297956\n",
            "Iteration 45, loss = 0.49220061\n",
            "Iteration 46, loss = 0.50229094\n",
            "Iteration 47, loss = 0.50487428\n",
            "Iteration 48, loss = 0.50783368\n",
            "Iteration 49, loss = 0.52660183\n",
            "Iteration 50, loss = 0.52016757\n",
            "Iteration 51, loss = 0.52897909\n",
            "Iteration 52, loss = 0.50470952\n",
            "Iteration 53, loss = 0.51730600\n",
            "Iteration 54, loss = 0.52365933\n",
            "Iteration 55, loss = 0.51043259\n",
            "Iteration 56, loss = 0.53177037\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 57, loss = 0.44881068\n",
            "Iteration 58, loss = 0.44574386\n",
            "Iteration 59, loss = 0.44342380\n",
            "Iteration 60, loss = 0.44048258\n",
            "Iteration 61, loss = 0.43776635\n",
            "Iteration 62, loss = 0.43501747\n",
            "Iteration 63, loss = 0.43223574\n",
            "Iteration 64, loss = 0.42940126\n",
            "Iteration 65, loss = 0.42682178\n",
            "Iteration 66, loss = 0.42462162\n",
            "Iteration 67, loss = 0.42159933\n",
            "Iteration 68, loss = 0.41913031\n",
            "Iteration 69, loss = 0.41649823\n",
            "Iteration 70, loss = 0.41398589\n",
            "Iteration 71, loss = 0.41120312\n",
            "Iteration 72, loss = 0.40889046\n",
            "Iteration 73, loss = 0.40610989\n",
            "Iteration 74, loss = 0.40420414\n",
            "Iteration 75, loss = 0.40187292\n",
            "Iteration 76, loss = 0.39942919\n",
            "Iteration 77, loss = 0.39682752\n",
            "Iteration 78, loss = 0.39481111\n",
            "Iteration 79, loss = 0.39253552\n",
            "Iteration 80, loss = 0.39073221\n",
            "Iteration 81, loss = 0.38788513\n",
            "Iteration 82, loss = 0.38601088\n",
            "Iteration 83, loss = 0.38344790\n",
            "Iteration 84, loss = 0.38171490\n",
            "Iteration 85, loss = 0.37928286\n",
            "Iteration 86, loss = 0.37764126\n",
            "Iteration 87, loss = 0.37583477\n",
            "Iteration 88, loss = 0.37353590\n",
            "Iteration 89, loss = 0.37142079\n",
            "Iteration 90, loss = 0.36963722\n",
            "Iteration 91, loss = 0.36802930\n",
            "Iteration 92, loss = 0.36638806\n",
            "Iteration 93, loss = 0.36429176\n",
            "Iteration 94, loss = 0.36208085\n",
            "Iteration 95, loss = 0.36017680\n",
            "Iteration 96, loss = 0.35896515\n",
            "Iteration 97, loss = 0.35678075\n",
            "Iteration 98, loss = 0.35519338\n",
            "Iteration 99, loss = 0.35430107\n",
            "Iteration 100, loss = 0.35185859\n",
            "Iteration 101, loss = 0.35019518\n",
            "Iteration 102, loss = 0.34881573\n",
            "Iteration 103, loss = 0.34603593\n",
            "Iteration 104, loss = 0.34506333\n",
            "Iteration 105, loss = 0.34467625\n",
            "Iteration 106, loss = 0.34254596\n",
            "Iteration 107, loss = 0.34097339\n",
            "Iteration 108, loss = 0.33918432\n",
            "Iteration 109, loss = 0.33807593\n",
            "Iteration 110, loss = 0.33752423\n",
            "Iteration 111, loss = 0.33506525\n",
            "Iteration 112, loss = 0.33472019\n",
            "Iteration 113, loss = 0.33324077\n",
            "Iteration 114, loss = 0.33224217\n",
            "Iteration 115, loss = 0.33147396\n",
            "Iteration 116, loss = 0.32896245\n",
            "Iteration 117, loss = 0.32726780\n",
            "Iteration 118, loss = 0.32667476\n",
            "Iteration 119, loss = 0.32494755\n",
            "Iteration 120, loss = 0.32524023\n",
            "Iteration 121, loss = 0.32373959\n",
            "Iteration 122, loss = 0.32285989\n",
            "Iteration 123, loss = 0.32065014\n",
            "Iteration 124, loss = 0.32103318\n",
            "Iteration 125, loss = 0.31842052\n",
            "Iteration 126, loss = 0.31901402\n",
            "Iteration 127, loss = 0.31870404\n",
            "Iteration 128, loss = 0.31600451\n",
            "Iteration 129, loss = 0.31588260\n",
            "Iteration 130, loss = 0.31473274\n",
            "Iteration 131, loss = 0.31440059\n",
            "Iteration 132, loss = 0.31303952\n",
            "Iteration 133, loss = 0.31291672\n",
            "Iteration 134, loss = 0.31062350\n",
            "Iteration 135, loss = 0.31146281\n",
            "Iteration 136, loss = 0.30898960\n",
            "Iteration 137, loss = 0.30922129\n",
            "Iteration 138, loss = 0.30859468\n",
            "Iteration 139, loss = 0.30713443\n",
            "Iteration 140, loss = 0.30840660\n",
            "Iteration 141, loss = 0.30898516\n",
            "Iteration 142, loss = 0.30701054\n",
            "Iteration 143, loss = 0.30319441\n",
            "Iteration 144, loss = 0.30224665\n",
            "Iteration 145, loss = 0.30226545\n",
            "Iteration 146, loss = 0.30354781\n",
            "Iteration 147, loss = 0.30393051\n",
            "Iteration 148, loss = 0.30183863\n",
            "Iteration 149, loss = 0.30136733\n",
            "Iteration 150, loss = 0.30058256\n",
            "Iteration 151, loss = 0.30141646\n",
            "Iteration 152, loss = 0.30513394\n",
            "Iteration 153, loss = 0.30215711\n",
            "Iteration 154, loss = 0.30248780\n",
            "Iteration 155, loss = 0.30037561\n",
            "Iteration 156, loss = 0.29633198\n",
            "Iteration 157, loss = 0.29604437\n",
            "Iteration 158, loss = 0.30060147\n",
            "Iteration 159, loss = 0.29810578\n",
            "Iteration 160, loss = 0.29903441\n",
            "Iteration 161, loss = 0.29623144\n",
            "Iteration 162, loss = 0.29715914\n",
            "Iteration 163, loss = 0.29368445\n",
            "Iteration 164, loss = 0.30134756\n",
            "Iteration 165, loss = 0.29571957\n",
            "Iteration 166, loss = 0.29588368\n",
            "Iteration 167, loss = 0.29738972\n",
            "Iteration 168, loss = 0.30015477\n",
            "Iteration 169, loss = 0.29640769\n",
            "Iteration 170, loss = 0.29042717\n",
            "Iteration 171, loss = 0.29877666\n",
            "Iteration 172, loss = 0.29506387\n",
            "Iteration 173, loss = 0.28953304\n",
            "Iteration 174, loss = 0.29366291\n",
            "Iteration 175, loss = 0.29492512\n",
            "Iteration 176, loss = 0.29972642\n",
            "Iteration 177, loss = 0.29365938\n",
            "Iteration 178, loss = 0.29593863\n",
            "Iteration 179, loss = 0.30129264\n",
            "Iteration 180, loss = 0.29662034\n",
            "Iteration 181, loss = 0.29211357\n",
            "Iteration 182, loss = 0.29068181\n",
            "Iteration 183, loss = 0.29263161\n",
            "Iteration 184, loss = 0.29448006\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 185, loss = 0.27503575\n",
            "Iteration 186, loss = 0.27468761\n",
            "Iteration 187, loss = 0.27442981\n",
            "Iteration 188, loss = 0.27450686\n",
            "Iteration 189, loss = 0.27435737\n",
            "Iteration 190, loss = 0.27426584\n",
            "Iteration 191, loss = 0.27421057\n",
            "Iteration 192, loss = 0.27415719\n",
            "Iteration 193, loss = 0.27402242\n",
            "Iteration 194, loss = 0.27406128\n",
            "Iteration 195, loss = 0.27393969\n",
            "Iteration 196, loss = 0.27373545\n",
            "Iteration 197, loss = 0.27372470\n",
            "Iteration 198, loss = 0.27354924\n",
            "Iteration 199, loss = 0.27342389\n",
            "Iteration 200, loss = 0.27340978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67751389\n",
            "Iteration 3, loss = 0.66485271\n",
            "Iteration 4, loss = 0.64894959\n",
            "Iteration 5, loss = 0.64034491\n",
            "Iteration 6, loss = 0.63223296\n",
            "Iteration 7, loss = 0.62476741\n",
            "Iteration 8, loss = 0.61673327\n",
            "Iteration 9, loss = 0.60929412\n",
            "Iteration 10, loss = 0.60682921\n",
            "Iteration 11, loss = 0.60305097\n",
            "Iteration 12, loss = 0.59666132\n",
            "Iteration 13, loss = 0.59250370\n",
            "Iteration 14, loss = 0.59243055\n",
            "Iteration 15, loss = 0.58618221\n",
            "Iteration 16, loss = 0.58381305\n",
            "Iteration 17, loss = 0.58231265\n",
            "Iteration 18, loss = 0.58099125\n",
            "Iteration 19, loss = 0.57573055\n",
            "Iteration 20, loss = 0.57401234\n",
            "Iteration 21, loss = 0.57796771\n",
            "Iteration 22, loss = 0.58133133\n",
            "Iteration 23, loss = 0.58492379\n",
            "Iteration 24, loss = 0.58331479\n",
            "Iteration 25, loss = 0.57538473\n",
            "Iteration 26, loss = 0.57904847\n",
            "Iteration 27, loss = 0.57663697\n",
            "Iteration 28, loss = 0.57552262\n",
            "Iteration 29, loss = 0.57836430\n",
            "Iteration 30, loss = 0.57897275\n",
            "Iteration 31, loss = 0.57886560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.55002318\n",
            "Iteration 33, loss = 0.54053883\n",
            "Iteration 34, loss = 0.53634768\n",
            "Iteration 35, loss = 0.53113921\n",
            "Iteration 36, loss = 0.52616651\n",
            "Iteration 37, loss = 0.52164233\n",
            "Iteration 38, loss = 0.51618429\n",
            "Iteration 39, loss = 0.51211536\n",
            "Iteration 40, loss = 0.50746585\n",
            "Iteration 41, loss = 0.51316997\n",
            "Iteration 42, loss = 0.51355649\n",
            "Iteration 43, loss = 0.51297769\n",
            "Iteration 44, loss = 0.51563059\n",
            "Iteration 45, loss = 0.52555002\n",
            "Iteration 46, loss = 0.53181716\n",
            "Iteration 47, loss = 0.52076207\n",
            "Iteration 48, loss = 0.52437175\n",
            "Iteration 49, loss = 0.52138935\n",
            "Iteration 50, loss = 0.52375947\n",
            "Iteration 51, loss = 0.52441710\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.46702683\n",
            "Iteration 53, loss = 0.46352319\n",
            "Iteration 54, loss = 0.46091096\n",
            "Iteration 55, loss = 0.45930721\n",
            "Iteration 56, loss = 0.45694176\n",
            "Iteration 57, loss = 0.45528506\n",
            "Iteration 58, loss = 0.45303517\n",
            "Iteration 59, loss = 0.45066908\n",
            "Iteration 60, loss = 0.44842414\n",
            "Iteration 61, loss = 0.44659628\n",
            "Iteration 62, loss = 0.44433096\n",
            "Iteration 63, loss = 0.44235394\n",
            "Iteration 64, loss = 0.44048960\n",
            "Iteration 65, loss = 0.43818847\n",
            "Iteration 66, loss = 0.43604820\n",
            "Iteration 67, loss = 0.43401130\n",
            "Iteration 68, loss = 0.43183453\n",
            "Iteration 69, loss = 0.42992222\n",
            "Iteration 70, loss = 0.42780290\n",
            "Iteration 71, loss = 0.42533741\n",
            "Iteration 72, loss = 0.42353920\n",
            "Iteration 73, loss = 0.42175139\n",
            "Iteration 74, loss = 0.41932621\n",
            "Iteration 75, loss = 0.41708781\n",
            "Iteration 76, loss = 0.41633679\n",
            "Iteration 77, loss = 0.41375043\n",
            "Iteration 78, loss = 0.41191599\n",
            "Iteration 79, loss = 0.40955245\n",
            "Iteration 80, loss = 0.40756968\n",
            "Iteration 81, loss = 0.40585486\n",
            "Iteration 82, loss = 0.40333129\n",
            "Iteration 83, loss = 0.40141150\n",
            "Iteration 84, loss = 0.40043989\n",
            "Iteration 85, loss = 0.39783760\n",
            "Iteration 86, loss = 0.39603858\n",
            "Iteration 87, loss = 0.39394916\n",
            "Iteration 88, loss = 0.39179391\n",
            "Iteration 89, loss = 0.38998232\n",
            "Iteration 90, loss = 0.38907128\n",
            "Iteration 91, loss = 0.38630854\n",
            "Iteration 92, loss = 0.38486516\n",
            "Iteration 93, loss = 0.38341701\n",
            "Iteration 94, loss = 0.38074991\n",
            "Iteration 95, loss = 0.37911789\n",
            "Iteration 96, loss = 0.37846214\n",
            "Iteration 97, loss = 0.37611048\n",
            "Iteration 98, loss = 0.37467713\n",
            "Iteration 99, loss = 0.37257795\n",
            "Iteration 100, loss = 0.37043157\n",
            "Iteration 101, loss = 0.36895017\n",
            "Iteration 102, loss = 0.36815419\n",
            "Iteration 103, loss = 0.36691419\n",
            "Iteration 104, loss = 0.36431127\n",
            "Iteration 105, loss = 0.36306393\n",
            "Iteration 106, loss = 0.36174247\n",
            "Iteration 107, loss = 0.35972199\n",
            "Iteration 108, loss = 0.35927153\n",
            "Iteration 109, loss = 0.35658421\n",
            "Iteration 110, loss = 0.35541706\n",
            "Iteration 111, loss = 0.35501390\n",
            "Iteration 112, loss = 0.35290838\n",
            "Iteration 113, loss = 0.35152634\n",
            "Iteration 114, loss = 0.34946963\n",
            "Iteration 115, loss = 0.34773927\n",
            "Iteration 116, loss = 0.34756713\n",
            "Iteration 117, loss = 0.34585787\n",
            "Iteration 118, loss = 0.34345918\n",
            "Iteration 119, loss = 0.34362228\n",
            "Iteration 120, loss = 0.34111127\n",
            "Iteration 121, loss = 0.34012882\n",
            "Iteration 122, loss = 0.33805026\n",
            "Iteration 123, loss = 0.33979316\n",
            "Iteration 124, loss = 0.33888108\n",
            "Iteration 125, loss = 0.33508446\n",
            "Iteration 126, loss = 0.33508150\n",
            "Iteration 127, loss = 0.33405434\n",
            "Iteration 128, loss = 0.33268977\n",
            "Iteration 129, loss = 0.33115058\n",
            "Iteration 130, loss = 0.33084597\n",
            "Iteration 131, loss = 0.33015877\n",
            "Iteration 132, loss = 0.32756669\n",
            "Iteration 133, loss = 0.32585909\n",
            "Iteration 134, loss = 0.32713126\n",
            "Iteration 135, loss = 0.32401822\n",
            "Iteration 136, loss = 0.32549635\n",
            "Iteration 137, loss = 0.32399027\n",
            "Iteration 138, loss = 0.32176674\n",
            "Iteration 139, loss = 0.31963129\n",
            "Iteration 140, loss = 0.32219192\n",
            "Iteration 141, loss = 0.32023031\n",
            "Iteration 142, loss = 0.32005905\n",
            "Iteration 143, loss = 0.32026340\n",
            "Iteration 144, loss = 0.31993131\n",
            "Iteration 145, loss = 0.31577161\n",
            "Iteration 146, loss = 0.31509854\n",
            "Iteration 147, loss = 0.31979831\n",
            "Iteration 148, loss = 0.31396874\n",
            "Iteration 149, loss = 0.31578074\n",
            "Iteration 150, loss = 0.31325132\n",
            "Iteration 151, loss = 0.31528344\n",
            "Iteration 152, loss = 0.31419744\n",
            "Iteration 153, loss = 0.31228013\n",
            "Iteration 154, loss = 0.31225184\n",
            "Iteration 155, loss = 0.30692241\n",
            "Iteration 156, loss = 0.30977547\n",
            "Iteration 157, loss = 0.31213906\n",
            "Iteration 158, loss = 0.31232035\n",
            "Iteration 159, loss = 0.31181606\n",
            "Iteration 160, loss = 0.30952300\n",
            "Iteration 161, loss = 0.30879050\n",
            "Iteration 162, loss = 0.30508147\n",
            "Iteration 163, loss = 0.30969757\n",
            "Iteration 164, loss = 0.30978019\n",
            "Iteration 165, loss = 0.30721797\n",
            "Iteration 166, loss = 0.31111411\n",
            "Iteration 167, loss = 0.31299545\n",
            "Iteration 168, loss = 0.31449238\n",
            "Iteration 169, loss = 0.30728130\n",
            "Iteration 170, loss = 0.30829889\n",
            "Iteration 171, loss = 0.30738434\n",
            "Iteration 172, loss = 0.30990713\n",
            "Iteration 173, loss = 0.30927126\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 174, loss = 0.28739060\n",
            "Iteration 175, loss = 0.28715508\n",
            "Iteration 176, loss = 0.28690909\n",
            "Iteration 177, loss = 0.28689523\n",
            "Iteration 178, loss = 0.28667553\n",
            "Iteration 179, loss = 0.28659837\n",
            "Iteration 180, loss = 0.28655820\n",
            "Iteration 181, loss = 0.28633339\n",
            "Iteration 182, loss = 0.28610987\n",
            "Iteration 183, loss = 0.28600924\n",
            "Iteration 184, loss = 0.28607544\n",
            "Iteration 185, loss = 0.28579567\n",
            "Iteration 186, loss = 0.28566942\n",
            "Iteration 187, loss = 0.28565457\n",
            "Iteration 188, loss = 0.28539111\n",
            "Iteration 189, loss = 0.28528254\n",
            "Iteration 190, loss = 0.28528678\n",
            "Iteration 191, loss = 0.28511951\n",
            "Iteration 192, loss = 0.28498336\n",
            "Iteration 193, loss = 0.28492316\n",
            "Iteration 194, loss = 0.28488660\n",
            "Iteration 195, loss = 0.28460602\n",
            "Iteration 196, loss = 0.28454609\n",
            "Iteration 197, loss = 0.28435633\n",
            "Iteration 198, loss = 0.28424917\n",
            "Iteration 199, loss = 0.28405645\n",
            "Iteration 200, loss = 0.28422232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69950282\n",
            "Iteration 3, loss = 0.68418140\n",
            "Iteration 4, loss = 0.67496735\n",
            "Iteration 5, loss = 0.66071139\n",
            "Iteration 6, loss = 0.64822318\n",
            "Iteration 7, loss = 0.64043863\n",
            "Iteration 8, loss = 0.62984561\n",
            "Iteration 9, loss = 0.62095589\n",
            "Iteration 10, loss = 0.61509380\n",
            "Iteration 11, loss = 0.60985166\n",
            "Iteration 12, loss = 0.60513374\n",
            "Iteration 13, loss = 0.60067147\n",
            "Iteration 14, loss = 0.59932094\n",
            "Iteration 15, loss = 0.59461499\n",
            "Iteration 16, loss = 0.59322386\n",
            "Iteration 17, loss = 0.58663574\n",
            "Iteration 18, loss = 0.58743876\n",
            "Iteration 19, loss = 0.58701693\n",
            "Iteration 20, loss = 0.58162793\n",
            "Iteration 21, loss = 0.58305977\n",
            "Iteration 22, loss = 0.57813056\n",
            "Iteration 23, loss = 0.58198245\n",
            "Iteration 24, loss = 0.57534605\n",
            "Iteration 25, loss = 0.57734603\n",
            "Iteration 26, loss = 0.57826748\n",
            "Iteration 27, loss = 0.57380365\n",
            "Iteration 28, loss = 0.57518103\n",
            "Iteration 29, loss = 0.57332274\n",
            "Iteration 30, loss = 0.57258472\n",
            "Iteration 31, loss = 0.57672599\n",
            "Iteration 32, loss = 0.57079758\n",
            "Iteration 33, loss = 0.57244414\n",
            "Iteration 34, loss = 0.57256146\n",
            "Iteration 35, loss = 0.57103320\n",
            "Iteration 36, loss = 0.57276586\n",
            "Iteration 37, loss = 0.57033550\n",
            "Iteration 38, loss = 0.57162660\n",
            "Iteration 39, loss = 0.57299884\n",
            "Iteration 40, loss = 0.57132515\n",
            "Iteration 41, loss = 0.57093358\n",
            "Iteration 42, loss = 0.57246044\n",
            "Iteration 43, loss = 0.56803231\n",
            "Iteration 44, loss = 0.57121110\n",
            "Iteration 45, loss = 0.57097623\n",
            "Iteration 46, loss = 0.56939892\n",
            "Iteration 47, loss = 0.56985710\n",
            "Iteration 48, loss = 0.56967263\n",
            "Iteration 49, loss = 0.57161698\n",
            "Iteration 50, loss = 0.57035669\n",
            "Iteration 51, loss = 0.57049663\n",
            "Iteration 52, loss = 0.57179912\n",
            "Iteration 53, loss = 0.56831571\n",
            "Iteration 54, loss = 0.56942357\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 55, loss = 0.52398845\n",
            "Iteration 56, loss = 0.51357850\n",
            "Iteration 57, loss = 0.50501410\n",
            "Iteration 58, loss = 0.49837426\n",
            "Iteration 59, loss = 0.49256122\n",
            "Iteration 60, loss = 0.48765698\n",
            "Iteration 61, loss = 0.48211240\n",
            "Iteration 62, loss = 0.49254018\n",
            "Iteration 63, loss = 0.49503353\n",
            "Iteration 64, loss = 0.50677818\n",
            "Iteration 65, loss = 0.51693674\n",
            "Iteration 66, loss = 0.51039136\n",
            "Iteration 67, loss = 0.51615910\n",
            "Iteration 68, loss = 0.51592726\n",
            "Iteration 69, loss = 0.50473151\n",
            "Iteration 70, loss = 0.52725450\n",
            "Iteration 71, loss = 0.51409695\n",
            "Iteration 72, loss = 0.50823065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.44357777\n",
            "Iteration 74, loss = 0.44091219\n",
            "Iteration 75, loss = 0.43865274\n",
            "Iteration 76, loss = 0.43650291\n",
            "Iteration 77, loss = 0.43427391\n",
            "Iteration 78, loss = 0.43203098\n",
            "Iteration 79, loss = 0.42964179\n",
            "Iteration 80, loss = 0.42765268\n",
            "Iteration 81, loss = 0.42569616\n",
            "Iteration 82, loss = 0.42351140\n",
            "Iteration 83, loss = 0.42095155\n",
            "Iteration 84, loss = 0.41920223\n",
            "Iteration 85, loss = 0.41696943\n",
            "Iteration 86, loss = 0.41502161\n",
            "Iteration 87, loss = 0.41324217\n",
            "Iteration 88, loss = 0.41106689\n",
            "Iteration 89, loss = 0.40889193\n",
            "Iteration 90, loss = 0.40755453\n",
            "Iteration 91, loss = 0.40495704\n",
            "Iteration 92, loss = 0.40294256\n",
            "Iteration 93, loss = 0.40061277\n",
            "Iteration 94, loss = 0.39911611\n",
            "Iteration 95, loss = 0.39708082\n",
            "Iteration 96, loss = 0.39566215\n",
            "Iteration 97, loss = 0.39318401\n",
            "Iteration 98, loss = 0.39181974\n",
            "Iteration 99, loss = 0.38992414\n",
            "Iteration 100, loss = 0.38765501\n",
            "Iteration 101, loss = 0.38633282\n",
            "Iteration 102, loss = 0.38411408\n",
            "Iteration 103, loss = 0.38271510\n",
            "Iteration 104, loss = 0.38099927\n",
            "Iteration 105, loss = 0.37960907\n",
            "Iteration 106, loss = 0.37754163\n",
            "Iteration 107, loss = 0.37680579\n",
            "Iteration 108, loss = 0.37376942\n",
            "Iteration 109, loss = 0.37314392\n",
            "Iteration 110, loss = 0.37110564\n",
            "Iteration 111, loss = 0.36985996\n",
            "Iteration 112, loss = 0.36282785\n",
            "Iteration 113, loss = 0.35558578\n",
            "Iteration 114, loss = 0.35420856\n",
            "Iteration 115, loss = 0.35281474\n",
            "Iteration 116, loss = 0.35096018\n",
            "Iteration 117, loss = 0.34949254\n",
            "Iteration 118, loss = 0.34706840\n",
            "Iteration 119, loss = 0.34610996\n",
            "Iteration 120, loss = 0.34457888\n",
            "Iteration 121, loss = 0.34218156\n",
            "Iteration 122, loss = 0.34109851\n",
            "Iteration 123, loss = 0.34055611\n",
            "Iteration 124, loss = 0.33875463\n",
            "Iteration 125, loss = 0.33589364\n",
            "Iteration 126, loss = 0.33503323\n",
            "Iteration 127, loss = 0.33434033\n",
            "Iteration 128, loss = 0.33328267\n",
            "Iteration 129, loss = 0.33066804\n",
            "Iteration 130, loss = 0.32957098\n",
            "Iteration 131, loss = 0.32939160\n",
            "Iteration 132, loss = 0.32732608\n",
            "Iteration 133, loss = 0.32471666\n",
            "Iteration 134, loss = 0.32489124\n",
            "Iteration 135, loss = 0.32326696\n",
            "Iteration 136, loss = 0.32216754\n",
            "Iteration 137, loss = 0.32218291\n",
            "Iteration 138, loss = 0.32015623\n",
            "Iteration 139, loss = 0.32151317\n",
            "Iteration 140, loss = 0.31798440\n",
            "Iteration 141, loss = 0.31605074\n",
            "Iteration 142, loss = 0.31663921\n",
            "Iteration 143, loss = 0.31453712\n",
            "Iteration 144, loss = 0.31482818\n",
            "Iteration 145, loss = 0.31235982\n",
            "Iteration 146, loss = 0.31195442\n",
            "Iteration 147, loss = 0.31135667\n",
            "Iteration 148, loss = 0.31180932\n",
            "Iteration 149, loss = 0.31048550\n",
            "Iteration 150, loss = 0.31050942\n",
            "Iteration 151, loss = 0.30868924\n",
            "Iteration 152, loss = 0.30824399\n",
            "Iteration 153, loss = 0.30628266\n",
            "Iteration 154, loss = 0.30594953\n",
            "Iteration 155, loss = 0.30536887\n",
            "Iteration 156, loss = 0.30549792\n",
            "Iteration 157, loss = 0.30485274\n",
            "Iteration 158, loss = 0.30287099\n",
            "Iteration 159, loss = 0.30122913\n",
            "Iteration 160, loss = 0.29947915\n",
            "Iteration 161, loss = 0.30319854\n",
            "Iteration 162, loss = 0.29886627\n",
            "Iteration 163, loss = 0.29938124\n",
            "Iteration 164, loss = 0.29974591\n",
            "Iteration 165, loss = 0.29682476\n",
            "Iteration 166, loss = 0.29713502\n",
            "Iteration 167, loss = 0.29669425\n",
            "Iteration 168, loss = 0.29570233\n",
            "Iteration 169, loss = 0.29482145\n",
            "Iteration 170, loss = 0.29581531\n",
            "Iteration 171, loss = 0.29656312\n",
            "Iteration 172, loss = 0.29622136\n",
            "Iteration 173, loss = 0.29434867\n",
            "Iteration 174, loss = 0.29577131\n",
            "Iteration 175, loss = 0.29735832\n",
            "Iteration 176, loss = 0.29095797\n",
            "Iteration 177, loss = 0.29550043\n",
            "Iteration 178, loss = 0.29280254\n",
            "Iteration 179, loss = 0.29353339\n",
            "Iteration 180, loss = 0.28935900\n",
            "Iteration 181, loss = 0.29846371\n",
            "Iteration 182, loss = 0.29332848\n",
            "Iteration 183, loss = 0.29682604\n",
            "Iteration 184, loss = 0.29392707\n",
            "Iteration 185, loss = 0.29576478\n",
            "Iteration 186, loss = 0.29575592\n",
            "Iteration 187, loss = 0.28785619\n",
            "Iteration 188, loss = 0.28664605\n",
            "Iteration 189, loss = 0.29133274\n",
            "Iteration 190, loss = 0.28896748\n",
            "Iteration 191, loss = 0.29043928\n",
            "Iteration 192, loss = 0.29320714\n",
            "Iteration 193, loss = 0.29630300\n",
            "Iteration 194, loss = 0.29773967\n",
            "Iteration 195, loss = 0.29298326\n",
            "Iteration 196, loss = 0.28753045\n",
            "Iteration 197, loss = 0.29096395\n",
            "Iteration 198, loss = 0.30402007\n",
            "Iteration 199, loss = 0.29851978\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 200, loss = 0.27396290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70654408\n",
            "Iteration 3, loss = 0.68919857\n",
            "Iteration 4, loss = 0.67226896\n",
            "Iteration 5, loss = 0.65966615\n",
            "Iteration 6, loss = 0.65366776\n",
            "Iteration 7, loss = 0.63936184\n",
            "Iteration 8, loss = 0.63408497\n",
            "Iteration 9, loss = 0.62717534\n",
            "Iteration 10, loss = 0.62222521\n",
            "Iteration 11, loss = 0.63793060\n",
            "Iteration 12, loss = 0.62328866\n",
            "Iteration 13, loss = 0.61463264\n",
            "Iteration 14, loss = 0.61080091\n",
            "Iteration 15, loss = 0.60545116\n",
            "Iteration 16, loss = 0.60325458\n",
            "Iteration 17, loss = 0.59752728\n",
            "Iteration 18, loss = 0.59098767\n",
            "Iteration 19, loss = 0.59164567\n",
            "Iteration 20, loss = 0.59182639\n",
            "Iteration 21, loss = 0.58666622\n",
            "Iteration 22, loss = 0.58410203\n",
            "Iteration 23, loss = 0.58377787\n",
            "Iteration 24, loss = 0.58266957\n",
            "Iteration 25, loss = 0.58109916\n",
            "Iteration 26, loss = 0.58347318\n",
            "Iteration 27, loss = 0.57741257\n",
            "Iteration 28, loss = 0.58087635\n",
            "Iteration 29, loss = 0.57794809\n",
            "Iteration 30, loss = 0.57426883\n",
            "Iteration 31, loss = 0.57488099\n",
            "Iteration 32, loss = 0.57955862\n",
            "Iteration 33, loss = 0.57493633\n",
            "Iteration 34, loss = 0.57705218\n",
            "Iteration 35, loss = 0.57414645\n",
            "Iteration 36, loss = 0.57814632\n",
            "Iteration 37, loss = 0.59186170\n",
            "Iteration 38, loss = 0.61990384\n",
            "Iteration 39, loss = 0.60671884\n",
            "Iteration 40, loss = 0.59863205\n",
            "Iteration 41, loss = 0.59822906\n",
            "Iteration 42, loss = 0.59142885\n",
            "Iteration 43, loss = 0.58751374\n",
            "Iteration 44, loss = 0.58641442\n",
            "Iteration 45, loss = 0.58453382\n",
            "Iteration 46, loss = 0.58056083\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.56713821\n",
            "Iteration 48, loss = 0.56442982\n",
            "Iteration 49, loss = 0.56320063\n",
            "Iteration 50, loss = 0.56170857\n",
            "Iteration 51, loss = 0.56036607\n",
            "Iteration 52, loss = 0.55853708\n",
            "Iteration 53, loss = 0.55771855\n",
            "Iteration 54, loss = 0.55468362\n",
            "Iteration 55, loss = 0.55331118\n",
            "Iteration 56, loss = 0.55250647\n",
            "Iteration 57, loss = 0.54973106\n",
            "Iteration 58, loss = 0.54727020\n",
            "Iteration 59, loss = 0.54637691\n",
            "Iteration 60, loss = 0.54257524\n",
            "Iteration 61, loss = 0.54221800\n",
            "Iteration 62, loss = 0.54091921\n",
            "Iteration 63, loss = 0.54021214\n",
            "Iteration 64, loss = 0.54036119\n",
            "Iteration 65, loss = 0.53790253\n",
            "Iteration 66, loss = 0.53911231\n",
            "Iteration 67, loss = 0.54384433\n",
            "Iteration 68, loss = 0.54183350\n",
            "Iteration 69, loss = 0.53620071\n",
            "Iteration 70, loss = 0.54253974\n",
            "Iteration 71, loss = 0.54887659\n",
            "Iteration 72, loss = 0.54043606\n",
            "Iteration 73, loss = 0.54486616\n",
            "Iteration 74, loss = 0.54484947\n",
            "Iteration 75, loss = 0.54023322\n",
            "Iteration 76, loss = 0.54210252\n",
            "Iteration 77, loss = 0.54006725\n",
            "Iteration 78, loss = 0.54298869\n",
            "Iteration 79, loss = 0.53644938\n",
            "Iteration 80, loss = 0.53711745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.49397239\n",
            "Iteration 82, loss = 0.49112462\n",
            "Iteration 83, loss = 0.48929904\n",
            "Iteration 84, loss = 0.48791267\n",
            "Iteration 85, loss = 0.48612819\n",
            "Iteration 86, loss = 0.48450168\n",
            "Iteration 87, loss = 0.48247183\n",
            "Iteration 88, loss = 0.48134297\n",
            "Iteration 89, loss = 0.47975094\n",
            "Iteration 90, loss = 0.47801940\n",
            "Iteration 91, loss = 0.47651106\n",
            "Iteration 92, loss = 0.47518638\n",
            "Iteration 93, loss = 0.47315731\n",
            "Iteration 94, loss = 0.47166412\n",
            "Iteration 95, loss = 0.46987707\n",
            "Iteration 96, loss = 0.46863913\n",
            "Iteration 97, loss = 0.46690074\n",
            "Iteration 98, loss = 0.46480597\n",
            "Iteration 99, loss = 0.46374544\n",
            "Iteration 100, loss = 0.46174603\n",
            "Iteration 101, loss = 0.46048250\n",
            "Iteration 102, loss = 0.45889012\n",
            "Iteration 103, loss = 0.45740712\n",
            "Iteration 104, loss = 0.45516582\n",
            "Iteration 105, loss = 0.45429783\n",
            "Iteration 106, loss = 0.45285640\n",
            "Iteration 107, loss = 0.45075290\n",
            "Iteration 108, loss = 0.44966593\n",
            "Iteration 109, loss = 0.44806363\n",
            "Iteration 110, loss = 0.44631077\n",
            "Iteration 111, loss = 0.44501308\n",
            "Iteration 112, loss = 0.44370247\n",
            "Iteration 113, loss = 0.44188942\n",
            "Iteration 114, loss = 0.44149444\n",
            "Iteration 115, loss = 0.43932912\n",
            "Iteration 116, loss = 0.43763147\n",
            "Iteration 117, loss = 0.43593541\n",
            "Iteration 118, loss = 0.43544757\n",
            "Iteration 119, loss = 0.43394195\n",
            "Iteration 120, loss = 0.43126373\n",
            "Iteration 121, loss = 0.43086874\n",
            "Iteration 122, loss = 0.42901281\n",
            "Iteration 123, loss = 0.42655268\n",
            "Iteration 124, loss = 0.42631187\n",
            "Iteration 125, loss = 0.42769110\n",
            "Iteration 126, loss = 0.42328518\n",
            "Iteration 127, loss = 0.42441893\n",
            "Iteration 128, loss = 0.42125223\n",
            "Iteration 129, loss = 0.42176534\n",
            "Iteration 130, loss = 0.42124065\n",
            "Iteration 131, loss = 0.41940823\n",
            "Iteration 132, loss = 0.41726486\n",
            "Iteration 133, loss = 0.41842572\n",
            "Iteration 134, loss = 0.41698636\n",
            "Iteration 135, loss = 0.41369559\n",
            "Iteration 136, loss = 0.41291292\n",
            "Iteration 137, loss = 0.40967871\n",
            "Iteration 138, loss = 0.41046783\n",
            "Iteration 139, loss = 0.41004790\n",
            "Iteration 140, loss = 0.40768699\n",
            "Iteration 141, loss = 0.40957070\n",
            "Iteration 142, loss = 0.40840404\n",
            "Iteration 143, loss = 0.40660501\n",
            "Iteration 144, loss = 0.40712869\n",
            "Iteration 145, loss = 0.40807293\n",
            "Iteration 146, loss = 0.40643238\n",
            "Iteration 147, loss = 0.40445802\n",
            "Iteration 148, loss = 0.40213037\n",
            "Iteration 149, loss = 0.40115059\n",
            "Iteration 150, loss = 0.40754772\n",
            "Iteration 151, loss = 0.40673716\n",
            "Iteration 152, loss = 0.40186698\n",
            "Iteration 153, loss = 0.40445593\n",
            "Iteration 154, loss = 0.39974058\n",
            "Iteration 155, loss = 0.40552719\n",
            "Iteration 156, loss = 0.40367733\n",
            "Iteration 157, loss = 0.40172166\n",
            "Iteration 158, loss = 0.40026847\n",
            "Iteration 159, loss = 0.39502414\n",
            "Iteration 160, loss = 0.40505636\n",
            "Iteration 161, loss = 0.39984051\n",
            "Iteration 162, loss = 0.39992123\n",
            "Iteration 163, loss = 0.39850756\n",
            "Iteration 164, loss = 0.40031872\n",
            "Iteration 165, loss = 0.39437435\n",
            "Iteration 166, loss = 0.39753143\n",
            "Iteration 167, loss = 0.40410820\n",
            "Iteration 168, loss = 0.39745892\n",
            "Iteration 169, loss = 0.40019344\n",
            "Iteration 170, loss = 0.39944371\n",
            "Iteration 171, loss = 0.39349736\n",
            "Iteration 172, loss = 0.39317458\n",
            "Iteration 173, loss = 0.39507933\n",
            "Iteration 174, loss = 0.39335537\n",
            "Iteration 175, loss = 0.39621949\n",
            "Iteration 176, loss = 0.39401561\n",
            "Iteration 177, loss = 0.39642596\n",
            "Iteration 178, loss = 0.39161975\n",
            "Iteration 179, loss = 0.40110982\n",
            "Iteration 180, loss = 0.39298718\n",
            "Iteration 181, loss = 0.39462236\n",
            "Iteration 182, loss = 0.40063759\n",
            "Iteration 183, loss = 0.39507038\n",
            "Iteration 184, loss = 0.39048948\n",
            "Iteration 185, loss = 0.39904569\n",
            "Iteration 186, loss = 0.38816662\n",
            "Iteration 187, loss = 0.39431917\n",
            "Iteration 188, loss = 0.39486204\n",
            "Iteration 189, loss = 0.39368591\n",
            "Iteration 190, loss = 0.39471873\n",
            "Iteration 191, loss = 0.39490734\n",
            "Iteration 192, loss = 0.39563697\n",
            "Iteration 193, loss = 0.39990394\n",
            "Iteration 194, loss = 0.39400809\n",
            "Iteration 195, loss = 0.39283760\n",
            "Iteration 196, loss = 0.39475557\n",
            "Iteration 197, loss = 0.39750165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 198, loss = 0.35289008\n",
            "Iteration 199, loss = 0.35142650\n",
            "Iteration 200, loss = 0.35133665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68983348\n",
            "Iteration 3, loss = 0.67783699\n",
            "Iteration 4, loss = 0.66497604\n",
            "Iteration 5, loss = 0.65071859\n",
            "Iteration 6, loss = 0.64660330\n",
            "Iteration 7, loss = 0.63428141\n",
            "Iteration 8, loss = 0.62379744\n",
            "Iteration 9, loss = 0.62636262\n",
            "Iteration 10, loss = 0.61709278\n",
            "Iteration 11, loss = 0.61037918\n",
            "Iteration 12, loss = 0.60404346\n",
            "Iteration 13, loss = 0.60537057\n",
            "Iteration 14, loss = 0.59941712\n",
            "Iteration 15, loss = 0.59992711\n",
            "Iteration 16, loss = 0.59189473\n",
            "Iteration 17, loss = 0.59356421\n",
            "Iteration 18, loss = 0.58810867\n",
            "Iteration 19, loss = 0.58804745\n",
            "Iteration 20, loss = 0.58775677\n",
            "Iteration 21, loss = 0.58672621\n",
            "Iteration 22, loss = 0.58171830\n",
            "Iteration 23, loss = 0.58209928\n",
            "Iteration 24, loss = 0.57946440\n",
            "Iteration 25, loss = 0.57861811\n",
            "Iteration 26, loss = 0.58137560\n",
            "Iteration 27, loss = 0.57832008\n",
            "Iteration 28, loss = 0.57491831\n",
            "Iteration 29, loss = 0.58011069\n",
            "Iteration 30, loss = 0.57670524\n",
            "Iteration 31, loss = 0.57822162\n",
            "Iteration 32, loss = 0.57511370\n",
            "Iteration 33, loss = 0.57940273\n",
            "Iteration 34, loss = 0.57793617\n",
            "Iteration 35, loss = 0.57353633\n",
            "Iteration 36, loss = 0.57875220\n",
            "Iteration 37, loss = 0.57386571\n",
            "Iteration 38, loss = 0.57579084\n",
            "Iteration 39, loss = 0.57767749\n",
            "Iteration 40, loss = 0.57386916\n",
            "Iteration 41, loss = 0.57639545\n",
            "Iteration 42, loss = 0.57358299\n",
            "Iteration 43, loss = 0.57722019\n",
            "Iteration 44, loss = 0.57583553\n",
            "Iteration 45, loss = 0.57382743\n",
            "Iteration 46, loss = 0.57273877\n",
            "Iteration 47, loss = 0.57190902\n",
            "Iteration 48, loss = 0.57434243\n",
            "Iteration 49, loss = 0.57352240\n",
            "Iteration 50, loss = 0.57474000\n",
            "Iteration 51, loss = 0.61300109\n",
            "Iteration 52, loss = 0.61957521\n",
            "Iteration 53, loss = 0.60764161\n",
            "Iteration 54, loss = 0.60265797\n",
            "Iteration 55, loss = 0.59693423\n",
            "Iteration 56, loss = 0.59397164\n",
            "Iteration 57, loss = 0.58929543\n",
            "Iteration 58, loss = 0.58545934\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 59, loss = 0.56956591\n",
            "Iteration 60, loss = 0.56729914\n",
            "Iteration 61, loss = 0.56532513\n",
            "Iteration 62, loss = 0.56374906\n",
            "Iteration 63, loss = 0.56195146\n",
            "Iteration 64, loss = 0.56018633\n",
            "Iteration 65, loss = 0.55824409\n",
            "Iteration 66, loss = 0.55697420\n",
            "Iteration 67, loss = 0.55455346\n",
            "Iteration 68, loss = 0.55311811\n",
            "Iteration 69, loss = 0.55036211\n",
            "Iteration 70, loss = 0.54850585\n",
            "Iteration 71, loss = 0.54496187\n",
            "Iteration 72, loss = 0.54287647\n",
            "Iteration 73, loss = 0.53977270\n",
            "Iteration 74, loss = 0.53990349\n",
            "Iteration 75, loss = 0.54130534\n",
            "Iteration 76, loss = 0.54083487\n",
            "Iteration 77, loss = 0.53653862\n",
            "Iteration 78, loss = 0.53791972\n",
            "Iteration 79, loss = 0.54335770\n",
            "Iteration 80, loss = 0.53673389\n",
            "Iteration 81, loss = 0.54544657\n",
            "Iteration 82, loss = 0.54412777\n",
            "Iteration 83, loss = 0.55071313\n",
            "Iteration 84, loss = 0.54161136\n",
            "Iteration 85, loss = 0.64347169\n",
            "Iteration 86, loss = 0.63212865\n",
            "Iteration 87, loss = 0.62799677\n",
            "Iteration 88, loss = 0.62476709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 89, loss = 0.62221996\n",
            "Iteration 90, loss = 0.62136423\n",
            "Iteration 91, loss = 0.62062772\n",
            "Iteration 92, loss = 0.61986587\n",
            "Iteration 93, loss = 0.61888194\n",
            "Iteration 94, loss = 0.61807563\n",
            "Iteration 95, loss = 0.61703811\n",
            "Iteration 96, loss = 0.61615968\n",
            "Iteration 97, loss = 0.61508315\n",
            "Iteration 98, loss = 0.61402337\n",
            "Iteration 99, loss = 0.61302292\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 100, loss = 0.61218262\n",
            "Iteration 101, loss = 0.61180530\n",
            "Iteration 102, loss = 0.61161836\n",
            "Iteration 103, loss = 0.61130277\n",
            "Iteration 104, loss = 0.61112950\n",
            "Iteration 105, loss = 0.61084640\n",
            "Iteration 106, loss = 0.61061634\n",
            "Iteration 107, loss = 0.61039484\n",
            "Iteration 108, loss = 0.61016871\n",
            "Iteration 109, loss = 0.60986357\n",
            "Iteration 110, loss = 0.60964907\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 111, loss = 0.60942605\n",
            "Iteration 112, loss = 0.60937003\n",
            "Iteration 113, loss = 0.60931958\n",
            "Iteration 114, loss = 0.60927071\n",
            "Iteration 115, loss = 0.60922833\n",
            "Iteration 116, loss = 0.60917212\n",
            "Iteration 117, loss = 0.60912446\n",
            "Iteration 118, loss = 0.60906760\n",
            "Iteration 119, loss = 0.60901344\n",
            "Iteration 120, loss = 0.60897102\n",
            "Iteration 121, loss = 0.60890864\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.60888411\n",
            "Iteration 123, loss = 0.60886608\n",
            "Iteration 124, loss = 0.60885292\n",
            "Iteration 125, loss = 0.60884297\n",
            "Iteration 126, loss = 0.60883297\n",
            "Iteration 127, loss = 0.60882171\n",
            "Iteration 128, loss = 0.60881285\n",
            "Iteration 129, loss = 0.60880116\n",
            "Iteration 130, loss = 0.60879263\n",
            "Iteration 131, loss = 0.60878336\n",
            "Iteration 132, loss = 0.60877209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69296701\n",
            "Iteration 3, loss = 0.67264590\n",
            "Iteration 4, loss = 0.66131815\n",
            "Iteration 5, loss = 0.64873949\n",
            "Iteration 6, loss = 0.64052579\n",
            "Iteration 7, loss = 0.62974293\n",
            "Iteration 8, loss = 0.62504469\n",
            "Iteration 9, loss = 0.61685360\n",
            "Iteration 10, loss = 0.61231498\n",
            "Iteration 11, loss = 0.60561605\n",
            "Iteration 12, loss = 0.60233022\n",
            "Iteration 13, loss = 0.59722803\n",
            "Iteration 14, loss = 0.59383546\n",
            "Iteration 15, loss = 0.59257042\n",
            "Iteration 16, loss = 0.58701304\n",
            "Iteration 17, loss = 0.58353253\n",
            "Iteration 18, loss = 0.58177686\n",
            "Iteration 19, loss = 0.58251627\n",
            "Iteration 20, loss = 0.58015917\n",
            "Iteration 21, loss = 0.57787913\n",
            "Iteration 22, loss = 0.57751994\n",
            "Iteration 23, loss = 0.57429236\n",
            "Iteration 24, loss = 0.57662047\n",
            "Iteration 25, loss = 0.57209430\n",
            "Iteration 26, loss = 0.57446832\n",
            "Iteration 27, loss = 0.57475293\n",
            "Iteration 28, loss = 0.57249882\n",
            "Iteration 29, loss = 0.57010238\n",
            "Iteration 30, loss = 0.57225709\n",
            "Iteration 31, loss = 0.57089932\n",
            "Iteration 32, loss = 0.56933550\n",
            "Iteration 33, loss = 0.57000216\n",
            "Iteration 34, loss = 0.57073543\n",
            "Iteration 35, loss = 0.56987274\n",
            "Iteration 36, loss = 0.56775946\n",
            "Iteration 37, loss = 0.57187166\n",
            "Iteration 38, loss = 0.56674414\n",
            "Iteration 39, loss = 0.57064350\n",
            "Iteration 40, loss = 0.56804599\n",
            "Iteration 41, loss = 0.56840218\n",
            "Iteration 42, loss = 0.57260047\n",
            "Iteration 43, loss = 0.56743684\n",
            "Iteration 44, loss = 0.57097967\n",
            "Iteration 45, loss = 0.56674279\n",
            "Iteration 46, loss = 0.56755401\n",
            "Iteration 47, loss = 0.57155727\n",
            "Iteration 48, loss = 0.57100830\n",
            "Iteration 49, loss = 0.56610210\n",
            "Iteration 50, loss = 0.57109807\n",
            "Iteration 51, loss = 0.56856247\n",
            "Iteration 52, loss = 0.56700925\n",
            "Iteration 53, loss = 0.57184531\n",
            "Iteration 54, loss = 0.56739503\n",
            "Iteration 55, loss = 0.57164586\n",
            "Iteration 56, loss = 0.56769311\n",
            "Iteration 57, loss = 0.56860567\n",
            "Iteration 58, loss = 0.57341119\n",
            "Iteration 59, loss = 0.56669987\n",
            "Iteration 60, loss = 0.56874095\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 61, loss = 0.53198485\n",
            "Iteration 62, loss = 0.52207699\n",
            "Iteration 63, loss = 0.51552028\n",
            "Iteration 64, loss = 0.50847964\n",
            "Iteration 65, loss = 0.50175974\n",
            "Iteration 66, loss = 0.49439418\n",
            "Iteration 67, loss = 0.48809686\n",
            "Iteration 68, loss = 0.50140614\n",
            "Iteration 69, loss = 0.50034846\n",
            "Iteration 70, loss = 0.52513199\n",
            "Iteration 71, loss = 0.52287292\n",
            "Iteration 72, loss = 0.51551847\n",
            "Iteration 73, loss = 0.52147394\n",
            "Iteration 74, loss = 0.52339874\n",
            "Iteration 75, loss = 0.52220721\n",
            "Iteration 76, loss = 0.51818526\n",
            "Iteration 77, loss = 0.51158643\n",
            "Iteration 78, loss = 0.53486771\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 79, loss = 0.45225047\n",
            "Iteration 80, loss = 0.44343227\n",
            "Iteration 81, loss = 0.44040565\n",
            "Iteration 82, loss = 0.43745928\n",
            "Iteration 83, loss = 0.43475593\n",
            "Iteration 84, loss = 0.43137655\n",
            "Iteration 85, loss = 0.42837081\n",
            "Iteration 86, loss = 0.42610198\n",
            "Iteration 87, loss = 0.42251898\n",
            "Iteration 88, loss = 0.42002695\n",
            "Iteration 89, loss = 0.41753897\n",
            "Iteration 90, loss = 0.41445213\n",
            "Iteration 91, loss = 0.41224066\n",
            "Iteration 92, loss = 0.40972864\n",
            "Iteration 93, loss = 0.40631727\n",
            "Iteration 94, loss = 0.40349475\n",
            "Iteration 95, loss = 0.40131563\n",
            "Iteration 96, loss = 0.39852453\n",
            "Iteration 97, loss = 0.39597925\n",
            "Iteration 98, loss = 0.39386682\n",
            "Iteration 99, loss = 0.39057751\n",
            "Iteration 100, loss = 0.38897942\n",
            "Iteration 101, loss = 0.38694959\n",
            "Iteration 102, loss = 0.38462795\n",
            "Iteration 103, loss = 0.38168656\n",
            "Iteration 104, loss = 0.37941590\n",
            "Iteration 105, loss = 0.37710533\n",
            "Iteration 106, loss = 0.37503094\n",
            "Iteration 107, loss = 0.37364548\n",
            "Iteration 108, loss = 0.37105656\n",
            "Iteration 109, loss = 0.36810426\n",
            "Iteration 110, loss = 0.36656481\n",
            "Iteration 111, loss = 0.36509621\n",
            "Iteration 112, loss = 0.36280229\n",
            "Iteration 113, loss = 0.36071571\n",
            "Iteration 114, loss = 0.35833466\n",
            "Iteration 115, loss = 0.35696816\n",
            "Iteration 116, loss = 0.35450703\n",
            "Iteration 117, loss = 0.35315110\n",
            "Iteration 118, loss = 0.35158355\n",
            "Iteration 119, loss = 0.35002353\n",
            "Iteration 120, loss = 0.34837240\n",
            "Iteration 121, loss = 0.34724259\n",
            "Iteration 122, loss = 0.34546386\n",
            "Iteration 123, loss = 0.34383658\n",
            "Iteration 124, loss = 0.34159484\n",
            "Iteration 125, loss = 0.33940586\n",
            "Iteration 126, loss = 0.33775413\n",
            "Iteration 127, loss = 0.33692438\n",
            "Iteration 128, loss = 0.33610242\n",
            "Iteration 129, loss = 0.33476161\n",
            "Iteration 130, loss = 0.33282583\n",
            "Iteration 131, loss = 0.33144975\n",
            "Iteration 132, loss = 0.32987300\n",
            "Iteration 133, loss = 0.32863589\n",
            "Iteration 134, loss = 0.32879613\n",
            "Iteration 135, loss = 0.32605174\n",
            "Iteration 136, loss = 0.32407587\n",
            "Iteration 137, loss = 0.32387736\n",
            "Iteration 138, loss = 0.32236111\n",
            "Iteration 139, loss = 0.32268348\n",
            "Iteration 140, loss = 0.31970380\n",
            "Iteration 141, loss = 0.31990304\n",
            "Iteration 142, loss = 0.31825716\n",
            "Iteration 143, loss = 0.31617535\n",
            "Iteration 144, loss = 0.31754591\n",
            "Iteration 145, loss = 0.31618869\n",
            "Iteration 146, loss = 0.31354234\n",
            "Iteration 147, loss = 0.31231955\n",
            "Iteration 148, loss = 0.31329335\n",
            "Iteration 149, loss = 0.31049323\n",
            "Iteration 150, loss = 0.31078565\n",
            "Iteration 151, loss = 0.30922453\n",
            "Iteration 152, loss = 0.30983289\n",
            "Iteration 153, loss = 0.30580941\n",
            "Iteration 154, loss = 0.30618612\n",
            "Iteration 155, loss = 0.30515516\n",
            "Iteration 156, loss = 0.30534438\n",
            "Iteration 157, loss = 0.30562843\n",
            "Iteration 158, loss = 0.30349640\n",
            "Iteration 159, loss = 0.30489969\n",
            "Iteration 160, loss = 0.30619995\n",
            "Iteration 161, loss = 0.30016084\n",
            "Iteration 162, loss = 0.30156047\n",
            "Iteration 163, loss = 0.30043983\n",
            "Iteration 164, loss = 0.30183847\n",
            "Iteration 165, loss = 0.30143168\n",
            "Iteration 166, loss = 0.29646500\n",
            "Iteration 167, loss = 0.29791658\n",
            "Iteration 168, loss = 0.29637110\n",
            "Iteration 169, loss = 0.29585039\n",
            "Iteration 170, loss = 0.29412008\n",
            "Iteration 171, loss = 0.29789890\n",
            "Iteration 172, loss = 0.29522054\n",
            "Iteration 173, loss = 0.29458115\n",
            "Iteration 174, loss = 0.29636291\n",
            "Iteration 175, loss = 0.29399172\n",
            "Iteration 176, loss = 0.29247643\n",
            "Iteration 177, loss = 0.29424197\n",
            "Iteration 178, loss = 0.29321105\n",
            "Iteration 179, loss = 0.29706646\n",
            "Iteration 180, loss = 0.29331233\n",
            "Iteration 181, loss = 0.29002581\n",
            "Iteration 182, loss = 0.29036108\n",
            "Iteration 183, loss = 0.29493493\n",
            "Iteration 184, loss = 0.29148848\n",
            "Iteration 185, loss = 0.29599306\n",
            "Iteration 186, loss = 0.28635592\n",
            "Iteration 187, loss = 0.28940384\n",
            "Iteration 188, loss = 0.29137970\n",
            "Iteration 189, loss = 0.28780853\n",
            "Iteration 190, loss = 0.28560629\n",
            "Iteration 191, loss = 0.29223965\n",
            "Iteration 192, loss = 0.29204530\n",
            "Iteration 193, loss = 0.28907156\n",
            "Iteration 194, loss = 0.29045978\n",
            "Iteration 195, loss = 0.28380432\n",
            "Iteration 196, loss = 0.28923134\n",
            "Iteration 197, loss = 0.28701450\n",
            "Iteration 198, loss = 0.28224992\n",
            "Iteration 199, loss = 0.29215074\n",
            "Iteration 200, loss = 0.28544831\n",
            "----------------------------------\n",
            "[[33518   209]\n",
            " [ 1072 15847]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  9.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     33727\n",
            "           1       0.99      0.94      0.96     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.98      0.97      0.97     50646\n",
            "weighted avg       0.98      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.842% (0.283%)\n",
            "Prediction Accuracy (Shuffle Split) : 99.092% (12.911%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63948967\n",
            "Iteration 3, loss = 0.61445188\n",
            "Iteration 4, loss = 0.59947794\n",
            "Iteration 5, loss = 0.59283053\n",
            "Iteration 6, loss = 0.58798096\n",
            "Iteration 7, loss = 0.58555742\n",
            "Iteration 8, loss = 0.58239082\n",
            "Iteration 9, loss = 0.58071421\n",
            "Iteration 10, loss = 0.58062682\n",
            "Iteration 11, loss = 0.58097684\n",
            "Iteration 12, loss = 0.58041338\n",
            "Iteration 13, loss = 0.58074181\n",
            "Iteration 14, loss = 0.58042856\n",
            "Iteration 15, loss = 0.58007921\n",
            "Iteration 16, loss = 0.58002255\n",
            "Iteration 17, loss = 0.57861285\n",
            "Iteration 18, loss = 0.57922420\n",
            "Iteration 19, loss = 0.57855219\n",
            "Iteration 20, loss = 0.57871868\n",
            "Iteration 21, loss = 0.57890447\n",
            "Iteration 22, loss = 0.57568677\n",
            "Iteration 23, loss = 0.57843487\n",
            "Iteration 24, loss = 0.57791224\n",
            "Iteration 25, loss = 0.57767374\n",
            "Iteration 26, loss = 0.57905375\n",
            "Iteration 27, loss = 0.58044042\n",
            "Iteration 28, loss = 0.57886738\n",
            "Iteration 29, loss = 0.57929857\n",
            "Iteration 30, loss = 0.57897064\n",
            "Iteration 31, loss = 0.57978567\n",
            "Iteration 32, loss = 0.57938653\n",
            "Iteration 33, loss = 0.57846890\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.52105841\n",
            "Iteration 35, loss = 0.51089802\n",
            "Iteration 36, loss = 0.52677439\n",
            "Iteration 37, loss = 0.52948590\n",
            "Iteration 38, loss = 0.52564422\n",
            "Iteration 39, loss = 0.52972526\n",
            "Iteration 40, loss = 0.52710661\n",
            "Iteration 41, loss = 0.52802932\n",
            "Iteration 42, loss = 0.52729578\n",
            "Iteration 43, loss = 0.52896422\n",
            "Iteration 44, loss = 0.52617287\n",
            "Iteration 45, loss = 0.53092593\n",
            "Iteration 46, loss = 0.52812007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.44054083\n",
            "Iteration 48, loss = 0.42595058\n",
            "Iteration 49, loss = 0.41309315\n",
            "Iteration 50, loss = 0.40181222\n",
            "Iteration 51, loss = 0.39170868\n",
            "Iteration 52, loss = 0.38299239\n",
            "Iteration 53, loss = 0.37511641\n",
            "Iteration 54, loss = 0.36827385\n",
            "Iteration 55, loss = 0.36199807\n",
            "Iteration 56, loss = 0.35695174\n",
            "Iteration 57, loss = 0.35221607\n",
            "Iteration 58, loss = 0.34868453\n",
            "Iteration 59, loss = 0.34466912\n",
            "Iteration 60, loss = 0.34181476\n",
            "Iteration 61, loss = 0.33918691\n",
            "Iteration 62, loss = 0.33685041\n",
            "Iteration 63, loss = 0.33467076\n",
            "Iteration 64, loss = 0.33243960\n",
            "Iteration 65, loss = 0.33053393\n",
            "Iteration 66, loss = 0.32868133\n",
            "Iteration 67, loss = 0.32727887\n",
            "Iteration 68, loss = 0.32657739\n",
            "Iteration 69, loss = 0.32750835\n",
            "Iteration 70, loss = 0.32554647\n",
            "Iteration 71, loss = 0.32434080\n",
            "Iteration 72, loss = 0.32379371\n",
            "Iteration 73, loss = 0.32389973\n",
            "Iteration 74, loss = 0.32308101\n",
            "Iteration 75, loss = 0.32415516\n",
            "Iteration 76, loss = 0.32362395\n",
            "Iteration 77, loss = 0.32304953\n",
            "Iteration 78, loss = 0.32633062\n",
            "Iteration 79, loss = 0.32369675\n",
            "Iteration 80, loss = 0.32223596\n",
            "Iteration 81, loss = 0.32336690\n",
            "Iteration 82, loss = 0.32854412\n",
            "Iteration 83, loss = 0.32417651\n",
            "Iteration 84, loss = 0.32262836\n",
            "Iteration 85, loss = 0.32772340\n",
            "Iteration 86, loss = 0.32445271\n",
            "Iteration 87, loss = 0.32459570\n",
            "Iteration 88, loss = 0.32498340\n",
            "Iteration 89, loss = 0.33032815\n",
            "Iteration 90, loss = 0.32716294\n",
            "Iteration 91, loss = 0.32789117\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.30788484\n",
            "Iteration 93, loss = 0.30762189\n",
            "Iteration 94, loss = 0.30754465\n",
            "Iteration 95, loss = 0.30750054\n",
            "Iteration 96, loss = 0.30746130\n",
            "Iteration 97, loss = 0.30735424\n",
            "Iteration 98, loss = 0.30728768\n",
            "Iteration 99, loss = 0.30720320\n",
            "Iteration 100, loss = 0.30712222\n",
            "Iteration 101, loss = 0.30713878\n",
            "Iteration 102, loss = 0.30702888\n",
            "Iteration 103, loss = 0.30697476\n",
            "Iteration 104, loss = 0.30703382\n",
            "Iteration 105, loss = 0.30681257\n",
            "Iteration 106, loss = 0.30689896\n",
            "Iteration 107, loss = 0.30672123\n",
            "Iteration 108, loss = 0.30673189\n",
            "Iteration 109, loss = 0.30668981\n",
            "Iteration 110, loss = 0.30663143\n",
            "Iteration 111, loss = 0.30654058\n",
            "Iteration 112, loss = 0.30655390\n",
            "Iteration 113, loss = 0.30654467\n",
            "Iteration 114, loss = 0.30637914\n",
            "Iteration 115, loss = 0.30639672\n",
            "Iteration 116, loss = 0.30638897\n",
            "Iteration 117, loss = 0.30630194\n",
            "Iteration 118, loss = 0.30621844\n",
            "Iteration 119, loss = 0.30625175\n",
            "Iteration 120, loss = 0.30619079\n",
            "Iteration 121, loss = 0.30613381\n",
            "Iteration 122, loss = 0.30611383\n",
            "Iteration 123, loss = 0.30592266\n",
            "Iteration 124, loss = 0.30594874\n",
            "Iteration 125, loss = 0.30596864\n",
            "Iteration 126, loss = 0.30593289\n",
            "Iteration 127, loss = 0.30585576\n",
            "Iteration 128, loss = 0.30585829\n",
            "Iteration 129, loss = 0.30574911\n",
            "Iteration 130, loss = 0.30568656\n",
            "Iteration 131, loss = 0.30565477\n",
            "Iteration 132, loss = 0.30555298\n",
            "Iteration 133, loss = 0.30550544\n",
            "Iteration 134, loss = 0.30547698\n",
            "Iteration 135, loss = 0.30548181\n",
            "Iteration 136, loss = 0.30547072\n",
            "Iteration 137, loss = 0.30540302\n",
            "Iteration 138, loss = 0.30539493\n",
            "Iteration 139, loss = 0.30534217\n",
            "Iteration 140, loss = 0.30523904\n",
            "Iteration 141, loss = 0.30525844\n",
            "Iteration 142, loss = 0.30520175\n",
            "Iteration 143, loss = 0.30512531\n",
            "Iteration 144, loss = 0.30514253\n",
            "Iteration 145, loss = 0.30505334\n",
            "Iteration 146, loss = 0.30500863\n",
            "Iteration 147, loss = 0.30502946\n",
            "Iteration 148, loss = 0.30503479\n",
            "Iteration 149, loss = 0.30491721\n",
            "Iteration 150, loss = 0.30487747\n",
            "Iteration 151, loss = 0.30495776\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 152, loss = 0.30437600\n",
            "Iteration 153, loss = 0.30438692\n",
            "Iteration 154, loss = 0.30433702\n",
            "Iteration 155, loss = 0.30434890\n",
            "Iteration 156, loss = 0.30435920\n",
            "Iteration 157, loss = 0.30432931\n",
            "Iteration 158, loss = 0.30437603\n",
            "Iteration 159, loss = 0.30429427\n",
            "Iteration 160, loss = 0.30434634\n",
            "Iteration 161, loss = 0.30433115\n",
            "Iteration 162, loss = 0.30430542\n",
            "Iteration 163, loss = 0.30431845\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 164, loss = 0.30413739\n",
            "Iteration 165, loss = 0.30414604\n",
            "Iteration 166, loss = 0.30413405\n",
            "Iteration 167, loss = 0.30414209\n",
            "Iteration 168, loss = 0.30413377\n",
            "Iteration 169, loss = 0.30413925\n",
            "Iteration 170, loss = 0.30411590\n",
            "Iteration 171, loss = 0.30413508\n",
            "Iteration 172, loss = 0.30413748\n",
            "Iteration 173, loss = 0.30414260\n",
            "Iteration 174, loss = 0.30411907\n",
            "Iteration 175, loss = 0.30413935\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63096473\n",
            "Iteration 3, loss = 0.60606486\n",
            "Iteration 4, loss = 0.58930299\n",
            "Iteration 5, loss = 0.59078684\n",
            "Iteration 6, loss = 0.59023936\n",
            "Iteration 7, loss = 0.58077618\n",
            "Iteration 8, loss = 0.58621170\n",
            "Iteration 9, loss = 0.57844124\n",
            "Iteration 10, loss = 0.57894804\n",
            "Iteration 11, loss = 0.57665813\n",
            "Iteration 12, loss = 0.61022091\n",
            "Iteration 13, loss = 0.58744285\n",
            "Iteration 14, loss = 0.57934565\n",
            "Iteration 15, loss = 0.60484046\n",
            "Iteration 16, loss = 0.59045232\n",
            "Iteration 17, loss = 0.58014498\n",
            "Iteration 18, loss = 0.57485677\n",
            "Iteration 19, loss = 0.57199539\n",
            "Iteration 20, loss = 0.57219087\n",
            "Iteration 21, loss = 0.57049656\n",
            "Iteration 22, loss = 0.57111008\n",
            "Iteration 23, loss = 0.57169105\n",
            "Iteration 24, loss = 0.57315214\n",
            "Iteration 25, loss = 0.57237956\n",
            "Iteration 26, loss = 0.57124286\n",
            "Iteration 27, loss = 0.57176872\n",
            "Iteration 28, loss = 0.57238588\n",
            "Iteration 29, loss = 0.57154875\n",
            "Iteration 30, loss = 0.57333672\n",
            "Iteration 31, loss = 0.57369909\n",
            "Iteration 32, loss = 0.57240261\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 33, loss = 0.50756361\n",
            "Iteration 34, loss = 0.49009378\n",
            "Iteration 35, loss = 0.51570619\n",
            "Iteration 36, loss = 0.51584112\n",
            "Iteration 37, loss = 0.51376381\n",
            "Iteration 38, loss = 0.51868165\n",
            "Iteration 39, loss = 0.51814668\n",
            "Iteration 40, loss = 0.51284670\n",
            "Iteration 41, loss = 0.51975380\n",
            "Iteration 42, loss = 0.51646442\n",
            "Iteration 43, loss = 0.51245996\n",
            "Iteration 44, loss = 0.51738988\n",
            "Iteration 45, loss = 0.51690339\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.41530201\n",
            "Iteration 47, loss = 0.39994834\n",
            "Iteration 48, loss = 0.38652889\n",
            "Iteration 49, loss = 0.37420653\n",
            "Iteration 50, loss = 0.36297593\n",
            "Iteration 51, loss = 0.35312269\n",
            "Iteration 52, loss = 0.34448534\n",
            "Iteration 53, loss = 0.33596152\n",
            "Iteration 54, loss = 0.32882657\n",
            "Iteration 55, loss = 0.32218096\n",
            "Iteration 56, loss = 0.31627891\n",
            "Iteration 57, loss = 0.31084796\n",
            "Iteration 58, loss = 0.30609916\n",
            "Iteration 59, loss = 0.30209262\n",
            "Iteration 60, loss = 0.29801901\n",
            "Iteration 61, loss = 0.29444986\n",
            "Iteration 62, loss = 0.29095038\n",
            "Iteration 63, loss = 0.28867791\n",
            "Iteration 64, loss = 0.28580828\n",
            "Iteration 65, loss = 0.28378998\n",
            "Iteration 66, loss = 0.28156378\n",
            "Iteration 67, loss = 0.27966856\n",
            "Iteration 68, loss = 0.27732989\n",
            "Iteration 69, loss = 0.27567462\n",
            "Iteration 70, loss = 0.27507652\n",
            "Iteration 71, loss = 0.27336343\n",
            "Iteration 72, loss = 0.27121030\n",
            "Iteration 73, loss = 0.27153043\n",
            "Iteration 74, loss = 0.27057899\n",
            "Iteration 75, loss = 0.26901014\n",
            "Iteration 76, loss = 0.26879715\n",
            "Iteration 77, loss = 0.26761928\n",
            "Iteration 78, loss = 0.26795069\n",
            "Iteration 79, loss = 0.26527920\n",
            "Iteration 80, loss = 0.26663773\n",
            "Iteration 81, loss = 0.26477970\n",
            "Iteration 82, loss = 0.26383976\n",
            "Iteration 83, loss = 0.26449005\n",
            "Iteration 84, loss = 0.26544787\n",
            "Iteration 85, loss = 0.26442585\n",
            "Iteration 86, loss = 0.26583275\n",
            "Iteration 87, loss = 0.26607072\n",
            "Iteration 88, loss = 0.26707136\n",
            "Iteration 89, loss = 0.26408888\n",
            "Iteration 90, loss = 0.26372589\n",
            "Iteration 91, loss = 0.26548110\n",
            "Iteration 92, loss = 0.27020756\n",
            "Iteration 93, loss = 0.26507353\n",
            "Iteration 94, loss = 0.26314979\n",
            "Iteration 95, loss = 0.28344703\n",
            "Iteration 96, loss = 0.27095725\n",
            "Iteration 97, loss = 0.26767829\n",
            "Iteration 98, loss = 0.27979250\n",
            "Iteration 99, loss = 0.27557689\n",
            "Iteration 100, loss = 0.27340360\n",
            "Iteration 101, loss = 0.27093948\n",
            "Iteration 102, loss = 0.28381776\n",
            "Iteration 103, loss = 0.26420737\n",
            "Iteration 104, loss = 0.26396891\n",
            "Iteration 105, loss = 0.28290513\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 106, loss = 0.25050478\n",
            "Iteration 107, loss = 0.25038234\n",
            "Iteration 108, loss = 0.25033299\n",
            "Iteration 109, loss = 0.25026168\n",
            "Iteration 110, loss = 0.25027114\n",
            "Iteration 111, loss = 0.25020241\n",
            "Iteration 112, loss = 0.25018628\n",
            "Iteration 113, loss = 0.25016804\n",
            "Iteration 114, loss = 0.25006746\n",
            "Iteration 115, loss = 0.25002806\n",
            "Iteration 116, loss = 0.24998571\n",
            "Iteration 117, loss = 0.24993875\n",
            "Iteration 118, loss = 0.24995809\n",
            "Iteration 119, loss = 0.24980814\n",
            "Iteration 120, loss = 0.24981902\n",
            "Iteration 121, loss = 0.24979532\n",
            "Iteration 122, loss = 0.24975696\n",
            "Iteration 123, loss = 0.24974399\n",
            "Iteration 124, loss = 0.24969938\n",
            "Iteration 125, loss = 0.24964199\n",
            "Iteration 126, loss = 0.24961164\n",
            "Iteration 127, loss = 0.24958852\n",
            "Iteration 128, loss = 0.24953869\n",
            "Iteration 129, loss = 0.24955519\n",
            "Iteration 130, loss = 0.24944503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 131, loss = 0.24919480\n",
            "Iteration 132, loss = 0.24918298\n",
            "Iteration 133, loss = 0.24916880\n",
            "Iteration 134, loss = 0.24916696\n",
            "Iteration 135, loss = 0.24919137\n",
            "Iteration 136, loss = 0.24915651\n",
            "Iteration 137, loss = 0.24911825\n",
            "Iteration 138, loss = 0.24914205\n",
            "Iteration 139, loss = 0.24915241\n",
            "Iteration 140, loss = 0.24913913\n",
            "Iteration 141, loss = 0.24912631\n",
            "Iteration 142, loss = 0.24912029\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 143, loss = 0.24903036\n",
            "Iteration 144, loss = 0.24902872\n",
            "Iteration 145, loss = 0.24901963\n",
            "Iteration 146, loss = 0.24902707\n",
            "Iteration 147, loss = 0.24902613\n",
            "Iteration 148, loss = 0.24902100\n",
            "Iteration 149, loss = 0.24902803\n",
            "Iteration 150, loss = 0.24901568\n",
            "Iteration 151, loss = 0.24901384\n",
            "Iteration 152, loss = 0.24901346\n",
            "Iteration 153, loss = 0.24900465\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64849186\n",
            "Iteration 3, loss = 0.61624549\n",
            "Iteration 4, loss = 0.59808332\n",
            "Iteration 5, loss = 0.60569095\n",
            "Iteration 6, loss = 0.59971492\n",
            "Iteration 7, loss = 0.58579787\n",
            "Iteration 8, loss = 0.57967433\n",
            "Iteration 9, loss = 0.57553805\n",
            "Iteration 10, loss = 0.57572923\n",
            "Iteration 11, loss = 0.57238787\n",
            "Iteration 12, loss = 0.57305198\n",
            "Iteration 13, loss = 0.57363398\n",
            "Iteration 14, loss = 0.57304561\n",
            "Iteration 15, loss = 0.57250274\n",
            "Iteration 16, loss = 0.57408593\n",
            "Iteration 17, loss = 0.57132612\n",
            "Iteration 18, loss = 0.57576674\n",
            "Iteration 19, loss = 0.57328530\n",
            "Iteration 20, loss = 0.57620112\n",
            "Iteration 21, loss = 0.57565292\n",
            "Iteration 22, loss = 0.57465907\n",
            "Iteration 23, loss = 0.57575478\n",
            "Iteration 24, loss = 0.57595249\n",
            "Iteration 25, loss = 0.57631788\n",
            "Iteration 26, loss = 0.57508308\n",
            "Iteration 27, loss = 0.57610785\n",
            "Iteration 28, loss = 0.57513328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.52244290\n",
            "Iteration 30, loss = 0.50386339\n",
            "Iteration 31, loss = 0.52630634\n",
            "Iteration 32, loss = 0.52438351\n",
            "Iteration 33, loss = 0.52836567\n",
            "Iteration 34, loss = 0.52690283\n",
            "Iteration 35, loss = 0.52524317\n",
            "Iteration 36, loss = 0.52650850\n",
            "Iteration 37, loss = 0.52481572\n",
            "Iteration 38, loss = 0.52584506\n",
            "Iteration 39, loss = 0.52574510\n",
            "Iteration 40, loss = 0.52422672\n",
            "Iteration 41, loss = 0.52646802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 42, loss = 0.44129243\n",
            "Iteration 43, loss = 0.42754632\n",
            "Iteration 44, loss = 0.41591517\n",
            "Iteration 45, loss = 0.40495097\n",
            "Iteration 46, loss = 0.39515259\n",
            "Iteration 47, loss = 0.38640085\n",
            "Iteration 48, loss = 0.37813613\n",
            "Iteration 49, loss = 0.37112045\n",
            "Iteration 50, loss = 0.36521860\n",
            "Iteration 51, loss = 0.35922815\n",
            "Iteration 52, loss = 0.35478617\n",
            "Iteration 53, loss = 0.35009240\n",
            "Iteration 54, loss = 0.34598003\n",
            "Iteration 55, loss = 0.34386514\n",
            "Iteration 56, loss = 0.34085204\n",
            "Iteration 57, loss = 0.33856091\n",
            "Iteration 58, loss = 0.33670961\n",
            "Iteration 59, loss = 0.33487088\n",
            "Iteration 60, loss = 0.33403623\n",
            "Iteration 61, loss = 0.33354609\n",
            "Iteration 62, loss = 0.33100270\n",
            "Iteration 63, loss = 0.33005025\n",
            "Iteration 64, loss = 0.32967776\n",
            "Iteration 65, loss = 0.32851024\n",
            "Iteration 66, loss = 0.33093735\n",
            "Iteration 67, loss = 0.33082230\n",
            "Iteration 68, loss = 0.33164505\n",
            "Iteration 69, loss = 0.32820307\n",
            "Iteration 70, loss = 0.33200030\n",
            "Iteration 71, loss = 0.32733162\n",
            "Iteration 72, loss = 0.33409428\n",
            "Iteration 73, loss = 0.33255515\n",
            "Iteration 74, loss = 0.33172609\n",
            "Iteration 75, loss = 0.33122368\n",
            "Iteration 76, loss = 0.33145032\n",
            "Iteration 77, loss = 0.33020300\n",
            "Iteration 78, loss = 0.33150176\n",
            "Iteration 79, loss = 0.33116225\n",
            "Iteration 80, loss = 0.33069172\n",
            "Iteration 81, loss = 0.33320729\n",
            "Iteration 82, loss = 0.32953485\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 83, loss = 0.29481614\n",
            "Iteration 84, loss = 0.29444884\n",
            "Iteration 85, loss = 0.29418618\n",
            "Iteration 86, loss = 0.29398971\n",
            "Iteration 87, loss = 0.29379885\n",
            "Iteration 88, loss = 0.29355181\n",
            "Iteration 89, loss = 0.29341750\n",
            "Iteration 90, loss = 0.29323728\n",
            "Iteration 91, loss = 0.29304393\n",
            "Iteration 92, loss = 0.29285326\n",
            "Iteration 93, loss = 0.29266924\n",
            "Iteration 94, loss = 0.29251939\n",
            "Iteration 95, loss = 0.29236334\n",
            "Iteration 96, loss = 0.29215694\n",
            "Iteration 97, loss = 0.29198103\n",
            "Iteration 98, loss = 0.29187757\n",
            "Iteration 99, loss = 0.29168268\n",
            "Iteration 100, loss = 0.29152895\n",
            "Iteration 101, loss = 0.29135316\n",
            "Iteration 102, loss = 0.29122898\n",
            "Iteration 103, loss = 0.29104751\n",
            "Iteration 104, loss = 0.29088119\n",
            "Iteration 105, loss = 0.29075085\n",
            "Iteration 106, loss = 0.29059609\n",
            "Iteration 107, loss = 0.29040811\n",
            "Iteration 108, loss = 0.29025272\n",
            "Iteration 109, loss = 0.29012834\n",
            "Iteration 110, loss = 0.28995256\n",
            "Iteration 111, loss = 0.28975430\n",
            "Iteration 112, loss = 0.28966938\n",
            "Iteration 113, loss = 0.28950589\n",
            "Iteration 114, loss = 0.28940959\n",
            "Iteration 115, loss = 0.28923113\n",
            "Iteration 116, loss = 0.28904241\n",
            "Iteration 117, loss = 0.28890884\n",
            "Iteration 118, loss = 0.28882589\n",
            "Iteration 119, loss = 0.28866526\n",
            "Iteration 120, loss = 0.28850508\n",
            "Iteration 121, loss = 0.28833705\n",
            "Iteration 122, loss = 0.28822361\n",
            "Iteration 123, loss = 0.28812999\n",
            "Iteration 124, loss = 0.28796257\n",
            "Iteration 125, loss = 0.28788956\n",
            "Iteration 126, loss = 0.28774316\n",
            "Iteration 127, loss = 0.28756804\n",
            "Iteration 128, loss = 0.28746559\n",
            "Iteration 129, loss = 0.28731742\n",
            "Iteration 130, loss = 0.28718102\n",
            "Iteration 131, loss = 0.28707971\n",
            "Iteration 132, loss = 0.28688613\n",
            "Iteration 133, loss = 0.28685203\n",
            "Iteration 134, loss = 0.28669806\n",
            "Iteration 135, loss = 0.28655477\n",
            "Iteration 136, loss = 0.28644934\n",
            "Iteration 137, loss = 0.28628481\n",
            "Iteration 138, loss = 0.28619021\n",
            "Iteration 139, loss = 0.28601279\n",
            "Iteration 140, loss = 0.28594935\n",
            "Iteration 141, loss = 0.28579679\n",
            "Iteration 142, loss = 0.28569069\n",
            "Iteration 143, loss = 0.28558310\n",
            "Iteration 144, loss = 0.28549301\n",
            "Iteration 145, loss = 0.28542688\n",
            "Iteration 146, loss = 0.28532348\n",
            "Iteration 147, loss = 0.28507188\n",
            "Iteration 148, loss = 0.28506516\n",
            "Iteration 149, loss = 0.28491098\n",
            "Iteration 150, loss = 0.28482563\n",
            "Iteration 151, loss = 0.28466098\n",
            "Iteration 152, loss = 0.28450195\n",
            "Iteration 153, loss = 0.28448747\n",
            "Iteration 154, loss = 0.28433888\n",
            "Iteration 155, loss = 0.28426771\n",
            "Iteration 156, loss = 0.28416118\n",
            "Iteration 157, loss = 0.28404592\n",
            "Iteration 158, loss = 0.28389581\n",
            "Iteration 159, loss = 0.28381930\n",
            "Iteration 160, loss = 0.28372600\n",
            "Iteration 161, loss = 0.28361271\n",
            "Iteration 162, loss = 0.28346779\n",
            "Iteration 163, loss = 0.28346790\n",
            "Iteration 164, loss = 0.28332304\n",
            "Iteration 165, loss = 0.28325207\n",
            "Iteration 166, loss = 0.28312545\n",
            "Iteration 167, loss = 0.28300909\n",
            "Iteration 168, loss = 0.28293023\n",
            "Iteration 169, loss = 0.28282310\n",
            "Iteration 170, loss = 0.28276045\n",
            "Iteration 171, loss = 0.28264475\n",
            "Iteration 172, loss = 0.28248497\n",
            "Iteration 173, loss = 0.28243281\n",
            "Iteration 174, loss = 0.28230720\n",
            "Iteration 175, loss = 0.28229167\n",
            "Iteration 176, loss = 0.28213010\n",
            "Iteration 177, loss = 0.28205046\n",
            "Iteration 178, loss = 0.28199188\n",
            "Iteration 179, loss = 0.28188908\n",
            "Iteration 180, loss = 0.28179399\n",
            "Iteration 181, loss = 0.28168774\n",
            "Iteration 182, loss = 0.28155547\n",
            "Iteration 183, loss = 0.28150619\n",
            "Iteration 184, loss = 0.28141038\n",
            "Iteration 185, loss = 0.28132797\n",
            "Iteration 186, loss = 0.28124380\n",
            "Iteration 187, loss = 0.28114964\n",
            "Iteration 188, loss = 0.28104032\n",
            "Iteration 189, loss = 0.28094096\n",
            "Iteration 190, loss = 0.28089031\n",
            "Iteration 191, loss = 0.28079133\n",
            "Iteration 192, loss = 0.28071028\n",
            "Iteration 193, loss = 0.28059775\n",
            "Iteration 194, loss = 0.28056255\n",
            "Iteration 195, loss = 0.28044386\n",
            "Iteration 196, loss = 0.28036793\n",
            "Iteration 197, loss = 0.28034405\n",
            "Iteration 198, loss = 0.28026267\n",
            "Iteration 199, loss = 0.28009011\n",
            "Iteration 200, loss = 0.28001472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68167776\n",
            "Iteration 3, loss = 0.64685540\n",
            "Iteration 4, loss = 0.61783505\n",
            "Iteration 5, loss = 0.60173291\n",
            "Iteration 6, loss = 0.59105052\n",
            "Iteration 7, loss = 0.58531198\n",
            "Iteration 8, loss = 0.66234175\n",
            "Iteration 9, loss = 0.63091973\n",
            "Iteration 10, loss = 0.60507051\n",
            "Iteration 11, loss = 0.59127877\n",
            "Iteration 12, loss = 0.58283664\n",
            "Iteration 13, loss = 0.57817843\n",
            "Iteration 14, loss = 0.57606734\n",
            "Iteration 15, loss = 0.61648085\n",
            "Iteration 16, loss = 0.63745965\n",
            "Iteration 17, loss = 0.62832206\n",
            "Iteration 18, loss = 0.60661204\n",
            "Iteration 19, loss = 0.59023163\n",
            "Iteration 20, loss = 0.58224525\n",
            "Iteration 21, loss = 0.57861680\n",
            "Iteration 22, loss = 0.57561382\n",
            "Iteration 23, loss = 0.57570824\n",
            "Iteration 24, loss = 0.57425633\n",
            "Iteration 25, loss = 0.57439433\n",
            "Iteration 26, loss = 0.57125962\n",
            "Iteration 27, loss = 0.57151190\n",
            "Iteration 28, loss = 0.59797602\n",
            "Iteration 29, loss = 0.60063551\n",
            "Iteration 30, loss = 0.58719989\n",
            "Iteration 31, loss = 0.57894367\n",
            "Iteration 32, loss = 0.57504020\n",
            "Iteration 33, loss = 0.57210800\n",
            "Iteration 34, loss = 0.57354457\n",
            "Iteration 35, loss = 0.57162718\n",
            "Iteration 36, loss = 0.57039927\n",
            "Iteration 37, loss = 0.57218658\n",
            "Iteration 38, loss = 0.59907947\n",
            "Iteration 39, loss = 0.60035029\n",
            "Iteration 40, loss = 0.58714631\n",
            "Iteration 41, loss = 0.57880858\n",
            "Iteration 42, loss = 0.57537637\n",
            "Iteration 43, loss = 0.57447359\n",
            "Iteration 44, loss = 0.57192012\n",
            "Iteration 45, loss = 0.57139717\n",
            "Iteration 46, loss = 0.57213635\n",
            "Iteration 47, loss = 0.57173685\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.52881389\n",
            "Iteration 49, loss = 0.51254613\n",
            "Iteration 50, loss = 0.51947080\n",
            "Iteration 51, loss = 0.52861137\n",
            "Iteration 52, loss = 0.52819330\n",
            "Iteration 53, loss = 0.52819548\n",
            "Iteration 54, loss = 0.53107093\n",
            "Iteration 55, loss = 0.52557820\n",
            "Iteration 56, loss = 0.52616023\n",
            "Iteration 57, loss = 0.52439871\n",
            "Iteration 58, loss = 0.52345549\n",
            "Iteration 59, loss = 0.52696549\n",
            "Iteration 60, loss = 0.52397532\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 61, loss = 0.44508795\n",
            "Iteration 62, loss = 0.43294899\n",
            "Iteration 63, loss = 0.42203280\n",
            "Iteration 64, loss = 0.41171992\n",
            "Iteration 65, loss = 0.40215271\n",
            "Iteration 66, loss = 0.39355471\n",
            "Iteration 67, loss = 0.38581037\n",
            "Iteration 68, loss = 0.37860461\n",
            "Iteration 69, loss = 0.37252769\n",
            "Iteration 70, loss = 0.36695327\n",
            "Iteration 71, loss = 0.36191176\n",
            "Iteration 72, loss = 0.35756213\n",
            "Iteration 73, loss = 0.35440283\n",
            "Iteration 74, loss = 0.35070614\n",
            "Iteration 75, loss = 0.34797600\n",
            "Iteration 76, loss = 0.34704429\n",
            "Iteration 77, loss = 0.34405819\n",
            "Iteration 78, loss = 0.34230331\n",
            "Iteration 79, loss = 0.34212334\n",
            "Iteration 80, loss = 0.34147657\n",
            "Iteration 81, loss = 0.34107135\n",
            "Iteration 82, loss = 0.33930132\n",
            "Iteration 83, loss = 0.33825502\n",
            "Iteration 84, loss = 0.33992369\n",
            "Iteration 85, loss = 0.33987401\n",
            "Iteration 86, loss = 0.34069472\n",
            "Iteration 87, loss = 0.33773942\n",
            "Iteration 88, loss = 0.33933528\n",
            "Iteration 89, loss = 0.34312594\n",
            "Iteration 90, loss = 0.33640467\n",
            "Iteration 91, loss = 0.33988441\n",
            "Iteration 92, loss = 0.33689076\n",
            "Iteration 93, loss = 0.33907262\n",
            "Iteration 94, loss = 0.33557286\n",
            "Iteration 95, loss = 0.33705645\n",
            "Iteration 96, loss = 0.34205649\n",
            "Iteration 97, loss = 0.34263665\n",
            "Iteration 98, loss = 0.33574093\n",
            "Iteration 99, loss = 0.34083365\n",
            "Iteration 100, loss = 0.33618411\n",
            "Iteration 101, loss = 0.33851058\n",
            "Iteration 102, loss = 0.33519779\n",
            "Iteration 103, loss = 0.33367576\n",
            "Iteration 104, loss = 0.33714426\n",
            "Iteration 105, loss = 0.33540098\n",
            "Iteration 106, loss = 0.33857226\n",
            "Iteration 107, loss = 0.33273692\n",
            "Iteration 108, loss = 0.33579924\n",
            "Iteration 109, loss = 0.33069426\n",
            "Iteration 110, loss = 0.33610710\n",
            "Iteration 111, loss = 0.33693458\n",
            "Iteration 112, loss = 0.32729007\n",
            "Iteration 113, loss = 0.33058738\n",
            "Iteration 114, loss = 0.32858256\n",
            "Iteration 115, loss = 0.33367888\n",
            "Iteration 116, loss = 0.33235222\n",
            "Iteration 117, loss = 0.33119792\n",
            "Iteration 118, loss = 0.32268779\n",
            "Iteration 119, loss = 0.33038131\n",
            "Iteration 120, loss = 0.33067754\n",
            "Iteration 121, loss = 0.32899659\n",
            "Iteration 122, loss = 0.32750004\n",
            "Iteration 123, loss = 0.33154543\n",
            "Iteration 124, loss = 0.32489241\n",
            "Iteration 125, loss = 0.33085065\n",
            "Iteration 126, loss = 0.32206153\n",
            "Iteration 127, loss = 0.32416685\n",
            "Iteration 128, loss = 0.32605117\n",
            "Iteration 129, loss = 0.32935469\n",
            "Iteration 130, loss = 0.32397607\n",
            "Iteration 131, loss = 0.32598380\n",
            "Iteration 132, loss = 0.32817354\n",
            "Iteration 133, loss = 0.31924465\n",
            "Iteration 134, loss = 0.32078937\n",
            "Iteration 135, loss = 0.32098448\n",
            "Iteration 136, loss = 0.31678663\n",
            "Iteration 137, loss = 0.31603566\n",
            "Iteration 138, loss = 0.31840929\n",
            "Iteration 139, loss = 0.32622280\n",
            "Iteration 140, loss = 0.32086686\n",
            "Iteration 141, loss = 0.32027055\n",
            "Iteration 142, loss = 0.31857470\n",
            "Iteration 143, loss = 0.31795098\n",
            "Iteration 144, loss = 0.31953237\n",
            "Iteration 145, loss = 0.31466209\n",
            "Iteration 146, loss = 0.31633313\n",
            "Iteration 147, loss = 0.32662621\n",
            "Iteration 148, loss = 0.31742389\n",
            "Iteration 149, loss = 0.31375171\n",
            "Iteration 150, loss = 0.32216513\n",
            "Iteration 151, loss = 0.31785399\n",
            "Iteration 152, loss = 0.31977197\n",
            "Iteration 153, loss = 0.31434088\n",
            "Iteration 154, loss = 0.31576160\n",
            "Iteration 155, loss = 0.31887703\n",
            "Iteration 156, loss = 0.31542603\n",
            "Iteration 157, loss = 0.31130908\n",
            "Iteration 158, loss = 0.32105039\n",
            "Iteration 159, loss = 0.31746025\n",
            "Iteration 160, loss = 0.30960475\n",
            "Iteration 161, loss = 0.32253186\n",
            "Iteration 162, loss = 0.31137537\n",
            "Iteration 163, loss = 0.30671961\n",
            "Iteration 164, loss = 0.30756877\n",
            "Iteration 165, loss = 0.30809718\n",
            "Iteration 166, loss = 0.31475585\n",
            "Iteration 167, loss = 0.31145094\n",
            "Iteration 168, loss = 0.31652163\n",
            "Iteration 169, loss = 0.30674547\n",
            "Iteration 170, loss = 0.30844608\n",
            "Iteration 171, loss = 0.31199120\n",
            "Iteration 172, loss = 0.31288648\n",
            "Iteration 173, loss = 0.30849780\n",
            "Iteration 174, loss = 0.32154610\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 175, loss = 0.27183190\n",
            "Iteration 176, loss = 0.27146941\n",
            "Iteration 177, loss = 0.27133060\n",
            "Iteration 178, loss = 0.27123251\n",
            "Iteration 179, loss = 0.27108832\n",
            "Iteration 180, loss = 0.27100768\n",
            "Iteration 181, loss = 0.27104637\n",
            "Iteration 182, loss = 0.27085186\n",
            "Iteration 183, loss = 0.27086460\n",
            "Iteration 184, loss = 0.27082467\n",
            "Iteration 185, loss = 0.27075730\n",
            "Iteration 186, loss = 0.27073728\n",
            "Iteration 187, loss = 0.27058032\n",
            "Iteration 188, loss = 0.27057316\n",
            "Iteration 189, loss = 0.27047058\n",
            "Iteration 190, loss = 0.27041036\n",
            "Iteration 191, loss = 0.27036043\n",
            "Iteration 192, loss = 0.27027028\n",
            "Iteration 193, loss = 0.27024278\n",
            "Iteration 194, loss = 0.27021437\n",
            "Iteration 195, loss = 0.27009883\n",
            "Iteration 196, loss = 0.27006245\n",
            "Iteration 197, loss = 0.27009112\n",
            "Iteration 198, loss = 0.26995034\n",
            "Iteration 199, loss = 0.26992904\n",
            "Iteration 200, loss = 0.26983828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.83072577\n",
            "Iteration 2, loss = 0.63392611\n",
            "Iteration 3, loss = 0.60797999\n",
            "Iteration 4, loss = 0.59419431\n",
            "Iteration 5, loss = 0.58535079\n",
            "Iteration 6, loss = 0.57960905\n",
            "Iteration 7, loss = 0.57440599\n",
            "Iteration 8, loss = 0.57850854\n",
            "Iteration 9, loss = 0.57802650\n",
            "Iteration 10, loss = 0.57609867\n",
            "Iteration 11, loss = 0.57482908\n",
            "Iteration 12, loss = 0.57374190\n",
            "Iteration 13, loss = 0.57438040\n",
            "Iteration 14, loss = 0.57412444\n",
            "Iteration 15, loss = 0.57586323\n",
            "Iteration 16, loss = 0.57493861\n",
            "Iteration 17, loss = 0.57434883\n",
            "Iteration 18, loss = 0.57738212\n",
            "Iteration 19, loss = 0.57378276\n",
            "Iteration 20, loss = 0.57488093\n",
            "Iteration 21, loss = 0.57571318\n",
            "Iteration 22, loss = 0.57682992\n",
            "Iteration 23, loss = 0.57368778\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.52110192\n",
            "Iteration 25, loss = 0.50378547\n",
            "Iteration 26, loss = 0.52229882\n",
            "Iteration 27, loss = 0.52702431\n",
            "Iteration 28, loss = 0.52735438\n",
            "Iteration 29, loss = 0.52470837\n",
            "Iteration 30, loss = 0.52689136\n",
            "Iteration 31, loss = 0.52465421\n",
            "Iteration 32, loss = 0.52327858\n",
            "Iteration 33, loss = 0.52664824\n",
            "Iteration 34, loss = 0.52591212\n",
            "Iteration 35, loss = 0.52265324\n",
            "Iteration 36, loss = 0.52151927\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.43655800\n",
            "Iteration 38, loss = 0.42346697\n",
            "Iteration 39, loss = 0.41167666\n",
            "Iteration 40, loss = 0.40088967\n",
            "Iteration 41, loss = 0.39113463\n",
            "Iteration 42, loss = 0.38234008\n",
            "Iteration 43, loss = 0.37403375\n",
            "Iteration 44, loss = 0.36729397\n",
            "Iteration 45, loss = 0.36119108\n",
            "Iteration 46, loss = 0.35557679\n",
            "Iteration 47, loss = 0.35102668\n",
            "Iteration 48, loss = 0.34670317\n",
            "Iteration 49, loss = 0.34422367\n",
            "Iteration 50, loss = 0.33954158\n",
            "Iteration 51, loss = 0.33844447\n",
            "Iteration 52, loss = 0.33573239\n",
            "Iteration 53, loss = 0.33378854\n",
            "Iteration 54, loss = 0.33182364\n",
            "Iteration 55, loss = 0.32981953\n",
            "Iteration 56, loss = 0.33036117\n",
            "Iteration 57, loss = 0.32720439\n",
            "Iteration 58, loss = 0.32983652\n",
            "Iteration 59, loss = 0.32972239\n",
            "Iteration 60, loss = 0.32801974\n",
            "Iteration 61, loss = 0.32677702\n",
            "Iteration 62, loss = 0.32873043\n",
            "Iteration 63, loss = 0.32801882\n",
            "Iteration 64, loss = 0.32747351\n",
            "Iteration 65, loss = 0.32754550\n",
            "Iteration 66, loss = 0.32783065\n",
            "Iteration 67, loss = 0.33125659\n",
            "Iteration 68, loss = 0.32840865\n",
            "Iteration 69, loss = 0.33242657\n",
            "Iteration 70, loss = 0.33083741\n",
            "Iteration 71, loss = 0.32678710\n",
            "Iteration 72, loss = 0.33128051\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 73, loss = 0.29556434\n",
            "Iteration 74, loss = 0.29476927\n",
            "Iteration 75, loss = 0.29447839\n",
            "Iteration 76, loss = 0.29419229\n",
            "Iteration 77, loss = 0.29403937\n",
            "Iteration 78, loss = 0.29376428\n",
            "Iteration 79, loss = 0.29358005\n",
            "Iteration 80, loss = 0.29333665\n",
            "Iteration 81, loss = 0.29318980\n",
            "Iteration 82, loss = 0.29296315\n",
            "Iteration 83, loss = 0.29263647\n",
            "Iteration 84, loss = 0.29249285\n",
            "Iteration 85, loss = 0.29239047\n",
            "Iteration 86, loss = 0.29217671\n",
            "Iteration 87, loss = 0.29194968\n",
            "Iteration 88, loss = 0.29175360\n",
            "Iteration 89, loss = 0.29156598\n",
            "Iteration 90, loss = 0.29140617\n",
            "Iteration 91, loss = 0.29125004\n",
            "Iteration 92, loss = 0.29097354\n",
            "Iteration 93, loss = 0.29080371\n",
            "Iteration 94, loss = 0.29064569\n",
            "Iteration 95, loss = 0.29046030\n",
            "Iteration 96, loss = 0.29030118\n",
            "Iteration 97, loss = 0.29000718\n",
            "Iteration 98, loss = 0.28991700\n",
            "Iteration 99, loss = 0.28970938\n",
            "Iteration 100, loss = 0.28957671\n",
            "Iteration 101, loss = 0.28941067\n",
            "Iteration 102, loss = 0.28923248\n",
            "Iteration 103, loss = 0.28908618\n",
            "Iteration 104, loss = 0.28888654\n",
            "Iteration 105, loss = 0.28874028\n",
            "Iteration 106, loss = 0.28859709\n",
            "Iteration 107, loss = 0.28839211\n",
            "Iteration 108, loss = 0.28829434\n",
            "Iteration 109, loss = 0.28801452\n",
            "Iteration 110, loss = 0.28799987\n",
            "Iteration 111, loss = 0.28776422\n",
            "Iteration 112, loss = 0.28761398\n",
            "Iteration 113, loss = 0.28749694\n",
            "Iteration 114, loss = 0.28728841\n",
            "Iteration 115, loss = 0.28721753\n",
            "Iteration 116, loss = 0.28701137\n",
            "Iteration 117, loss = 0.28685256\n",
            "Iteration 118, loss = 0.28667521\n",
            "Iteration 119, loss = 0.28657283\n",
            "Iteration 120, loss = 0.28640602\n",
            "Iteration 121, loss = 0.28627644\n",
            "Iteration 122, loss = 0.28612574\n",
            "Iteration 123, loss = 0.28595024\n",
            "Iteration 124, loss = 0.28586882\n",
            "Iteration 125, loss = 0.28566095\n",
            "Iteration 126, loss = 0.28554802\n",
            "Iteration 127, loss = 0.28537100\n",
            "Iteration 128, loss = 0.28522461\n",
            "Iteration 129, loss = 0.28510053\n",
            "Iteration 130, loss = 0.28501275\n",
            "Iteration 131, loss = 0.28494618\n",
            "Iteration 132, loss = 0.28474976\n",
            "Iteration 133, loss = 0.28465331\n",
            "Iteration 134, loss = 0.28442923\n",
            "Iteration 135, loss = 0.28431790\n",
            "Iteration 136, loss = 0.28418486\n",
            "Iteration 137, loss = 0.28411699\n",
            "Iteration 138, loss = 0.28393844\n",
            "Iteration 139, loss = 0.28384664\n",
            "Iteration 140, loss = 0.28371315\n",
            "Iteration 141, loss = 0.28358704\n",
            "Iteration 142, loss = 0.28351894\n",
            "Iteration 143, loss = 0.28328412\n",
            "Iteration 144, loss = 0.28317922\n",
            "Iteration 145, loss = 0.28307261\n",
            "Iteration 146, loss = 0.28299616\n",
            "Iteration 147, loss = 0.28285077\n",
            "Iteration 148, loss = 0.28270105\n",
            "Iteration 149, loss = 0.28256398\n",
            "Iteration 150, loss = 0.28249266\n",
            "Iteration 151, loss = 0.28236193\n",
            "Iteration 152, loss = 0.28221925\n",
            "Iteration 153, loss = 0.28211089\n",
            "Iteration 154, loss = 0.28204271\n",
            "Iteration 155, loss = 0.28191754\n",
            "Iteration 156, loss = 0.28178250\n",
            "Iteration 157, loss = 0.28167141\n",
            "Iteration 158, loss = 0.28155112\n",
            "Iteration 159, loss = 0.28143483\n",
            "Iteration 160, loss = 0.28136076\n",
            "Iteration 161, loss = 0.28127746\n",
            "Iteration 162, loss = 0.28111683\n",
            "Iteration 163, loss = 0.28104077\n",
            "Iteration 164, loss = 0.28087749\n",
            "Iteration 165, loss = 0.28077319\n",
            "Iteration 166, loss = 0.28068895\n",
            "Iteration 167, loss = 0.28063773\n",
            "Iteration 168, loss = 0.28054964\n",
            "Iteration 169, loss = 0.28044116\n",
            "Iteration 170, loss = 0.28025736\n",
            "Iteration 171, loss = 0.28015037\n",
            "Iteration 172, loss = 0.28006022\n",
            "Iteration 173, loss = 0.28000298\n",
            "Iteration 174, loss = 0.27982057\n",
            "Iteration 175, loss = 0.27982585\n",
            "Iteration 176, loss = 0.27971781\n",
            "Iteration 177, loss = 0.27964127\n",
            "Iteration 178, loss = 0.27944944\n",
            "Iteration 179, loss = 0.27941226\n",
            "Iteration 180, loss = 0.27925904\n",
            "Iteration 181, loss = 0.27925110\n",
            "Iteration 182, loss = 0.27911883\n",
            "Iteration 183, loss = 0.27900346\n",
            "Iteration 184, loss = 0.27893095\n",
            "Iteration 185, loss = 0.27890041\n",
            "Iteration 186, loss = 0.27875092\n",
            "Iteration 187, loss = 0.27866610\n",
            "Iteration 188, loss = 0.27851188\n",
            "Iteration 189, loss = 0.27840270\n",
            "Iteration 190, loss = 0.27833001\n",
            "Iteration 191, loss = 0.27822715\n",
            "Iteration 192, loss = 0.27819487\n",
            "Iteration 193, loss = 0.27808653\n",
            "Iteration 194, loss = 0.27801127\n",
            "Iteration 195, loss = 0.27790603\n",
            "Iteration 196, loss = 0.27783329\n",
            "Iteration 197, loss = 0.27775813\n",
            "Iteration 198, loss = 0.27762938\n",
            "Iteration 199, loss = 0.27756582\n",
            "Iteration 200, loss = 0.27743867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64758633\n",
            "Iteration 3, loss = 0.61799692\n",
            "Iteration 4, loss = 0.60040043\n",
            "Iteration 5, loss = 0.59797005\n",
            "Iteration 6, loss = 0.60282700\n",
            "Iteration 7, loss = 0.58747734\n",
            "Iteration 8, loss = 0.57967032\n",
            "Iteration 9, loss = 0.57723205\n",
            "Iteration 10, loss = 0.57580547\n",
            "Iteration 11, loss = 0.57376591\n",
            "Iteration 12, loss = 0.57315806\n",
            "Iteration 13, loss = 0.57202810\n",
            "Iteration 14, loss = 0.57400372\n",
            "Iteration 15, loss = 0.57421889\n",
            "Iteration 16, loss = 0.57315229\n",
            "Iteration 17, loss = 0.57403268\n",
            "Iteration 18, loss = 0.57254395\n",
            "Iteration 19, loss = 0.57313959\n",
            "Iteration 20, loss = 0.57355484\n",
            "Iteration 21, loss = 0.57489905\n",
            "Iteration 22, loss = 0.57403027\n",
            "Iteration 23, loss = 0.57365650\n",
            "Iteration 24, loss = 0.57511629\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.52376894\n",
            "Iteration 26, loss = 0.49658873\n",
            "Iteration 27, loss = 0.51828443\n",
            "Iteration 28, loss = 0.52182394\n",
            "Iteration 29, loss = 0.52509277\n",
            "Iteration 30, loss = 0.52596729\n",
            "Iteration 31, loss = 0.51908705\n",
            "Iteration 32, loss = 0.52475347\n",
            "Iteration 33, loss = 0.52552689\n",
            "Iteration 34, loss = 0.52208741\n",
            "Iteration 35, loss = 0.52306030\n",
            "Iteration 36, loss = 0.52095315\n",
            "Iteration 37, loss = 0.52251426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.42329409\n",
            "Iteration 39, loss = 0.40827597\n",
            "Iteration 40, loss = 0.39483300\n",
            "Iteration 41, loss = 0.38266706\n",
            "Iteration 42, loss = 0.37161145\n",
            "Iteration 43, loss = 0.36165481\n",
            "Iteration 44, loss = 0.35267800\n",
            "Iteration 45, loss = 0.34429213\n",
            "Iteration 46, loss = 0.33710461\n",
            "Iteration 47, loss = 0.33019548\n",
            "Iteration 48, loss = 0.32459437\n",
            "Iteration 49, loss = 0.31906343\n",
            "Iteration 50, loss = 0.31423441\n",
            "Iteration 51, loss = 0.30973506\n",
            "Iteration 52, loss = 0.30577815\n",
            "Iteration 53, loss = 0.30176444\n",
            "Iteration 54, loss = 0.29904766\n",
            "Iteration 55, loss = 0.29537146\n",
            "Iteration 56, loss = 0.29290779\n",
            "Iteration 57, loss = 0.29006074\n",
            "Iteration 58, loss = 0.28882477\n",
            "Iteration 59, loss = 0.28602894\n",
            "Iteration 60, loss = 0.28450154\n",
            "Iteration 61, loss = 0.28260913\n",
            "Iteration 62, loss = 0.28155013\n",
            "Iteration 63, loss = 0.28014863\n",
            "Iteration 64, loss = 0.27817283\n",
            "Iteration 65, loss = 0.27832804\n",
            "Iteration 66, loss = 0.27571967\n",
            "Iteration 67, loss = 0.27622869\n",
            "Iteration 68, loss = 0.27467938\n",
            "Iteration 69, loss = 0.27716760\n",
            "Iteration 70, loss = 0.27571881\n",
            "Iteration 71, loss = 0.27355368\n",
            "Iteration 72, loss = 0.27211181\n",
            "Iteration 73, loss = 0.27646288\n",
            "Iteration 74, loss = 0.27445922\n",
            "Iteration 75, loss = 0.27121249\n",
            "Iteration 76, loss = 0.27219930\n",
            "Iteration 77, loss = 0.27548439\n",
            "Iteration 78, loss = 0.27330319\n",
            "Iteration 79, loss = 0.27538176\n",
            "Iteration 80, loss = 0.27457521\n",
            "Iteration 81, loss = 0.27415866\n",
            "Iteration 82, loss = 0.27436108\n",
            "Iteration 83, loss = 0.27764385\n",
            "Iteration 84, loss = 0.27330334\n",
            "Iteration 85, loss = 0.28088425\n",
            "Iteration 86, loss = 0.27970547\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 87, loss = 0.25766595\n",
            "Iteration 88, loss = 0.25720864\n",
            "Iteration 89, loss = 0.25718559\n",
            "Iteration 90, loss = 0.25707029\n",
            "Iteration 91, loss = 0.25696881\n",
            "Iteration 92, loss = 0.25694370\n",
            "Iteration 93, loss = 0.25687272\n",
            "Iteration 94, loss = 0.25676369\n",
            "Iteration 95, loss = 0.25676389\n",
            "Iteration 96, loss = 0.25665280\n",
            "Iteration 97, loss = 0.25652945\n",
            "Iteration 98, loss = 0.25653477\n",
            "Iteration 99, loss = 0.25641396\n",
            "Iteration 100, loss = 0.25647792\n",
            "Iteration 101, loss = 0.25630756\n",
            "Iteration 102, loss = 0.25621771\n",
            "Iteration 103, loss = 0.25621158\n",
            "Iteration 104, loss = 0.25617548\n",
            "Iteration 105, loss = 0.25614432\n",
            "Iteration 106, loss = 0.25607883\n",
            "Iteration 107, loss = 0.25597964\n",
            "Iteration 108, loss = 0.25592196\n",
            "Iteration 109, loss = 0.25585028\n",
            "Iteration 110, loss = 0.25586706\n",
            "Iteration 111, loss = 0.25582198\n",
            "Iteration 112, loss = 0.25572272\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.25540767\n",
            "Iteration 114, loss = 0.25539533\n",
            "Iteration 115, loss = 0.25538670\n",
            "Iteration 116, loss = 0.25534941\n",
            "Iteration 117, loss = 0.25535300\n",
            "Iteration 118, loss = 0.25534116\n",
            "Iteration 119, loss = 0.25535414\n",
            "Iteration 120, loss = 0.25533483\n",
            "Iteration 121, loss = 0.25529972\n",
            "Iteration 122, loss = 0.25528563\n",
            "Iteration 123, loss = 0.25527389\n",
            "Iteration 124, loss = 0.25529007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 125, loss = 0.25518687\n",
            "Iteration 126, loss = 0.25517444\n",
            "Iteration 127, loss = 0.25517169\n",
            "Iteration 128, loss = 0.25517253\n",
            "Iteration 129, loss = 0.25517040\n",
            "Iteration 130, loss = 0.25516992\n",
            "Iteration 131, loss = 0.25516629\n",
            "Iteration 132, loss = 0.25516957\n",
            "Iteration 133, loss = 0.25516521\n",
            "Iteration 134, loss = 0.25516934\n",
            "Iteration 135, loss = 0.25516397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63174209\n",
            "Iteration 3, loss = 0.60814376\n",
            "Iteration 4, loss = 0.59269107\n",
            "Iteration 5, loss = 0.58303246\n",
            "Iteration 6, loss = 0.57876553\n",
            "Iteration 7, loss = 0.57434586\n",
            "Iteration 8, loss = 0.57296944\n",
            "Iteration 9, loss = 0.57086549\n",
            "Iteration 10, loss = 0.57055852\n",
            "Iteration 11, loss = 0.57042580\n",
            "Iteration 12, loss = 0.57009910\n",
            "Iteration 13, loss = 0.56967661\n",
            "Iteration 14, loss = 0.57087435\n",
            "Iteration 15, loss = 0.56986465\n",
            "Iteration 16, loss = 0.57190304\n",
            "Iteration 17, loss = 0.57149868\n",
            "Iteration 18, loss = 0.57152632\n",
            "Iteration 19, loss = 0.57160536\n",
            "Iteration 20, loss = 0.57230494\n",
            "Iteration 21, loss = 0.57106155\n",
            "Iteration 22, loss = 0.57671916\n",
            "Iteration 23, loss = 0.58115893\n",
            "Iteration 24, loss = 0.57624911\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.52758304\n",
            "Iteration 26, loss = 0.50739476\n",
            "Iteration 27, loss = 0.50572710\n",
            "Iteration 28, loss = 0.53641027\n",
            "Iteration 29, loss = 0.52581592\n",
            "Iteration 30, loss = 0.52516183\n",
            "Iteration 31, loss = 0.51959494\n",
            "Iteration 32, loss = 0.52403197\n",
            "Iteration 33, loss = 0.51979780\n",
            "Iteration 34, loss = 0.51705844\n",
            "Iteration 35, loss = 0.51707174\n",
            "Iteration 36, loss = 0.52222447\n",
            "Iteration 37, loss = 0.51658919\n",
            "Iteration 38, loss = 0.51967488\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.42134123\n",
            "Iteration 40, loss = 0.40713649\n",
            "Iteration 41, loss = 0.39435227\n",
            "Iteration 42, loss = 0.38228904\n",
            "Iteration 43, loss = 0.37113075\n",
            "Iteration 44, loss = 0.36105621\n",
            "Iteration 45, loss = 0.35223823\n",
            "Iteration 46, loss = 0.34394342\n",
            "Iteration 47, loss = 0.33650461\n",
            "Iteration 48, loss = 0.32974525\n",
            "Iteration 49, loss = 0.32370019\n",
            "Iteration 50, loss = 0.31800898\n",
            "Iteration 51, loss = 0.31296861\n",
            "Iteration 52, loss = 0.30839815\n",
            "Iteration 53, loss = 0.30374863\n",
            "Iteration 54, loss = 0.30014408\n",
            "Iteration 55, loss = 0.29751455\n",
            "Iteration 56, loss = 0.29387345\n",
            "Iteration 57, loss = 0.29056839\n",
            "Iteration 58, loss = 0.28819078\n",
            "Iteration 59, loss = 0.28686901\n",
            "Iteration 60, loss = 0.28335330\n",
            "Iteration 61, loss = 0.28214060\n",
            "Iteration 62, loss = 0.28054817\n",
            "Iteration 63, loss = 0.27759822\n",
            "Iteration 64, loss = 0.27828270\n",
            "Iteration 65, loss = 0.27611844\n",
            "Iteration 66, loss = 0.27358996\n",
            "Iteration 67, loss = 0.27254025\n",
            "Iteration 68, loss = 0.27308845\n",
            "Iteration 69, loss = 0.27089147\n",
            "Iteration 70, loss = 0.27019242\n",
            "Iteration 71, loss = 0.27086916\n",
            "Iteration 72, loss = 0.27179900\n",
            "Iteration 73, loss = 0.27074023\n",
            "Iteration 74, loss = 0.27135804\n",
            "Iteration 75, loss = 0.26946835\n",
            "Iteration 76, loss = 0.26756302\n",
            "Iteration 77, loss = 0.26963129\n",
            "Iteration 78, loss = 0.27018751\n",
            "Iteration 79, loss = 0.27141163\n",
            "Iteration 80, loss = 0.27066587\n",
            "Iteration 81, loss = 0.26877475\n",
            "Iteration 82, loss = 0.26932424\n",
            "Iteration 83, loss = 0.27054388\n",
            "Iteration 84, loss = 0.27318709\n",
            "Iteration 85, loss = 0.27367128\n",
            "Iteration 86, loss = 0.27539174\n",
            "Iteration 87, loss = 0.28018585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 88, loss = 0.25368735\n",
            "Iteration 89, loss = 0.25358311\n",
            "Iteration 90, loss = 0.25347644\n",
            "Iteration 91, loss = 0.25337967\n",
            "Iteration 92, loss = 0.25332871\n",
            "Iteration 93, loss = 0.25324737\n",
            "Iteration 94, loss = 0.25319582\n",
            "Iteration 95, loss = 0.25312300\n",
            "Iteration 96, loss = 0.25306461\n",
            "Iteration 97, loss = 0.25299313\n",
            "Iteration 98, loss = 0.25291148\n",
            "Iteration 99, loss = 0.25285763\n",
            "Iteration 100, loss = 0.25286874\n",
            "Iteration 101, loss = 0.25272629\n",
            "Iteration 102, loss = 0.25269001\n",
            "Iteration 103, loss = 0.25263325\n",
            "Iteration 104, loss = 0.25255874\n",
            "Iteration 105, loss = 0.25246166\n",
            "Iteration 106, loss = 0.25242404\n",
            "Iteration 107, loss = 0.25229368\n",
            "Iteration 108, loss = 0.25230392\n",
            "Iteration 109, loss = 0.25221313\n",
            "Iteration 110, loss = 0.25215904\n",
            "Iteration 111, loss = 0.25215940\n",
            "Iteration 112, loss = 0.25207255\n",
            "Iteration 113, loss = 0.25202875\n",
            "Iteration 114, loss = 0.25193338\n",
            "Iteration 115, loss = 0.25194123\n",
            "Iteration 116, loss = 0.25181378\n",
            "Iteration 117, loss = 0.25177106\n",
            "Iteration 118, loss = 0.25170533\n",
            "Iteration 119, loss = 0.25173816\n",
            "Iteration 120, loss = 0.25166576\n",
            "Iteration 121, loss = 0.25160544\n",
            "Iteration 122, loss = 0.25154510\n",
            "Iteration 123, loss = 0.25151488\n",
            "Iteration 124, loss = 0.25144610\n",
            "Iteration 125, loss = 0.25136403\n",
            "Iteration 126, loss = 0.25134271\n",
            "Iteration 127, loss = 0.25128545\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 128, loss = 0.25102445\n",
            "Iteration 129, loss = 0.25099219\n",
            "Iteration 130, loss = 0.25097193\n",
            "Iteration 131, loss = 0.25095790\n",
            "Iteration 132, loss = 0.25095162\n",
            "Iteration 133, loss = 0.25095640\n",
            "Iteration 134, loss = 0.25093376\n",
            "Iteration 135, loss = 0.25093378\n",
            "Iteration 136, loss = 0.25092281\n",
            "Iteration 137, loss = 0.25092969\n",
            "Iteration 138, loss = 0.25090135\n",
            "Iteration 139, loss = 0.25089354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 140, loss = 0.25080291\n",
            "Iteration 141, loss = 0.25078862\n",
            "Iteration 142, loss = 0.25078782\n",
            "Iteration 143, loss = 0.25078456\n",
            "Iteration 144, loss = 0.25078374\n",
            "Iteration 145, loss = 0.25079415\n",
            "Iteration 146, loss = 0.25078809\n",
            "Iteration 147, loss = 0.25077714\n",
            "Iteration 148, loss = 0.25077699\n",
            "Iteration 149, loss = 0.25078654\n",
            "Iteration 150, loss = 0.25077007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.91885474\n",
            "Iteration 2, loss = 0.64824275\n",
            "Iteration 3, loss = 0.61869462\n",
            "Iteration 4, loss = 0.60025842\n",
            "Iteration 5, loss = 0.59094847\n",
            "Iteration 6, loss = 0.58385235\n",
            "Iteration 7, loss = 0.57958900\n",
            "Iteration 8, loss = 0.57780495\n",
            "Iteration 9, loss = 0.57544661\n",
            "Iteration 10, loss = 0.57375854\n",
            "Iteration 11, loss = 0.57356463\n",
            "Iteration 12, loss = 0.57415482\n",
            "Iteration 13, loss = 0.57368189\n",
            "Iteration 14, loss = 0.57364917\n",
            "Iteration 15, loss = 0.57324668\n",
            "Iteration 16, loss = 0.57371181\n",
            "Iteration 17, loss = 0.57400596\n",
            "Iteration 18, loss = 0.57307837\n",
            "Iteration 19, loss = 0.57426920\n",
            "Iteration 20, loss = 0.57339261\n",
            "Iteration 21, loss = 0.57436667\n",
            "Iteration 22, loss = 0.57556545\n",
            "Iteration 23, loss = 0.57429033\n",
            "Iteration 24, loss = 0.57409499\n",
            "Iteration 25, loss = 0.57514326\n",
            "Iteration 26, loss = 0.57423895\n",
            "Iteration 27, loss = 0.57453426\n",
            "Iteration 28, loss = 0.57461829\n",
            "Iteration 29, loss = 0.62343066\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.58464023\n",
            "Iteration 31, loss = 0.57634091\n",
            "Iteration 32, loss = 0.56857388\n",
            "Iteration 33, loss = 0.56022367\n",
            "Iteration 34, loss = 0.55362328\n",
            "Iteration 35, loss = 0.54944765\n",
            "Iteration 36, loss = 0.54877940\n",
            "Iteration 37, loss = 0.54944576\n",
            "Iteration 38, loss = 0.55138614\n",
            "Iteration 39, loss = 0.55332859\n",
            "Iteration 40, loss = 0.55149477\n",
            "Iteration 41, loss = 0.54635873\n",
            "Iteration 42, loss = 0.54837251\n",
            "Iteration 43, loss = 0.55327043\n",
            "Iteration 44, loss = 0.55595952\n",
            "Iteration 45, loss = 0.55305291\n",
            "Iteration 46, loss = 0.55824279\n",
            "Iteration 47, loss = 0.55852200\n",
            "Iteration 48, loss = 0.55363523\n",
            "Iteration 49, loss = 0.54925787\n",
            "Iteration 50, loss = 0.54719237\n",
            "Iteration 51, loss = 0.54527387\n",
            "Iteration 52, loss = 0.54161853\n",
            "Iteration 53, loss = 0.54091603\n",
            "Iteration 54, loss = 0.53911507\n",
            "Iteration 55, loss = 0.53477428\n",
            "Iteration 56, loss = 0.53865457\n",
            "Iteration 57, loss = 0.53313519\n",
            "Iteration 58, loss = 0.53458196\n",
            "Iteration 59, loss = 0.53057006\n",
            "Iteration 60, loss = 0.52917869\n",
            "Iteration 61, loss = 0.52989265\n",
            "Iteration 62, loss = 0.53507841\n",
            "Iteration 63, loss = 0.52978821\n",
            "Iteration 64, loss = 0.53131305\n",
            "Iteration 65, loss = 0.53103234\n",
            "Iteration 66, loss = 0.52545739\n",
            "Iteration 67, loss = 0.52858016\n",
            "Iteration 68, loss = 0.52825596\n",
            "Iteration 69, loss = 0.52839193\n",
            "Iteration 70, loss = 0.52691697\n",
            "Iteration 71, loss = 0.52595659\n",
            "Iteration 72, loss = 0.53219032\n",
            "Iteration 73, loss = 0.52831313\n",
            "Iteration 74, loss = 0.52720230\n",
            "Iteration 75, loss = 0.53034554\n",
            "Iteration 76, loss = 0.52593272\n",
            "Iteration 77, loss = 0.52742381\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 78, loss = 0.42629199\n",
            "Iteration 79, loss = 0.41095102\n",
            "Iteration 80, loss = 0.39760789\n",
            "Iteration 81, loss = 0.38563981\n",
            "Iteration 82, loss = 0.37522801\n",
            "Iteration 83, loss = 0.36586623\n",
            "Iteration 84, loss = 0.35793541\n",
            "Iteration 85, loss = 0.35067892\n",
            "Iteration 86, loss = 0.34471357\n",
            "Iteration 87, loss = 0.33981549\n",
            "Iteration 88, loss = 0.33473151\n",
            "Iteration 89, loss = 0.33061745\n",
            "Iteration 90, loss = 0.32744977\n",
            "Iteration 91, loss = 0.32398636\n",
            "Iteration 92, loss = 0.32177533\n",
            "Iteration 93, loss = 0.31889574\n",
            "Iteration 94, loss = 0.31698508\n",
            "Iteration 95, loss = 0.31478217\n",
            "Iteration 96, loss = 0.31326121\n",
            "Iteration 97, loss = 0.31206083\n",
            "Iteration 98, loss = 0.31008040\n",
            "Iteration 99, loss = 0.30914880\n",
            "Iteration 100, loss = 0.30817948\n",
            "Iteration 101, loss = 0.30844555\n",
            "Iteration 102, loss = 0.30720859\n",
            "Iteration 103, loss = 0.30587604\n",
            "Iteration 104, loss = 0.30650146\n",
            "Iteration 105, loss = 0.30679252\n",
            "Iteration 106, loss = 0.30535004\n",
            "Iteration 107, loss = 0.30534925\n",
            "Iteration 108, loss = 0.30601473\n",
            "Iteration 109, loss = 0.30729218\n",
            "Iteration 110, loss = 0.30635047\n",
            "Iteration 111, loss = 0.30469721\n",
            "Iteration 112, loss = 0.30519851\n",
            "Iteration 113, loss = 0.30633355\n",
            "Iteration 114, loss = 0.30593802\n",
            "Iteration 115, loss = 0.30569629\n",
            "Iteration 116, loss = 0.30980732\n",
            "Iteration 117, loss = 0.30577685\n",
            "Iteration 118, loss = 0.31173623\n",
            "Iteration 119, loss = 0.30887049\n",
            "Iteration 120, loss = 0.30725770\n",
            "Iteration 121, loss = 0.30846862\n",
            "Iteration 122, loss = 0.31161861\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 123, loss = 0.28274135\n",
            "Iteration 124, loss = 0.28235132\n",
            "Iteration 125, loss = 0.28217904\n",
            "Iteration 126, loss = 0.28206984\n",
            "Iteration 127, loss = 0.28193992\n",
            "Iteration 128, loss = 0.28180151\n",
            "Iteration 129, loss = 0.28165396\n",
            "Iteration 130, loss = 0.28157108\n",
            "Iteration 131, loss = 0.28144056\n",
            "Iteration 132, loss = 0.28134058\n",
            "Iteration 133, loss = 0.28121011\n",
            "Iteration 134, loss = 0.28109668\n",
            "Iteration 135, loss = 0.28100946\n",
            "Iteration 136, loss = 0.28092832\n",
            "Iteration 137, loss = 0.28075968\n",
            "Iteration 138, loss = 0.28073677\n",
            "Iteration 139, loss = 0.28058605\n",
            "Iteration 140, loss = 0.28044293\n",
            "Iteration 141, loss = 0.28037110\n",
            "Iteration 142, loss = 0.28025005\n",
            "Iteration 143, loss = 0.28013598\n",
            "Iteration 144, loss = 0.28005168\n",
            "Iteration 145, loss = 0.27993269\n",
            "Iteration 146, loss = 0.27979429\n",
            "Iteration 147, loss = 0.27976756\n",
            "Iteration 148, loss = 0.27960565\n",
            "Iteration 149, loss = 0.27951365\n",
            "Iteration 150, loss = 0.27944502\n",
            "Iteration 151, loss = 0.27931060\n",
            "Iteration 152, loss = 0.27921466\n",
            "Iteration 153, loss = 0.27921441\n",
            "Iteration 154, loss = 0.27907767\n",
            "Iteration 155, loss = 0.27892513\n",
            "Iteration 156, loss = 0.27885139\n",
            "Iteration 157, loss = 0.27880795\n",
            "Iteration 158, loss = 0.27864051\n",
            "Iteration 159, loss = 0.27854836\n",
            "Iteration 160, loss = 0.27853113\n",
            "Iteration 161, loss = 0.27839571\n",
            "Iteration 162, loss = 0.27830617\n",
            "Iteration 163, loss = 0.27817707\n",
            "Iteration 164, loss = 0.27812530\n",
            "Iteration 165, loss = 0.27802167\n",
            "Iteration 166, loss = 0.27790913\n",
            "Iteration 167, loss = 0.27782345\n",
            "Iteration 168, loss = 0.27775619\n",
            "Iteration 169, loss = 0.27773550\n",
            "Iteration 170, loss = 0.27756206\n",
            "Iteration 171, loss = 0.27755504\n",
            "Iteration 172, loss = 0.27745715\n",
            "Iteration 173, loss = 0.27730017\n",
            "Iteration 174, loss = 0.27726716\n",
            "Iteration 175, loss = 0.27717079\n",
            "Iteration 176, loss = 0.27705527\n",
            "Iteration 177, loss = 0.27699007\n",
            "Iteration 178, loss = 0.27690059\n",
            "Iteration 179, loss = 0.27683904\n",
            "Iteration 180, loss = 0.27677828\n",
            "Iteration 181, loss = 0.27671023\n",
            "Iteration 182, loss = 0.27654883\n",
            "Iteration 183, loss = 0.27654545\n",
            "Iteration 184, loss = 0.27650365\n",
            "Iteration 185, loss = 0.27632085\n",
            "Iteration 186, loss = 0.27627024\n",
            "Iteration 187, loss = 0.27620558\n",
            "Iteration 188, loss = 0.27611624\n",
            "Iteration 189, loss = 0.27605230\n",
            "Iteration 190, loss = 0.27595540\n",
            "Iteration 191, loss = 0.27586306\n",
            "Iteration 192, loss = 0.27581290\n",
            "Iteration 193, loss = 0.27573777\n",
            "Iteration 194, loss = 0.27560707\n",
            "Iteration 195, loss = 0.27557617\n",
            "Iteration 196, loss = 0.27554259\n",
            "Iteration 197, loss = 0.27544111\n",
            "Iteration 198, loss = 0.27539284\n",
            "Iteration 199, loss = 0.27528386\n",
            "Iteration 200, loss = 0.27523665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63205707\n",
            "Iteration 3, loss = 0.60730993\n",
            "Iteration 4, loss = 0.59339954\n",
            "Iteration 5, loss = 0.58488065\n",
            "Iteration 6, loss = 0.57782301\n",
            "Iteration 7, loss = 0.57466884\n",
            "Iteration 8, loss = 0.57291869\n",
            "Iteration 9, loss = 0.57003047\n",
            "Iteration 10, loss = 0.56981636\n",
            "Iteration 11, loss = 0.57105685\n",
            "Iteration 12, loss = 0.56902681\n",
            "Iteration 13, loss = 0.57094070\n",
            "Iteration 14, loss = 0.57023481\n",
            "Iteration 15, loss = 0.56964250\n",
            "Iteration 16, loss = 0.56953107\n",
            "Iteration 17, loss = 0.57027237\n",
            "Iteration 18, loss = 0.57072624\n",
            "Iteration 19, loss = 0.56981411\n",
            "Iteration 20, loss = 0.56970856\n",
            "Iteration 21, loss = 0.56924761\n",
            "Iteration 22, loss = 0.56947847\n",
            "Iteration 23, loss = 0.57050321\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.51172951\n",
            "Iteration 25, loss = 0.48755303\n",
            "Iteration 26, loss = 0.50780786\n",
            "Iteration 27, loss = 0.52835628\n",
            "Iteration 28, loss = 0.51927187\n",
            "Iteration 29, loss = 0.51700446\n",
            "Iteration 30, loss = 0.52093816\n",
            "Iteration 31, loss = 0.51619526\n",
            "Iteration 32, loss = 0.51576509\n",
            "Iteration 33, loss = 0.51852308\n",
            "Iteration 34, loss = 0.51195029\n",
            "Iteration 35, loss = 0.51812239\n",
            "Iteration 36, loss = 0.52007599\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.41665021\n",
            "Iteration 38, loss = 0.40109105\n",
            "Iteration 39, loss = 0.38759066\n",
            "Iteration 40, loss = 0.37514796\n",
            "Iteration 41, loss = 0.36418475\n",
            "Iteration 42, loss = 0.35411225\n",
            "Iteration 43, loss = 0.34495110\n",
            "Iteration 44, loss = 0.33677617\n",
            "Iteration 45, loss = 0.32969356\n",
            "Iteration 46, loss = 0.32316698\n",
            "Iteration 47, loss = 0.31695401\n",
            "Iteration 48, loss = 0.31158896\n",
            "Iteration 49, loss = 0.30724903\n",
            "Iteration 50, loss = 0.30225676\n",
            "Iteration 51, loss = 0.29866750\n",
            "Iteration 52, loss = 0.29481259\n",
            "Iteration 53, loss = 0.29160990\n",
            "Iteration 54, loss = 0.28897539\n",
            "Iteration 55, loss = 0.28607332\n",
            "Iteration 56, loss = 0.28398818\n",
            "Iteration 57, loss = 0.28139341\n",
            "Iteration 58, loss = 0.27947819\n",
            "Iteration 59, loss = 0.27875338\n",
            "Iteration 60, loss = 0.27589390\n",
            "Iteration 61, loss = 0.27527454\n",
            "Iteration 62, loss = 0.27317597\n",
            "Iteration 63, loss = 0.27225478\n",
            "Iteration 64, loss = 0.27013986\n",
            "Iteration 65, loss = 0.27082883\n",
            "Iteration 66, loss = 0.26927411\n",
            "Iteration 67, loss = 0.26894782\n",
            "Iteration 68, loss = 0.26772368\n",
            "Iteration 69, loss = 0.26693992\n",
            "Iteration 70, loss = 0.26628045\n",
            "Iteration 71, loss = 0.26748639\n",
            "Iteration 72, loss = 0.26558182\n",
            "Iteration 73, loss = 0.26499404\n",
            "Iteration 74, loss = 0.26617717\n",
            "Iteration 75, loss = 0.26269165\n",
            "Iteration 76, loss = 0.26213978\n",
            "Iteration 77, loss = 0.26379104\n",
            "Iteration 78, loss = 0.26428097\n",
            "Iteration 79, loss = 0.26329097\n",
            "Iteration 80, loss = 0.26628608\n",
            "Iteration 81, loss = 0.26234197\n",
            "Iteration 82, loss = 0.26867204\n",
            "Iteration 83, loss = 0.27545566\n",
            "Iteration 84, loss = 0.26553887\n",
            "Iteration 85, loss = 0.27072054\n",
            "Iteration 86, loss = 0.26968705\n",
            "Iteration 87, loss = 0.26581004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 88, loss = 0.25197630\n",
            "Iteration 89, loss = 0.25182856\n",
            "Iteration 90, loss = 0.25182071\n",
            "Iteration 91, loss = 0.25173612\n",
            "Iteration 92, loss = 0.25169385\n",
            "Iteration 93, loss = 0.25165283\n",
            "Iteration 94, loss = 0.25164155\n",
            "Iteration 95, loss = 0.25154643\n",
            "Iteration 96, loss = 0.25149585\n",
            "Iteration 97, loss = 0.25142469\n",
            "Iteration 98, loss = 0.25141670\n",
            "Iteration 99, loss = 0.25137223\n",
            "Iteration 100, loss = 0.25122407\n",
            "Iteration 101, loss = 0.25129104\n",
            "Iteration 102, loss = 0.25120638\n",
            "Iteration 103, loss = 0.25114453\n",
            "Iteration 104, loss = 0.25109342\n",
            "Iteration 105, loss = 0.25102247\n",
            "Iteration 106, loss = 0.25093850\n",
            "Iteration 107, loss = 0.25091302\n",
            "Iteration 108, loss = 0.25089473\n",
            "Iteration 109, loss = 0.25083692\n",
            "Iteration 110, loss = 0.25081060\n",
            "Iteration 111, loss = 0.25073582\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 112, loss = 0.25044681\n",
            "Iteration 113, loss = 0.25043004\n",
            "Iteration 114, loss = 0.25042006\n",
            "Iteration 115, loss = 0.25043193\n",
            "Iteration 116, loss = 0.25041952\n",
            "Iteration 117, loss = 0.25041609\n",
            "Iteration 118, loss = 0.25037865\n",
            "Iteration 119, loss = 0.25041933\n",
            "Iteration 120, loss = 0.25038043\n",
            "Iteration 121, loss = 0.25037917\n",
            "Iteration 122, loss = 0.25035760\n",
            "Iteration 123, loss = 0.25035187\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 124, loss = 0.25026171\n",
            "Iteration 125, loss = 0.25026426\n",
            "Iteration 126, loss = 0.25025338\n",
            "Iteration 127, loss = 0.25025720\n",
            "Iteration 128, loss = 0.25025645\n",
            "Iteration 129, loss = 0.25025237\n",
            "Iteration 130, loss = 0.25025529\n",
            "Iteration 131, loss = 0.25024105\n",
            "Iteration 132, loss = 0.25024609\n",
            "Iteration 133, loss = 0.25024081\n",
            "Iteration 134, loss = 0.25024523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64484803\n",
            "Iteration 3, loss = 0.61426067\n",
            "Iteration 4, loss = 0.59689346\n",
            "Iteration 5, loss = 0.58516560\n",
            "Iteration 6, loss = 0.57866681\n",
            "Iteration 7, loss = 0.57486159\n",
            "Iteration 8, loss = 0.57158542\n",
            "Iteration 9, loss = 0.57159459\n",
            "Iteration 10, loss = 0.57030120\n",
            "Iteration 11, loss = 0.56899587\n",
            "Iteration 12, loss = 0.57126051\n",
            "Iteration 13, loss = 0.56948560\n",
            "Iteration 14, loss = 0.56987662\n",
            "Iteration 15, loss = 0.57066432\n",
            "Iteration 16, loss = 0.56986723\n",
            "Iteration 17, loss = 0.57096032\n",
            "Iteration 18, loss = 0.57040532\n",
            "Iteration 19, loss = 0.57129499\n",
            "Iteration 20, loss = 0.57052586\n",
            "Iteration 21, loss = 0.57071236\n",
            "Iteration 22, loss = 0.57054079\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.51269954\n",
            "Iteration 24, loss = 0.49074356\n",
            "Iteration 25, loss = 0.51833680\n",
            "Iteration 26, loss = 0.51943569\n",
            "Iteration 27, loss = 0.51963251\n",
            "Iteration 28, loss = 0.51791555\n",
            "Iteration 29, loss = 0.51783766\n",
            "Iteration 30, loss = 0.51763925\n",
            "Iteration 31, loss = 0.51723131\n",
            "Iteration 32, loss = 0.51812540\n",
            "Iteration 33, loss = 0.51610679\n",
            "Iteration 34, loss = 0.51623340\n",
            "Iteration 35, loss = 0.51780226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.41711644\n",
            "Iteration 37, loss = 0.40115633\n",
            "Iteration 38, loss = 0.38806436\n",
            "Iteration 39, loss = 0.37605044\n",
            "Iteration 40, loss = 0.36501527\n",
            "Iteration 41, loss = 0.35547999\n",
            "Iteration 42, loss = 0.34661790\n",
            "Iteration 43, loss = 0.33843488\n",
            "Iteration 44, loss = 0.33114723\n",
            "Iteration 45, loss = 0.32492154\n",
            "Iteration 46, loss = 0.31915808\n",
            "Iteration 47, loss = 0.31375906\n",
            "Iteration 48, loss = 0.30899936\n",
            "Iteration 49, loss = 0.30489806\n",
            "Iteration 50, loss = 0.30040658\n",
            "Iteration 51, loss = 0.29713274\n",
            "Iteration 52, loss = 0.29436954\n",
            "Iteration 53, loss = 0.29161144\n",
            "Iteration 54, loss = 0.28861611\n",
            "Iteration 55, loss = 0.28595654\n",
            "Iteration 56, loss = 0.28391124\n",
            "Iteration 57, loss = 0.28243585\n",
            "Iteration 58, loss = 0.27989293\n",
            "Iteration 59, loss = 0.27880689\n",
            "Iteration 60, loss = 0.27712216\n",
            "Iteration 61, loss = 0.27542708\n",
            "Iteration 62, loss = 0.27449048\n",
            "Iteration 63, loss = 0.27381925\n",
            "Iteration 64, loss = 0.27228718\n",
            "Iteration 65, loss = 0.27269819\n",
            "Iteration 66, loss = 0.27326448\n",
            "Iteration 67, loss = 0.27085962\n",
            "Iteration 68, loss = 0.27046535\n",
            "Iteration 69, loss = 0.26744579\n",
            "Iteration 70, loss = 0.26869619\n",
            "Iteration 71, loss = 0.26989238\n",
            "Iteration 72, loss = 0.26832973\n",
            "Iteration 73, loss = 0.26817115\n",
            "Iteration 74, loss = 0.26844952\n",
            "Iteration 75, loss = 0.26983146\n",
            "Iteration 76, loss = 0.26698458\n",
            "Iteration 77, loss = 0.26695125\n",
            "Iteration 78, loss = 0.26790171\n",
            "Iteration 79, loss = 0.26525948\n",
            "Iteration 80, loss = 0.26795749\n",
            "Iteration 81, loss = 0.26827280\n",
            "Iteration 82, loss = 0.26699470\n",
            "Iteration 83, loss = 0.26536515\n",
            "Iteration 84, loss = 0.26874167\n",
            "Iteration 85, loss = 0.27514913\n",
            "Iteration 86, loss = 0.27713994\n",
            "Iteration 87, loss = 0.27757610\n",
            "Iteration 88, loss = 0.27376910\n",
            "Iteration 89, loss = 0.26967448\n",
            "Iteration 90, loss = 0.27499860\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 91, loss = 0.25375958\n",
            "Iteration 92, loss = 0.25371299\n",
            "Iteration 93, loss = 0.25362009\n",
            "Iteration 94, loss = 0.25356267\n",
            "Iteration 95, loss = 0.25356149\n",
            "Iteration 96, loss = 0.25346246\n",
            "Iteration 97, loss = 0.25345035\n",
            "Iteration 98, loss = 0.25341195\n",
            "Iteration 99, loss = 0.25341637\n",
            "Iteration 100, loss = 0.25328321\n",
            "Iteration 101, loss = 0.25324491\n",
            "Iteration 102, loss = 0.25320951\n",
            "Iteration 103, loss = 0.25319193\n",
            "Iteration 104, loss = 0.25312631\n",
            "Iteration 105, loss = 0.25301913\n",
            "Iteration 106, loss = 0.25301962\n",
            "Iteration 107, loss = 0.25302291\n",
            "Iteration 108, loss = 0.25292356\n",
            "Iteration 109, loss = 0.25289268\n",
            "Iteration 110, loss = 0.25286169\n",
            "Iteration 111, loss = 0.25283548\n",
            "Iteration 112, loss = 0.25279573\n",
            "Iteration 113, loss = 0.25271066\n",
            "Iteration 114, loss = 0.25264763\n",
            "Iteration 115, loss = 0.25269435\n",
            "Iteration 116, loss = 0.25259828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 117, loss = 0.25233483\n",
            "Iteration 118, loss = 0.25231014\n",
            "Iteration 119, loss = 0.25233326\n",
            "Iteration 120, loss = 0.25233240\n",
            "Iteration 121, loss = 0.25230678\n",
            "Iteration 122, loss = 0.25228330\n",
            "Iteration 123, loss = 0.25226512\n",
            "Iteration 124, loss = 0.25229999\n",
            "Iteration 125, loss = 0.25227657\n",
            "Iteration 126, loss = 0.25226560\n",
            "Iteration 127, loss = 0.25225414\n",
            "Iteration 128, loss = 0.25227363\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 129, loss = 0.25215061\n",
            "Iteration 130, loss = 0.25215634\n",
            "Iteration 131, loss = 0.25215869\n",
            "Iteration 132, loss = 0.25214895\n",
            "Iteration 133, loss = 0.25214874\n",
            "Iteration 134, loss = 0.25215275\n",
            "Iteration 135, loss = 0.25214537\n",
            "Iteration 136, loss = 0.25214698\n",
            "Iteration 137, loss = 0.25214573\n",
            "Iteration 138, loss = 0.25213551\n",
            "Iteration 139, loss = 0.25214295\n",
            "Iteration 140, loss = 0.25214032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.92424277\n",
            "Iteration 2, loss = 0.64679925\n",
            "Iteration 3, loss = 0.61672334\n",
            "Iteration 4, loss = 0.59854224\n",
            "Iteration 5, loss = 0.58839079\n",
            "Iteration 6, loss = 0.58036507\n",
            "Iteration 7, loss = 0.57832924\n",
            "Iteration 8, loss = 0.58043772\n",
            "Iteration 9, loss = 0.61199267\n",
            "Iteration 10, loss = 0.59145556\n",
            "Iteration 11, loss = 0.58059257\n",
            "Iteration 12, loss = 0.57604447\n",
            "Iteration 13, loss = 0.57556317\n",
            "Iteration 14, loss = 0.57445101\n",
            "Iteration 15, loss = 0.57258316\n",
            "Iteration 16, loss = 0.57248649\n",
            "Iteration 17, loss = 0.57221560\n",
            "Iteration 18, loss = 0.57284112\n",
            "Iteration 19, loss = 0.57304504\n",
            "Iteration 20, loss = 0.57416709\n",
            "Iteration 21, loss = 0.57354406\n",
            "Iteration 22, loss = 0.57319254\n",
            "Iteration 23, loss = 0.57419406\n",
            "Iteration 24, loss = 0.57591413\n",
            "Iteration 25, loss = 0.57458463\n",
            "Iteration 26, loss = 0.57541954\n",
            "Iteration 27, loss = 0.57544558\n",
            "Iteration 28, loss = 0.57498852\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.52067373\n",
            "Iteration 30, loss = 0.50610219\n",
            "Iteration 31, loss = 0.52163852\n",
            "Iteration 32, loss = 0.52633041\n",
            "Iteration 33, loss = 0.52905106\n",
            "Iteration 34, loss = 0.52474896\n",
            "Iteration 35, loss = 0.53037663\n",
            "Iteration 36, loss = 0.52432520\n",
            "Iteration 37, loss = 0.52754356\n",
            "Iteration 38, loss = 0.52127495\n",
            "Iteration 39, loss = 0.52657951\n",
            "Iteration 40, loss = 0.52552894\n",
            "Iteration 41, loss = 0.52551302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 42, loss = 0.44147807\n",
            "Iteration 43, loss = 0.42788164\n",
            "Iteration 44, loss = 0.41618897\n",
            "Iteration 45, loss = 0.40527095\n",
            "Iteration 46, loss = 0.39583007\n",
            "Iteration 47, loss = 0.38660475\n",
            "Iteration 48, loss = 0.37873134\n",
            "Iteration 49, loss = 0.37169171\n",
            "Iteration 50, loss = 0.36563949\n",
            "Iteration 51, loss = 0.36063959\n",
            "Iteration 52, loss = 0.35526932\n",
            "Iteration 53, loss = 0.35089681\n",
            "Iteration 54, loss = 0.34741050\n",
            "Iteration 55, loss = 0.34431011\n",
            "Iteration 56, loss = 0.34144736\n",
            "Iteration 57, loss = 0.33899602\n",
            "Iteration 58, loss = 0.33773347\n",
            "Iteration 59, loss = 0.33520280\n",
            "Iteration 60, loss = 0.33331132\n",
            "Iteration 61, loss = 0.33443624\n",
            "Iteration 62, loss = 0.33156933\n",
            "Iteration 63, loss = 0.33010141\n",
            "Iteration 64, loss = 0.33071909\n",
            "Iteration 65, loss = 0.33065851\n",
            "Iteration 66, loss = 0.32979278\n",
            "Iteration 67, loss = 0.33084081\n",
            "Iteration 68, loss = 0.33242345\n",
            "Iteration 69, loss = 0.33163559\n",
            "Iteration 70, loss = 0.33041921\n",
            "Iteration 71, loss = 0.33327688\n",
            "Iteration 72, loss = 0.33066747\n",
            "Iteration 73, loss = 0.33103432\n",
            "Iteration 74, loss = 0.32897290\n",
            "Iteration 75, loss = 0.33580723\n",
            "Iteration 76, loss = 0.33630312\n",
            "Iteration 77, loss = 0.33269641\n",
            "Iteration 78, loss = 0.32894960\n",
            "Iteration 79, loss = 0.33562209\n",
            "Iteration 80, loss = 0.33634802\n",
            "Iteration 81, loss = 0.33267707\n",
            "Iteration 82, loss = 0.32952275\n",
            "Iteration 83, loss = 0.33125363\n",
            "Iteration 84, loss = 0.33051497\n",
            "Iteration 85, loss = 0.32781788\n",
            "Iteration 86, loss = 0.32729984\n",
            "Iteration 87, loss = 0.32546667\n",
            "Iteration 88, loss = 0.33262003\n",
            "Iteration 89, loss = 0.33273553\n",
            "Iteration 90, loss = 0.33356998\n",
            "Iteration 91, loss = 0.33124514\n",
            "Iteration 92, loss = 0.32549092\n",
            "Iteration 93, loss = 0.33131130\n",
            "Iteration 94, loss = 0.33279534\n",
            "Iteration 95, loss = 0.32481323\n",
            "Iteration 96, loss = 0.33316813\n",
            "Iteration 97, loss = 0.32961342\n",
            "Iteration 98, loss = 0.32768308\n",
            "Iteration 99, loss = 0.33390255\n",
            "Iteration 100, loss = 0.32436982\n",
            "Iteration 101, loss = 0.32672407\n",
            "Iteration 102, loss = 0.32890935\n",
            "Iteration 103, loss = 0.31827614\n",
            "Iteration 104, loss = 0.32101481\n",
            "Iteration 105, loss = 0.32240041\n",
            "Iteration 106, loss = 0.32791793\n",
            "Iteration 107, loss = 0.32455974\n",
            "Iteration 108, loss = 0.32200447\n",
            "Iteration 109, loss = 0.32616701\n",
            "Iteration 110, loss = 0.32363366\n",
            "Iteration 111, loss = 0.32259190\n",
            "Iteration 112, loss = 0.32565742\n",
            "Iteration 113, loss = 0.32765636\n",
            "Iteration 114, loss = 0.32590177\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 115, loss = 0.28088078\n",
            "Iteration 116, loss = 0.28053839\n",
            "Iteration 117, loss = 0.28032307\n",
            "Iteration 118, loss = 0.28024985\n",
            "Iteration 119, loss = 0.28013521\n",
            "Iteration 120, loss = 0.27994712\n",
            "Iteration 121, loss = 0.27989591\n",
            "Iteration 122, loss = 0.27981673\n",
            "Iteration 123, loss = 0.27969044\n",
            "Iteration 124, loss = 0.27961838\n",
            "Iteration 125, loss = 0.27951461\n",
            "Iteration 126, loss = 0.27941810\n",
            "Iteration 127, loss = 0.27935057\n",
            "Iteration 128, loss = 0.27924043\n",
            "Iteration 129, loss = 0.27916257\n",
            "Iteration 130, loss = 0.27905065\n",
            "Iteration 131, loss = 0.27898780\n",
            "Iteration 132, loss = 0.27890900\n",
            "Iteration 133, loss = 0.27881911\n",
            "Iteration 134, loss = 0.27872840\n",
            "Iteration 135, loss = 0.27866440\n",
            "Iteration 136, loss = 0.27858711\n",
            "Iteration 137, loss = 0.27850466\n",
            "Iteration 138, loss = 0.27837672\n",
            "Iteration 139, loss = 0.27832614\n",
            "Iteration 140, loss = 0.27825938\n",
            "Iteration 141, loss = 0.27817713\n",
            "Iteration 142, loss = 0.27806456\n",
            "Iteration 143, loss = 0.27791587\n",
            "Iteration 144, loss = 0.27790239\n",
            "Iteration 145, loss = 0.27779361\n",
            "Iteration 146, loss = 0.27772404\n",
            "Iteration 147, loss = 0.27767149\n",
            "Iteration 148, loss = 0.27753694\n",
            "Iteration 149, loss = 0.27753334\n",
            "Iteration 150, loss = 0.27738128\n",
            "Iteration 151, loss = 0.27729572\n",
            "Iteration 152, loss = 0.27726295\n",
            "Iteration 153, loss = 0.27720375\n",
            "Iteration 154, loss = 0.27713459\n",
            "Iteration 155, loss = 0.27709472\n",
            "Iteration 156, loss = 0.27698560\n",
            "Iteration 157, loss = 0.27690142\n",
            "Iteration 158, loss = 0.27679993\n",
            "Iteration 159, loss = 0.27673993\n",
            "Iteration 160, loss = 0.27666238\n",
            "Iteration 161, loss = 0.27666852\n",
            "Iteration 162, loss = 0.27652144\n",
            "Iteration 163, loss = 0.27644402\n",
            "Iteration 164, loss = 0.27640425\n",
            "Iteration 165, loss = 0.27624183\n",
            "Iteration 166, loss = 0.27624722\n",
            "Iteration 167, loss = 0.27614914\n",
            "Iteration 168, loss = 0.27605378\n",
            "Iteration 169, loss = 0.27597200\n",
            "Iteration 170, loss = 0.27591285\n",
            "Iteration 171, loss = 0.27593663\n",
            "Iteration 172, loss = 0.27581986\n",
            "Iteration 173, loss = 0.27575423\n",
            "Iteration 174, loss = 0.27568847\n",
            "Iteration 175, loss = 0.27563509\n",
            "Iteration 176, loss = 0.27555416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 177, loss = 0.27519545\n",
            "Iteration 178, loss = 0.27515837\n",
            "Iteration 179, loss = 0.27515822\n",
            "Iteration 180, loss = 0.27518156\n",
            "Iteration 181, loss = 0.27512857\n",
            "Iteration 182, loss = 0.27510312\n",
            "Iteration 183, loss = 0.27513446\n",
            "Iteration 184, loss = 0.27507195\n",
            "Iteration 185, loss = 0.27509567\n",
            "Iteration 186, loss = 0.27507111\n",
            "Iteration 187, loss = 0.27506092\n",
            "Iteration 188, loss = 0.27502483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 189, loss = 0.27494702\n",
            "Iteration 190, loss = 0.27493464\n",
            "Iteration 191, loss = 0.27494317\n",
            "Iteration 192, loss = 0.27492392\n",
            "Iteration 193, loss = 0.27492956\n",
            "Iteration 194, loss = 0.27491905\n",
            "Iteration 195, loss = 0.27493073\n",
            "Iteration 196, loss = 0.27492411\n",
            "Iteration 197, loss = 0.27491404\n",
            "Iteration 198, loss = 0.27491656\n",
            "Iteration 199, loss = 0.27491020\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 38.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69256098\n",
            "Iteration 3, loss = 0.68195775\n",
            "Iteration 4, loss = 0.66778721\n",
            "Iteration 5, loss = 0.68449109\n",
            "Iteration 6, loss = 0.67689217\n",
            "Iteration 7, loss = 0.66282003\n",
            "Iteration 8, loss = 0.64582718\n",
            "Iteration 9, loss = 0.63530720\n",
            "Iteration 10, loss = 0.62663381\n",
            "Iteration 11, loss = 0.62319229\n",
            "Iteration 12, loss = 0.61701983\n",
            "Iteration 13, loss = 0.61099280\n",
            "Iteration 14, loss = 0.60577213\n",
            "Iteration 15, loss = 0.60142772\n",
            "Iteration 16, loss = 0.59914871\n",
            "Iteration 17, loss = 0.59532404\n",
            "Iteration 18, loss = 0.59331502\n",
            "Iteration 19, loss = 0.58991675\n",
            "Iteration 20, loss = 0.58797869\n",
            "Iteration 21, loss = 0.59140575\n",
            "Iteration 22, loss = 0.58393972\n",
            "Iteration 23, loss = 0.58497045\n",
            "Iteration 24, loss = 0.58045505\n",
            "Iteration 25, loss = 0.58121469\n",
            "Iteration 26, loss = 0.58047022\n",
            "Iteration 27, loss = 0.57865329\n",
            "Iteration 28, loss = 0.57971103\n",
            "Iteration 29, loss = 0.57773369\n",
            "Iteration 30, loss = 0.57661562\n",
            "Iteration 31, loss = 0.57868697\n",
            "Iteration 32, loss = 0.57879817\n",
            "Iteration 33, loss = 0.57730529\n",
            "Iteration 34, loss = 0.57203283\n",
            "Iteration 35, loss = 0.57960413\n",
            "Iteration 36, loss = 0.57423120\n",
            "Iteration 37, loss = 0.57191084\n",
            "Iteration 38, loss = 0.57409170\n",
            "Iteration 39, loss = 0.57793948\n",
            "Iteration 40, loss = 0.57688853\n",
            "Iteration 41, loss = 0.59408485\n",
            "Iteration 42, loss = 0.62457414\n",
            "Iteration 43, loss = 0.61172524\n",
            "Iteration 44, loss = 0.60495077\n",
            "Iteration 45, loss = 0.60063681\n",
            "Iteration 46, loss = 0.59348674\n",
            "Iteration 47, loss = 0.59215113\n",
            "Iteration 48, loss = 0.58351231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.57250775\n",
            "Iteration 50, loss = 0.57145618\n",
            "Iteration 51, loss = 0.56929113\n",
            "Iteration 52, loss = 0.56818622\n",
            "Iteration 53, loss = 0.56635914\n",
            "Iteration 54, loss = 0.56485004\n",
            "Iteration 55, loss = 0.56359878\n",
            "Iteration 56, loss = 0.56227025\n",
            "Iteration 57, loss = 0.56067462\n",
            "Iteration 58, loss = 0.55904708\n",
            "Iteration 59, loss = 0.55610736\n",
            "Iteration 60, loss = 0.55474572\n",
            "Iteration 61, loss = 0.55386297\n",
            "Iteration 62, loss = 0.55107553\n",
            "Iteration 63, loss = 0.54939234\n",
            "Iteration 64, loss = 0.54922801\n",
            "Iteration 65, loss = 0.54604191\n",
            "Iteration 66, loss = 0.54492569\n",
            "Iteration 67, loss = 0.54299084\n",
            "Iteration 68, loss = 0.54023190\n",
            "Iteration 69, loss = 0.54037722\n",
            "Iteration 70, loss = 0.54143677\n",
            "Iteration 71, loss = 0.54210350\n",
            "Iteration 72, loss = 0.54424881\n",
            "Iteration 73, loss = 0.54557250\n",
            "Iteration 74, loss = 0.54373595\n",
            "Iteration 75, loss = 0.54358501\n",
            "Iteration 76, loss = 0.54362265\n",
            "Iteration 77, loss = 0.53662714\n",
            "Iteration 78, loss = 0.54489607\n",
            "Iteration 79, loss = 0.54085769\n",
            "Iteration 80, loss = 0.53808526\n",
            "Iteration 81, loss = 0.53984204\n",
            "Iteration 82, loss = 0.55161462\n",
            "Iteration 83, loss = 0.54239067\n",
            "Iteration 84, loss = 0.54217872\n",
            "Iteration 85, loss = 0.53643890\n",
            "Iteration 86, loss = 0.54047251\n",
            "Iteration 87, loss = 0.53982765\n",
            "Iteration 88, loss = 0.54049054\n",
            "Iteration 89, loss = 0.54076820\n",
            "Iteration 90, loss = 0.53216521\n",
            "Iteration 91, loss = 0.53154485\n",
            "Iteration 92, loss = 0.54056184\n",
            "Iteration 93, loss = 0.53996741\n",
            "Iteration 94, loss = 0.53129904\n",
            "Iteration 95, loss = 0.53606212\n",
            "Iteration 96, loss = 0.53998503\n",
            "Iteration 97, loss = 0.53247992\n",
            "Iteration 98, loss = 0.53265701\n",
            "Iteration 99, loss = 0.53798357\n",
            "Iteration 100, loss = 0.52915074\n",
            "Iteration 101, loss = 0.54149725\n",
            "Iteration 102, loss = 0.53370961\n",
            "Iteration 103, loss = 0.53795649\n",
            "Iteration 104, loss = 0.53379695\n",
            "Iteration 105, loss = 0.53435457\n",
            "Iteration 106, loss = 0.53153398\n",
            "Iteration 107, loss = 0.52656541\n",
            "Iteration 108, loss = 0.53316439\n",
            "Iteration 109, loss = 0.53600699\n",
            "Iteration 110, loss = 0.52874423\n",
            "Iteration 111, loss = 0.52940025\n",
            "Iteration 112, loss = 0.53186792\n",
            "Iteration 113, loss = 0.52305317\n",
            "Iteration 114, loss = 0.53508995\n",
            "Iteration 115, loss = 0.53379674\n",
            "Iteration 116, loss = 0.52583736\n",
            "Iteration 117, loss = 0.52336692\n",
            "Iteration 118, loss = 0.52731458\n",
            "Iteration 119, loss = 0.53107233\n",
            "Iteration 120, loss = 0.52774647\n",
            "Iteration 121, loss = 0.52911782\n",
            "Iteration 122, loss = 0.52435059\n",
            "Iteration 123, loss = 0.53607994\n",
            "Iteration 124, loss = 0.53091253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 125, loss = 0.46145650\n",
            "Iteration 126, loss = 0.45729549\n",
            "Iteration 127, loss = 0.45429216\n",
            "Iteration 128, loss = 0.45197819\n",
            "Iteration 129, loss = 0.44934061\n",
            "Iteration 130, loss = 0.44632611\n",
            "Iteration 131, loss = 0.44393710\n",
            "Iteration 132, loss = 0.44149922\n",
            "Iteration 133, loss = 0.43893180\n",
            "Iteration 134, loss = 0.43619602\n",
            "Iteration 135, loss = 0.43418447\n",
            "Iteration 136, loss = 0.43171789\n",
            "Iteration 137, loss = 0.42925215\n",
            "Iteration 138, loss = 0.42702109\n",
            "Iteration 139, loss = 0.42481905\n",
            "Iteration 140, loss = 0.42255397\n",
            "Iteration 141, loss = 0.42004037\n",
            "Iteration 142, loss = 0.41819988\n",
            "Iteration 143, loss = 0.41565706\n",
            "Iteration 144, loss = 0.41359228\n",
            "Iteration 145, loss = 0.41148638\n",
            "Iteration 146, loss = 0.40976044\n",
            "Iteration 147, loss = 0.40760224\n",
            "Iteration 148, loss = 0.40561653\n",
            "Iteration 149, loss = 0.40380713\n",
            "Iteration 150, loss = 0.40152602\n",
            "Iteration 151, loss = 0.40004242\n",
            "Iteration 152, loss = 0.39866476\n",
            "Iteration 153, loss = 0.39587516\n",
            "Iteration 154, loss = 0.39457515\n",
            "Iteration 155, loss = 0.39327763\n",
            "Iteration 156, loss = 0.39071260\n",
            "Iteration 157, loss = 0.38957463\n",
            "Iteration 158, loss = 0.38823202\n",
            "Iteration 159, loss = 0.38753725\n",
            "Iteration 160, loss = 0.38493107\n",
            "Iteration 161, loss = 0.38452333\n",
            "Iteration 162, loss = 0.38135870\n",
            "Iteration 163, loss = 0.38121190\n",
            "Iteration 164, loss = 0.37971612\n",
            "Iteration 165, loss = 0.37962008\n",
            "Iteration 166, loss = 0.37784055\n",
            "Iteration 167, loss = 0.37548383\n",
            "Iteration 168, loss = 0.37373910\n",
            "Iteration 169, loss = 0.37437797\n",
            "Iteration 170, loss = 0.37290004\n",
            "Iteration 171, loss = 0.37108545\n",
            "Iteration 172, loss = 0.37040404\n",
            "Iteration 173, loss = 0.36993587\n",
            "Iteration 174, loss = 0.36957084\n",
            "Iteration 175, loss = 0.36852882\n",
            "Iteration 176, loss = 0.36784682\n",
            "Iteration 177, loss = 0.36471704\n",
            "Iteration 178, loss = 0.36448171\n",
            "Iteration 179, loss = 0.36435025\n",
            "Iteration 180, loss = 0.36363779\n",
            "Iteration 181, loss = 0.36365668\n",
            "Iteration 182, loss = 0.36181604\n",
            "Iteration 183, loss = 0.36024827\n",
            "Iteration 184, loss = 0.36272050\n",
            "Iteration 185, loss = 0.36087176\n",
            "Iteration 186, loss = 0.35946982\n",
            "Iteration 187, loss = 0.35935207\n",
            "Iteration 188, loss = 0.35969850\n",
            "Iteration 189, loss = 0.36031567\n",
            "Iteration 190, loss = 0.35907714\n",
            "Iteration 191, loss = 0.35844547\n",
            "Iteration 192, loss = 0.35842649\n",
            "Iteration 193, loss = 0.35587989\n",
            "Iteration 194, loss = 0.35959841\n",
            "Iteration 195, loss = 0.35422618\n",
            "Iteration 196, loss = 0.35287927\n",
            "Iteration 197, loss = 0.35391020\n",
            "Iteration 198, loss = 0.35482066\n",
            "Iteration 199, loss = 0.35457479\n",
            "Iteration 200, loss = 0.35488959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69409146\n",
            "Iteration 3, loss = 0.67945735\n",
            "Iteration 4, loss = 0.66271721\n",
            "Iteration 5, loss = 0.65165638\n",
            "Iteration 6, loss = 0.64810564\n",
            "Iteration 7, loss = 0.63510116\n",
            "Iteration 8, loss = 0.62687784\n",
            "Iteration 9, loss = 0.62157490\n",
            "Iteration 10, loss = 0.61624949\n",
            "Iteration 11, loss = 0.61293524\n",
            "Iteration 12, loss = 0.60714576\n",
            "Iteration 13, loss = 0.60370952\n",
            "Iteration 14, loss = 0.59956800\n",
            "Iteration 15, loss = 0.59870782\n",
            "Iteration 16, loss = 0.59571724\n",
            "Iteration 17, loss = 0.64094069\n",
            "Iteration 18, loss = 0.62504124\n",
            "Iteration 19, loss = 0.61296122\n",
            "Iteration 20, loss = 0.60590533\n",
            "Iteration 21, loss = 0.60125949\n",
            "Iteration 22, loss = 0.59806246\n",
            "Iteration 23, loss = 0.59174177\n",
            "Iteration 24, loss = 0.59073264\n",
            "Iteration 25, loss = 0.58628529\n",
            "Iteration 26, loss = 0.58970977\n",
            "Iteration 27, loss = 0.58331127\n",
            "Iteration 28, loss = 0.58226279\n",
            "Iteration 29, loss = 0.58183610\n",
            "Iteration 30, loss = 0.57956809\n",
            "Iteration 31, loss = 0.58062538\n",
            "Iteration 32, loss = 0.57672485\n",
            "Iteration 33, loss = 0.57796794\n",
            "Iteration 34, loss = 0.57823203\n",
            "Iteration 35, loss = 0.57648880\n",
            "Iteration 36, loss = 0.57392134\n",
            "Iteration 37, loss = 0.57497310\n",
            "Iteration 38, loss = 0.57495559\n",
            "Iteration 39, loss = 0.57546370\n",
            "Iteration 40, loss = 0.57349428\n",
            "Iteration 41, loss = 0.57472477\n",
            "Iteration 42, loss = 0.57456217\n",
            "Iteration 43, loss = 0.57252471\n",
            "Iteration 44, loss = 0.57381281\n",
            "Iteration 45, loss = 0.57452448\n",
            "Iteration 46, loss = 0.57354688\n",
            "Iteration 47, loss = 0.57332322\n",
            "Iteration 48, loss = 0.57483245\n",
            "Iteration 49, loss = 0.57662215\n",
            "Iteration 50, loss = 0.57085218\n",
            "Iteration 51, loss = 0.57622754\n",
            "Iteration 52, loss = 0.57683745\n",
            "Iteration 53, loss = 0.57319122\n",
            "Iteration 54, loss = 0.57279765\n",
            "Iteration 55, loss = 0.57402214\n",
            "Iteration 56, loss = 0.57319143\n",
            "Iteration 57, loss = 0.57764411\n",
            "Iteration 58, loss = 0.57347408\n",
            "Iteration 59, loss = 0.57256281\n",
            "Iteration 60, loss = 0.57256169\n",
            "Iteration 61, loss = 0.57624296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 62, loss = 0.54257580\n",
            "Iteration 63, loss = 0.53197958\n",
            "Iteration 64, loss = 0.52586542\n",
            "Iteration 65, loss = 0.52068897\n",
            "Iteration 66, loss = 0.51540527\n",
            "Iteration 67, loss = 0.51206792\n",
            "Iteration 68, loss = 0.50921411\n",
            "Iteration 69, loss = 0.51427729\n",
            "Iteration 70, loss = 0.51604668\n",
            "Iteration 71, loss = 0.51699740\n",
            "Iteration 72, loss = 0.52095054\n",
            "Iteration 73, loss = 0.52455776\n",
            "Iteration 74, loss = 0.52751882\n",
            "Iteration 75, loss = 0.53543860\n",
            "Iteration 76, loss = 0.53469493\n",
            "Iteration 77, loss = 0.53006113\n",
            "Iteration 78, loss = 0.52314658\n",
            "Iteration 79, loss = 0.52887775\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.46911109\n",
            "Iteration 81, loss = 0.46643077\n",
            "Iteration 82, loss = 0.46412203\n",
            "Iteration 83, loss = 0.46201731\n",
            "Iteration 84, loss = 0.45959397\n",
            "Iteration 85, loss = 0.45741663\n",
            "Iteration 86, loss = 0.45526812\n",
            "Iteration 87, loss = 0.45310597\n",
            "Iteration 88, loss = 0.45092671\n",
            "Iteration 89, loss = 0.44862972\n",
            "Iteration 90, loss = 0.44639606\n",
            "Iteration 91, loss = 0.44477754\n",
            "Iteration 92, loss = 0.44237424\n",
            "Iteration 93, loss = 0.44065807\n",
            "Iteration 94, loss = 0.43856359\n",
            "Iteration 95, loss = 0.43649493\n",
            "Iteration 96, loss = 0.43449464\n",
            "Iteration 97, loss = 0.43217570\n",
            "Iteration 98, loss = 0.43040345\n",
            "Iteration 99, loss = 0.42848017\n",
            "Iteration 100, loss = 0.42702083\n",
            "Iteration 101, loss = 0.42483931\n",
            "Iteration 102, loss = 0.42279785\n",
            "Iteration 103, loss = 0.42126085\n",
            "Iteration 104, loss = 0.41900726\n",
            "Iteration 105, loss = 0.41715333\n",
            "Iteration 106, loss = 0.41558391\n",
            "Iteration 107, loss = 0.41394615\n",
            "Iteration 108, loss = 0.41283508\n",
            "Iteration 109, loss = 0.41130918\n",
            "Iteration 110, loss = 0.40854732\n",
            "Iteration 111, loss = 0.40793990\n",
            "Iteration 112, loss = 0.40531089\n",
            "Iteration 113, loss = 0.40513080\n",
            "Iteration 114, loss = 0.40267038\n",
            "Iteration 115, loss = 0.40098713\n",
            "Iteration 116, loss = 0.40028285\n",
            "Iteration 117, loss = 0.39866479\n",
            "Iteration 118, loss = 0.39686764\n",
            "Iteration 119, loss = 0.39622418\n",
            "Iteration 120, loss = 0.39518710\n",
            "Iteration 121, loss = 0.39426905\n",
            "Iteration 122, loss = 0.39330267\n",
            "Iteration 123, loss = 0.39158253\n",
            "Iteration 124, loss = 0.39021918\n",
            "Iteration 125, loss = 0.38913331\n",
            "Iteration 126, loss = 0.38768741\n",
            "Iteration 127, loss = 0.38593450\n",
            "Iteration 128, loss = 0.38599662\n",
            "Iteration 129, loss = 0.38543483\n",
            "Iteration 130, loss = 0.38312980\n",
            "Iteration 131, loss = 0.38310978\n",
            "Iteration 132, loss = 0.38223228\n",
            "Iteration 133, loss = 0.38214685\n",
            "Iteration 134, loss = 0.37935555\n",
            "Iteration 135, loss = 0.38273666\n",
            "Iteration 136, loss = 0.38379358\n",
            "Iteration 137, loss = 0.37960982\n",
            "Iteration 138, loss = 0.37781952\n",
            "Iteration 139, loss = 0.37654501\n",
            "Iteration 140, loss = 0.37678201\n",
            "Iteration 141, loss = 0.37569660\n",
            "Iteration 142, loss = 0.37713622\n",
            "Iteration 143, loss = 0.37466614\n",
            "Iteration 144, loss = 0.37495963\n",
            "Iteration 145, loss = 0.37601260\n",
            "Iteration 146, loss = 0.37438149\n",
            "Iteration 147, loss = 0.37523591\n",
            "Iteration 148, loss = 0.37288476\n",
            "Iteration 149, loss = 0.37239040\n",
            "Iteration 150, loss = 0.37255062\n",
            "Iteration 151, loss = 0.37415040\n",
            "Iteration 152, loss = 0.37294580\n",
            "Iteration 153, loss = 0.36959385\n",
            "Iteration 154, loss = 0.37246305\n",
            "Iteration 155, loss = 0.37470558\n",
            "Iteration 156, loss = 0.37359845\n",
            "Iteration 157, loss = 0.37462663\n",
            "Iteration 158, loss = 0.37649210\n",
            "Iteration 159, loss = 0.37174397\n",
            "Iteration 160, loss = 0.36736202\n",
            "Iteration 161, loss = 0.37221939\n",
            "Iteration 162, loss = 0.37286631\n",
            "Iteration 163, loss = 0.36687791\n",
            "Iteration 164, loss = 0.36619913\n",
            "Iteration 165, loss = 0.37127764\n",
            "Iteration 166, loss = 0.36697225\n",
            "Iteration 167, loss = 0.36999231\n",
            "Iteration 168, loss = 0.37376988\n",
            "Iteration 169, loss = 0.37373764\n",
            "Iteration 170, loss = 0.37172847\n",
            "Iteration 171, loss = 0.37291472\n",
            "Iteration 172, loss = 0.36636376\n",
            "Iteration 173, loss = 0.36919535\n",
            "Iteration 174, loss = 0.36967956\n",
            "Iteration 175, loss = 0.36288290\n",
            "Iteration 176, loss = 0.37066113\n",
            "Iteration 177, loss = 0.36685066\n",
            "Iteration 178, loss = 0.37153830\n",
            "Iteration 179, loss = 0.37056789\n",
            "Iteration 180, loss = 0.36637346\n",
            "Iteration 181, loss = 0.36962132\n",
            "Iteration 182, loss = 0.36916056\n",
            "Iteration 183, loss = 0.36575423\n",
            "Iteration 184, loss = 0.36607315\n",
            "Iteration 185, loss = 0.36501567\n",
            "Iteration 186, loss = 0.37156344\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 187, loss = 0.33481533\n",
            "Iteration 188, loss = 0.33298960\n",
            "Iteration 189, loss = 0.33285867\n",
            "Iteration 190, loss = 0.33283339\n",
            "Iteration 191, loss = 0.33253848\n",
            "Iteration 192, loss = 0.33250133\n",
            "Iteration 193, loss = 0.33229247\n",
            "Iteration 194, loss = 0.33216140\n",
            "Iteration 195, loss = 0.33195324\n",
            "Iteration 196, loss = 0.33190528\n",
            "Iteration 197, loss = 0.33180029\n",
            "Iteration 198, loss = 0.33144610\n",
            "Iteration 199, loss = 0.33139059\n",
            "Iteration 200, loss = 0.33126434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70820546\n",
            "Iteration 3, loss = 0.70954324\n",
            "Iteration 4, loss = 0.69026548\n",
            "Iteration 5, loss = 0.66804098\n",
            "Iteration 6, loss = 0.65740199\n",
            "Iteration 7, loss = 0.64774951\n",
            "Iteration 8, loss = 0.63535561\n",
            "Iteration 9, loss = 0.62848149\n",
            "Iteration 10, loss = 0.62106404\n",
            "Iteration 11, loss = 0.61744540\n",
            "Iteration 12, loss = 0.61205968\n",
            "Iteration 13, loss = 0.60784910\n",
            "Iteration 14, loss = 0.60282423\n",
            "Iteration 15, loss = 0.60048009\n",
            "Iteration 16, loss = 0.59680562\n",
            "Iteration 17, loss = 0.59556410\n",
            "Iteration 18, loss = 0.59250289\n",
            "Iteration 19, loss = 0.59268226\n",
            "Iteration 20, loss = 0.58959882\n",
            "Iteration 21, loss = 0.58653087\n",
            "Iteration 22, loss = 0.58500205\n",
            "Iteration 23, loss = 0.58351354\n",
            "Iteration 24, loss = 0.58310503\n",
            "Iteration 25, loss = 0.58085953\n",
            "Iteration 26, loss = 0.58009851\n",
            "Iteration 27, loss = 0.57753854\n",
            "Iteration 28, loss = 0.57912863\n",
            "Iteration 29, loss = 0.57695704\n",
            "Iteration 30, loss = 0.57957854\n",
            "Iteration 31, loss = 0.57976743\n",
            "Iteration 32, loss = 0.57744725\n",
            "Iteration 33, loss = 0.57636935\n",
            "Iteration 34, loss = 0.57898971\n",
            "Iteration 35, loss = 0.64913826\n",
            "Iteration 36, loss = 0.62120005\n",
            "Iteration 37, loss = 0.60815959\n",
            "Iteration 38, loss = 0.60429886\n",
            "Iteration 39, loss = 0.59729013\n",
            "Iteration 40, loss = 0.59614464\n",
            "Iteration 41, loss = 0.59349125\n",
            "Iteration 42, loss = 0.58833430\n",
            "Iteration 43, loss = 0.58595972\n",
            "Iteration 44, loss = 0.58578723\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.57223036\n",
            "Iteration 46, loss = 0.56946969\n",
            "Iteration 47, loss = 0.56765929\n",
            "Iteration 48, loss = 0.56603478\n",
            "Iteration 49, loss = 0.56488334\n",
            "Iteration 50, loss = 0.56309620\n",
            "Iteration 51, loss = 0.56165187\n",
            "Iteration 52, loss = 0.55952137\n",
            "Iteration 53, loss = 0.55828820\n",
            "Iteration 54, loss = 0.55625931\n",
            "Iteration 55, loss = 0.55340782\n",
            "Iteration 56, loss = 0.55106540\n",
            "Iteration 57, loss = 0.54981160\n",
            "Iteration 58, loss = 0.54853107\n",
            "Iteration 59, loss = 0.54656622\n",
            "Iteration 60, loss = 0.54174458\n",
            "Iteration 61, loss = 0.54270587\n",
            "Iteration 62, loss = 0.53937199\n",
            "Iteration 63, loss = 0.54163431\n",
            "Iteration 64, loss = 0.54304974\n",
            "Iteration 65, loss = 0.54610078\n",
            "Iteration 66, loss = 0.54435130\n",
            "Iteration 67, loss = 0.54960739\n",
            "Iteration 68, loss = 0.54592641\n",
            "Iteration 69, loss = 0.54719207\n",
            "Iteration 70, loss = 0.54503150\n",
            "Iteration 71, loss = 0.54332197\n",
            "Iteration 72, loss = 0.55680462\n",
            "Iteration 73, loss = 0.53918715\n",
            "Iteration 74, loss = 0.54328653\n",
            "Iteration 75, loss = 0.54733862\n",
            "Iteration 76, loss = 0.54139333\n",
            "Iteration 77, loss = 0.54663660\n",
            "Iteration 78, loss = 0.54527392\n",
            "Iteration 79, loss = 0.54398230\n",
            "Iteration 80, loss = 0.54521753\n",
            "Iteration 81, loss = 0.69513322\n",
            "Iteration 82, loss = 0.64555794\n",
            "Iteration 83, loss = 0.63799994\n",
            "Iteration 84, loss = 0.63531369\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 85, loss = 0.63394419\n",
            "Iteration 86, loss = 0.63273006\n",
            "Iteration 87, loss = 0.63222801\n",
            "Iteration 88, loss = 0.63158919\n",
            "Iteration 89, loss = 0.63118176\n",
            "Iteration 90, loss = 0.63049767\n",
            "Iteration 91, loss = 0.62972074\n",
            "Iteration 92, loss = 0.62932333\n",
            "Iteration 93, loss = 0.62844263\n",
            "Iteration 94, loss = 0.62766322\n",
            "Iteration 95, loss = 0.62693883\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 96, loss = 0.62626969\n",
            "Iteration 97, loss = 0.62608083\n",
            "Iteration 98, loss = 0.62592292\n",
            "Iteration 99, loss = 0.62579792\n",
            "Iteration 100, loss = 0.62555096\n",
            "Iteration 101, loss = 0.62537200\n",
            "Iteration 102, loss = 0.62528155\n",
            "Iteration 103, loss = 0.62508036\n",
            "Iteration 104, loss = 0.62490363\n",
            "Iteration 105, loss = 0.62469353\n",
            "Iteration 106, loss = 0.62457131\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 107, loss = 0.62442004\n",
            "Iteration 108, loss = 0.62435231\n",
            "Iteration 109, loss = 0.62431115\n",
            "Iteration 110, loss = 0.62427609\n",
            "Iteration 111, loss = 0.62424948\n",
            "Iteration 112, loss = 0.62421616\n",
            "Iteration 113, loss = 0.62417565\n",
            "Iteration 114, loss = 0.62412863\n",
            "Iteration 115, loss = 0.62409463\n",
            "Iteration 116, loss = 0.62406941\n",
            "Iteration 117, loss = 0.62403133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 118, loss = 0.62399131\n",
            "Iteration 119, loss = 0.62398476\n",
            "Iteration 120, loss = 0.62397618\n",
            "Iteration 121, loss = 0.62396739\n",
            "Iteration 122, loss = 0.62396224\n",
            "Iteration 123, loss = 0.62395403\n",
            "Iteration 124, loss = 0.62394722\n",
            "Iteration 125, loss = 0.62393902\n",
            "Iteration 126, loss = 0.62393297\n",
            "Iteration 127, loss = 0.62392609\n",
            "Iteration 128, loss = 0.62391863\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72806569\n",
            "Iteration 3, loss = 0.71279427\n",
            "Iteration 4, loss = 0.70005803\n",
            "Iteration 5, loss = 0.68763451\n",
            "Iteration 6, loss = 0.67173966\n",
            "Iteration 7, loss = 0.65237788\n",
            "Iteration 8, loss = 0.66922304\n",
            "Iteration 9, loss = 0.65219111\n",
            "Iteration 10, loss = 0.63625684\n",
            "Iteration 11, loss = 0.62756275\n",
            "Iteration 12, loss = 0.62163424\n",
            "Iteration 13, loss = 0.61167884\n",
            "Iteration 14, loss = 0.60826048\n",
            "Iteration 15, loss = 0.60208712\n",
            "Iteration 16, loss = 0.59942463\n",
            "Iteration 17, loss = 0.59373924\n",
            "Iteration 18, loss = 0.59090096\n",
            "Iteration 19, loss = 0.58989708\n",
            "Iteration 20, loss = 0.59052408\n",
            "Iteration 21, loss = 0.58426423\n",
            "Iteration 22, loss = 0.58557706\n",
            "Iteration 23, loss = 0.58441106\n",
            "Iteration 24, loss = 0.58232208\n",
            "Iteration 25, loss = 0.58326394\n",
            "Iteration 26, loss = 0.57859738\n",
            "Iteration 27, loss = 0.57890471\n",
            "Iteration 28, loss = 0.58028030\n",
            "Iteration 29, loss = 0.57608178\n",
            "Iteration 30, loss = 0.57928213\n",
            "Iteration 31, loss = 0.57560764\n",
            "Iteration 32, loss = 0.57576225\n",
            "Iteration 33, loss = 0.57557850\n",
            "Iteration 34, loss = 0.57496316\n",
            "Iteration 35, loss = 0.62142091\n",
            "Iteration 36, loss = 0.61969611\n",
            "Iteration 37, loss = 0.60696115\n",
            "Iteration 38, loss = 0.59937936\n",
            "Iteration 39, loss = 0.59229395\n",
            "Iteration 40, loss = 0.58948366\n",
            "Iteration 41, loss = 0.58909446\n",
            "Iteration 42, loss = 0.58340115\n",
            "Iteration 43, loss = 0.58310443\n",
            "Iteration 44, loss = 0.57963672\n",
            "Iteration 45, loss = 0.58051800\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 46, loss = 0.56450751\n",
            "Iteration 47, loss = 0.56098139\n",
            "Iteration 48, loss = 0.55949836\n",
            "Iteration 49, loss = 0.55780943\n",
            "Iteration 50, loss = 0.55591451\n",
            "Iteration 51, loss = 0.55468482\n",
            "Iteration 52, loss = 0.55224980\n",
            "Iteration 53, loss = 0.55026515\n",
            "Iteration 54, loss = 0.54800001\n",
            "Iteration 55, loss = 0.54577489\n",
            "Iteration 56, loss = 0.54346338\n",
            "Iteration 57, loss = 0.54082279\n",
            "Iteration 58, loss = 0.53768649\n",
            "Iteration 59, loss = 0.53754477\n",
            "Iteration 60, loss = 0.53474694\n",
            "Iteration 61, loss = 0.53473451\n",
            "Iteration 62, loss = 0.53602148\n",
            "Iteration 63, loss = 0.53441114\n",
            "Iteration 64, loss = 0.53963459\n",
            "Iteration 65, loss = 0.53761904\n",
            "Iteration 66, loss = 0.54242350\n",
            "Iteration 67, loss = 0.53982223\n",
            "Iteration 68, loss = 0.53688474\n",
            "Iteration 69, loss = 0.54195575\n",
            "Iteration 70, loss = 0.53798866\n",
            "Iteration 71, loss = 0.54440872\n",
            "Iteration 72, loss = 0.54191438\n",
            "Iteration 73, loss = 0.53834191\n",
            "Iteration 74, loss = 0.53604612\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 75, loss = 0.49384436\n",
            "Iteration 76, loss = 0.49230014\n",
            "Iteration 77, loss = 0.49048431\n",
            "Iteration 78, loss = 0.48947074\n",
            "Iteration 79, loss = 0.48748057\n",
            "Iteration 80, loss = 0.48634368\n",
            "Iteration 81, loss = 0.48463025\n",
            "Iteration 82, loss = 0.48315605\n",
            "Iteration 83, loss = 0.48134211\n",
            "Iteration 84, loss = 0.47999158\n",
            "Iteration 85, loss = 0.47851924\n",
            "Iteration 86, loss = 0.47667707\n",
            "Iteration 87, loss = 0.47476952\n",
            "Iteration 88, loss = 0.47403461\n",
            "Iteration 89, loss = 0.47160546\n",
            "Iteration 90, loss = 0.47048766\n",
            "Iteration 91, loss = 0.46874184\n",
            "Iteration 92, loss = 0.46753959\n",
            "Iteration 93, loss = 0.46535378\n",
            "Iteration 94, loss = 0.46371866\n",
            "Iteration 95, loss = 0.46272624\n",
            "Iteration 96, loss = 0.46081425\n",
            "Iteration 97, loss = 0.45948984\n",
            "Iteration 98, loss = 0.45754519\n",
            "Iteration 99, loss = 0.45611441\n",
            "Iteration 100, loss = 0.45513423\n",
            "Iteration 101, loss = 0.45359025\n",
            "Iteration 102, loss = 0.45224221\n",
            "Iteration 103, loss = 0.45036439\n",
            "Iteration 104, loss = 0.44870842\n",
            "Iteration 105, loss = 0.44804967\n",
            "Iteration 106, loss = 0.44629737\n",
            "Iteration 107, loss = 0.44427341\n",
            "Iteration 108, loss = 0.44310404\n",
            "Iteration 109, loss = 0.44130765\n",
            "Iteration 110, loss = 0.44005614\n",
            "Iteration 111, loss = 0.43782928\n",
            "Iteration 112, loss = 0.43699946\n",
            "Iteration 113, loss = 0.43528757\n",
            "Iteration 114, loss = 0.43377773\n",
            "Iteration 115, loss = 0.43384857\n",
            "Iteration 116, loss = 0.43122116\n",
            "Iteration 117, loss = 0.43051689\n",
            "Iteration 118, loss = 0.42846253\n",
            "Iteration 119, loss = 0.42789109\n",
            "Iteration 120, loss = 0.42610679\n",
            "Iteration 121, loss = 0.42508174\n",
            "Iteration 122, loss = 0.42362681\n",
            "Iteration 123, loss = 0.42286535\n",
            "Iteration 124, loss = 0.42223688\n",
            "Iteration 125, loss = 0.41924968\n",
            "Iteration 126, loss = 0.41893769\n",
            "Iteration 127, loss = 0.41828646\n",
            "Iteration 128, loss = 0.41853909\n",
            "Iteration 129, loss = 0.41627212\n",
            "Iteration 130, loss = 0.41374630\n",
            "Iteration 131, loss = 0.41577714\n",
            "Iteration 132, loss = 0.41208491\n",
            "Iteration 133, loss = 0.41258248\n",
            "Iteration 134, loss = 0.41141918\n",
            "Iteration 135, loss = 0.41095635\n",
            "Iteration 136, loss = 0.40656725\n",
            "Iteration 137, loss = 0.40873968\n",
            "Iteration 138, loss = 0.40804117\n",
            "Iteration 139, loss = 0.40759108\n",
            "Iteration 140, loss = 0.40491170\n",
            "Iteration 141, loss = 0.40822510\n",
            "Iteration 142, loss = 0.40625570\n",
            "Iteration 143, loss = 0.40605168\n",
            "Iteration 144, loss = 0.40635193\n",
            "Iteration 145, loss = 0.40681619\n",
            "Iteration 146, loss = 0.40035753\n",
            "Iteration 147, loss = 0.40345178\n",
            "Iteration 148, loss = 0.39817062\n",
            "Iteration 149, loss = 0.40299141\n",
            "Iteration 150, loss = 0.40524336\n",
            "Iteration 151, loss = 0.39873590\n",
            "Iteration 152, loss = 0.39923903\n",
            "Iteration 153, loss = 0.40282067\n",
            "Iteration 154, loss = 0.40025218\n",
            "Iteration 155, loss = 0.40327777\n",
            "Iteration 156, loss = 0.40388227\n",
            "Iteration 157, loss = 0.40679338\n",
            "Iteration 158, loss = 0.40117186\n",
            "Iteration 159, loss = 0.40143956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 160, loss = 0.37678339\n",
            "Iteration 161, loss = 0.37621858\n",
            "Iteration 162, loss = 0.37615657\n",
            "Iteration 163, loss = 0.37579248\n",
            "Iteration 164, loss = 0.37574063\n",
            "Iteration 165, loss = 0.37528725\n",
            "Iteration 166, loss = 0.37526207\n",
            "Iteration 167, loss = 0.37493283\n",
            "Iteration 168, loss = 0.37476352\n",
            "Iteration 169, loss = 0.37458751\n",
            "Iteration 170, loss = 0.37432537\n",
            "Iteration 171, loss = 0.37417351\n",
            "Iteration 172, loss = 0.37378415\n",
            "Iteration 173, loss = 0.37362124\n",
            "Iteration 174, loss = 0.37354448\n",
            "Iteration 175, loss = 0.37322988\n",
            "Iteration 176, loss = 0.37300058\n",
            "Iteration 177, loss = 0.37289208\n",
            "Iteration 178, loss = 0.37264119\n",
            "Iteration 179, loss = 0.37240448\n",
            "Iteration 180, loss = 0.37230631\n",
            "Iteration 181, loss = 0.37226668\n",
            "Iteration 182, loss = 0.37196552\n",
            "Iteration 183, loss = 0.37185268\n",
            "Iteration 184, loss = 0.37147423\n",
            "Iteration 185, loss = 0.37142705\n",
            "Iteration 186, loss = 0.37109199\n",
            "Iteration 187, loss = 0.37077621\n",
            "Iteration 188, loss = 0.37059849\n",
            "Iteration 189, loss = 0.37042267\n",
            "Iteration 190, loss = 0.37034273\n",
            "Iteration 191, loss = 0.37015309\n",
            "Iteration 192, loss = 0.36992496\n",
            "Iteration 193, loss = 0.36967122\n",
            "Iteration 194, loss = 0.36951204\n",
            "Iteration 195, loss = 0.36934798\n",
            "Iteration 196, loss = 0.36921853\n",
            "Iteration 197, loss = 0.36890403\n",
            "Iteration 198, loss = 0.36866196\n",
            "Iteration 199, loss = 0.36879122\n",
            "Iteration 200, loss = 0.36856464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72660557\n",
            "Iteration 3, loss = 0.70681795\n",
            "Iteration 4, loss = 0.68504117\n",
            "Iteration 5, loss = 0.66956648\n",
            "Iteration 6, loss = 0.65787513\n",
            "Iteration 7, loss = 0.64221320\n",
            "Iteration 8, loss = 0.63655021\n",
            "Iteration 9, loss = 0.62619089\n",
            "Iteration 10, loss = 0.67787264\n",
            "Iteration 11, loss = 0.66500257\n",
            "Iteration 12, loss = 0.65743148\n",
            "Iteration 13, loss = 0.65155179\n",
            "Iteration 14, loss = 0.64688081\n",
            "Iteration 15, loss = 0.64232991\n",
            "Iteration 16, loss = 0.63884577\n",
            "Iteration 17, loss = 0.63575556\n",
            "Iteration 18, loss = 0.63295673\n",
            "Iteration 19, loss = 0.63022894\n",
            "Iteration 20, loss = 0.62719663\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 21, loss = 0.62532692\n",
            "Iteration 22, loss = 0.62475700\n",
            "Iteration 23, loss = 0.62443781\n",
            "Iteration 24, loss = 0.62398959\n",
            "Iteration 25, loss = 0.62354613\n",
            "Iteration 26, loss = 0.62322965\n",
            "Iteration 27, loss = 0.62251386\n",
            "Iteration 28, loss = 0.62235771\n",
            "Iteration 29, loss = 0.62177314\n",
            "Iteration 30, loss = 0.62157906\n",
            "Iteration 31, loss = 0.62115658\n",
            "Iteration 32, loss = 0.62073806\n",
            "Iteration 33, loss = 0.62035099\n",
            "Iteration 34, loss = 0.61994080\n",
            "Iteration 35, loss = 0.61969561\n",
            "Iteration 36, loss = 0.61927428\n",
            "Iteration 37, loss = 0.61922778\n",
            "Iteration 38, loss = 0.61864703\n",
            "Iteration 39, loss = 0.61853872\n",
            "Iteration 40, loss = 0.61808727\n",
            "Iteration 41, loss = 0.61780254\n",
            "Iteration 42, loss = 0.61748917\n",
            "Iteration 43, loss = 0.61729439\n",
            "Iteration 44, loss = 0.61686331\n",
            "Iteration 45, loss = 0.61676317\n",
            "Iteration 46, loss = 0.61623534\n",
            "Iteration 47, loss = 0.61619354\n",
            "Iteration 48, loss = 0.61586725\n",
            "Iteration 49, loss = 0.61562672\n",
            "Iteration 50, loss = 0.61510407\n",
            "Iteration 51, loss = 0.61507866\n",
            "Iteration 52, loss = 0.61490934\n",
            "Iteration 53, loss = 0.61459329\n",
            "Iteration 54, loss = 0.61437482\n",
            "Iteration 55, loss = 0.61414311\n",
            "Iteration 56, loss = 0.61384282\n",
            "Iteration 57, loss = 0.61348806\n",
            "Iteration 58, loss = 0.61351553\n",
            "Iteration 59, loss = 0.61315135\n",
            "Iteration 60, loss = 0.61322334\n",
            "Iteration 61, loss = 0.61274649\n",
            "Iteration 62, loss = 0.61259115\n",
            "Iteration 63, loss = 0.61228828\n",
            "Iteration 64, loss = 0.61201058\n",
            "Iteration 65, loss = 0.61198831\n",
            "Iteration 66, loss = 0.61169299\n",
            "Iteration 67, loss = 0.61139767\n",
            "Iteration 68, loss = 0.61095300\n",
            "Iteration 69, loss = 0.60677804\n",
            "Iteration 70, loss = 0.60006411\n",
            "Iteration 71, loss = 0.59584391\n",
            "Iteration 72, loss = 0.59407257\n",
            "Iteration 73, loss = 0.59282371\n",
            "Iteration 74, loss = 0.59180338\n",
            "Iteration 75, loss = 0.59110906\n",
            "Iteration 76, loss = 0.58894160\n",
            "Iteration 77, loss = 0.58793102\n",
            "Iteration 78, loss = 0.58784062\n",
            "Iteration 79, loss = 0.58584799\n",
            "Iteration 80, loss = 0.58503233\n",
            "Iteration 81, loss = 0.58393455\n",
            "Iteration 82, loss = 0.58246648\n",
            "Iteration 83, loss = 0.58105789\n",
            "Iteration 84, loss = 0.57969956\n",
            "Iteration 85, loss = 0.57836257\n",
            "Iteration 86, loss = 0.57613525\n",
            "Iteration 87, loss = 0.57543793\n",
            "Iteration 88, loss = 0.57374901\n",
            "Iteration 89, loss = 0.57208632\n",
            "Iteration 90, loss = 0.57164234\n",
            "Iteration 91, loss = 0.57098308\n",
            "Iteration 92, loss = 0.56901886\n",
            "Iteration 93, loss = 0.56779833\n",
            "Iteration 94, loss = 0.56432812\n",
            "Iteration 95, loss = 0.56376518\n",
            "Iteration 96, loss = 0.56219117\n",
            "Iteration 97, loss = 0.56061331\n",
            "Iteration 98, loss = 0.56030787\n",
            "Iteration 99, loss = 0.55662234\n",
            "Iteration 100, loss = 0.55939063\n",
            "Iteration 101, loss = 0.55557831\n",
            "Iteration 102, loss = 0.55434264\n",
            "Iteration 103, loss = 0.55181644\n",
            "Iteration 104, loss = 0.55312855\n",
            "Iteration 105, loss = 0.54865852\n",
            "Iteration 106, loss = 0.55300367\n",
            "Iteration 107, loss = 0.55626792\n",
            "Iteration 108, loss = 0.55276533\n",
            "Iteration 109, loss = 0.55222798\n",
            "Iteration 110, loss = 0.55083078\n",
            "Iteration 111, loss = 0.54907673\n",
            "Iteration 112, loss = 0.55374306\n",
            "Iteration 113, loss = 0.55216027\n",
            "Iteration 114, loss = 0.70466822\n",
            "Iteration 115, loss = 0.69266485\n",
            "Iteration 116, loss = 0.68279999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 117, loss = 0.67746797\n",
            "Iteration 118, loss = 0.67593851\n",
            "Iteration 119, loss = 0.67448093\n",
            "Iteration 120, loss = 0.67308478\n",
            "Iteration 121, loss = 0.67174229\n",
            "Iteration 122, loss = 0.67044870\n",
            "Iteration 123, loss = 0.66920795\n",
            "Iteration 124, loss = 0.66801585\n",
            "Iteration 125, loss = 0.66687343\n",
            "Iteration 126, loss = 0.66577418\n",
            "Iteration 127, loss = 0.66471533\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 128, loss = 0.66406328\n",
            "Iteration 129, loss = 0.66386016\n",
            "Iteration 130, loss = 0.66366043\n",
            "Iteration 131, loss = 0.66346262\n",
            "Iteration 132, loss = 0.66326572\n",
            "Iteration 133, loss = 0.66307054\n",
            "Iteration 134, loss = 0.66287700\n",
            "Iteration 135, loss = 0.66268546\n",
            "Iteration 136, loss = 0.66249531\n",
            "Iteration 137, loss = 0.66230648\n",
            "Iteration 138, loss = 0.66211917\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 139, loss = 0.66200157\n",
            "Iteration 140, loss = 0.66196428\n",
            "Iteration 141, loss = 0.66192720\n",
            "Iteration 142, loss = 0.66189023\n",
            "Iteration 143, loss = 0.66185330\n",
            "Iteration 144, loss = 0.66181637\n",
            "Iteration 145, loss = 0.66177958\n",
            "Iteration 146, loss = 0.66174289\n",
            "Iteration 147, loss = 0.66170608\n",
            "Iteration 148, loss = 0.66166956\n",
            "Iteration 149, loss = 0.66163293\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 150, loss = 0.66160977\n",
            "Iteration 151, loss = 0.66160242\n",
            "Iteration 152, loss = 0.66159512\n",
            "Iteration 153, loss = 0.66158779\n",
            "Iteration 154, loss = 0.66158049\n",
            "Iteration 155, loss = 0.66157320\n",
            "Iteration 156, loss = 0.66156594\n",
            "Iteration 157, loss = 0.66155864\n",
            "Iteration 158, loss = 0.66155133\n",
            "Iteration 159, loss = 0.66154403\n",
            "Iteration 160, loss = 0.66153675\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69310640\n",
            "Iteration 3, loss = 0.67544714\n",
            "Iteration 4, loss = 0.66264748\n",
            "Iteration 5, loss = 0.64947399\n",
            "Iteration 6, loss = 0.63949090\n",
            "Iteration 7, loss = 0.63105547\n",
            "Iteration 8, loss = 0.62314614\n",
            "Iteration 9, loss = 0.61747911\n",
            "Iteration 10, loss = 0.61037166\n",
            "Iteration 11, loss = 0.60433283\n",
            "Iteration 12, loss = 0.60262942\n",
            "Iteration 13, loss = 0.59667849\n",
            "Iteration 14, loss = 0.59392034\n",
            "Iteration 15, loss = 0.59157528\n",
            "Iteration 16, loss = 0.58744508\n",
            "Iteration 17, loss = 0.58857886\n",
            "Iteration 18, loss = 0.59011441\n",
            "Iteration 19, loss = 0.58716596\n",
            "Iteration 20, loss = 0.59160912\n",
            "Iteration 21, loss = 0.58463995\n",
            "Iteration 22, loss = 0.58416262\n",
            "Iteration 23, loss = 0.58214514\n",
            "Iteration 24, loss = 0.58209476\n",
            "Iteration 25, loss = 0.58221378\n",
            "Iteration 26, loss = 0.57904158\n",
            "Iteration 27, loss = 0.57917841\n",
            "Iteration 28, loss = 0.58161009\n",
            "Iteration 29, loss = 0.57636717\n",
            "Iteration 30, loss = 0.57719199\n",
            "Iteration 31, loss = 0.57920387\n",
            "Iteration 32, loss = 0.57741337\n",
            "Iteration 33, loss = 0.57709096\n",
            "Iteration 34, loss = 0.57777843\n",
            "Iteration 35, loss = 0.57490294\n",
            "Iteration 36, loss = 0.57895858\n",
            "Iteration 37, loss = 0.57474088\n",
            "Iteration 38, loss = 0.57528268\n",
            "Iteration 39, loss = 0.57419159\n",
            "Iteration 40, loss = 0.57226384\n",
            "Iteration 41, loss = 0.57680294\n",
            "Iteration 42, loss = 0.59601806\n",
            "Iteration 43, loss = 0.62557252\n",
            "Iteration 44, loss = 0.61246997\n",
            "Iteration 45, loss = 0.60143563\n",
            "Iteration 46, loss = 0.59947559\n",
            "Iteration 47, loss = 0.59243204\n",
            "Iteration 48, loss = 0.58789301\n",
            "Iteration 49, loss = 0.58624912\n",
            "Iteration 50, loss = 0.58560529\n",
            "Iteration 51, loss = 0.58085007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.56630976\n",
            "Iteration 53, loss = 0.56381865\n",
            "Iteration 54, loss = 0.56192674\n",
            "Iteration 55, loss = 0.55972948\n",
            "Iteration 56, loss = 0.55767361\n",
            "Iteration 57, loss = 0.55651198\n",
            "Iteration 58, loss = 0.55432950\n",
            "Iteration 59, loss = 0.55182606\n",
            "Iteration 60, loss = 0.55016584\n",
            "Iteration 61, loss = 0.54792589\n",
            "Iteration 62, loss = 0.54625597\n",
            "Iteration 63, loss = 0.54411248\n",
            "Iteration 64, loss = 0.54264432\n",
            "Iteration 65, loss = 0.54128298\n",
            "Iteration 66, loss = 0.53735643\n",
            "Iteration 67, loss = 0.53715188\n",
            "Iteration 68, loss = 0.53738080\n",
            "Iteration 69, loss = 0.53556409\n",
            "Iteration 70, loss = 0.53737046\n",
            "Iteration 71, loss = 0.53703806\n",
            "Iteration 72, loss = 0.54616288\n",
            "Iteration 73, loss = 0.53936616\n",
            "Iteration 74, loss = 0.54223706\n",
            "Iteration 75, loss = 0.54027008\n",
            "Iteration 76, loss = 0.53797813\n",
            "Iteration 77, loss = 0.53780578\n",
            "Iteration 78, loss = 0.54176243\n",
            "Iteration 79, loss = 0.54488925\n",
            "Iteration 80, loss = 0.53752715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.49536909\n",
            "Iteration 82, loss = 0.49298506\n",
            "Iteration 83, loss = 0.49124249\n",
            "Iteration 84, loss = 0.49005614\n",
            "Iteration 85, loss = 0.48799395\n",
            "Iteration 86, loss = 0.48696739\n",
            "Iteration 87, loss = 0.48531994\n",
            "Iteration 88, loss = 0.48343957\n",
            "Iteration 89, loss = 0.48221131\n",
            "Iteration 90, loss = 0.48041688\n",
            "Iteration 91, loss = 0.47888523\n",
            "Iteration 92, loss = 0.47718047\n",
            "Iteration 93, loss = 0.47540791\n",
            "Iteration 94, loss = 0.47393296\n",
            "Iteration 95, loss = 0.47240635\n",
            "Iteration 96, loss = 0.47071636\n",
            "Iteration 97, loss = 0.46927211\n",
            "Iteration 98, loss = 0.46758167\n",
            "Iteration 99, loss = 0.46587450\n",
            "Iteration 100, loss = 0.46426105\n",
            "Iteration 101, loss = 0.46282108\n",
            "Iteration 102, loss = 0.46099141\n",
            "Iteration 103, loss = 0.45922073\n",
            "Iteration 104, loss = 0.45715963\n",
            "Iteration 105, loss = 0.45594405\n",
            "Iteration 106, loss = 0.45457348\n",
            "Iteration 107, loss = 0.45298041\n",
            "Iteration 108, loss = 0.45164359\n",
            "Iteration 109, loss = 0.45043830\n",
            "Iteration 110, loss = 0.44877926\n",
            "Iteration 111, loss = 0.44756725\n",
            "Iteration 112, loss = 0.44536713\n",
            "Iteration 113, loss = 0.44355827\n",
            "Iteration 114, loss = 0.44217582\n",
            "Iteration 115, loss = 0.44068074\n",
            "Iteration 116, loss = 0.43965302\n",
            "Iteration 117, loss = 0.43770987\n",
            "Iteration 118, loss = 0.43649141\n",
            "Iteration 119, loss = 0.43569714\n",
            "Iteration 120, loss = 0.43404946\n",
            "Iteration 121, loss = 0.43251632\n",
            "Iteration 122, loss = 0.43045184\n",
            "Iteration 123, loss = 0.43002181\n",
            "Iteration 124, loss = 0.42825550\n",
            "Iteration 125, loss = 0.42647779\n",
            "Iteration 126, loss = 0.42428874\n",
            "Iteration 127, loss = 0.42433995\n",
            "Iteration 128, loss = 0.42221502\n",
            "Iteration 129, loss = 0.42129874\n",
            "Iteration 130, loss = 0.41953207\n",
            "Iteration 131, loss = 0.42133412\n",
            "Iteration 132, loss = 0.41836746\n",
            "Iteration 133, loss = 0.41624175\n",
            "Iteration 134, loss = 0.41505776\n",
            "Iteration 135, loss = 0.41516145\n",
            "Iteration 136, loss = 0.41427343\n",
            "Iteration 137, loss = 0.41348585\n",
            "Iteration 138, loss = 0.41090879\n",
            "Iteration 139, loss = 0.40985586\n",
            "Iteration 140, loss = 0.41056064\n",
            "Iteration 141, loss = 0.40982517\n",
            "Iteration 142, loss = 0.40945241\n",
            "Iteration 143, loss = 0.40858957\n",
            "Iteration 144, loss = 0.40715841\n",
            "Iteration 145, loss = 0.41051958\n",
            "Iteration 146, loss = 0.40270266\n",
            "Iteration 147, loss = 0.40100448\n",
            "Iteration 148, loss = 0.40415381\n",
            "Iteration 149, loss = 0.40396409\n",
            "Iteration 150, loss = 0.40467441\n",
            "Iteration 151, loss = 0.40358799\n",
            "Iteration 152, loss = 0.40212391\n",
            "Iteration 153, loss = 0.39979076\n",
            "Iteration 154, loss = 0.40275412\n",
            "Iteration 155, loss = 0.39991354\n",
            "Iteration 156, loss = 0.40101216\n",
            "Iteration 157, loss = 0.40287155\n",
            "Iteration 158, loss = 0.40097020\n",
            "Iteration 159, loss = 0.40110963\n",
            "Iteration 160, loss = 0.39717449\n",
            "Iteration 161, loss = 0.39923954\n",
            "Iteration 162, loss = 0.39544021\n",
            "Iteration 163, loss = 0.39633689\n",
            "Iteration 164, loss = 0.40258957\n",
            "Iteration 165, loss = 0.39840481\n",
            "Iteration 166, loss = 0.40208577\n",
            "Iteration 167, loss = 0.39715655\n",
            "Iteration 168, loss = 0.39759701\n",
            "Iteration 169, loss = 0.39779082\n",
            "Iteration 170, loss = 0.40154767\n",
            "Iteration 171, loss = 0.39608314\n",
            "Iteration 172, loss = 0.40241379\n",
            "Iteration 173, loss = 0.38796717\n",
            "Iteration 174, loss = 0.39838327\n",
            "Iteration 175, loss = 0.40280370\n",
            "Iteration 176, loss = 0.40046709\n",
            "Iteration 177, loss = 0.39660552\n",
            "Iteration 178, loss = 0.40325387\n",
            "Iteration 179, loss = 0.39717604\n",
            "Iteration 180, loss = 0.38970508\n",
            "Iteration 181, loss = 0.39409195\n",
            "Iteration 182, loss = 0.39462691\n",
            "Iteration 183, loss = 0.40541702\n",
            "Iteration 184, loss = 0.39628403\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 185, loss = 0.35936707\n",
            "Iteration 186, loss = 0.35895660\n",
            "Iteration 187, loss = 0.35865628\n",
            "Iteration 188, loss = 0.35835517\n",
            "Iteration 189, loss = 0.35800012\n",
            "Iteration 190, loss = 0.35800957\n",
            "Iteration 191, loss = 0.35772368\n",
            "Iteration 192, loss = 0.35764030\n",
            "Iteration 193, loss = 0.35739527\n",
            "Iteration 194, loss = 0.35724763\n",
            "Iteration 195, loss = 0.35700652\n",
            "Iteration 196, loss = 0.35689050\n",
            "Iteration 197, loss = 0.35680782\n",
            "Iteration 198, loss = 0.35653417\n",
            "Iteration 199, loss = 0.35623985\n",
            "Iteration 200, loss = 0.35612222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68488899\n",
            "Iteration 3, loss = 0.67095953\n",
            "Iteration 4, loss = 0.65662080\n",
            "Iteration 5, loss = 0.64688853\n",
            "Iteration 6, loss = 0.63760439\n",
            "Iteration 7, loss = 0.62736548\n",
            "Iteration 8, loss = 0.62514379\n",
            "Iteration 9, loss = 0.61586486\n",
            "Iteration 10, loss = 0.61275702\n",
            "Iteration 11, loss = 0.60433055\n",
            "Iteration 12, loss = 0.60409426\n",
            "Iteration 13, loss = 0.60028090\n",
            "Iteration 14, loss = 0.59501931\n",
            "Iteration 15, loss = 0.59473262\n",
            "Iteration 16, loss = 0.59029563\n",
            "Iteration 17, loss = 0.58747933\n",
            "Iteration 18, loss = 0.58581605\n",
            "Iteration 19, loss = 0.58651139\n",
            "Iteration 20, loss = 0.58072274\n",
            "Iteration 21, loss = 0.58343006\n",
            "Iteration 22, loss = 0.57932564\n",
            "Iteration 23, loss = 0.57939603\n",
            "Iteration 24, loss = 0.57687998\n",
            "Iteration 25, loss = 0.57507355\n",
            "Iteration 26, loss = 0.57886106\n",
            "Iteration 27, loss = 0.57853605\n",
            "Iteration 28, loss = 0.58173748\n",
            "Iteration 29, loss = 0.57546137\n",
            "Iteration 30, loss = 0.57847504\n",
            "Iteration 31, loss = 0.57442910\n",
            "Iteration 32, loss = 0.57520789\n",
            "Iteration 33, loss = 0.57507316\n",
            "Iteration 34, loss = 0.57269442\n",
            "Iteration 35, loss = 0.57302393\n",
            "Iteration 36, loss = 0.57202423\n",
            "Iteration 37, loss = 0.57096420\n",
            "Iteration 38, loss = 0.57248609\n",
            "Iteration 39, loss = 0.57359815\n",
            "Iteration 40, loss = 0.57401801\n",
            "Iteration 41, loss = 0.57174324\n",
            "Iteration 42, loss = 0.57457442\n",
            "Iteration 43, loss = 0.57248424\n",
            "Iteration 44, loss = 0.57051468\n",
            "Iteration 45, loss = 0.57380805\n",
            "Iteration 46, loss = 0.57053230\n",
            "Iteration 47, loss = 0.57067818\n",
            "Iteration 48, loss = 0.56959375\n",
            "Iteration 49, loss = 0.57173612\n",
            "Iteration 50, loss = 0.57269388\n",
            "Iteration 51, loss = 0.57274790\n",
            "Iteration 52, loss = 0.57146034\n",
            "Iteration 53, loss = 0.56632588\n",
            "Iteration 54, loss = 0.57241557\n",
            "Iteration 55, loss = 0.57731787\n",
            "Iteration 56, loss = 0.57384024\n",
            "Iteration 57, loss = 0.57424393\n",
            "Iteration 58, loss = 0.57494805\n",
            "Iteration 59, loss = 0.57278786\n",
            "Iteration 60, loss = 0.57086517\n",
            "Iteration 61, loss = 0.57092011\n",
            "Iteration 62, loss = 0.57118490\n",
            "Iteration 63, loss = 0.57148186\n",
            "Iteration 64, loss = 0.57054912\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 65, loss = 0.53258271\n",
            "Iteration 66, loss = 0.51920705\n",
            "Iteration 67, loss = 0.51210314\n",
            "Iteration 68, loss = 0.50565384\n",
            "Iteration 69, loss = 0.49827094\n",
            "Iteration 70, loss = 0.49429373\n",
            "Iteration 71, loss = 0.48895189\n",
            "Iteration 72, loss = 0.49036591\n",
            "Iteration 73, loss = 0.49459321\n",
            "Iteration 74, loss = 0.50427236\n",
            "Iteration 75, loss = 0.52244782\n",
            "Iteration 76, loss = 0.51438873\n",
            "Iteration 77, loss = 0.51301300\n",
            "Iteration 78, loss = 0.51609295\n",
            "Iteration 79, loss = 0.52018928\n",
            "Iteration 80, loss = 0.51782146\n",
            "Iteration 81, loss = 0.52222651\n",
            "Iteration 82, loss = 0.52359071\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 83, loss = 0.45018981\n",
            "Iteration 84, loss = 0.44653211\n",
            "Iteration 85, loss = 0.44428119\n",
            "Iteration 86, loss = 0.44174467\n",
            "Iteration 87, loss = 0.43996922\n",
            "Iteration 88, loss = 0.43780189\n",
            "Iteration 89, loss = 0.43544004\n",
            "Iteration 90, loss = 0.43330645\n",
            "Iteration 91, loss = 0.43101865\n",
            "Iteration 92, loss = 0.42880555\n",
            "Iteration 93, loss = 0.42660080\n",
            "Iteration 94, loss = 0.42447938\n",
            "Iteration 95, loss = 0.42268521\n",
            "Iteration 96, loss = 0.42019808\n",
            "Iteration 97, loss = 0.41887657\n",
            "Iteration 98, loss = 0.41669470\n",
            "Iteration 99, loss = 0.41424123\n",
            "Iteration 100, loss = 0.41271305\n",
            "Iteration 101, loss = 0.41050691\n",
            "Iteration 102, loss = 0.40872039\n",
            "Iteration 103, loss = 0.40605130\n",
            "Iteration 104, loss = 0.40450604\n",
            "Iteration 105, loss = 0.40272353\n",
            "Iteration 106, loss = 0.40086100\n",
            "Iteration 107, loss = 0.39872616\n",
            "Iteration 108, loss = 0.39694705\n",
            "Iteration 109, loss = 0.39534158\n",
            "Iteration 110, loss = 0.39396903\n",
            "Iteration 111, loss = 0.39183215\n",
            "Iteration 112, loss = 0.39001960\n",
            "Iteration 113, loss = 0.38873681\n",
            "Iteration 114, loss = 0.38672061\n",
            "Iteration 115, loss = 0.38521126\n",
            "Iteration 116, loss = 0.38335386\n",
            "Iteration 117, loss = 0.38155128\n",
            "Iteration 118, loss = 0.37999547\n",
            "Iteration 119, loss = 0.37882414\n",
            "Iteration 120, loss = 0.37686517\n",
            "Iteration 121, loss = 0.37530699\n",
            "Iteration 122, loss = 0.37421421\n",
            "Iteration 123, loss = 0.37230021\n",
            "Iteration 124, loss = 0.37118183\n",
            "Iteration 125, loss = 0.36967740\n",
            "Iteration 126, loss = 0.36835232\n",
            "Iteration 127, loss = 0.36650902\n",
            "Iteration 128, loss = 0.36529988\n",
            "Iteration 129, loss = 0.36405627\n",
            "Iteration 130, loss = 0.36285427\n",
            "Iteration 131, loss = 0.36115225\n",
            "Iteration 132, loss = 0.36043020\n",
            "Iteration 133, loss = 0.35967636\n",
            "Iteration 134, loss = 0.35696556\n",
            "Iteration 135, loss = 0.35665242\n",
            "Iteration 136, loss = 0.35640928\n",
            "Iteration 137, loss = 0.35448338\n",
            "Iteration 138, loss = 0.35308742\n",
            "Iteration 139, loss = 0.35220960\n",
            "Iteration 140, loss = 0.35069461\n",
            "Iteration 141, loss = 0.34938587\n",
            "Iteration 142, loss = 0.33784859\n",
            "Iteration 143, loss = 0.33403224\n",
            "Iteration 144, loss = 0.33321463\n",
            "Iteration 145, loss = 0.33142236\n",
            "Iteration 146, loss = 0.32984731\n",
            "Iteration 147, loss = 0.32866332\n",
            "Iteration 148, loss = 0.32745421\n",
            "Iteration 149, loss = 0.32778408\n",
            "Iteration 150, loss = 0.32637294\n",
            "Iteration 151, loss = 0.32408427\n",
            "Iteration 152, loss = 0.32252290\n",
            "Iteration 153, loss = 0.32247279\n",
            "Iteration 154, loss = 0.32040511\n",
            "Iteration 155, loss = 0.32051416\n",
            "Iteration 156, loss = 0.31897744\n",
            "Iteration 157, loss = 0.31637751\n",
            "Iteration 158, loss = 0.31788487\n",
            "Iteration 159, loss = 0.31917419\n",
            "Iteration 160, loss = 0.31581191\n",
            "Iteration 161, loss = 0.31522631\n",
            "Iteration 162, loss = 0.31202248\n",
            "Iteration 163, loss = 0.31080338\n",
            "Iteration 164, loss = 0.31163912\n",
            "Iteration 165, loss = 0.31004815\n",
            "Iteration 166, loss = 0.31120438\n",
            "Iteration 167, loss = 0.30741067\n",
            "Iteration 168, loss = 0.30976923\n",
            "Iteration 169, loss = 0.30971808\n",
            "Iteration 170, loss = 0.30820699\n",
            "Iteration 171, loss = 0.30750200\n",
            "Iteration 172, loss = 0.30783654\n",
            "Iteration 173, loss = 0.30384488\n",
            "Iteration 174, loss = 0.30346162\n",
            "Iteration 175, loss = 0.30321645\n",
            "Iteration 176, loss = 0.30348376\n",
            "Iteration 177, loss = 0.30335770\n",
            "Iteration 178, loss = 0.30119552\n",
            "Iteration 179, loss = 0.30519830\n",
            "Iteration 180, loss = 0.29963780\n",
            "Iteration 181, loss = 0.29892065\n",
            "Iteration 182, loss = 0.30183146\n",
            "Iteration 183, loss = 0.30122802\n",
            "Iteration 184, loss = 0.29898731\n",
            "Iteration 185, loss = 0.29701368\n",
            "Iteration 186, loss = 0.29610540\n",
            "Iteration 187, loss = 0.29659555\n",
            "Iteration 188, loss = 0.30158843\n",
            "Iteration 189, loss = 0.29771644\n",
            "Iteration 190, loss = 0.30074308\n",
            "Iteration 191, loss = 0.29324930\n",
            "Iteration 192, loss = 0.29682928\n",
            "Iteration 193, loss = 0.29937268\n",
            "Iteration 194, loss = 0.29460122\n",
            "Iteration 195, loss = 0.29910644\n",
            "Iteration 196, loss = 0.29486369\n",
            "Iteration 197, loss = 0.29654931\n",
            "Iteration 198, loss = 0.29380902\n",
            "Iteration 199, loss = 0.30851934\n",
            "Iteration 200, loss = 0.29595210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67664654\n",
            "Iteration 3, loss = 0.65955560\n",
            "Iteration 4, loss = 0.65038723\n",
            "Iteration 5, loss = 0.63999753\n",
            "Iteration 6, loss = 0.63263950\n",
            "Iteration 7, loss = 0.62212629\n",
            "Iteration 8, loss = 0.61727975\n",
            "Iteration 9, loss = 0.61090503\n",
            "Iteration 10, loss = 0.60430133\n",
            "Iteration 11, loss = 0.60154639\n",
            "Iteration 12, loss = 0.59507243\n",
            "Iteration 13, loss = 0.59508931\n",
            "Iteration 14, loss = 0.58966118\n",
            "Iteration 15, loss = 0.59125945\n",
            "Iteration 16, loss = 0.58662800\n",
            "Iteration 17, loss = 0.58400915\n",
            "Iteration 18, loss = 0.58092598\n",
            "Iteration 19, loss = 0.58244364\n",
            "Iteration 20, loss = 0.57964476\n",
            "Iteration 21, loss = 0.57620044\n",
            "Iteration 22, loss = 0.58064749\n",
            "Iteration 23, loss = 0.57756083\n",
            "Iteration 24, loss = 0.57559293\n",
            "Iteration 25, loss = 0.57798790\n",
            "Iteration 26, loss = 0.57491366\n",
            "Iteration 27, loss = 0.57450307\n",
            "Iteration 28, loss = 0.57310429\n",
            "Iteration 29, loss = 0.57413884\n",
            "Iteration 30, loss = 0.57200851\n",
            "Iteration 31, loss = 0.57243196\n",
            "Iteration 32, loss = 0.56948196\n",
            "Iteration 33, loss = 0.56902466\n",
            "Iteration 34, loss = 0.57187294\n",
            "Iteration 35, loss = 0.57170929\n",
            "Iteration 36, loss = 0.57071224\n",
            "Iteration 37, loss = 0.57428939\n",
            "Iteration 38, loss = 0.56872062\n",
            "Iteration 39, loss = 0.57141760\n",
            "Iteration 40, loss = 0.56984436\n",
            "Iteration 41, loss = 0.56896430\n",
            "Iteration 42, loss = 0.57102419\n",
            "Iteration 43, loss = 0.56977743\n",
            "Iteration 44, loss = 0.57249450\n",
            "Iteration 45, loss = 0.57009322\n",
            "Iteration 46, loss = 0.56565979\n",
            "Iteration 47, loss = 0.57315215\n",
            "Iteration 48, loss = 0.56862218\n",
            "Iteration 49, loss = 0.57007750\n",
            "Iteration 50, loss = 0.56930612\n",
            "Iteration 51, loss = 0.57392312\n",
            "Iteration 52, loss = 0.57246057\n",
            "Iteration 53, loss = 0.56935391\n",
            "Iteration 54, loss = 0.56968818\n",
            "Iteration 55, loss = 0.57010080\n",
            "Iteration 56, loss = 0.56941966\n",
            "Iteration 57, loss = 0.57070679\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 58, loss = 0.52839755\n",
            "Iteration 59, loss = 0.51539466\n",
            "Iteration 60, loss = 0.50776719\n",
            "Iteration 61, loss = 0.50047619\n",
            "Iteration 62, loss = 0.49447509\n",
            "Iteration 63, loss = 0.48695695\n",
            "Iteration 64, loss = 0.48691124\n",
            "Iteration 65, loss = 0.48488940\n",
            "Iteration 66, loss = 0.49999632\n",
            "Iteration 67, loss = 0.49893095\n",
            "Iteration 68, loss = 0.50273697\n",
            "Iteration 69, loss = 0.52191650\n",
            "Iteration 70, loss = 0.51788998\n",
            "Iteration 71, loss = 0.50124181\n",
            "Iteration 72, loss = 0.52795118\n",
            "Iteration 73, loss = 0.50825594\n",
            "Iteration 74, loss = 0.51121707\n",
            "Iteration 75, loss = 0.52398851\n",
            "Iteration 76, loss = 0.51189654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 77, loss = 0.44445244\n",
            "Iteration 78, loss = 0.44155019\n",
            "Iteration 79, loss = 0.43944186\n",
            "Iteration 80, loss = 0.43748589\n",
            "Iteration 81, loss = 0.43493970\n",
            "Iteration 82, loss = 0.43298450\n",
            "Iteration 83, loss = 0.43058031\n",
            "Iteration 84, loss = 0.42870225\n",
            "Iteration 85, loss = 0.42686269\n",
            "Iteration 86, loss = 0.42476510\n",
            "Iteration 87, loss = 0.42238066\n",
            "Iteration 88, loss = 0.42031698\n",
            "Iteration 89, loss = 0.41827545\n",
            "Iteration 90, loss = 0.41630055\n",
            "Iteration 91, loss = 0.41440813\n",
            "Iteration 92, loss = 0.41202462\n",
            "Iteration 93, loss = 0.41065489\n",
            "Iteration 94, loss = 0.40842542\n",
            "Iteration 95, loss = 0.40688149\n",
            "Iteration 96, loss = 0.40446841\n",
            "Iteration 97, loss = 0.40296967\n",
            "Iteration 98, loss = 0.40079980\n",
            "Iteration 99, loss = 0.39744152\n",
            "Iteration 100, loss = 0.38684504\n",
            "Iteration 101, loss = 0.38414099\n",
            "Iteration 102, loss = 0.38164729\n",
            "Iteration 103, loss = 0.37891222\n",
            "Iteration 104, loss = 0.37701571\n",
            "Iteration 105, loss = 0.37499795\n",
            "Iteration 106, loss = 0.37246232\n",
            "Iteration 107, loss = 0.37050664\n",
            "Iteration 108, loss = 0.36915247\n",
            "Iteration 109, loss = 0.36700110\n",
            "Iteration 110, loss = 0.36544203\n",
            "Iteration 111, loss = 0.36328849\n",
            "Iteration 112, loss = 0.36036132\n",
            "Iteration 113, loss = 0.35915854\n",
            "Iteration 114, loss = 0.35804160\n",
            "Iteration 115, loss = 0.35593202\n",
            "Iteration 116, loss = 0.35401675\n",
            "Iteration 117, loss = 0.35236873\n",
            "Iteration 118, loss = 0.35048415\n",
            "Iteration 119, loss = 0.34919767\n",
            "Iteration 120, loss = 0.34720283\n",
            "Iteration 121, loss = 0.34591463\n",
            "Iteration 122, loss = 0.34459980\n",
            "Iteration 123, loss = 0.34228044\n",
            "Iteration 124, loss = 0.34149470\n",
            "Iteration 125, loss = 0.34005297\n",
            "Iteration 126, loss = 0.33937194\n",
            "Iteration 127, loss = 0.33650270\n",
            "Iteration 128, loss = 0.33484151\n",
            "Iteration 129, loss = 0.33382630\n",
            "Iteration 130, loss = 0.33272410\n",
            "Iteration 131, loss = 0.33206549\n",
            "Iteration 132, loss = 0.33059256\n",
            "Iteration 133, loss = 0.32908105\n",
            "Iteration 134, loss = 0.32647915\n",
            "Iteration 135, loss = 0.32644595\n",
            "Iteration 136, loss = 0.32592781\n",
            "Iteration 137, loss = 0.32353399\n",
            "Iteration 138, loss = 0.32299485\n",
            "Iteration 139, loss = 0.32205971\n",
            "Iteration 140, loss = 0.32131705\n",
            "Iteration 141, loss = 0.31985103\n",
            "Iteration 142, loss = 0.31916127\n",
            "Iteration 143, loss = 0.31922065\n",
            "Iteration 144, loss = 0.31800121\n",
            "Iteration 145, loss = 0.31518553\n",
            "Iteration 146, loss = 0.31539965\n",
            "Iteration 147, loss = 0.31384728\n",
            "Iteration 148, loss = 0.31240866\n",
            "Iteration 149, loss = 0.31236478\n",
            "Iteration 150, loss = 0.31060610\n",
            "Iteration 151, loss = 0.31143035\n",
            "Iteration 152, loss = 0.30873337\n",
            "Iteration 153, loss = 0.30960263\n",
            "Iteration 154, loss = 0.30754663\n",
            "Iteration 155, loss = 0.30771564\n",
            "Iteration 156, loss = 0.30502153\n",
            "Iteration 157, loss = 0.30462466\n",
            "Iteration 158, loss = 0.30405728\n",
            "Iteration 159, loss = 0.30470236\n",
            "Iteration 160, loss = 0.30400404\n",
            "Iteration 161, loss = 0.30316252\n",
            "Iteration 162, loss = 0.30133902\n",
            "Iteration 163, loss = 0.30160005\n",
            "Iteration 164, loss = 0.30128468\n",
            "Iteration 165, loss = 0.30028846\n",
            "Iteration 166, loss = 0.29888069\n",
            "Iteration 167, loss = 0.30047016\n",
            "Iteration 168, loss = 0.29755597\n",
            "Iteration 169, loss = 0.30064490\n",
            "Iteration 170, loss = 0.29930479\n",
            "Iteration 171, loss = 0.29867145\n",
            "Iteration 172, loss = 0.29684394\n",
            "Iteration 173, loss = 0.29693949\n",
            "Iteration 174, loss = 0.29556141\n",
            "Iteration 175, loss = 0.29733342\n",
            "Iteration 176, loss = 0.29310793\n",
            "Iteration 177, loss = 0.29568155\n",
            "Iteration 178, loss = 0.29660564\n",
            "Iteration 179, loss = 0.29558032\n",
            "Iteration 180, loss = 0.29635959\n",
            "Iteration 181, loss = 0.29250957\n",
            "Iteration 182, loss = 0.29098544\n",
            "Iteration 183, loss = 0.30371917\n",
            "Iteration 184, loss = 0.29163139\n",
            "Iteration 185, loss = 0.29019552\n",
            "Iteration 186, loss = 0.29657155\n",
            "Iteration 187, loss = 0.29260735\n",
            "Iteration 188, loss = 0.28851899\n",
            "Iteration 189, loss = 0.29018943\n",
            "Iteration 190, loss = 0.29217036\n",
            "Iteration 191, loss = 0.29608950\n",
            "Iteration 192, loss = 0.28691551\n",
            "Iteration 193, loss = 0.28880794\n",
            "Iteration 194, loss = 0.29739907\n",
            "Iteration 195, loss = 0.29304549\n",
            "Iteration 196, loss = 0.28658413\n",
            "Iteration 197, loss = 0.29651552\n",
            "Iteration 198, loss = 0.28845803\n",
            "Iteration 199, loss = 0.30332101\n",
            "Iteration 200, loss = 0.29458024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70641937\n",
            "Iteration 3, loss = 0.68440265\n",
            "Iteration 4, loss = 0.66978842\n",
            "Iteration 5, loss = 0.66116987\n",
            "Iteration 6, loss = 0.65011744\n",
            "Iteration 7, loss = 0.64158182\n",
            "Iteration 8, loss = 0.63254018\n",
            "Iteration 9, loss = 0.62264028\n",
            "Iteration 10, loss = 0.62100659\n",
            "Iteration 11, loss = 0.62014518\n",
            "Iteration 12, loss = 0.60818022\n",
            "Iteration 13, loss = 0.60739527\n",
            "Iteration 14, loss = 0.60296966\n",
            "Iteration 15, loss = 0.60050179\n",
            "Iteration 16, loss = 0.59556704\n",
            "Iteration 17, loss = 0.59310172\n",
            "Iteration 18, loss = 0.59271973\n",
            "Iteration 19, loss = 0.59080826\n",
            "Iteration 20, loss = 0.58968239\n",
            "Iteration 21, loss = 0.58612840\n",
            "Iteration 22, loss = 0.58977699\n",
            "Iteration 23, loss = 0.58210521\n",
            "Iteration 24, loss = 0.58184917\n",
            "Iteration 25, loss = 0.58302035\n",
            "Iteration 26, loss = 0.58272959\n",
            "Iteration 27, loss = 0.60678576\n",
            "Iteration 28, loss = 0.62833829\n",
            "Iteration 29, loss = 0.61911141\n",
            "Iteration 30, loss = 0.60699620\n",
            "Iteration 31, loss = 0.59886698\n",
            "Iteration 32, loss = 0.59513470\n",
            "Iteration 33, loss = 0.59093090\n",
            "Iteration 34, loss = 0.58687316\n",
            "Iteration 35, loss = 0.58418961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.57012408\n",
            "Iteration 37, loss = 0.56880383\n",
            "Iteration 38, loss = 0.56745514\n",
            "Iteration 39, loss = 0.56633117\n",
            "Iteration 40, loss = 0.56456532\n",
            "Iteration 41, loss = 0.56286021\n",
            "Iteration 42, loss = 0.56160745\n",
            "Iteration 43, loss = 0.56007414\n",
            "Iteration 44, loss = 0.55786472\n",
            "Iteration 45, loss = 0.55670981\n",
            "Iteration 46, loss = 0.55412818\n",
            "Iteration 47, loss = 0.55240734\n",
            "Iteration 48, loss = 0.55092927\n",
            "Iteration 49, loss = 0.54895444\n",
            "Iteration 50, loss = 0.54740278\n",
            "Iteration 51, loss = 0.54531913\n",
            "Iteration 52, loss = 0.54279248\n",
            "Iteration 53, loss = 0.54008949\n",
            "Iteration 54, loss = 0.53898522\n",
            "Iteration 55, loss = 0.54120889\n",
            "Iteration 56, loss = 0.54226880\n",
            "Iteration 57, loss = 0.54394561\n",
            "Iteration 58, loss = 0.54544308\n",
            "Iteration 59, loss = 0.54252107\n",
            "Iteration 60, loss = 0.54412957\n",
            "Iteration 61, loss = 0.54672015\n",
            "Iteration 62, loss = 0.54147530\n",
            "Iteration 63, loss = 0.54731996\n",
            "Iteration 64, loss = 0.54338697\n",
            "Iteration 65, loss = 0.54636065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.50336132\n",
            "Iteration 67, loss = 0.50167790\n",
            "Iteration 68, loss = 0.50041078\n",
            "Iteration 69, loss = 0.49900284\n",
            "Iteration 70, loss = 0.49780579\n",
            "Iteration 71, loss = 0.49634811\n",
            "Iteration 72, loss = 0.49452820\n",
            "Iteration 73, loss = 0.49359490\n",
            "Iteration 74, loss = 0.49196848\n",
            "Iteration 75, loss = 0.49040883\n",
            "Iteration 76, loss = 0.48926817\n",
            "Iteration 77, loss = 0.48789495\n",
            "Iteration 78, loss = 0.48624236\n",
            "Iteration 79, loss = 0.48483197\n",
            "Iteration 80, loss = 0.48344471\n",
            "Iteration 81, loss = 0.48174981\n",
            "Iteration 82, loss = 0.48032079\n",
            "Iteration 83, loss = 0.47901844\n",
            "Iteration 84, loss = 0.47759683\n",
            "Iteration 85, loss = 0.47571573\n",
            "Iteration 86, loss = 0.47463288\n",
            "Iteration 87, loss = 0.47306711\n",
            "Iteration 88, loss = 0.47133062\n",
            "Iteration 89, loss = 0.47041876\n",
            "Iteration 90, loss = 0.46851653\n",
            "Iteration 91, loss = 0.46764344\n",
            "Iteration 92, loss = 0.46596527\n",
            "Iteration 93, loss = 0.46365855\n",
            "Iteration 94, loss = 0.46265039\n",
            "Iteration 95, loss = 0.46140977\n",
            "Iteration 96, loss = 0.45980798\n",
            "Iteration 97, loss = 0.45828225\n",
            "Iteration 98, loss = 0.45622701\n",
            "Iteration 99, loss = 0.45505061\n",
            "Iteration 100, loss = 0.45369215\n",
            "Iteration 101, loss = 0.45261716\n",
            "Iteration 102, loss = 0.45152133\n",
            "Iteration 103, loss = 0.44992980\n",
            "Iteration 104, loss = 0.44887081\n",
            "Iteration 105, loss = 0.44666093\n",
            "Iteration 106, loss = 0.44531381\n",
            "Iteration 107, loss = 0.44438687\n",
            "Iteration 108, loss = 0.44324547\n",
            "Iteration 109, loss = 0.44106565\n",
            "Iteration 110, loss = 0.44118098\n",
            "Iteration 111, loss = 0.43785581\n",
            "Iteration 112, loss = 0.43710566\n",
            "Iteration 113, loss = 0.43594168\n",
            "Iteration 114, loss = 0.43536383\n",
            "Iteration 115, loss = 0.43391820\n",
            "Iteration 116, loss = 0.43348533\n",
            "Iteration 117, loss = 0.43211236\n",
            "Iteration 118, loss = 0.43012943\n",
            "Iteration 119, loss = 0.42947258\n",
            "Iteration 120, loss = 0.42834123\n",
            "Iteration 121, loss = 0.42616920\n",
            "Iteration 122, loss = 0.42542522\n",
            "Iteration 123, loss = 0.42620234\n",
            "Iteration 124, loss = 0.42423745\n",
            "Iteration 125, loss = 0.42392516\n",
            "Iteration 126, loss = 0.42440119\n",
            "Iteration 127, loss = 0.42191189\n",
            "Iteration 128, loss = 0.42017956\n",
            "Iteration 129, loss = 0.42050087\n",
            "Iteration 130, loss = 0.41958256\n",
            "Iteration 131, loss = 0.41802070\n",
            "Iteration 132, loss = 0.41794584\n",
            "Iteration 133, loss = 0.41526630\n",
            "Iteration 134, loss = 0.41584956\n",
            "Iteration 135, loss = 0.41424965\n",
            "Iteration 136, loss = 0.41626808\n",
            "Iteration 137, loss = 0.41476623\n",
            "Iteration 138, loss = 0.41474204\n",
            "Iteration 139, loss = 0.41661454\n",
            "Iteration 140, loss = 0.40966653\n",
            "Iteration 141, loss = 0.41507416\n",
            "Iteration 142, loss = 0.41056800\n",
            "Iteration 143, loss = 0.41149683\n",
            "Iteration 144, loss = 0.40948544\n",
            "Iteration 145, loss = 0.40908716\n",
            "Iteration 146, loss = 0.41490422\n",
            "Iteration 147, loss = 0.40830813\n",
            "Iteration 148, loss = 0.40647504\n",
            "Iteration 149, loss = 0.40968277\n",
            "Iteration 150, loss = 0.40820755\n",
            "Iteration 151, loss = 0.40979938\n",
            "Iteration 152, loss = 0.40614228\n",
            "Iteration 153, loss = 0.40601570\n",
            "Iteration 154, loss = 0.40310573\n",
            "Iteration 155, loss = 0.40766703\n",
            "Iteration 156, loss = 0.40579992\n",
            "Iteration 157, loss = 0.40928105\n",
            "Iteration 158, loss = 0.41166731\n",
            "Iteration 159, loss = 0.40564771\n",
            "Iteration 160, loss = 0.39915506\n",
            "Iteration 161, loss = 0.41085749\n",
            "Iteration 162, loss = 0.40548251\n",
            "Iteration 163, loss = 0.39842686\n",
            "Iteration 164, loss = 0.40924181\n",
            "Iteration 165, loss = 0.40659202\n",
            "Iteration 166, loss = 0.40237988\n",
            "Iteration 167, loss = 0.40509083\n",
            "Iteration 168, loss = 0.39659088\n",
            "Iteration 169, loss = 0.40125890\n",
            "Iteration 170, loss = 0.40351375\n",
            "Iteration 171, loss = 0.40500436\n",
            "Iteration 172, loss = 0.40457597\n",
            "Iteration 173, loss = 0.40402168\n",
            "Iteration 174, loss = 0.39953275\n",
            "Iteration 175, loss = 0.40644650\n",
            "Iteration 176, loss = 0.40167222\n",
            "Iteration 177, loss = 0.40423770\n",
            "Iteration 178, loss = 0.40110252\n",
            "Iteration 179, loss = 0.40749191\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 180, loss = 0.36344516\n",
            "Iteration 181, loss = 0.36278796\n",
            "Iteration 182, loss = 0.36282491\n",
            "Iteration 183, loss = 0.36240145\n",
            "Iteration 184, loss = 0.36225700\n",
            "Iteration 185, loss = 0.36199598\n",
            "Iteration 186, loss = 0.36183704\n",
            "Iteration 187, loss = 0.36153715\n",
            "Iteration 188, loss = 0.36151338\n",
            "Iteration 189, loss = 0.36120315\n",
            "Iteration 190, loss = 0.36096503\n",
            "Iteration 191, loss = 0.36086426\n",
            "Iteration 192, loss = 0.36058930\n",
            "Iteration 193, loss = 0.36040830\n",
            "Iteration 194, loss = 0.36028697\n",
            "Iteration 195, loss = 0.36015594\n",
            "Iteration 196, loss = 0.36008299\n",
            "Iteration 197, loss = 0.35971314\n",
            "Iteration 198, loss = 0.35962130\n",
            "Iteration 199, loss = 0.35939433\n",
            "Iteration 200, loss = 0.35943309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.60374067\n",
            "Iteration 2, loss = 0.69546769\n",
            "Iteration 3, loss = 0.67885205\n",
            "Iteration 4, loss = 0.66610907\n",
            "Iteration 5, loss = 0.65326438\n",
            "Iteration 6, loss = 0.64345356\n",
            "Iteration 7, loss = 0.63328302\n",
            "Iteration 8, loss = 0.62782667\n",
            "Iteration 9, loss = 0.62120328\n",
            "Iteration 10, loss = 0.61599994\n",
            "Iteration 11, loss = 0.60826041\n",
            "Iteration 12, loss = 0.60640637\n",
            "Iteration 13, loss = 0.59937725\n",
            "Iteration 14, loss = 0.60114232\n",
            "Iteration 15, loss = 0.59674146\n",
            "Iteration 16, loss = 0.58935621\n",
            "Iteration 17, loss = 0.59093371\n",
            "Iteration 18, loss = 0.58413911\n",
            "Iteration 19, loss = 0.58347867\n",
            "Iteration 20, loss = 0.58330417\n",
            "Iteration 21, loss = 0.58190276\n",
            "Iteration 22, loss = 0.57953167\n",
            "Iteration 23, loss = 0.58440069\n",
            "Iteration 24, loss = 0.57810489\n",
            "Iteration 25, loss = 0.57683951\n",
            "Iteration 26, loss = 0.57740070\n",
            "Iteration 27, loss = 0.57696008\n",
            "Iteration 28, loss = 0.57744348\n",
            "Iteration 29, loss = 0.57552341\n",
            "Iteration 30, loss = 0.57553327\n",
            "Iteration 31, loss = 0.57734991\n",
            "Iteration 32, loss = 0.57517458\n",
            "Iteration 33, loss = 0.57204537\n",
            "Iteration 34, loss = 0.57023435\n",
            "Iteration 35, loss = 0.57589855\n",
            "Iteration 36, loss = 0.57164680\n",
            "Iteration 37, loss = 0.56946269\n",
            "Iteration 38, loss = 0.57264779\n",
            "Iteration 39, loss = 0.57365461\n",
            "Iteration 40, loss = 0.57041345\n",
            "Iteration 41, loss = 0.56989266\n",
            "Iteration 42, loss = 0.57416351\n",
            "Iteration 43, loss = 0.57006630\n",
            "Iteration 44, loss = 0.57766568\n",
            "Iteration 45, loss = 0.57031771\n",
            "Iteration 46, loss = 0.57458722\n",
            "Iteration 47, loss = 0.57177139\n",
            "Iteration 48, loss = 0.57429745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.53905501\n",
            "Iteration 50, loss = 0.52268581\n",
            "Iteration 51, loss = 0.51191865\n",
            "Iteration 52, loss = 0.50419000\n",
            "Iteration 53, loss = 0.49570538\n",
            "Iteration 54, loss = 0.48906004\n",
            "Iteration 55, loss = 0.48394605\n",
            "Iteration 56, loss = 0.48251878\n",
            "Iteration 57, loss = 0.50048626\n",
            "Iteration 58, loss = 0.51868407\n",
            "Iteration 59, loss = 0.49775749\n",
            "Iteration 60, loss = 0.52310762\n",
            "Iteration 61, loss = 0.52811189\n",
            "Iteration 62, loss = 0.52922897\n",
            "Iteration 63, loss = 0.51171368\n",
            "Iteration 64, loss = 0.50602248\n",
            "Iteration 65, loss = 0.51847053\n",
            "Iteration 66, loss = 0.52132644\n",
            "Iteration 67, loss = 0.51075627\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.43995190\n",
            "Iteration 69, loss = 0.43649089\n",
            "Iteration 70, loss = 0.43377162\n",
            "Iteration 71, loss = 0.43053450\n",
            "Iteration 72, loss = 0.42720713\n",
            "Iteration 73, loss = 0.42470577\n",
            "Iteration 74, loss = 0.42180714\n",
            "Iteration 75, loss = 0.41902523\n",
            "Iteration 76, loss = 0.41628953\n",
            "Iteration 77, loss = 0.41324176\n",
            "Iteration 78, loss = 0.41041455\n",
            "Iteration 79, loss = 0.40753001\n",
            "Iteration 80, loss = 0.40473011\n",
            "Iteration 81, loss = 0.40237085\n",
            "Iteration 82, loss = 0.40020425\n",
            "Iteration 83, loss = 0.39723247\n",
            "Iteration 84, loss = 0.39414543\n",
            "Iteration 85, loss = 0.39234307\n",
            "Iteration 86, loss = 0.38937194\n",
            "Iteration 87, loss = 0.38756465\n",
            "Iteration 88, loss = 0.38455056\n",
            "Iteration 89, loss = 0.38290666\n",
            "Iteration 90, loss = 0.38000026\n",
            "Iteration 91, loss = 0.37748844\n",
            "Iteration 92, loss = 0.37576550\n",
            "Iteration 93, loss = 0.37361727\n",
            "Iteration 94, loss = 0.37098558\n",
            "Iteration 95, loss = 0.36923557\n",
            "Iteration 96, loss = 0.36747522\n",
            "Iteration 97, loss = 0.36514192\n",
            "Iteration 98, loss = 0.36317230\n",
            "Iteration 99, loss = 0.36123305\n",
            "Iteration 100, loss = 0.35904412\n",
            "Iteration 101, loss = 0.35725968\n",
            "Iteration 102, loss = 0.35511219\n",
            "Iteration 103, loss = 0.35353120\n",
            "Iteration 104, loss = 0.35205274\n",
            "Iteration 105, loss = 0.35066623\n",
            "Iteration 106, loss = 0.34827592\n",
            "Iteration 107, loss = 0.34672128\n",
            "Iteration 108, loss = 0.34419847\n",
            "Iteration 109, loss = 0.34335745\n",
            "Iteration 110, loss = 0.34084969\n",
            "Iteration 111, loss = 0.33903544\n",
            "Iteration 112, loss = 0.33806942\n",
            "Iteration 113, loss = 0.33651292\n",
            "Iteration 114, loss = 0.33515409\n",
            "Iteration 115, loss = 0.33345090\n",
            "Iteration 116, loss = 0.33194814\n",
            "Iteration 117, loss = 0.33053647\n",
            "Iteration 118, loss = 0.32874866\n",
            "Iteration 119, loss = 0.32846230\n",
            "Iteration 120, loss = 0.32724989\n",
            "Iteration 121, loss = 0.32532875\n",
            "Iteration 122, loss = 0.32529307\n",
            "Iteration 123, loss = 0.32409696\n",
            "Iteration 124, loss = 0.32249417\n",
            "Iteration 125, loss = 0.32010246\n",
            "Iteration 126, loss = 0.31919521\n",
            "Iteration 127, loss = 0.31849356\n",
            "Iteration 128, loss = 0.31711551\n",
            "Iteration 129, loss = 0.31536853\n",
            "Iteration 130, loss = 0.31526454\n",
            "Iteration 131, loss = 0.31443371\n",
            "Iteration 132, loss = 0.31312652\n",
            "Iteration 133, loss = 0.31068659\n",
            "Iteration 134, loss = 0.31106323\n",
            "Iteration 135, loss = 0.31105978\n",
            "Iteration 136, loss = 0.30975953\n",
            "Iteration 137, loss = 0.30903235\n",
            "Iteration 138, loss = 0.30784144\n",
            "Iteration 139, loss = 0.30512811\n",
            "Iteration 140, loss = 0.30618867\n",
            "Iteration 141, loss = 0.30489948\n",
            "Iteration 142, loss = 0.30354478\n",
            "Iteration 143, loss = 0.30331148\n",
            "Iteration 144, loss = 0.30107685\n",
            "Iteration 145, loss = 0.30319672\n",
            "Iteration 146, loss = 0.30212489\n",
            "Iteration 147, loss = 0.30018610\n",
            "Iteration 148, loss = 0.30190903\n",
            "Iteration 149, loss = 0.30097212\n",
            "Iteration 150, loss = 0.29751935\n",
            "Iteration 151, loss = 0.29968778\n",
            "Iteration 152, loss = 0.29907659\n",
            "Iteration 153, loss = 0.29642591\n",
            "Iteration 154, loss = 0.29293553\n",
            "Iteration 155, loss = 0.29475902\n",
            "Iteration 156, loss = 0.29718938\n",
            "Iteration 157, loss = 0.29294627\n",
            "Iteration 158, loss = 0.29191155\n",
            "Iteration 159, loss = 0.29094579\n",
            "Iteration 160, loss = 0.29189663\n",
            "Iteration 161, loss = 0.29502825\n",
            "Iteration 162, loss = 0.28807112\n",
            "Iteration 163, loss = 0.29239528\n",
            "Iteration 164, loss = 0.29057277\n",
            "Iteration 165, loss = 0.29001963\n",
            "Iteration 166, loss = 0.29095487\n",
            "Iteration 167, loss = 0.29361790\n",
            "Iteration 168, loss = 0.28780524\n",
            "Iteration 169, loss = 0.29088814\n",
            "Iteration 170, loss = 0.29009289\n",
            "Iteration 171, loss = 0.29531480\n",
            "Iteration 172, loss = 0.28743183\n",
            "Iteration 173, loss = 0.29204767\n",
            "Iteration 174, loss = 0.28422284\n",
            "Iteration 175, loss = 0.28435085\n",
            "Iteration 176, loss = 0.28847499\n",
            "Iteration 177, loss = 0.28445750\n",
            "Iteration 178, loss = 0.29746437\n",
            "Iteration 179, loss = 0.28710585\n",
            "Iteration 180, loss = 0.28839175\n",
            "Iteration 181, loss = 0.28619503\n",
            "Iteration 182, loss = 0.28321292\n",
            "Iteration 183, loss = 0.28950698\n",
            "Iteration 184, loss = 0.28653031\n",
            "Iteration 185, loss = 0.28831454\n",
            "Iteration 186, loss = 0.28475560\n",
            "Iteration 187, loss = 0.28806964\n",
            "Iteration 188, loss = 0.29444874\n",
            "Iteration 189, loss = 0.28402168\n",
            "Iteration 190, loss = 0.28287596\n",
            "Iteration 191, loss = 0.29251541\n",
            "Iteration 192, loss = 0.28016872\n",
            "Iteration 193, loss = 0.28467090\n",
            "Iteration 194, loss = 0.28273878\n",
            "Iteration 195, loss = 0.29911669\n",
            "Iteration 196, loss = 0.28423244\n",
            "Iteration 197, loss = 0.29287583\n",
            "Iteration 198, loss = 0.29156298\n",
            "Iteration 199, loss = 0.28563966\n",
            "Iteration 200, loss = 0.28011149\n",
            "----------------------------------\n",
            "[[33523   204]\n",
            " [ 1058 15861]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 10.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     33727\n",
            "           1       0.99      0.94      0.96     16919\n",
            "\n",
            "    accuracy                           0.98     50646\n",
            "   macro avg       0.98      0.97      0.97     50646\n",
            "weighted avg       0.98      0.98      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.766% (0.207%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.526% (10.858%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64863911\n",
            "Iteration 3, loss = 0.62015919\n",
            "Iteration 4, loss = 0.60226033\n",
            "Iteration 5, loss = 0.59361997\n",
            "Iteration 6, loss = 0.58893125\n",
            "Iteration 7, loss = 0.58594367\n",
            "Iteration 8, loss = 0.58481489\n",
            "Iteration 9, loss = 0.58269165\n",
            "Iteration 10, loss = 0.58235534\n",
            "Iteration 11, loss = 0.58272359\n",
            "Iteration 12, loss = 0.58331896\n",
            "Iteration 13, loss = 0.58203194\n",
            "Iteration 14, loss = 0.58114424\n",
            "Iteration 15, loss = 0.58264158\n",
            "Iteration 16, loss = 0.58084625\n",
            "Iteration 17, loss = 0.58221424\n",
            "Iteration 18, loss = 0.59769718\n",
            "Iteration 19, loss = 0.58740478\n",
            "Iteration 20, loss = 0.58589326\n",
            "Iteration 21, loss = 0.63990181\n",
            "Iteration 22, loss = 0.62414816\n",
            "Iteration 23, loss = 0.61386372\n",
            "Iteration 24, loss = 0.61366878\n",
            "Iteration 25, loss = 0.60299291\n",
            "Iteration 26, loss = 0.59365678\n",
            "Iteration 27, loss = 0.58799509\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 28, loss = 0.57355533\n",
            "Iteration 29, loss = 0.56771962\n",
            "Iteration 30, loss = 0.56056831\n",
            "Iteration 31, loss = 0.55282411\n",
            "Iteration 32, loss = 0.55197496\n",
            "Iteration 33, loss = 0.55643128\n",
            "Iteration 34, loss = 0.55531144\n",
            "Iteration 35, loss = 0.60785675\n",
            "Iteration 36, loss = 0.62840946\n",
            "Iteration 37, loss = 0.61333914\n",
            "Iteration 38, loss = 0.59785152\n",
            "Iteration 39, loss = 0.58962440\n",
            "Iteration 40, loss = 0.58349959\n",
            "Iteration 41, loss = 0.57724887\n",
            "Iteration 42, loss = 0.57204091\n",
            "Iteration 43, loss = 0.56691220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.54835755\n",
            "Iteration 45, loss = 0.54430564\n",
            "Iteration 46, loss = 0.54016056\n",
            "Iteration 47, loss = 0.53578252\n",
            "Iteration 48, loss = 0.53106279\n",
            "Iteration 49, loss = 0.52638101\n",
            "Iteration 50, loss = 0.52156369\n",
            "Iteration 51, loss = 0.51658433\n",
            "Iteration 52, loss = 0.51116431\n",
            "Iteration 53, loss = 0.50596077\n",
            "Iteration 54, loss = 0.50098217\n",
            "Iteration 55, loss = 0.49546210\n",
            "Iteration 56, loss = 0.49013632\n",
            "Iteration 57, loss = 0.48508346\n",
            "Iteration 58, loss = 0.48055937\n",
            "Iteration 59, loss = 0.47573716\n",
            "Iteration 60, loss = 0.47140413\n",
            "Iteration 61, loss = 0.46788315\n",
            "Iteration 62, loss = 0.46645060\n",
            "Iteration 63, loss = 0.46235261\n",
            "Iteration 64, loss = 0.46157973\n",
            "Iteration 65, loss = 0.45753913\n",
            "Iteration 66, loss = 0.45580911\n",
            "Iteration 67, loss = 0.45353295\n",
            "Iteration 68, loss = 0.45256282\n",
            "Iteration 69, loss = 0.45129376\n",
            "Iteration 70, loss = 0.44927482\n",
            "Iteration 71, loss = 0.45015885\n",
            "Iteration 72, loss = 0.44796414\n",
            "Iteration 73, loss = 0.44414111\n",
            "Iteration 74, loss = 0.44243920\n",
            "Iteration 75, loss = 0.44361197\n",
            "Iteration 76, loss = 0.44460275\n",
            "Iteration 77, loss = 0.43892188\n",
            "Iteration 78, loss = 0.43992925\n",
            "Iteration 79, loss = 0.43864690\n",
            "Iteration 80, loss = 0.43574309\n",
            "Iteration 81, loss = 0.43292374\n",
            "Iteration 82, loss = 0.43422623\n",
            "Iteration 83, loss = 0.43420921\n",
            "Iteration 84, loss = 0.43121757\n",
            "Iteration 85, loss = 0.42869360\n",
            "Iteration 86, loss = 0.42827675\n",
            "Iteration 87, loss = 0.42743551\n",
            "Iteration 88, loss = 0.42502737\n",
            "Iteration 89, loss = 0.42445818\n",
            "Iteration 90, loss = 0.42208783\n",
            "Iteration 91, loss = 0.42130510\n",
            "Iteration 92, loss = 0.42139572\n",
            "Iteration 93, loss = 0.41588773\n",
            "Iteration 94, loss = 0.42109122\n",
            "Iteration 95, loss = 0.41830742\n",
            "Iteration 96, loss = 0.41246073\n",
            "Iteration 97, loss = 0.41699162\n",
            "Iteration 98, loss = 0.41155166\n",
            "Iteration 99, loss = 0.41015981\n",
            "Iteration 100, loss = 0.41006515\n",
            "Iteration 101, loss = 0.41352974\n",
            "Iteration 102, loss = 0.40851949\n",
            "Iteration 103, loss = 0.40598769\n",
            "Iteration 104, loss = 0.40276386\n",
            "Iteration 105, loss = 0.40552022\n",
            "Iteration 106, loss = 0.40551240\n",
            "Iteration 107, loss = 0.40140114\n",
            "Iteration 108, loss = 0.40508479\n",
            "Iteration 109, loss = 0.40350807\n",
            "Iteration 110, loss = 0.39610264\n",
            "Iteration 111, loss = 0.40224574\n",
            "Iteration 112, loss = 0.39964507\n",
            "Iteration 113, loss = 0.39735509\n",
            "Iteration 114, loss = 0.39685722\n",
            "Iteration 115, loss = 0.39830755\n",
            "Iteration 116, loss = 0.39500872\n",
            "Iteration 117, loss = 0.39211128\n",
            "Iteration 118, loss = 0.39514080\n",
            "Iteration 119, loss = 0.39181609\n",
            "Iteration 120, loss = 0.39653301\n",
            "Iteration 121, loss = 0.39645825\n",
            "Iteration 122, loss = 0.38828045\n",
            "Iteration 123, loss = 0.39534326\n",
            "Iteration 124, loss = 0.38978579\n",
            "Iteration 125, loss = 0.38762453\n",
            "Iteration 126, loss = 0.39143408\n",
            "Iteration 127, loss = 0.38990424\n",
            "Iteration 128, loss = 0.38666231\n",
            "Iteration 129, loss = 0.38712224\n",
            "Iteration 130, loss = 0.38854520\n",
            "Iteration 131, loss = 0.38929527\n",
            "Iteration 132, loss = 0.38236309\n",
            "Iteration 133, loss = 0.38504272\n",
            "Iteration 134, loss = 0.38697938\n",
            "Iteration 135, loss = 0.38080068\n",
            "Iteration 136, loss = 0.38723906\n",
            "Iteration 137, loss = 0.38553223\n",
            "Iteration 138, loss = 0.38246104\n",
            "Iteration 139, loss = 0.38545319\n",
            "Iteration 140, loss = 0.38301263\n",
            "Iteration 141, loss = 0.38762221\n",
            "Iteration 142, loss = 0.38215892\n",
            "Iteration 143, loss = 0.38423221\n",
            "Iteration 144, loss = 0.38158541\n",
            "Iteration 145, loss = 0.38095853\n",
            "Iteration 146, loss = 0.38077570\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 147, loss = 0.34045906\n",
            "Iteration 148, loss = 0.34009253\n",
            "Iteration 149, loss = 0.33994710\n",
            "Iteration 150, loss = 0.33977038\n",
            "Iteration 151, loss = 0.33975902\n",
            "Iteration 152, loss = 0.33958535\n",
            "Iteration 153, loss = 0.33946046\n",
            "Iteration 154, loss = 0.33930538\n",
            "Iteration 155, loss = 0.33933868\n",
            "Iteration 156, loss = 0.33910018\n",
            "Iteration 157, loss = 0.33903963\n",
            "Iteration 158, loss = 0.33889373\n",
            "Iteration 159, loss = 0.33879113\n",
            "Iteration 160, loss = 0.33867119\n",
            "Iteration 161, loss = 0.33858028\n",
            "Iteration 162, loss = 0.33846380\n",
            "Iteration 163, loss = 0.33842037\n",
            "Iteration 164, loss = 0.33827698\n",
            "Iteration 165, loss = 0.33820600\n",
            "Iteration 166, loss = 0.33816279\n",
            "Iteration 167, loss = 0.33801735\n",
            "Iteration 168, loss = 0.33792603\n",
            "Iteration 169, loss = 0.33786499\n",
            "Iteration 170, loss = 0.33774138\n",
            "Iteration 171, loss = 0.33761436\n",
            "Iteration 172, loss = 0.33753199\n",
            "Iteration 173, loss = 0.33749684\n",
            "Iteration 174, loss = 0.33733099\n",
            "Iteration 175, loss = 0.33728342\n",
            "Iteration 176, loss = 0.33718764\n",
            "Iteration 177, loss = 0.33708802\n",
            "Iteration 178, loss = 0.33696284\n",
            "Iteration 179, loss = 0.33690992\n",
            "Iteration 180, loss = 0.33679243\n",
            "Iteration 181, loss = 0.33671512\n",
            "Iteration 182, loss = 0.33671532\n",
            "Iteration 183, loss = 0.33660748\n",
            "Iteration 184, loss = 0.33655143\n",
            "Iteration 185, loss = 0.33636521\n",
            "Iteration 186, loss = 0.33636719\n",
            "Iteration 187, loss = 0.33624456\n",
            "Iteration 188, loss = 0.33615931\n",
            "Iteration 189, loss = 0.33604036\n",
            "Iteration 190, loss = 0.33602939\n",
            "Iteration 191, loss = 0.33596239\n",
            "Iteration 192, loss = 0.33580780\n",
            "Iteration 193, loss = 0.33579242\n",
            "Iteration 194, loss = 0.33572040\n",
            "Iteration 195, loss = 0.33558926\n",
            "Iteration 196, loss = 0.33548771\n",
            "Iteration 197, loss = 0.33538420\n",
            "Iteration 198, loss = 0.33534069\n",
            "Iteration 199, loss = 0.33528524\n",
            "Iteration 200, loss = 0.33524072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63563989\n",
            "Iteration 3, loss = 0.60816806\n",
            "Iteration 4, loss = 0.59250549\n",
            "Iteration 5, loss = 0.58359248\n",
            "Iteration 6, loss = 0.57691440\n",
            "Iteration 7, loss = 0.57300667\n",
            "Iteration 8, loss = 0.57147971\n",
            "Iteration 9, loss = 0.56918040\n",
            "Iteration 10, loss = 0.56878670\n",
            "Iteration 11, loss = 0.56802256\n",
            "Iteration 12, loss = 0.56848523\n",
            "Iteration 13, loss = 0.56557041\n",
            "Iteration 14, loss = 0.56813606\n",
            "Iteration 15, loss = 0.56677001\n",
            "Iteration 16, loss = 0.56812004\n",
            "Iteration 17, loss = 0.56823250\n",
            "Iteration 18, loss = 0.56758308\n",
            "Iteration 19, loss = 0.56635628\n",
            "Iteration 20, loss = 0.56758159\n",
            "Iteration 21, loss = 0.56785946\n",
            "Iteration 22, loss = 0.56739396\n",
            "Iteration 23, loss = 0.56509815\n",
            "Iteration 24, loss = 0.56835071\n",
            "Iteration 25, loss = 0.56553546\n",
            "Iteration 26, loss = 0.56630599\n",
            "Iteration 27, loss = 0.56699071\n",
            "Iteration 28, loss = 0.56686475\n",
            "Iteration 29, loss = 0.56602142\n",
            "Iteration 30, loss = 0.56637522\n",
            "Iteration 31, loss = 0.56444228\n",
            "Iteration 32, loss = 0.56437747\n",
            "Iteration 33, loss = 0.56563483\n",
            "Iteration 34, loss = 0.56414469\n",
            "Iteration 35, loss = 0.56503779\n",
            "Iteration 36, loss = 0.56341354\n",
            "Iteration 37, loss = 0.63503901\n",
            "Iteration 38, loss = 0.62835224\n",
            "Iteration 39, loss = 0.61011377\n",
            "Iteration 40, loss = 0.60121562\n",
            "Iteration 41, loss = 0.58741241\n",
            "Iteration 42, loss = 0.59800519\n",
            "Iteration 43, loss = 0.58180134\n",
            "Iteration 44, loss = 0.57509855\n",
            "Iteration 45, loss = 0.57076074\n",
            "Iteration 46, loss = 0.59760418\n",
            "Iteration 47, loss = 0.58418519\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.56333626\n",
            "Iteration 49, loss = 0.55850754\n",
            "Iteration 50, loss = 0.55306893\n",
            "Iteration 51, loss = 0.54634718\n",
            "Iteration 52, loss = 0.53998038\n",
            "Iteration 53, loss = 0.53537347\n",
            "Iteration 54, loss = 0.53708319\n",
            "Iteration 55, loss = 0.53566597\n",
            "Iteration 56, loss = 0.53707165\n",
            "Iteration 57, loss = 0.53596011\n",
            "Iteration 58, loss = 0.53419697\n",
            "Iteration 59, loss = 0.53320149\n",
            "Iteration 60, loss = 0.53139353\n",
            "Iteration 61, loss = 0.53092306\n",
            "Iteration 62, loss = 0.52970260\n",
            "Iteration 63, loss = 0.52629401\n",
            "Iteration 64, loss = 0.52621395\n",
            "Iteration 65, loss = 0.52914882\n",
            "Iteration 66, loss = 0.52405396\n",
            "Iteration 67, loss = 0.52605125\n",
            "Iteration 68, loss = 0.52180129\n",
            "Iteration 69, loss = 0.52331208\n",
            "Iteration 70, loss = 0.52135156\n",
            "Iteration 71, loss = 0.51858289\n",
            "Iteration 72, loss = 0.52372111\n",
            "Iteration 73, loss = 0.51981146\n",
            "Iteration 74, loss = 0.52318971\n",
            "Iteration 75, loss = 0.52082072\n",
            "Iteration 76, loss = 0.52228449\n",
            "Iteration 77, loss = 0.51939617\n",
            "Iteration 78, loss = 0.51913959\n",
            "Iteration 79, loss = 0.51829182\n",
            "Iteration 80, loss = 0.52049715\n",
            "Iteration 81, loss = 0.52018614\n",
            "Iteration 82, loss = 0.52238516\n",
            "Iteration 83, loss = 0.51740789\n",
            "Iteration 84, loss = 0.52000833\n",
            "Iteration 85, loss = 0.52350064\n",
            "Iteration 86, loss = 0.52071930\n",
            "Iteration 87, loss = 0.52120879\n",
            "Iteration 88, loss = 0.52404071\n",
            "Iteration 89, loss = 0.51935890\n",
            "Iteration 90, loss = 0.52587289\n",
            "Iteration 91, loss = 0.52158288\n",
            "Iteration 92, loss = 0.52118250\n",
            "Iteration 93, loss = 0.52545936\n",
            "Iteration 94, loss = 0.52007769\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 95, loss = 0.41827170\n",
            "Iteration 96, loss = 0.40303169\n",
            "Iteration 97, loss = 0.38998783\n",
            "Iteration 98, loss = 0.37816828\n",
            "Iteration 99, loss = 0.36820816\n",
            "Iteration 100, loss = 0.35893369\n",
            "Iteration 101, loss = 0.35145030\n",
            "Iteration 102, loss = 0.34455656\n",
            "Iteration 103, loss = 0.33842034\n",
            "Iteration 104, loss = 0.33377880\n",
            "Iteration 105, loss = 0.32906364\n",
            "Iteration 106, loss = 0.32518593\n",
            "Iteration 107, loss = 0.32179885\n",
            "Iteration 108, loss = 0.31848849\n",
            "Iteration 109, loss = 0.31528044\n",
            "Iteration 110, loss = 0.31377333\n",
            "Iteration 111, loss = 0.31144708\n",
            "Iteration 112, loss = 0.30971548\n",
            "Iteration 113, loss = 0.30763071\n",
            "Iteration 114, loss = 0.30658421\n",
            "Iteration 115, loss = 0.30435021\n",
            "Iteration 116, loss = 0.30322806\n",
            "Iteration 117, loss = 0.30125768\n",
            "Iteration 118, loss = 0.30078893\n",
            "Iteration 119, loss = 0.29944972\n",
            "Iteration 120, loss = 0.29960754\n",
            "Iteration 121, loss = 0.29827175\n",
            "Iteration 122, loss = 0.29799847\n",
            "Iteration 123, loss = 0.29648035\n",
            "Iteration 124, loss = 0.29806219\n",
            "Iteration 125, loss = 0.29866998\n",
            "Iteration 126, loss = 0.29705883\n",
            "Iteration 127, loss = 0.29783812\n",
            "Iteration 128, loss = 0.29790579\n",
            "Iteration 129, loss = 0.29752012\n",
            "Iteration 130, loss = 0.29588636\n",
            "Iteration 131, loss = 0.29800515\n",
            "Iteration 132, loss = 0.29306042\n",
            "Iteration 133, loss = 0.29698362\n",
            "Iteration 134, loss = 0.29359951\n",
            "Iteration 135, loss = 0.29649943\n",
            "Iteration 136, loss = 0.30136694\n",
            "Iteration 137, loss = 0.29757903\n",
            "Iteration 138, loss = 0.29784817\n",
            "Iteration 139, loss = 0.29682932\n",
            "Iteration 140, loss = 0.30268726\n",
            "Iteration 141, loss = 0.29888039\n",
            "Iteration 142, loss = 0.30047162\n",
            "Iteration 143, loss = 0.30043961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 144, loss = 0.27748377\n",
            "Iteration 145, loss = 0.27731583\n",
            "Iteration 146, loss = 0.27714078\n",
            "Iteration 147, loss = 0.27706682\n",
            "Iteration 148, loss = 0.27698848\n",
            "Iteration 149, loss = 0.27682429\n",
            "Iteration 150, loss = 0.27672768\n",
            "Iteration 151, loss = 0.27666429\n",
            "Iteration 152, loss = 0.27656750\n",
            "Iteration 153, loss = 0.27644278\n",
            "Iteration 154, loss = 0.27635389\n",
            "Iteration 155, loss = 0.27632071\n",
            "Iteration 156, loss = 0.27622053\n",
            "Iteration 157, loss = 0.27609252\n",
            "Iteration 158, loss = 0.27603851\n",
            "Iteration 159, loss = 0.27600043\n",
            "Iteration 160, loss = 0.27587834\n",
            "Iteration 161, loss = 0.27575315\n",
            "Iteration 162, loss = 0.27570591\n",
            "Iteration 163, loss = 0.27559797\n",
            "Iteration 164, loss = 0.27546080\n",
            "Iteration 165, loss = 0.27539084\n",
            "Iteration 166, loss = 0.27533149\n",
            "Iteration 167, loss = 0.27521401\n",
            "Iteration 168, loss = 0.27515974\n",
            "Iteration 169, loss = 0.27511164\n",
            "Iteration 170, loss = 0.27498470\n",
            "Iteration 171, loss = 0.27492270\n",
            "Iteration 172, loss = 0.27487597\n",
            "Iteration 173, loss = 0.27474963\n",
            "Iteration 174, loss = 0.27468279\n",
            "Iteration 175, loss = 0.27459959\n",
            "Iteration 176, loss = 0.27448348\n",
            "Iteration 177, loss = 0.27440066\n",
            "Iteration 178, loss = 0.27439453\n",
            "Iteration 179, loss = 0.27426537\n",
            "Iteration 180, loss = 0.27420201\n",
            "Iteration 181, loss = 0.27409691\n",
            "Iteration 182, loss = 0.27401662\n",
            "Iteration 183, loss = 0.27394286\n",
            "Iteration 184, loss = 0.27391702\n",
            "Iteration 185, loss = 0.27380404\n",
            "Iteration 186, loss = 0.27372720\n",
            "Iteration 187, loss = 0.27370640\n",
            "Iteration 188, loss = 0.27362163\n",
            "Iteration 189, loss = 0.27354134\n",
            "Iteration 190, loss = 0.27350616\n",
            "Iteration 191, loss = 0.27338655\n",
            "Iteration 192, loss = 0.27331370\n",
            "Iteration 193, loss = 0.27322917\n",
            "Iteration 194, loss = 0.27321707\n",
            "Iteration 195, loss = 0.27310398\n",
            "Iteration 196, loss = 0.27307750\n",
            "Iteration 197, loss = 0.27294955\n",
            "Iteration 198, loss = 0.27291820\n",
            "Iteration 199, loss = 0.27285128\n",
            "Iteration 200, loss = 0.27280476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.87932989\n",
            "Iteration 2, loss = 0.63411788\n",
            "Iteration 3, loss = 0.60859667\n",
            "Iteration 4, loss = 0.59162986\n",
            "Iteration 5, loss = 0.57933763\n",
            "Iteration 6, loss = 0.57395633\n",
            "Iteration 7, loss = 0.56966841\n",
            "Iteration 8, loss = 0.56664240\n",
            "Iteration 9, loss = 0.56617430\n",
            "Iteration 10, loss = 0.56554946\n",
            "Iteration 11, loss = 0.56549623\n",
            "Iteration 12, loss = 0.56606160\n",
            "Iteration 13, loss = 0.56545825\n",
            "Iteration 14, loss = 0.56492199\n",
            "Iteration 15, loss = 0.56501130\n",
            "Iteration 16, loss = 0.56565355\n",
            "Iteration 17, loss = 0.56557839\n",
            "Iteration 18, loss = 0.56612553\n",
            "Iteration 19, loss = 0.56524450\n",
            "Iteration 20, loss = 0.56484048\n",
            "Iteration 21, loss = 0.56578153\n",
            "Iteration 22, loss = 0.56450694\n",
            "Iteration 23, loss = 0.56515146\n",
            "Iteration 24, loss = 0.56647464\n",
            "Iteration 25, loss = 0.56448909\n",
            "Iteration 26, loss = 0.56455699\n",
            "Iteration 27, loss = 0.56659246\n",
            "Iteration 28, loss = 0.56547945\n",
            "Iteration 29, loss = 0.56591373\n",
            "Iteration 30, loss = 0.56568411\n",
            "Iteration 31, loss = 0.56648753\n",
            "Iteration 32, loss = 0.56683227\n",
            "Iteration 33, loss = 0.56633233\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.50748266\n",
            "Iteration 35, loss = 0.48237646\n",
            "Iteration 36, loss = 0.49921368\n",
            "Iteration 37, loss = 0.51379838\n",
            "Iteration 38, loss = 0.50871161\n",
            "Iteration 39, loss = 0.51175478\n",
            "Iteration 40, loss = 0.50509621\n",
            "Iteration 41, loss = 0.51053800\n",
            "Iteration 42, loss = 0.50713745\n",
            "Iteration 43, loss = 0.51269736\n",
            "Iteration 44, loss = 0.50795016\n",
            "Iteration 45, loss = 0.51212815\n",
            "Iteration 46, loss = 0.51044728\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.41743084\n",
            "Iteration 48, loss = 0.40680575\n",
            "Iteration 49, loss = 0.39685732\n",
            "Iteration 50, loss = 0.38762367\n",
            "Iteration 51, loss = 0.37914266\n",
            "Iteration 52, loss = 0.37113767\n",
            "Iteration 53, loss = 0.36394989\n",
            "Iteration 54, loss = 0.35702391\n",
            "Iteration 55, loss = 0.35097329\n",
            "Iteration 56, loss = 0.34556245\n",
            "Iteration 57, loss = 0.34053955\n",
            "Iteration 58, loss = 0.33607316\n",
            "Iteration 59, loss = 0.33163750\n",
            "Iteration 60, loss = 0.32800819\n",
            "Iteration 61, loss = 0.32452429\n",
            "Iteration 62, loss = 0.32165941\n",
            "Iteration 63, loss = 0.31892327\n",
            "Iteration 64, loss = 0.31685135\n",
            "Iteration 65, loss = 0.31384124\n",
            "Iteration 66, loss = 0.31230292\n",
            "Iteration 67, loss = 0.29865144\n",
            "Iteration 68, loss = 0.28994811\n",
            "Iteration 69, loss = 0.28758844\n",
            "Iteration 70, loss = 0.28551151\n",
            "Iteration 71, loss = 0.28406477\n",
            "Iteration 72, loss = 0.28194165\n",
            "Iteration 73, loss = 0.28016443\n",
            "Iteration 74, loss = 0.27870991\n",
            "Iteration 75, loss = 0.27768313\n",
            "Iteration 76, loss = 0.27603863\n",
            "Iteration 77, loss = 0.27529601\n",
            "Iteration 78, loss = 0.27356697\n",
            "Iteration 79, loss = 0.27546668\n",
            "Iteration 80, loss = 0.27283007\n",
            "Iteration 81, loss = 0.27065682\n",
            "Iteration 82, loss = 0.27200551\n",
            "Iteration 83, loss = 0.27245058\n",
            "Iteration 84, loss = 0.27207584\n",
            "Iteration 85, loss = 0.27323026\n",
            "Iteration 86, loss = 0.27055320\n",
            "Iteration 87, loss = 0.26960346\n",
            "Iteration 88, loss = 0.26779306\n",
            "Iteration 89, loss = 0.26685282\n",
            "Iteration 90, loss = 0.26988004\n",
            "Iteration 91, loss = 0.27075930\n",
            "Iteration 92, loss = 0.27310008\n",
            "Iteration 93, loss = 0.26960880\n",
            "Iteration 94, loss = 0.26995207\n",
            "Iteration 95, loss = 0.27351451\n",
            "Iteration 96, loss = 0.26889951\n",
            "Iteration 97, loss = 0.27394514\n",
            "Iteration 98, loss = 0.27525305\n",
            "Iteration 99, loss = 0.28496406\n",
            "Iteration 100, loss = 0.27668067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.25473072\n",
            "Iteration 102, loss = 0.25467245\n",
            "Iteration 103, loss = 0.25465772\n",
            "Iteration 104, loss = 0.25461356\n",
            "Iteration 105, loss = 0.25451910\n",
            "Iteration 106, loss = 0.25445181\n",
            "Iteration 107, loss = 0.25442287\n",
            "Iteration 108, loss = 0.25438397\n",
            "Iteration 109, loss = 0.25426721\n",
            "Iteration 110, loss = 0.25421930\n",
            "Iteration 111, loss = 0.25415601\n",
            "Iteration 112, loss = 0.25407944\n",
            "Iteration 113, loss = 0.25407677\n",
            "Iteration 114, loss = 0.25402567\n",
            "Iteration 115, loss = 0.25390181\n",
            "Iteration 116, loss = 0.25394016\n",
            "Iteration 117, loss = 0.25384791\n",
            "Iteration 118, loss = 0.25377538\n",
            "Iteration 119, loss = 0.25375154\n",
            "Iteration 120, loss = 0.25371000\n",
            "Iteration 121, loss = 0.25360845\n",
            "Iteration 122, loss = 0.25357937\n",
            "Iteration 123, loss = 0.25352384\n",
            "Iteration 124, loss = 0.25351023\n",
            "Iteration 125, loss = 0.25342990\n",
            "Iteration 126, loss = 0.25337767\n",
            "Iteration 127, loss = 0.25335129\n",
            "Iteration 128, loss = 0.25328567\n",
            "Iteration 129, loss = 0.25326166\n",
            "Iteration 130, loss = 0.25323961\n",
            "Iteration 131, loss = 0.25315626\n",
            "Iteration 132, loss = 0.25311558\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 133, loss = 0.25279168\n",
            "Iteration 134, loss = 0.25280641\n",
            "Iteration 135, loss = 0.25281329\n",
            "Iteration 136, loss = 0.25281116\n",
            "Iteration 137, loss = 0.25275293\n",
            "Iteration 138, loss = 0.25279331\n",
            "Iteration 139, loss = 0.25275611\n",
            "Iteration 140, loss = 0.25275708\n",
            "Iteration 141, loss = 0.25275909\n",
            "Iteration 142, loss = 0.25271242\n",
            "Iteration 143, loss = 0.25274744\n",
            "Iteration 144, loss = 0.25272708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 145, loss = 0.25261732\n",
            "Iteration 146, loss = 0.25261015\n",
            "Iteration 147, loss = 0.25260697\n",
            "Iteration 148, loss = 0.25261474\n",
            "Iteration 149, loss = 0.25259566\n",
            "Iteration 150, loss = 0.25259679\n",
            "Iteration 151, loss = 0.25259566\n",
            "Iteration 152, loss = 0.25260676\n",
            "Iteration 153, loss = 0.25259387\n",
            "Iteration 154, loss = 0.25258864\n",
            "Iteration 155, loss = 0.25257552\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63689241\n",
            "Iteration 3, loss = 0.60741986\n",
            "Iteration 4, loss = 0.58981436\n",
            "Iteration 5, loss = 0.58021932\n",
            "Iteration 6, loss = 0.58750693\n",
            "Iteration 7, loss = 0.60802818\n",
            "Iteration 8, loss = 0.58699874\n",
            "Iteration 9, loss = 0.57918446\n",
            "Iteration 10, loss = 0.57346523\n",
            "Iteration 11, loss = 0.59398739\n",
            "Iteration 12, loss = 0.58692485\n",
            "Iteration 13, loss = 0.57976028\n",
            "Iteration 14, loss = 0.57444778\n",
            "Iteration 15, loss = 0.57122475\n",
            "Iteration 16, loss = 0.56957306\n",
            "Iteration 17, loss = 0.56779409\n",
            "Iteration 18, loss = 0.56762742\n",
            "Iteration 19, loss = 0.56885274\n",
            "Iteration 20, loss = 0.56729095\n",
            "Iteration 21, loss = 0.57020685\n",
            "Iteration 22, loss = 0.56861761\n",
            "Iteration 23, loss = 0.56707655\n",
            "Iteration 24, loss = 0.57056977\n",
            "Iteration 25, loss = 0.56913961\n",
            "Iteration 26, loss = 0.56901959\n",
            "Iteration 27, loss = 0.56908817\n",
            "Iteration 28, loss = 0.57144722\n",
            "Iteration 29, loss = 0.56980358\n",
            "Iteration 30, loss = 0.56952811\n",
            "Iteration 31, loss = 0.56927243\n",
            "Iteration 32, loss = 0.57005322\n",
            "Iteration 33, loss = 0.56863310\n",
            "Iteration 34, loss = 0.57068963\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.50741180\n",
            "Iteration 36, loss = 0.48401161\n",
            "Iteration 37, loss = 0.51271090\n",
            "Iteration 38, loss = 0.51094366\n",
            "Iteration 39, loss = 0.51791227\n",
            "Iteration 40, loss = 0.50943096\n",
            "Iteration 41, loss = 0.51254759\n",
            "Iteration 42, loss = 0.51426671\n",
            "Iteration 43, loss = 0.51446250\n",
            "Iteration 44, loss = 0.50948332\n",
            "Iteration 45, loss = 0.51768701\n",
            "Iteration 46, loss = 0.51129457\n",
            "Iteration 47, loss = 0.51431610\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 48, loss = 0.41353288\n",
            "Iteration 49, loss = 0.39906938\n",
            "Iteration 50, loss = 0.38576985\n",
            "Iteration 51, loss = 0.37412327\n",
            "Iteration 52, loss = 0.36312784\n",
            "Iteration 53, loss = 0.35361193\n",
            "Iteration 54, loss = 0.34490820\n",
            "Iteration 55, loss = 0.33685988\n",
            "Iteration 56, loss = 0.32990002\n",
            "Iteration 57, loss = 0.32359744\n",
            "Iteration 58, loss = 0.31759603\n",
            "Iteration 59, loss = 0.31271855\n",
            "Iteration 60, loss = 0.30785932\n",
            "Iteration 61, loss = 0.30387255\n",
            "Iteration 62, loss = 0.29914747\n",
            "Iteration 63, loss = 0.29602760\n",
            "Iteration 64, loss = 0.29335494\n",
            "Iteration 65, loss = 0.28993467\n",
            "Iteration 66, loss = 0.28796760\n",
            "Iteration 67, loss = 0.28523919\n",
            "Iteration 68, loss = 0.28338892\n",
            "Iteration 69, loss = 0.28056813\n",
            "Iteration 70, loss = 0.27931650\n",
            "Iteration 71, loss = 0.27747408\n",
            "Iteration 72, loss = 0.27523583\n",
            "Iteration 73, loss = 0.27540333\n",
            "Iteration 74, loss = 0.27435402\n",
            "Iteration 75, loss = 0.27224379\n",
            "Iteration 76, loss = 0.27145263\n",
            "Iteration 77, loss = 0.27092607\n",
            "Iteration 78, loss = 0.27049871\n",
            "Iteration 79, loss = 0.26990610\n",
            "Iteration 80, loss = 0.26963997\n",
            "Iteration 81, loss = 0.26826202\n",
            "Iteration 82, loss = 0.26927649\n",
            "Iteration 83, loss = 0.26947518\n",
            "Iteration 84, loss = 0.26595985\n",
            "Iteration 85, loss = 0.26697969\n",
            "Iteration 86, loss = 0.27056731\n",
            "Iteration 87, loss = 0.26932335\n",
            "Iteration 88, loss = 0.26859863\n",
            "Iteration 89, loss = 0.26588402\n",
            "Iteration 90, loss = 0.26744443\n",
            "Iteration 91, loss = 0.26444142\n",
            "Iteration 92, loss = 0.26896025\n",
            "Iteration 93, loss = 0.26744382\n",
            "Iteration 94, loss = 0.27056233\n",
            "Iteration 95, loss = 0.26561389\n",
            "Iteration 96, loss = 0.26667881\n",
            "Iteration 97, loss = 0.27624241\n",
            "Iteration 98, loss = 0.27085846\n",
            "Iteration 99, loss = 0.26737687\n",
            "Iteration 100, loss = 0.28058725\n",
            "Iteration 101, loss = 0.26712812\n",
            "Iteration 102, loss = 0.27054171\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 103, loss = 0.25337824\n",
            "Iteration 104, loss = 0.25322696\n",
            "Iteration 105, loss = 0.25319506\n",
            "Iteration 106, loss = 0.25312620\n",
            "Iteration 107, loss = 0.25309634\n",
            "Iteration 108, loss = 0.25300820\n",
            "Iteration 109, loss = 0.25298323\n",
            "Iteration 110, loss = 0.25286406\n",
            "Iteration 111, loss = 0.25286597\n",
            "Iteration 112, loss = 0.25285877\n",
            "Iteration 113, loss = 0.25280848\n",
            "Iteration 114, loss = 0.25277624\n",
            "Iteration 115, loss = 0.25269432\n",
            "Iteration 116, loss = 0.25266462\n",
            "Iteration 117, loss = 0.25261503\n",
            "Iteration 118, loss = 0.25252708\n",
            "Iteration 119, loss = 0.25261511\n",
            "Iteration 120, loss = 0.25253232\n",
            "Iteration 121, loss = 0.25246957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 122, loss = 0.25218781\n",
            "Iteration 123, loss = 0.25214696\n",
            "Iteration 124, loss = 0.25213537\n",
            "Iteration 125, loss = 0.25214940\n",
            "Iteration 126, loss = 0.25213888\n",
            "Iteration 127, loss = 0.25214093\n",
            "Iteration 128, loss = 0.25212475\n",
            "Iteration 129, loss = 0.25213353\n",
            "Iteration 130, loss = 0.25208530\n",
            "Iteration 131, loss = 0.25211865\n",
            "Iteration 132, loss = 0.25207123\n",
            "Iteration 133, loss = 0.25205794\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 134, loss = 0.25198897\n",
            "Iteration 135, loss = 0.25198765\n",
            "Iteration 136, loss = 0.25197125\n",
            "Iteration 137, loss = 0.25197912\n",
            "Iteration 138, loss = 0.25197617\n",
            "Iteration 139, loss = 0.25198084\n",
            "Iteration 140, loss = 0.25197332\n",
            "Iteration 141, loss = 0.25197594\n",
            "Iteration 142, loss = 0.25196819\n",
            "Iteration 143, loss = 0.25196602\n",
            "Iteration 144, loss = 0.25196614\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63650295\n",
            "Iteration 3, loss = 0.60762834\n",
            "Iteration 4, loss = 0.59054427\n",
            "Iteration 5, loss = 0.58253098\n",
            "Iteration 6, loss = 0.57673984\n",
            "Iteration 7, loss = 0.57084202\n",
            "Iteration 8, loss = 0.56886954\n",
            "Iteration 9, loss = 0.56561465\n",
            "Iteration 10, loss = 0.56522392\n",
            "Iteration 11, loss = 0.56792334\n",
            "Iteration 12, loss = 0.56529185\n",
            "Iteration 13, loss = 0.56402271\n",
            "Iteration 14, loss = 0.56495668\n",
            "Iteration 15, loss = 0.56586147\n",
            "Iteration 16, loss = 0.56629645\n",
            "Iteration 17, loss = 0.56437937\n",
            "Iteration 18, loss = 0.56505171\n",
            "Iteration 19, loss = 0.56565614\n",
            "Iteration 20, loss = 0.56599913\n",
            "Iteration 21, loss = 0.56485425\n",
            "Iteration 22, loss = 0.56488516\n",
            "Iteration 23, loss = 0.56454577\n",
            "Iteration 24, loss = 0.56540141\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.50583143\n",
            "Iteration 26, loss = 0.48555241\n",
            "Iteration 27, loss = 0.50834379\n",
            "Iteration 28, loss = 0.51218744\n",
            "Iteration 29, loss = 0.51282967\n",
            "Iteration 30, loss = 0.51076349\n",
            "Iteration 31, loss = 0.51119338\n",
            "Iteration 32, loss = 0.51110666\n",
            "Iteration 33, loss = 0.51414057\n",
            "Iteration 34, loss = 0.51347885\n",
            "Iteration 35, loss = 0.50995514\n",
            "Iteration 36, loss = 0.51093696\n",
            "Iteration 37, loss = 0.50823447\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.41070353\n",
            "Iteration 39, loss = 0.39571797\n",
            "Iteration 40, loss = 0.38272296\n",
            "Iteration 41, loss = 0.37099530\n",
            "Iteration 42, loss = 0.36062346\n",
            "Iteration 43, loss = 0.35076576\n",
            "Iteration 44, loss = 0.34224282\n",
            "Iteration 45, loss = 0.33469645\n",
            "Iteration 46, loss = 0.32749202\n",
            "Iteration 47, loss = 0.32135553\n",
            "Iteration 48, loss = 0.31558524\n",
            "Iteration 49, loss = 0.31030263\n",
            "Iteration 50, loss = 0.30579151\n",
            "Iteration 51, loss = 0.30152236\n",
            "Iteration 52, loss = 0.29783153\n",
            "Iteration 53, loss = 0.29422423\n",
            "Iteration 54, loss = 0.29111508\n",
            "Iteration 55, loss = 0.28846681\n",
            "Iteration 56, loss = 0.28626519\n",
            "Iteration 57, loss = 0.28346407\n",
            "Iteration 58, loss = 0.28101862\n",
            "Iteration 59, loss = 0.27899642\n",
            "Iteration 60, loss = 0.27757239\n",
            "Iteration 61, loss = 0.27626452\n",
            "Iteration 62, loss = 0.27430871\n",
            "Iteration 63, loss = 0.27269188\n",
            "Iteration 64, loss = 0.27217336\n",
            "Iteration 65, loss = 0.27094276\n",
            "Iteration 66, loss = 0.26991141\n",
            "Iteration 67, loss = 0.26877564\n",
            "Iteration 68, loss = 0.26977499\n",
            "Iteration 69, loss = 0.26759763\n",
            "Iteration 70, loss = 0.26799032\n",
            "Iteration 71, loss = 0.26682925\n",
            "Iteration 72, loss = 0.26638524\n",
            "Iteration 73, loss = 0.26485549\n",
            "Iteration 74, loss = 0.26627388\n",
            "Iteration 75, loss = 0.26496337\n",
            "Iteration 76, loss = 0.26737937\n",
            "Iteration 77, loss = 0.26432752\n",
            "Iteration 78, loss = 0.27074659\n",
            "Iteration 79, loss = 0.26509533\n",
            "Iteration 80, loss = 0.26587695\n",
            "Iteration 81, loss = 0.26516387\n",
            "Iteration 82, loss = 0.26253823\n",
            "Iteration 83, loss = 0.26305747\n",
            "Iteration 84, loss = 0.26781009\n",
            "Iteration 85, loss = 0.26490374\n",
            "Iteration 86, loss = 0.26914397\n",
            "Iteration 87, loss = 0.26337737\n",
            "Iteration 88, loss = 0.27185622\n",
            "Iteration 89, loss = 0.26562387\n",
            "Iteration 90, loss = 0.27079450\n",
            "Iteration 91, loss = 0.26882292\n",
            "Iteration 92, loss = 0.26630335\n",
            "Iteration 93, loss = 0.26984702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.25166721\n",
            "Iteration 95, loss = 0.25155335\n",
            "Iteration 96, loss = 0.25145242\n",
            "Iteration 97, loss = 0.25145591\n",
            "Iteration 98, loss = 0.25136599\n",
            "Iteration 99, loss = 0.25131268\n",
            "Iteration 100, loss = 0.25131403\n",
            "Iteration 101, loss = 0.25127750\n",
            "Iteration 102, loss = 0.25120732\n",
            "Iteration 103, loss = 0.25117673\n",
            "Iteration 104, loss = 0.25113058\n",
            "Iteration 105, loss = 0.25104118\n",
            "Iteration 106, loss = 0.25106968\n",
            "Iteration 107, loss = 0.25099917\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 108, loss = 0.25071950\n",
            "Iteration 109, loss = 0.25070233\n",
            "Iteration 110, loss = 0.25069326\n",
            "Iteration 111, loss = 0.25068139\n",
            "Iteration 112, loss = 0.25066338\n",
            "Iteration 113, loss = 0.25066523\n",
            "Iteration 114, loss = 0.25067316\n",
            "Iteration 115, loss = 0.25063139\n",
            "Iteration 116, loss = 0.25066583\n",
            "Iteration 117, loss = 0.25061995\n",
            "Iteration 118, loss = 0.25061800\n",
            "Iteration 119, loss = 0.25062305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 120, loss = 0.25053681\n",
            "Iteration 121, loss = 0.25052275\n",
            "Iteration 122, loss = 0.25052264\n",
            "Iteration 123, loss = 0.25051902\n",
            "Iteration 124, loss = 0.25051519\n",
            "Iteration 125, loss = 0.25051296\n",
            "Iteration 126, loss = 0.25051802\n",
            "Iteration 127, loss = 0.25051300\n",
            "Iteration 128, loss = 0.25052074\n",
            "Iteration 129, loss = 0.25052101\n",
            "Iteration 130, loss = 0.25051091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65199569\n",
            "Iteration 3, loss = 0.61928246\n",
            "Iteration 4, loss = 0.60094225\n",
            "Iteration 5, loss = 0.61383778\n",
            "Iteration 6, loss = 0.60023145\n",
            "Iteration 7, loss = 0.58490649\n",
            "Iteration 8, loss = 0.57722422\n",
            "Iteration 9, loss = 0.57286393\n",
            "Iteration 10, loss = 0.57148601\n",
            "Iteration 11, loss = 0.56975853\n",
            "Iteration 12, loss = 0.56949437\n",
            "Iteration 13, loss = 0.56795143\n",
            "Iteration 14, loss = 0.56848151\n",
            "Iteration 15, loss = 0.56853016\n",
            "Iteration 16, loss = 0.57052555\n",
            "Iteration 17, loss = 0.56852057\n",
            "Iteration 18, loss = 0.57015489\n",
            "Iteration 19, loss = 0.57042724\n",
            "Iteration 20, loss = 0.57097546\n",
            "Iteration 21, loss = 0.56912431\n",
            "Iteration 22, loss = 0.56937790\n",
            "Iteration 23, loss = 0.57018841\n",
            "Iteration 24, loss = 0.57798403\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.52048024\n",
            "Iteration 26, loss = 0.49861030\n",
            "Iteration 27, loss = 0.51760022\n",
            "Iteration 28, loss = 0.52481356\n",
            "Iteration 29, loss = 0.52779532\n",
            "Iteration 30, loss = 0.52130952\n",
            "Iteration 31, loss = 0.52277744\n",
            "Iteration 32, loss = 0.52454931\n",
            "Iteration 33, loss = 0.51912184\n",
            "Iteration 34, loss = 0.52156025\n",
            "Iteration 35, loss = 0.52152833\n",
            "Iteration 36, loss = 0.51855332\n",
            "Iteration 37, loss = 0.51967687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.41953946\n",
            "Iteration 39, loss = 0.40490031\n",
            "Iteration 40, loss = 0.39199005\n",
            "Iteration 41, loss = 0.37985707\n",
            "Iteration 42, loss = 0.36908206\n",
            "Iteration 43, loss = 0.35906451\n",
            "Iteration 44, loss = 0.35028812\n",
            "Iteration 45, loss = 0.34231828\n",
            "Iteration 46, loss = 0.33500756\n",
            "Iteration 47, loss = 0.32834954\n",
            "Iteration 48, loss = 0.32244255\n",
            "Iteration 49, loss = 0.31740245\n",
            "Iteration 50, loss = 0.31236948\n",
            "Iteration 51, loss = 0.30783712\n",
            "Iteration 52, loss = 0.30385463\n",
            "Iteration 53, loss = 0.29966087\n",
            "Iteration 54, loss = 0.29742761\n",
            "Iteration 55, loss = 0.29398812\n",
            "Iteration 56, loss = 0.29167863\n",
            "Iteration 57, loss = 0.28924010\n",
            "Iteration 58, loss = 0.28598272\n",
            "Iteration 59, loss = 0.28477792\n",
            "Iteration 60, loss = 0.28309759\n",
            "Iteration 61, loss = 0.28176407\n",
            "Iteration 62, loss = 0.28039116\n",
            "Iteration 63, loss = 0.27844410\n",
            "Iteration 64, loss = 0.27717419\n",
            "Iteration 65, loss = 0.27693364\n",
            "Iteration 66, loss = 0.27648380\n",
            "Iteration 67, loss = 0.27459313\n",
            "Iteration 68, loss = 0.27386426\n",
            "Iteration 69, loss = 0.27382490\n",
            "Iteration 70, loss = 0.27239912\n",
            "Iteration 71, loss = 0.26999251\n",
            "Iteration 72, loss = 0.27228387\n",
            "Iteration 73, loss = 0.27097295\n",
            "Iteration 74, loss = 0.27135729\n",
            "Iteration 75, loss = 0.27010058\n",
            "Iteration 76, loss = 0.27726311\n",
            "Iteration 77, loss = 0.27503328\n",
            "Iteration 78, loss = 0.26969671\n",
            "Iteration 79, loss = 0.26971984\n",
            "Iteration 80, loss = 0.27260035\n",
            "Iteration 81, loss = 0.27488190\n",
            "Iteration 82, loss = 0.27506710\n",
            "Iteration 83, loss = 0.27441194\n",
            "Iteration 84, loss = 0.27020623\n",
            "Iteration 85, loss = 0.27993545\n",
            "Iteration 86, loss = 0.28357652\n",
            "Iteration 87, loss = 0.27186451\n",
            "Iteration 88, loss = 0.28166675\n",
            "Iteration 89, loss = 0.26786637\n",
            "Iteration 90, loss = 0.26986253\n",
            "Iteration 91, loss = 0.27773320\n",
            "Iteration 92, loss = 0.27032575\n",
            "Iteration 93, loss = 0.29326494\n",
            "Iteration 94, loss = 0.28656623\n",
            "Iteration 95, loss = 0.28105531\n",
            "Iteration 96, loss = 0.27928476\n",
            "Iteration 97, loss = 0.27228294\n",
            "Iteration 98, loss = 0.27595008\n",
            "Iteration 99, loss = 0.27205060\n",
            "Iteration 100, loss = 0.27286562\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.25421789\n",
            "Iteration 102, loss = 0.25401589\n",
            "Iteration 103, loss = 0.25399702\n",
            "Iteration 104, loss = 0.25394277\n",
            "Iteration 105, loss = 0.25394533\n",
            "Iteration 106, loss = 0.25391967\n",
            "Iteration 107, loss = 0.25385652\n",
            "Iteration 108, loss = 0.25375533\n",
            "Iteration 109, loss = 0.25375949\n",
            "Iteration 110, loss = 0.25376468\n",
            "Iteration 111, loss = 0.25368963\n",
            "Iteration 112, loss = 0.25365361\n",
            "Iteration 113, loss = 0.25362185\n",
            "Iteration 114, loss = 0.25358248\n",
            "Iteration 115, loss = 0.25352556\n",
            "Iteration 116, loss = 0.25353582\n",
            "Iteration 117, loss = 0.25350028\n",
            "Iteration 118, loss = 0.25343230\n",
            "Iteration 119, loss = 0.25339424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 120, loss = 0.25312717\n",
            "Iteration 121, loss = 0.25311403\n",
            "Iteration 122, loss = 0.25310572\n",
            "Iteration 123, loss = 0.25309227\n",
            "Iteration 124, loss = 0.25307334\n",
            "Iteration 125, loss = 0.25305466\n",
            "Iteration 126, loss = 0.25306963\n",
            "Iteration 127, loss = 0.25306914\n",
            "Iteration 128, loss = 0.25303217\n",
            "Iteration 129, loss = 0.25303616\n",
            "Iteration 130, loss = 0.25302700\n",
            "Iteration 131, loss = 0.25304582\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 132, loss = 0.25293747\n",
            "Iteration 133, loss = 0.25294001\n",
            "Iteration 134, loss = 0.25292965\n",
            "Iteration 135, loss = 0.25292520\n",
            "Iteration 136, loss = 0.25292480\n",
            "Iteration 137, loss = 0.25293906\n",
            "Iteration 138, loss = 0.25294072\n",
            "Iteration 139, loss = 0.25293525\n",
            "Iteration 140, loss = 0.25292965\n",
            "Iteration 141, loss = 0.25292202\n",
            "Iteration 142, loss = 0.25291880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 1.00611047\n",
            "Iteration 2, loss = 0.64798397\n",
            "Iteration 3, loss = 0.63499112\n",
            "Iteration 4, loss = 0.60472742\n",
            "Iteration 5, loss = 0.59044318\n",
            "Iteration 6, loss = 0.58180491\n",
            "Iteration 7, loss = 0.59232496\n",
            "Iteration 8, loss = 0.59595891\n",
            "Iteration 9, loss = 0.59031491\n",
            "Iteration 10, loss = 0.59570319\n",
            "Iteration 11, loss = 0.57861586\n",
            "Iteration 12, loss = 0.57233170\n",
            "Iteration 13, loss = 0.56922703\n",
            "Iteration 14, loss = 0.56799468\n",
            "Iteration 15, loss = 0.56693181\n",
            "Iteration 16, loss = 0.60806789\n",
            "Iteration 17, loss = 0.58589607\n",
            "Iteration 18, loss = 0.57569555\n",
            "Iteration 19, loss = 0.57102377\n",
            "Iteration 20, loss = 0.56879352\n",
            "Iteration 21, loss = 0.56814737\n",
            "Iteration 22, loss = 0.56796398\n",
            "Iteration 23, loss = 0.56673195\n",
            "Iteration 24, loss = 0.56734156\n",
            "Iteration 25, loss = 0.56791229\n",
            "Iteration 26, loss = 0.56776600\n",
            "Iteration 27, loss = 0.56783491\n",
            "Iteration 28, loss = 0.56853252\n",
            "Iteration 29, loss = 0.56900355\n",
            "Iteration 30, loss = 0.56790321\n",
            "Iteration 31, loss = 0.56740277\n",
            "Iteration 32, loss = 0.56992503\n",
            "Iteration 33, loss = 0.56848934\n",
            "Iteration 34, loss = 0.56831389\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.50851885\n",
            "Iteration 36, loss = 0.48391076\n",
            "Iteration 37, loss = 0.50921758\n",
            "Iteration 38, loss = 0.51209445\n",
            "Iteration 39, loss = 0.51340974\n",
            "Iteration 40, loss = 0.51175690\n",
            "Iteration 41, loss = 0.50860762\n",
            "Iteration 42, loss = 0.51590125\n",
            "Iteration 43, loss = 0.51344171\n",
            "Iteration 44, loss = 0.51198242\n",
            "Iteration 45, loss = 0.51129328\n",
            "Iteration 46, loss = 0.51218567\n",
            "Iteration 47, loss = 0.51501266\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 48, loss = 0.41208145\n",
            "Iteration 49, loss = 0.39783560\n",
            "Iteration 50, loss = 0.38460428\n",
            "Iteration 51, loss = 0.37250206\n",
            "Iteration 52, loss = 0.36162519\n",
            "Iteration 53, loss = 0.35208265\n",
            "Iteration 54, loss = 0.34335576\n",
            "Iteration 55, loss = 0.33546013\n",
            "Iteration 56, loss = 0.32816624\n",
            "Iteration 57, loss = 0.32174917\n",
            "Iteration 58, loss = 0.31597514\n",
            "Iteration 59, loss = 0.31072427\n",
            "Iteration 60, loss = 0.30604623\n",
            "Iteration 61, loss = 0.30165722\n",
            "Iteration 62, loss = 0.29797414\n",
            "Iteration 63, loss = 0.29446945\n",
            "Iteration 64, loss = 0.29132498\n",
            "Iteration 65, loss = 0.28828082\n",
            "Iteration 66, loss = 0.28578797\n",
            "Iteration 67, loss = 0.28342802\n",
            "Iteration 68, loss = 0.28146217\n",
            "Iteration 69, loss = 0.27911667\n",
            "Iteration 70, loss = 0.27751029\n",
            "Iteration 71, loss = 0.27637506\n",
            "Iteration 72, loss = 0.27512397\n",
            "Iteration 73, loss = 0.27379422\n",
            "Iteration 74, loss = 0.27181027\n",
            "Iteration 75, loss = 0.27058361\n",
            "Iteration 76, loss = 0.27037693\n",
            "Iteration 77, loss = 0.26990211\n",
            "Iteration 78, loss = 0.26867935\n",
            "Iteration 79, loss = 0.26716148\n",
            "Iteration 80, loss = 0.26737698\n",
            "Iteration 81, loss = 0.26649423\n",
            "Iteration 82, loss = 0.26657314\n",
            "Iteration 83, loss = 0.26525823\n",
            "Iteration 84, loss = 0.26633947\n",
            "Iteration 85, loss = 0.26391500\n",
            "Iteration 86, loss = 0.26586326\n",
            "Iteration 87, loss = 0.26380330\n",
            "Iteration 88, loss = 0.26439958\n",
            "Iteration 89, loss = 0.26514981\n",
            "Iteration 90, loss = 0.26075237\n",
            "Iteration 91, loss = 0.26560720\n",
            "Iteration 92, loss = 0.26751861\n",
            "Iteration 93, loss = 0.26558728\n",
            "Iteration 94, loss = 0.26651804\n",
            "Iteration 95, loss = 0.26833384\n",
            "Iteration 96, loss = 0.26515522\n",
            "Iteration 97, loss = 0.26604690\n",
            "Iteration 98, loss = 0.26702728\n",
            "Iteration 99, loss = 0.26293131\n",
            "Iteration 100, loss = 0.27471359\n",
            "Iteration 101, loss = 0.27544477\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 102, loss = 0.25187756\n",
            "Iteration 103, loss = 0.25171725\n",
            "Iteration 104, loss = 0.25168055\n",
            "Iteration 105, loss = 0.25159049\n",
            "Iteration 106, loss = 0.25156374\n",
            "Iteration 107, loss = 0.25150125\n",
            "Iteration 108, loss = 0.25144801\n",
            "Iteration 109, loss = 0.25139135\n",
            "Iteration 110, loss = 0.25134823\n",
            "Iteration 111, loss = 0.25128514\n",
            "Iteration 112, loss = 0.25124096\n",
            "Iteration 113, loss = 0.25121147\n",
            "Iteration 114, loss = 0.25114616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 115, loss = 0.25089867\n",
            "Iteration 116, loss = 0.25089049\n",
            "Iteration 117, loss = 0.25087540\n",
            "Iteration 118, loss = 0.25086190\n",
            "Iteration 119, loss = 0.25084772\n",
            "Iteration 120, loss = 0.25085966\n",
            "Iteration 121, loss = 0.25081094\n",
            "Iteration 122, loss = 0.25082622\n",
            "Iteration 123, loss = 0.25082888\n",
            "Iteration 124, loss = 0.25080397\n",
            "Iteration 125, loss = 0.25080789\n",
            "Iteration 126, loss = 0.25079483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 127, loss = 0.25070010\n",
            "Iteration 128, loss = 0.25069307\n",
            "Iteration 129, loss = 0.25069909\n",
            "Iteration 130, loss = 0.25070537\n",
            "Iteration 131, loss = 0.25070268\n",
            "Iteration 132, loss = 0.25068978\n",
            "Iteration 133, loss = 0.25067450\n",
            "Iteration 134, loss = 0.25069021\n",
            "Iteration 135, loss = 0.25069252\n",
            "Iteration 136, loss = 0.25068602\n",
            "Iteration 137, loss = 0.25069848\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66431043\n",
            "Iteration 3, loss = 0.62973287\n",
            "Iteration 4, loss = 0.60424504\n",
            "Iteration 5, loss = 0.58896472\n",
            "Iteration 6, loss = 0.58201702\n",
            "Iteration 7, loss = 0.57502463\n",
            "Iteration 8, loss = 0.57230713\n",
            "Iteration 9, loss = 0.57117513\n",
            "Iteration 10, loss = 0.56948187\n",
            "Iteration 11, loss = 0.56908177\n",
            "Iteration 12, loss = 0.56841659\n",
            "Iteration 13, loss = 0.56940231\n",
            "Iteration 14, loss = 0.56991258\n",
            "Iteration 15, loss = 0.56959739\n",
            "Iteration 16, loss = 0.56895459\n",
            "Iteration 17, loss = 0.56746288\n",
            "Iteration 18, loss = 0.56984377\n",
            "Iteration 19, loss = 0.57007446\n",
            "Iteration 20, loss = 0.56951998\n",
            "Iteration 21, loss = 0.56979712\n",
            "Iteration 22, loss = 0.56894630\n",
            "Iteration 23, loss = 0.56910648\n",
            "Iteration 24, loss = 0.56836633\n",
            "Iteration 25, loss = 0.57001892\n",
            "Iteration 26, loss = 0.56802295\n",
            "Iteration 27, loss = 0.57021369\n",
            "Iteration 28, loss = 0.56840285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.50396089\n",
            "Iteration 30, loss = 0.48349611\n",
            "Iteration 31, loss = 0.50946586\n",
            "Iteration 32, loss = 0.51199824\n",
            "Iteration 33, loss = 0.51028717\n",
            "Iteration 34, loss = 0.50725692\n",
            "Iteration 35, loss = 0.50875230\n",
            "Iteration 36, loss = 0.51510742\n",
            "Iteration 37, loss = 0.50715237\n",
            "Iteration 38, loss = 0.50832429\n",
            "Iteration 39, loss = 0.51633402\n",
            "Iteration 40, loss = 0.51147118\n",
            "Iteration 41, loss = 0.51159137\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 42, loss = 0.41017387\n",
            "Iteration 43, loss = 0.39538769\n",
            "Iteration 44, loss = 0.38226912\n",
            "Iteration 45, loss = 0.37022569\n",
            "Iteration 46, loss = 0.35963521\n",
            "Iteration 47, loss = 0.34974673\n",
            "Iteration 48, loss = 0.34114904\n",
            "Iteration 49, loss = 0.33340928\n",
            "Iteration 50, loss = 0.32620441\n",
            "Iteration 51, loss = 0.32004909\n",
            "Iteration 52, loss = 0.31456027\n",
            "Iteration 53, loss = 0.30936981\n",
            "Iteration 54, loss = 0.30457769\n",
            "Iteration 55, loss = 0.30074313\n",
            "Iteration 56, loss = 0.29639035\n",
            "Iteration 57, loss = 0.29315539\n",
            "Iteration 58, loss = 0.28990273\n",
            "Iteration 59, loss = 0.28708283\n",
            "Iteration 60, loss = 0.28465028\n",
            "Iteration 61, loss = 0.28205235\n",
            "Iteration 62, loss = 0.28007989\n",
            "Iteration 63, loss = 0.27806250\n",
            "Iteration 64, loss = 0.27699065\n",
            "Iteration 65, loss = 0.27475319\n",
            "Iteration 66, loss = 0.27368107\n",
            "Iteration 67, loss = 0.27251414\n",
            "Iteration 68, loss = 0.27053197\n",
            "Iteration 69, loss = 0.27077113\n",
            "Iteration 70, loss = 0.26884030\n",
            "Iteration 71, loss = 0.26879118\n",
            "Iteration 72, loss = 0.26667219\n",
            "Iteration 73, loss = 0.26789215\n",
            "Iteration 74, loss = 0.26644922\n",
            "Iteration 75, loss = 0.26568730\n",
            "Iteration 76, loss = 0.26698602\n",
            "Iteration 77, loss = 0.26466303\n",
            "Iteration 78, loss = 0.26525157\n",
            "Iteration 79, loss = 0.26313346\n",
            "Iteration 80, loss = 0.26368333\n",
            "Iteration 81, loss = 0.26220001\n",
            "Iteration 82, loss = 0.26324115\n",
            "Iteration 83, loss = 0.26542012\n",
            "Iteration 84, loss = 0.26264572\n",
            "Iteration 85, loss = 0.26352563\n",
            "Iteration 86, loss = 0.26070642\n",
            "Iteration 87, loss = 0.26465643\n",
            "Iteration 88, loss = 0.26228413\n",
            "Iteration 89, loss = 0.26427968\n",
            "Iteration 90, loss = 0.26862293\n",
            "Iteration 91, loss = 0.27152243\n",
            "Iteration 92, loss = 0.27042247\n",
            "Iteration 93, loss = 0.27117390\n",
            "Iteration 94, loss = 0.26426603\n",
            "Iteration 95, loss = 0.26606465\n",
            "Iteration 96, loss = 0.26555316\n",
            "Iteration 97, loss = 0.26414323\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.25101666\n",
            "Iteration 99, loss = 0.25100856\n",
            "Iteration 100, loss = 0.25093053\n",
            "Iteration 101, loss = 0.25088250\n",
            "Iteration 102, loss = 0.25089194\n",
            "Iteration 103, loss = 0.25080636\n",
            "Iteration 104, loss = 0.25074278\n",
            "Iteration 105, loss = 0.25064856\n",
            "Iteration 106, loss = 0.25066623\n",
            "Iteration 107, loss = 0.25062438\n",
            "Iteration 108, loss = 0.25057388\n",
            "Iteration 109, loss = 0.25052953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.25027443\n",
            "Iteration 111, loss = 0.25027310\n",
            "Iteration 112, loss = 0.25024511\n",
            "Iteration 113, loss = 0.25023282\n",
            "Iteration 114, loss = 0.25024590\n",
            "Iteration 115, loss = 0.25021680\n",
            "Iteration 116, loss = 0.25023753\n",
            "Iteration 117, loss = 0.25019602\n",
            "Iteration 118, loss = 0.25018283\n",
            "Iteration 119, loss = 0.25016466\n",
            "Iteration 120, loss = 0.25017246\n",
            "Iteration 121, loss = 0.25017876\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.25008124\n",
            "Iteration 123, loss = 0.25009247\n",
            "Iteration 124, loss = 0.25008262\n",
            "Iteration 125, loss = 0.25007510\n",
            "Iteration 126, loss = 0.25006685\n",
            "Iteration 127, loss = 0.25008584\n",
            "Iteration 128, loss = 0.25007281\n",
            "Iteration 129, loss = 0.25007348\n",
            "Iteration 130, loss = 0.25006306\n",
            "Iteration 131, loss = 0.25007522\n",
            "Iteration 132, loss = 0.25006945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62906518\n",
            "Iteration 3, loss = 0.60403777\n",
            "Iteration 4, loss = 0.58840061\n",
            "Iteration 5, loss = 0.58108895\n",
            "Iteration 6, loss = 0.57363573\n",
            "Iteration 7, loss = 0.57028648\n",
            "Iteration 8, loss = 0.56806584\n",
            "Iteration 9, loss = 0.56649625\n",
            "Iteration 10, loss = 0.56593121\n",
            "Iteration 11, loss = 0.56591002\n",
            "Iteration 12, loss = 0.56605251\n",
            "Iteration 13, loss = 0.56400442\n",
            "Iteration 14, loss = 0.56578320\n",
            "Iteration 15, loss = 0.56558589\n",
            "Iteration 16, loss = 0.56515058\n",
            "Iteration 17, loss = 0.56542759\n",
            "Iteration 18, loss = 0.56474188\n",
            "Iteration 19, loss = 0.56401428\n",
            "Iteration 20, loss = 0.56551536\n",
            "Iteration 21, loss = 0.56338297\n",
            "Iteration 22, loss = 0.56412552\n",
            "Iteration 23, loss = 0.56328990\n",
            "Iteration 24, loss = 0.56397147\n",
            "Iteration 25, loss = 0.56446803\n",
            "Iteration 26, loss = 0.56361782\n",
            "Iteration 27, loss = 0.56315306\n",
            "Iteration 28, loss = 0.56369795\n",
            "Iteration 29, loss = 0.56576718\n",
            "Iteration 30, loss = 0.56299246\n",
            "Iteration 31, loss = 0.56328305\n",
            "Iteration 32, loss = 0.56308704\n",
            "Iteration 33, loss = 0.56293110\n",
            "Iteration 34, loss = 0.56085338\n",
            "Iteration 35, loss = 0.56285530\n",
            "Iteration 36, loss = 0.56187028\n",
            "Iteration 37, loss = 0.56126499\n",
            "Iteration 38, loss = 0.56248493\n",
            "Iteration 39, loss = 0.56117366\n",
            "Iteration 40, loss = 0.56202120\n",
            "Iteration 41, loss = 0.56107837\n",
            "Iteration 42, loss = 0.56148744\n",
            "Iteration 43, loss = 0.55942318\n",
            "Iteration 44, loss = 0.56104569\n",
            "Iteration 45, loss = 0.56040936\n",
            "Iteration 46, loss = 0.55931939\n",
            "Iteration 47, loss = 0.55931396\n",
            "Iteration 48, loss = 0.55901977\n",
            "Iteration 49, loss = 0.55958738\n",
            "Iteration 50, loss = 0.56151924\n",
            "Iteration 51, loss = 0.55808645\n",
            "Iteration 52, loss = 0.55921590\n",
            "Iteration 53, loss = 0.55851777\n",
            "Iteration 54, loss = 0.55990593\n",
            "Iteration 55, loss = 0.55592761\n",
            "Iteration 56, loss = 0.55917987\n",
            "Iteration 57, loss = 0.55962965\n",
            "Iteration 58, loss = 0.55787400\n",
            "Iteration 59, loss = 0.55883782\n",
            "Iteration 60, loss = 0.55685309\n",
            "Iteration 61, loss = 0.55867102\n",
            "Iteration 62, loss = 0.55769989\n",
            "Iteration 63, loss = 0.55785989\n",
            "Iteration 64, loss = 0.58237047\n",
            "Iteration 65, loss = 0.57197660\n",
            "Iteration 66, loss = 0.56238595\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 67, loss = 0.52140112\n",
            "Iteration 68, loss = 0.49979328\n",
            "Iteration 69, loss = 0.49931792\n",
            "Iteration 70, loss = 0.51506625\n",
            "Iteration 71, loss = 0.51337826\n",
            "Iteration 72, loss = 0.50824421\n",
            "Iteration 73, loss = 0.50552780\n",
            "Iteration 74, loss = 0.51012344\n",
            "Iteration 75, loss = 0.50767018\n",
            "Iteration 76, loss = 0.50632595\n",
            "Iteration 77, loss = 0.50163769\n",
            "Iteration 78, loss = 0.50562821\n",
            "Iteration 79, loss = 0.50503423\n",
            "Iteration 80, loss = 0.50089928\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.40933010\n",
            "Iteration 82, loss = 0.39584297\n",
            "Iteration 83, loss = 0.38349527\n",
            "Iteration 84, loss = 0.37199506\n",
            "Iteration 85, loss = 0.36153129\n",
            "Iteration 86, loss = 0.35194871\n",
            "Iteration 87, loss = 0.34290710\n",
            "Iteration 88, loss = 0.33531407\n",
            "Iteration 89, loss = 0.32774583\n",
            "Iteration 90, loss = 0.32144655\n",
            "Iteration 91, loss = 0.31533479\n",
            "Iteration 92, loss = 0.30955740\n",
            "Iteration 93, loss = 0.30445360\n",
            "Iteration 94, loss = 0.29975728\n",
            "Iteration 95, loss = 0.29613290\n",
            "Iteration 96, loss = 0.29234749\n",
            "Iteration 97, loss = 0.28864196\n",
            "Iteration 98, loss = 0.28564846\n",
            "Iteration 99, loss = 0.28295622\n",
            "Iteration 100, loss = 0.27982764\n",
            "Iteration 101, loss = 0.27795839\n",
            "Iteration 102, loss = 0.27610908\n",
            "Iteration 103, loss = 0.27374723\n",
            "Iteration 104, loss = 0.27243808\n",
            "Iteration 105, loss = 0.26984185\n",
            "Iteration 106, loss = 0.26837464\n",
            "Iteration 107, loss = 0.26720272\n",
            "Iteration 108, loss = 0.26611130\n",
            "Iteration 109, loss = 0.26587923\n",
            "Iteration 110, loss = 0.26487121\n",
            "Iteration 111, loss = 0.26341235\n",
            "Iteration 112, loss = 0.26256655\n",
            "Iteration 113, loss = 0.26334886\n",
            "Iteration 114, loss = 0.26092167\n",
            "Iteration 115, loss = 0.26237453\n",
            "Iteration 116, loss = 0.25998513\n",
            "Iteration 117, loss = 0.25996761\n",
            "Iteration 118, loss = 0.25847364\n",
            "Iteration 119, loss = 0.25977646\n",
            "Iteration 120, loss = 0.26163925\n",
            "Iteration 121, loss = 0.25948954\n",
            "Iteration 122, loss = 0.26132606\n",
            "Iteration 123, loss = 0.26040938\n",
            "Iteration 124, loss = 0.26036659\n",
            "Iteration 125, loss = 0.25767516\n",
            "Iteration 126, loss = 0.25745411\n",
            "Iteration 127, loss = 0.25878487\n",
            "Iteration 128, loss = 0.25487042\n",
            "Iteration 129, loss = 0.25995253\n",
            "Iteration 130, loss = 0.26999375\n",
            "Iteration 131, loss = 0.26584207\n",
            "Iteration 132, loss = 0.26488502\n",
            "Iteration 133, loss = 0.39212032\n",
            "Iteration 134, loss = 1.03827514\n",
            "Iteration 135, loss = 1.01452034\n",
            "Iteration 136, loss = 0.76735661\n",
            "Iteration 137, loss = 0.70949590\n",
            "Iteration 138, loss = 0.70398436\n",
            "Iteration 139, loss = 0.70014803\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 140, loss = 0.69772612\n",
            "Iteration 141, loss = 0.69691573\n",
            "Iteration 142, loss = 0.69614520\n",
            "Iteration 143, loss = 0.69536981\n",
            "Iteration 144, loss = 0.69456948\n",
            "Iteration 145, loss = 0.69378362\n",
            "Iteration 146, loss = 0.69297008\n",
            "Iteration 147, loss = 0.69214229\n",
            "Iteration 148, loss = 0.69127104\n",
            "Iteration 149, loss = 0.69042350\n",
            "Iteration 150, loss = 0.68958855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 151, loss = 0.68899983\n",
            "Iteration 152, loss = 0.68880469\n",
            "Iteration 153, loss = 0.68862245\n",
            "Iteration 154, loss = 0.68844601\n",
            "Iteration 155, loss = 0.68825699\n",
            "Iteration 156, loss = 0.68808004\n",
            "Iteration 157, loss = 0.68790305\n",
            "Iteration 158, loss = 0.68771399\n",
            "Iteration 159, loss = 0.68751868\n",
            "Iteration 160, loss = 0.68735215\n",
            "Iteration 161, loss = 0.68716306\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 162, loss = 0.68704214\n",
            "Iteration 163, loss = 0.68700271\n",
            "Iteration 164, loss = 0.68696811\n",
            "Iteration 165, loss = 0.68692995\n",
            "Iteration 166, loss = 0.68689189\n",
            "Iteration 167, loss = 0.68685608\n",
            "Iteration 168, loss = 0.68681948\n",
            "Iteration 169, loss = 0.68678107\n",
            "Iteration 170, loss = 0.68674319\n",
            "Iteration 171, loss = 0.68670634\n",
            "Iteration 172, loss = 0.68666995\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64283789\n",
            "Iteration 3, loss = 0.61458501\n",
            "Iteration 4, loss = 0.59525728\n",
            "Iteration 5, loss = 0.58470562\n",
            "Iteration 6, loss = 0.57925453\n",
            "Iteration 7, loss = 0.57363628\n",
            "Iteration 8, loss = 0.57409917\n",
            "Iteration 9, loss = 0.56953718\n",
            "Iteration 10, loss = 0.56973848\n",
            "Iteration 11, loss = 0.56906207\n",
            "Iteration 12, loss = 0.56910766\n",
            "Iteration 13, loss = 0.56829969\n",
            "Iteration 14, loss = 0.56930368\n",
            "Iteration 15, loss = 0.57016238\n",
            "Iteration 16, loss = 0.56834565\n",
            "Iteration 17, loss = 0.57029758\n",
            "Iteration 18, loss = 0.56940154\n",
            "Iteration 19, loss = 0.57001240\n",
            "Iteration 20, loss = 0.56900523\n",
            "Iteration 21, loss = 0.57007113\n",
            "Iteration 22, loss = 0.57021495\n",
            "Iteration 23, loss = 0.56989166\n",
            "Iteration 24, loss = 0.56809929\n",
            "Iteration 25, loss = 0.56993770\n",
            "Iteration 26, loss = 0.57036236\n",
            "Iteration 27, loss = 0.57130608\n",
            "Iteration 28, loss = 0.57006763\n",
            "Iteration 29, loss = 0.57110499\n",
            "Iteration 30, loss = 0.57062904\n",
            "Iteration 31, loss = 0.57002102\n",
            "Iteration 32, loss = 0.56984651\n",
            "Iteration 33, loss = 0.57153252\n",
            "Iteration 34, loss = 0.56975725\n",
            "Iteration 35, loss = 0.57002617\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.51490419\n",
            "Iteration 37, loss = 0.49445690\n",
            "Iteration 38, loss = 0.51618323\n",
            "Iteration 39, loss = 0.52057325\n",
            "Iteration 40, loss = 0.51974981\n",
            "Iteration 41, loss = 0.51999878\n",
            "Iteration 42, loss = 0.51825983\n",
            "Iteration 43, loss = 0.51855619\n",
            "Iteration 44, loss = 0.52283210\n",
            "Iteration 45, loss = 0.51846925\n",
            "Iteration 46, loss = 0.51838370\n",
            "Iteration 47, loss = 0.51939352\n",
            "Iteration 48, loss = 0.51861012\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.43249634\n",
            "Iteration 50, loss = 0.41961729\n",
            "Iteration 51, loss = 0.40783455\n",
            "Iteration 52, loss = 0.39728315\n",
            "Iteration 53, loss = 0.38749213\n",
            "Iteration 54, loss = 0.37869563\n",
            "Iteration 55, loss = 0.37057036\n",
            "Iteration 56, loss = 0.36342948\n",
            "Iteration 57, loss = 0.35755137\n",
            "Iteration 58, loss = 0.35263504\n",
            "Iteration 59, loss = 0.34677987\n",
            "Iteration 60, loss = 0.34266542\n",
            "Iteration 61, loss = 0.33881331\n",
            "Iteration 62, loss = 0.33561282\n",
            "Iteration 63, loss = 0.33294497\n",
            "Iteration 64, loss = 0.33004821\n",
            "Iteration 65, loss = 0.32797385\n",
            "Iteration 66, loss = 0.32592452\n",
            "Iteration 67, loss = 0.32411659\n",
            "Iteration 68, loss = 0.32285670\n",
            "Iteration 69, loss = 0.32240103\n",
            "Iteration 70, loss = 0.32041269\n",
            "Iteration 71, loss = 0.32035906\n",
            "Iteration 72, loss = 0.31977929\n",
            "Iteration 73, loss = 0.31942392\n",
            "Iteration 74, loss = 0.31950006\n",
            "Iteration 75, loss = 0.31961765\n",
            "Iteration 76, loss = 0.31831743\n",
            "Iteration 77, loss = 0.32427745\n",
            "Iteration 78, loss = 0.31990444\n",
            "Iteration 79, loss = 0.31830647\n",
            "Iteration 80, loss = 0.31764174\n",
            "Iteration 81, loss = 0.32104060\n",
            "Iteration 82, loss = 0.32088346\n",
            "Iteration 83, loss = 0.31834087\n",
            "Iteration 84, loss = 0.31855377\n",
            "Iteration 85, loss = 0.31704539\n",
            "Iteration 86, loss = 0.31644619\n",
            "Iteration 87, loss = 0.32256320\n",
            "Iteration 88, loss = 0.32193090\n",
            "Iteration 89, loss = 0.31711814\n",
            "Iteration 90, loss = 0.31874812\n",
            "Iteration 91, loss = 0.32136980\n",
            "Iteration 92, loss = 0.31961284\n",
            "Iteration 93, loss = 0.31812190\n",
            "Iteration 94, loss = 0.32169079\n",
            "Iteration 95, loss = 0.32061961\n",
            "Iteration 96, loss = 0.31629145\n",
            "Iteration 97, loss = 0.31680488\n",
            "Iteration 98, loss = 0.32561147\n",
            "Iteration 99, loss = 0.32184634\n",
            "Iteration 100, loss = 0.31881632\n",
            "Iteration 101, loss = 0.32537837\n",
            "Iteration 102, loss = 0.32415917\n",
            "Iteration 103, loss = 0.31926659\n",
            "Iteration 104, loss = 0.31414762\n",
            "Iteration 105, loss = 0.32184231\n",
            "Iteration 106, loss = 0.31566715\n",
            "Iteration 107, loss = 0.31973410\n",
            "Iteration 108, loss = 0.31829637\n",
            "Iteration 109, loss = 0.32580520\n",
            "Iteration 110, loss = 0.31202122\n",
            "Iteration 111, loss = 0.31642013\n",
            "Iteration 112, loss = 0.31132306\n",
            "Iteration 113, loss = 0.31756546\n",
            "Iteration 114, loss = 0.31550557\n",
            "Iteration 115, loss = 0.31602205\n",
            "Iteration 116, loss = 0.31543326\n",
            "Iteration 117, loss = 0.31314442\n",
            "Iteration 118, loss = 0.30926803\n",
            "Iteration 119, loss = 0.31344468\n",
            "Iteration 120, loss = 0.31624641\n",
            "Iteration 121, loss = 0.31632600\n",
            "Iteration 122, loss = 0.31620100\n",
            "Iteration 123, loss = 0.31893786\n",
            "Iteration 124, loss = 0.31681682\n",
            "Iteration 125, loss = 0.31785095\n",
            "Iteration 126, loss = 0.31464526\n",
            "Iteration 127, loss = 0.30915900\n",
            "Iteration 128, loss = 0.31419061\n",
            "Iteration 129, loss = 0.31720992\n",
            "Iteration 130, loss = 0.31319342\n",
            "Iteration 131, loss = 0.31701931\n",
            "Iteration 132, loss = 0.31162284\n",
            "Iteration 133, loss = 0.30882741\n",
            "Iteration 134, loss = 0.31526313\n",
            "Iteration 135, loss = 0.31222199\n",
            "Iteration 136, loss = 0.30889668\n",
            "Iteration 137, loss = 0.30438815\n",
            "Iteration 138, loss = 0.30876797\n",
            "Iteration 139, loss = 0.30847974\n",
            "Iteration 140, loss = 0.31416730\n",
            "Iteration 141, loss = 0.31262594\n",
            "Iteration 142, loss = 0.30659754\n",
            "Iteration 143, loss = 0.31275532\n",
            "Iteration 144, loss = 0.31381560\n",
            "Iteration 145, loss = 0.30770983\n",
            "Iteration 146, loss = 0.30829596\n",
            "Iteration 147, loss = 0.30211002\n",
            "Iteration 148, loss = 0.31114890\n",
            "Iteration 149, loss = 0.30393923\n",
            "Iteration 150, loss = 0.30866632\n",
            "Iteration 151, loss = 0.30712470\n",
            "Iteration 152, loss = 0.30510264\n",
            "Iteration 153, loss = 0.31050473\n",
            "Iteration 154, loss = 0.30990394\n",
            "Iteration 155, loss = 0.30080287\n",
            "Iteration 156, loss = 0.30698771\n",
            "Iteration 157, loss = 0.30456840\n",
            "Iteration 158, loss = 0.31125055\n",
            "Iteration 159, loss = 0.30989538\n",
            "Iteration 160, loss = 0.30388546\n",
            "Iteration 161, loss = 0.31001691\n",
            "Iteration 162, loss = 0.30396150\n",
            "Iteration 163, loss = 0.30311073\n",
            "Iteration 164, loss = 0.30999572\n",
            "Iteration 165, loss = 0.30284660\n",
            "Iteration 166, loss = 0.31233448\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 167, loss = 0.26874148\n",
            "Iteration 168, loss = 0.26841437\n",
            "Iteration 169, loss = 0.26825526\n",
            "Iteration 170, loss = 0.26816529\n",
            "Iteration 171, loss = 0.26807466\n",
            "Iteration 172, loss = 0.26803564\n",
            "Iteration 173, loss = 0.26796292\n",
            "Iteration 174, loss = 0.26792131\n",
            "Iteration 175, loss = 0.26779512\n",
            "Iteration 176, loss = 0.26779650\n",
            "Iteration 177, loss = 0.26775889\n",
            "Iteration 178, loss = 0.26772554\n",
            "Iteration 179, loss = 0.26769370\n",
            "Iteration 180, loss = 0.26759427\n",
            "Iteration 181, loss = 0.26761462\n",
            "Iteration 182, loss = 0.26746995\n",
            "Iteration 183, loss = 0.26748582\n",
            "Iteration 184, loss = 0.26738679\n",
            "Iteration 185, loss = 0.26733057\n",
            "Iteration 186, loss = 0.26729862\n",
            "Iteration 187, loss = 0.26729827\n",
            "Iteration 188, loss = 0.26722151\n",
            "Iteration 189, loss = 0.26713783\n",
            "Iteration 190, loss = 0.26712287\n",
            "Iteration 191, loss = 0.26700640\n",
            "Iteration 192, loss = 0.26700920\n",
            "Iteration 193, loss = 0.26695910\n",
            "Iteration 194, loss = 0.26696488\n",
            "Iteration 195, loss = 0.26685126\n",
            "Iteration 196, loss = 0.26686447\n",
            "Iteration 197, loss = 0.26677079\n",
            "Iteration 198, loss = 0.26670432\n",
            "Iteration 199, loss = 0.26672124\n",
            "Iteration 200, loss = 0.26662412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.92258208\n",
            "Iteration 2, loss = 0.62859405\n",
            "Iteration 3, loss = 0.60499517\n",
            "Iteration 4, loss = 0.58950941\n",
            "Iteration 5, loss = 0.58070060\n",
            "Iteration 6, loss = 0.57526330\n",
            "Iteration 7, loss = 0.57107391\n",
            "Iteration 8, loss = 0.56893921\n",
            "Iteration 9, loss = 0.56828778\n",
            "Iteration 10, loss = 0.56769686\n",
            "Iteration 11, loss = 0.56728947\n",
            "Iteration 12, loss = 0.56648088\n",
            "Iteration 13, loss = 0.56617878\n",
            "Iteration 14, loss = 0.56625600\n",
            "Iteration 15, loss = 0.57429958\n",
            "Iteration 16, loss = 0.57744544\n",
            "Iteration 17, loss = 0.57287543\n",
            "Iteration 18, loss = 0.57068224\n",
            "Iteration 19, loss = 0.56866831\n",
            "Iteration 20, loss = 0.56892117\n",
            "Iteration 21, loss = 0.56780017\n",
            "Iteration 22, loss = 0.56687158\n",
            "Iteration 23, loss = 0.56774477\n",
            "Iteration 24, loss = 0.56831213\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.50633547\n",
            "Iteration 26, loss = 0.48214000\n",
            "Iteration 27, loss = 0.50381057\n",
            "Iteration 28, loss = 0.51013060\n",
            "Iteration 29, loss = 0.50715354\n",
            "Iteration 30, loss = 0.51524733\n",
            "Iteration 31, loss = 0.51027236\n",
            "Iteration 32, loss = 0.50797078\n",
            "Iteration 33, loss = 0.50798644\n",
            "Iteration 34, loss = 0.51128451\n",
            "Iteration 35, loss = 0.50842680\n",
            "Iteration 36, loss = 0.51441399\n",
            "Iteration 37, loss = 0.51000203\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.41189967\n",
            "Iteration 39, loss = 0.39722443\n",
            "Iteration 40, loss = 0.38426370\n",
            "Iteration 41, loss = 0.37230772\n",
            "Iteration 42, loss = 0.36138003\n",
            "Iteration 43, loss = 0.35163176\n",
            "Iteration 44, loss = 0.34294567\n",
            "Iteration 45, loss = 0.33482061\n",
            "Iteration 46, loss = 0.32770620\n",
            "Iteration 47, loss = 0.32160650\n",
            "Iteration 48, loss = 0.31552610\n",
            "Iteration 49, loss = 0.31041164\n",
            "Iteration 50, loss = 0.30554938\n",
            "Iteration 51, loss = 0.30165170\n",
            "Iteration 52, loss = 0.29755499\n",
            "Iteration 53, loss = 0.29409467\n",
            "Iteration 54, loss = 0.29072025\n",
            "Iteration 55, loss = 0.28831375\n",
            "Iteration 56, loss = 0.28481117\n",
            "Iteration 57, loss = 0.28290123\n",
            "Iteration 58, loss = 0.28089277\n",
            "Iteration 59, loss = 0.27891575\n",
            "Iteration 60, loss = 0.27724462\n",
            "Iteration 61, loss = 0.27476521\n",
            "Iteration 62, loss = 0.27394039\n",
            "Iteration 63, loss = 0.27240422\n",
            "Iteration 64, loss = 0.27230064\n",
            "Iteration 65, loss = 0.27094747\n",
            "Iteration 66, loss = 0.26914832\n",
            "Iteration 67, loss = 0.26933857\n",
            "Iteration 68, loss = 0.26828541\n",
            "Iteration 69, loss = 0.26560523\n",
            "Iteration 70, loss = 0.26757612\n",
            "Iteration 71, loss = 0.26577408\n",
            "Iteration 72, loss = 0.26417611\n",
            "Iteration 73, loss = 0.26405161\n",
            "Iteration 74, loss = 0.26449564\n",
            "Iteration 75, loss = 0.26487110\n",
            "Iteration 76, loss = 0.26455846\n",
            "Iteration 77, loss = 0.26633277\n",
            "Iteration 78, loss = 0.26422152\n",
            "Iteration 79, loss = 0.26390216\n",
            "Iteration 80, loss = 0.26389583\n",
            "Iteration 81, loss = 0.26503130\n",
            "Iteration 82, loss = 0.26481294\n",
            "Iteration 83, loss = 0.26288195\n",
            "Iteration 84, loss = 0.26603458\n",
            "Iteration 85, loss = 0.26185862\n",
            "Iteration 86, loss = 0.26763237\n",
            "Iteration 87, loss = 0.26899972\n",
            "Iteration 88, loss = 0.26944899\n",
            "Iteration 89, loss = 0.26732047\n",
            "Iteration 90, loss = 0.26425397\n",
            "Iteration 91, loss = 0.26867818\n",
            "Iteration 92, loss = 0.27197634\n",
            "Iteration 93, loss = 0.26829833\n",
            "Iteration 94, loss = 0.26864815\n",
            "Iteration 95, loss = 0.26840522\n",
            "Iteration 96, loss = 0.26627701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.25059355\n",
            "Iteration 98, loss = 0.25049784\n",
            "Iteration 99, loss = 0.25048831\n",
            "Iteration 100, loss = 0.25040746\n",
            "Iteration 101, loss = 0.25037651\n",
            "Iteration 102, loss = 0.25028246\n",
            "Iteration 103, loss = 0.25027691\n",
            "Iteration 104, loss = 0.25025021\n",
            "Iteration 105, loss = 0.25018088\n",
            "Iteration 106, loss = 0.25018755\n",
            "Iteration 107, loss = 0.25016265\n",
            "Iteration 108, loss = 0.25011250\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.24982423\n",
            "Iteration 110, loss = 0.24979094\n",
            "Iteration 111, loss = 0.24977130\n",
            "Iteration 112, loss = 0.24977802\n",
            "Iteration 113, loss = 0.24978600\n",
            "Iteration 114, loss = 0.24976244\n",
            "Iteration 115, loss = 0.24977140\n",
            "Iteration 116, loss = 0.24973947\n",
            "Iteration 117, loss = 0.24975927\n",
            "Iteration 118, loss = 0.24973105\n",
            "Iteration 119, loss = 0.24972618\n",
            "Iteration 120, loss = 0.24972342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.24963531\n",
            "Iteration 122, loss = 0.24964884\n",
            "Iteration 123, loss = 0.24962607\n",
            "Iteration 124, loss = 0.24964143\n",
            "Iteration 125, loss = 0.24963266\n",
            "Iteration 126, loss = 0.24962860\n",
            "Iteration 127, loss = 0.24963095\n",
            "Iteration 128, loss = 0.24961951\n",
            "Iteration 129, loss = 0.24961606\n",
            "Iteration 130, loss = 0.24961863\n",
            "Iteration 131, loss = 0.24962122\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 34.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68298754\n",
            "Iteration 3, loss = 0.66941331\n",
            "Iteration 4, loss = 0.65544233\n",
            "Iteration 5, loss = 0.64076255\n",
            "Iteration 6, loss = 0.63277724\n",
            "Iteration 7, loss = 0.62418548\n",
            "Iteration 8, loss = 0.61711131\n",
            "Iteration 9, loss = 0.61019500\n",
            "Iteration 10, loss = 0.60634884\n",
            "Iteration 11, loss = 0.59520236\n",
            "Iteration 12, loss = 0.60103164\n",
            "Iteration 13, loss = 0.58932052\n",
            "Iteration 14, loss = 0.59065895\n",
            "Iteration 15, loss = 0.58872778\n",
            "Iteration 16, loss = 0.58802456\n",
            "Iteration 17, loss = 0.58416638\n",
            "Iteration 18, loss = 0.58682575\n",
            "Iteration 19, loss = 0.58268339\n",
            "Iteration 20, loss = 0.57814992\n",
            "Iteration 21, loss = 0.57579738\n",
            "Iteration 22, loss = 0.57334479\n",
            "Iteration 23, loss = 0.57883448\n",
            "Iteration 24, loss = 0.57228280\n",
            "Iteration 25, loss = 0.57232645\n",
            "Iteration 26, loss = 0.56966342\n",
            "Iteration 27, loss = 0.57232756\n",
            "Iteration 28, loss = 0.56944224\n",
            "Iteration 29, loss = 0.56913573\n",
            "Iteration 30, loss = 0.56389238\n",
            "Iteration 31, loss = 0.56841547\n",
            "Iteration 32, loss = 0.56627756\n",
            "Iteration 33, loss = 0.56685161\n",
            "Iteration 34, loss = 0.56581309\n",
            "Iteration 35, loss = 0.56731692\n",
            "Iteration 36, loss = 0.56563735\n",
            "Iteration 37, loss = 0.56851169\n",
            "Iteration 38, loss = 0.56894206\n",
            "Iteration 39, loss = 0.56464331\n",
            "Iteration 40, loss = 0.56684388\n",
            "Iteration 41, loss = 0.56637374\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 42, loss = 0.52179653\n",
            "Iteration 43, loss = 0.51162960\n",
            "Iteration 44, loss = 0.50542651\n",
            "Iteration 45, loss = 0.49877255\n",
            "Iteration 46, loss = 0.49275554\n",
            "Iteration 47, loss = 0.48421504\n",
            "Iteration 48, loss = 0.48162915\n",
            "Iteration 49, loss = 0.48445294\n",
            "Iteration 50, loss = 0.48325355\n",
            "Iteration 51, loss = 0.48810862\n",
            "Iteration 52, loss = 0.50410593\n",
            "Iteration 53, loss = 0.50718644\n",
            "Iteration 54, loss = 0.51101817\n",
            "Iteration 55, loss = 0.49971626\n",
            "Iteration 56, loss = 0.51380192\n",
            "Iteration 57, loss = 0.51236479\n",
            "Iteration 58, loss = 0.50628918\n",
            "Iteration 59, loss = 0.51183768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 60, loss = 0.44284922\n",
            "Iteration 61, loss = 0.43884717\n",
            "Iteration 62, loss = 0.43638357\n",
            "Iteration 63, loss = 0.43460305\n",
            "Iteration 64, loss = 0.43239684\n",
            "Iteration 65, loss = 0.43004192\n",
            "Iteration 66, loss = 0.42819438\n",
            "Iteration 67, loss = 0.42633640\n",
            "Iteration 68, loss = 0.42433366\n",
            "Iteration 69, loss = 0.42207333\n",
            "Iteration 70, loss = 0.42027388\n",
            "Iteration 71, loss = 0.41852089\n",
            "Iteration 72, loss = 0.41608858\n",
            "Iteration 73, loss = 0.41370504\n",
            "Iteration 74, loss = 0.41287837\n",
            "Iteration 75, loss = 0.41048173\n",
            "Iteration 76, loss = 0.40815504\n",
            "Iteration 77, loss = 0.40642430\n",
            "Iteration 78, loss = 0.40467342\n",
            "Iteration 79, loss = 0.40273256\n",
            "Iteration 80, loss = 0.40069158\n",
            "Iteration 81, loss = 0.39905108\n",
            "Iteration 82, loss = 0.39697697\n",
            "Iteration 83, loss = 0.39546556\n",
            "Iteration 84, loss = 0.39338771\n",
            "Iteration 85, loss = 0.39214321\n",
            "Iteration 86, loss = 0.38968830\n",
            "Iteration 87, loss = 0.38836900\n",
            "Iteration 88, loss = 0.38675493\n",
            "Iteration 89, loss = 0.38439404\n",
            "Iteration 90, loss = 0.38321559\n",
            "Iteration 91, loss = 0.38167663\n",
            "Iteration 92, loss = 0.38036154\n",
            "Iteration 93, loss = 0.37836183\n",
            "Iteration 94, loss = 0.37643855\n",
            "Iteration 95, loss = 0.37525561\n",
            "Iteration 96, loss = 0.37373665\n",
            "Iteration 97, loss = 0.37244424\n",
            "Iteration 98, loss = 0.37032078\n",
            "Iteration 99, loss = 0.36903938\n",
            "Iteration 100, loss = 0.36717738\n",
            "Iteration 101, loss = 0.36591502\n",
            "Iteration 102, loss = 0.36442620\n",
            "Iteration 103, loss = 0.36392538\n",
            "Iteration 104, loss = 0.36129215\n",
            "Iteration 105, loss = 0.35992744\n",
            "Iteration 106, loss = 0.35839219\n",
            "Iteration 107, loss = 0.35779793\n",
            "Iteration 108, loss = 0.35677702\n",
            "Iteration 109, loss = 0.35593136\n",
            "Iteration 110, loss = 0.35372622\n",
            "Iteration 111, loss = 0.35272746\n",
            "Iteration 112, loss = 0.35149118\n",
            "Iteration 113, loss = 0.35040672\n",
            "Iteration 114, loss = 0.34965211\n",
            "Iteration 115, loss = 0.34776545\n",
            "Iteration 116, loss = 0.34628098\n",
            "Iteration 117, loss = 0.34604141\n",
            "Iteration 118, loss = 0.34462905\n",
            "Iteration 119, loss = 0.34401688\n",
            "Iteration 120, loss = 0.34309282\n",
            "Iteration 121, loss = 0.34238899\n",
            "Iteration 122, loss = 0.34082700\n",
            "Iteration 123, loss = 0.33961340\n",
            "Iteration 124, loss = 0.33932938\n",
            "Iteration 125, loss = 0.33822348\n",
            "Iteration 126, loss = 0.33807630\n",
            "Iteration 127, loss = 0.33765836\n",
            "Iteration 128, loss = 0.33620715\n",
            "Iteration 129, loss = 0.33588688\n",
            "Iteration 130, loss = 0.33349585\n",
            "Iteration 131, loss = 0.33273324\n",
            "Iteration 132, loss = 0.33164415\n",
            "Iteration 133, loss = 0.33098995\n",
            "Iteration 134, loss = 0.33023809\n",
            "Iteration 135, loss = 0.33028392\n",
            "Iteration 136, loss = 0.32932853\n",
            "Iteration 137, loss = 0.32924151\n",
            "Iteration 138, loss = 0.32842631\n",
            "Iteration 139, loss = 0.32766692\n",
            "Iteration 140, loss = 0.32669298\n",
            "Iteration 141, loss = 0.32673275\n",
            "Iteration 142, loss = 0.32852448\n",
            "Iteration 143, loss = 0.32680056\n",
            "Iteration 144, loss = 0.32381489\n",
            "Iteration 145, loss = 0.32341355\n",
            "Iteration 146, loss = 0.32319311\n",
            "Iteration 147, loss = 0.32436808\n",
            "Iteration 148, loss = 0.32444164\n",
            "Iteration 149, loss = 0.32409206\n",
            "Iteration 150, loss = 0.31492724\n",
            "Iteration 151, loss = 0.30624776\n",
            "Iteration 152, loss = 0.30587031\n",
            "Iteration 153, loss = 0.30546018\n",
            "Iteration 154, loss = 0.30479767\n",
            "Iteration 155, loss = 0.30387263\n",
            "Iteration 156, loss = 0.30457002\n",
            "Iteration 157, loss = 0.30219367\n",
            "Iteration 158, loss = 0.30291401\n",
            "Iteration 159, loss = 0.30557452\n",
            "Iteration 160, loss = 0.30130565\n",
            "Iteration 161, loss = 0.30737822\n",
            "Iteration 162, loss = 0.29775253\n",
            "Iteration 163, loss = 0.30378496\n",
            "Iteration 164, loss = 0.29869749\n",
            "Iteration 165, loss = 0.29900910\n",
            "Iteration 166, loss = 0.30044887\n",
            "Iteration 167, loss = 0.29609092\n",
            "Iteration 168, loss = 0.29404629\n",
            "Iteration 169, loss = 0.29817062\n",
            "Iteration 170, loss = 0.29336982\n",
            "Iteration 171, loss = 0.29495610\n",
            "Iteration 172, loss = 0.29602658\n",
            "Iteration 173, loss = 0.29471197\n",
            "Iteration 174, loss = 0.29735259\n",
            "Iteration 175, loss = 0.29320808\n",
            "Iteration 176, loss = 0.29378506\n",
            "Iteration 177, loss = 0.29601526\n",
            "Iteration 178, loss = 0.29543034\n",
            "Iteration 179, loss = 0.29518170\n",
            "Iteration 180, loss = 0.29276903\n",
            "Iteration 181, loss = 0.29523780\n",
            "Iteration 182, loss = 0.29423200\n",
            "Iteration 183, loss = 0.29314458\n",
            "Iteration 184, loss = 0.29611331\n",
            "Iteration 185, loss = 0.29310378\n",
            "Iteration 186, loss = 0.29352995\n",
            "Iteration 187, loss = 0.29758144\n",
            "Iteration 188, loss = 0.29423330\n",
            "Iteration 189, loss = 0.29980052\n",
            "Iteration 190, loss = 0.29646082\n",
            "Iteration 191, loss = 0.29464489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 192, loss = 0.27652786\n",
            "Iteration 193, loss = 0.27532428\n",
            "Iteration 194, loss = 0.27526363\n",
            "Iteration 195, loss = 0.27524083\n",
            "Iteration 196, loss = 0.27502427\n",
            "Iteration 197, loss = 0.27500183\n",
            "Iteration 198, loss = 0.27483259\n",
            "Iteration 199, loss = 0.27483778\n",
            "Iteration 200, loss = 0.27466376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68782842\n",
            "Iteration 3, loss = 0.66876878\n",
            "Iteration 4, loss = 0.65476014\n",
            "Iteration 5, loss = 0.64429345\n",
            "Iteration 6, loss = 0.63339591\n",
            "Iteration 7, loss = 0.62393432\n",
            "Iteration 8, loss = 0.61622613\n",
            "Iteration 9, loss = 0.60829210\n",
            "Iteration 10, loss = 0.60592831\n",
            "Iteration 11, loss = 0.60048791\n",
            "Iteration 12, loss = 0.59417172\n",
            "Iteration 13, loss = 0.58958730\n",
            "Iteration 14, loss = 0.58875063\n",
            "Iteration 15, loss = 0.58799175\n",
            "Iteration 16, loss = 0.58150611\n",
            "Iteration 17, loss = 0.58134011\n",
            "Iteration 18, loss = 0.57887572\n",
            "Iteration 19, loss = 0.57443188\n",
            "Iteration 20, loss = 0.57180272\n",
            "Iteration 21, loss = 0.57356998\n",
            "Iteration 22, loss = 0.57163506\n",
            "Iteration 23, loss = 0.57207241\n",
            "Iteration 24, loss = 0.56977433\n",
            "Iteration 25, loss = 0.56937985\n",
            "Iteration 26, loss = 0.57097646\n",
            "Iteration 27, loss = 0.56488782\n",
            "Iteration 28, loss = 0.56937225\n",
            "Iteration 29, loss = 0.56511915\n",
            "Iteration 30, loss = 0.56673598\n",
            "Iteration 31, loss = 0.56543923\n",
            "Iteration 32, loss = 0.56512236\n",
            "Iteration 33, loss = 0.56393662\n",
            "Iteration 34, loss = 0.56403029\n",
            "Iteration 35, loss = 0.56735405\n",
            "Iteration 36, loss = 0.56051237\n",
            "Iteration 37, loss = 0.56672792\n",
            "Iteration 38, loss = 0.56579055\n",
            "Iteration 39, loss = 0.56167333\n",
            "Iteration 40, loss = 0.56741167\n",
            "Iteration 41, loss = 0.56211925\n",
            "Iteration 42, loss = 0.56329072\n",
            "Iteration 43, loss = 0.56370121\n",
            "Iteration 44, loss = 0.56417889\n",
            "Iteration 45, loss = 0.56902676\n",
            "Iteration 46, loss = 0.56839117\n",
            "Iteration 47, loss = 0.56885561\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.53170967\n",
            "Iteration 49, loss = 0.52004223\n",
            "Iteration 50, loss = 0.51379795\n",
            "Iteration 51, loss = 0.50795564\n",
            "Iteration 52, loss = 0.49977795\n",
            "Iteration 53, loss = 0.49160006\n",
            "Iteration 54, loss = 0.49023706\n",
            "Iteration 55, loss = 0.48784245\n",
            "Iteration 56, loss = 0.49302283\n",
            "Iteration 57, loss = 0.51709181\n",
            "Iteration 58, loss = 0.52047578\n",
            "Iteration 59, loss = 0.51766952\n",
            "Iteration 60, loss = 0.51458480\n",
            "Iteration 61, loss = 0.52601862\n",
            "Iteration 62, loss = 0.51617937\n",
            "Iteration 63, loss = 0.51441461\n",
            "Iteration 64, loss = 0.51413506\n",
            "Iteration 65, loss = 0.52295284\n",
            "Iteration 66, loss = 0.51132975\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.44401468\n",
            "Iteration 68, loss = 0.44071709\n",
            "Iteration 69, loss = 0.43805770\n",
            "Iteration 70, loss = 0.43539315\n",
            "Iteration 71, loss = 0.43228247\n",
            "Iteration 72, loss = 0.42967028\n",
            "Iteration 73, loss = 0.42676667\n",
            "Iteration 74, loss = 0.42433513\n",
            "Iteration 75, loss = 0.42179499\n",
            "Iteration 76, loss = 0.41905097\n",
            "Iteration 77, loss = 0.41620741\n",
            "Iteration 78, loss = 0.41393182\n",
            "Iteration 79, loss = 0.41157585\n",
            "Iteration 80, loss = 0.40891280\n",
            "Iteration 81, loss = 0.40634707\n",
            "Iteration 82, loss = 0.40389509\n",
            "Iteration 83, loss = 0.40209514\n",
            "Iteration 84, loss = 0.39934309\n",
            "Iteration 85, loss = 0.39657094\n",
            "Iteration 86, loss = 0.39422765\n",
            "Iteration 87, loss = 0.39210604\n",
            "Iteration 88, loss = 0.38910303\n",
            "Iteration 89, loss = 0.38751720\n",
            "Iteration 90, loss = 0.38490377\n",
            "Iteration 91, loss = 0.38319803\n",
            "Iteration 92, loss = 0.38105575\n",
            "Iteration 93, loss = 0.37885497\n",
            "Iteration 94, loss = 0.37647537\n",
            "Iteration 95, loss = 0.37451317\n",
            "Iteration 96, loss = 0.37201273\n",
            "Iteration 97, loss = 0.37031272\n",
            "Iteration 98, loss = 0.36820805\n",
            "Iteration 99, loss = 0.36661167\n",
            "Iteration 100, loss = 0.36477270\n",
            "Iteration 101, loss = 0.36237205\n",
            "Iteration 102, loss = 0.36077804\n",
            "Iteration 103, loss = 0.35835470\n",
            "Iteration 104, loss = 0.35653718\n",
            "Iteration 105, loss = 0.35581762\n",
            "Iteration 106, loss = 0.35379169\n",
            "Iteration 107, loss = 0.35223021\n",
            "Iteration 108, loss = 0.35094181\n",
            "Iteration 109, loss = 0.34791680\n",
            "Iteration 110, loss = 0.34721300\n",
            "Iteration 111, loss = 0.34564443\n",
            "Iteration 112, loss = 0.34418966\n",
            "Iteration 113, loss = 0.34212765\n",
            "Iteration 114, loss = 0.34147841\n",
            "Iteration 115, loss = 0.33885814\n",
            "Iteration 116, loss = 0.33778092\n",
            "Iteration 117, loss = 0.33601684\n",
            "Iteration 118, loss = 0.33482248\n",
            "Iteration 119, loss = 0.33279968\n",
            "Iteration 120, loss = 0.33278770\n",
            "Iteration 121, loss = 0.33053251\n",
            "Iteration 122, loss = 0.32948998\n",
            "Iteration 123, loss = 0.32741319\n",
            "Iteration 124, loss = 0.32713726\n",
            "Iteration 125, loss = 0.32713118\n",
            "Iteration 126, loss = 0.32470557\n",
            "Iteration 127, loss = 0.32345874\n",
            "Iteration 128, loss = 0.32282009\n",
            "Iteration 129, loss = 0.31996316\n",
            "Iteration 130, loss = 0.32001760\n",
            "Iteration 131, loss = 0.31898314\n",
            "Iteration 132, loss = 0.31683825\n",
            "Iteration 133, loss = 0.31656916\n",
            "Iteration 134, loss = 0.31493155\n",
            "Iteration 135, loss = 0.31590875\n",
            "Iteration 136, loss = 0.31319603\n",
            "Iteration 137, loss = 0.31295813\n",
            "Iteration 138, loss = 0.31230585\n",
            "Iteration 139, loss = 0.31264065\n",
            "Iteration 140, loss = 0.30949141\n",
            "Iteration 141, loss = 0.30848028\n",
            "Iteration 142, loss = 0.30910307\n",
            "Iteration 143, loss = 0.30718272\n",
            "Iteration 144, loss = 0.30628324\n",
            "Iteration 145, loss = 0.30541386\n",
            "Iteration 146, loss = 0.30686928\n",
            "Iteration 147, loss = 0.30415670\n",
            "Iteration 148, loss = 0.30335366\n",
            "Iteration 149, loss = 0.30270879\n",
            "Iteration 150, loss = 0.30643382\n",
            "Iteration 151, loss = 0.30317367\n",
            "Iteration 152, loss = 0.30229903\n",
            "Iteration 153, loss = 0.30027162\n",
            "Iteration 154, loss = 0.30033621\n",
            "Iteration 155, loss = 0.29911048\n",
            "Iteration 156, loss = 0.30009426\n",
            "Iteration 157, loss = 0.29565169\n",
            "Iteration 158, loss = 0.29824426\n",
            "Iteration 159, loss = 0.29754825\n",
            "Iteration 160, loss = 0.29869821\n",
            "Iteration 161, loss = 0.29400253\n",
            "Iteration 162, loss = 0.29428911\n",
            "Iteration 163, loss = 0.29491341\n",
            "Iteration 164, loss = 0.29575335\n",
            "Iteration 165, loss = 0.29234656\n",
            "Iteration 166, loss = 0.29459327\n",
            "Iteration 167, loss = 0.29530373\n",
            "Iteration 168, loss = 0.29559754\n",
            "Iteration 169, loss = 0.29173342\n",
            "Iteration 170, loss = 0.28800205\n",
            "Iteration 171, loss = 0.28894062\n",
            "Iteration 172, loss = 0.28977254\n",
            "Iteration 173, loss = 0.29340053\n",
            "Iteration 174, loss = 0.29110695\n",
            "Iteration 175, loss = 0.28808117\n",
            "Iteration 176, loss = 0.29477115\n",
            "Iteration 177, loss = 0.29224253\n",
            "Iteration 178, loss = 0.29069799\n",
            "Iteration 179, loss = 0.29230715\n",
            "Iteration 180, loss = 0.28657905\n",
            "Iteration 181, loss = 0.28824549\n",
            "Iteration 182, loss = 0.28657058\n",
            "Iteration 183, loss = 0.28494619\n",
            "Iteration 184, loss = 0.29123223\n",
            "Iteration 185, loss = 0.29171167\n",
            "Iteration 186, loss = 0.28741586\n",
            "Iteration 187, loss = 0.29210612\n",
            "Iteration 188, loss = 0.29002981\n",
            "Iteration 189, loss = 0.28860635\n",
            "Iteration 190, loss = 0.29364409\n",
            "Iteration 191, loss = 0.28974478\n",
            "Iteration 192, loss = 0.28326275\n",
            "Iteration 193, loss = 0.29201598\n",
            "Iteration 194, loss = 0.28772552\n",
            "Iteration 195, loss = 0.29079285\n",
            "Iteration 196, loss = 0.29390941\n",
            "Iteration 197, loss = 0.28164458\n",
            "Iteration 198, loss = 0.28524618\n",
            "Iteration 199, loss = 0.28103966\n",
            "Iteration 200, loss = 0.29791549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68256354\n",
            "Iteration 3, loss = 0.66691041\n",
            "Iteration 4, loss = 0.65235378\n",
            "Iteration 5, loss = 0.64154941\n",
            "Iteration 6, loss = 0.63140565\n",
            "Iteration 7, loss = 0.62080490\n",
            "Iteration 8, loss = 0.61871931\n",
            "Iteration 9, loss = 0.60755882\n",
            "Iteration 10, loss = 0.60989737\n",
            "Iteration 11, loss = 0.60883594\n",
            "Iteration 12, loss = 0.60312819\n",
            "Iteration 13, loss = 0.60118337\n",
            "Iteration 14, loss = 0.59166043\n",
            "Iteration 15, loss = 0.59140319\n",
            "Iteration 16, loss = 0.58668327\n",
            "Iteration 17, loss = 0.58250950\n",
            "Iteration 18, loss = 0.57858754\n",
            "Iteration 19, loss = 0.57406378\n",
            "Iteration 20, loss = 0.57988452\n",
            "Iteration 21, loss = 0.58029283\n",
            "Iteration 22, loss = 0.57536979\n",
            "Iteration 23, loss = 0.57787481\n",
            "Iteration 24, loss = 0.57359005\n",
            "Iteration 25, loss = 0.57020871\n",
            "Iteration 26, loss = 0.56709755\n",
            "Iteration 27, loss = 0.56957859\n",
            "Iteration 28, loss = 0.56641993\n",
            "Iteration 29, loss = 0.56666814\n",
            "Iteration 30, loss = 0.56509943\n",
            "Iteration 31, loss = 0.56122363\n",
            "Iteration 32, loss = 0.55905461\n",
            "Iteration 33, loss = 0.56754192\n",
            "Iteration 34, loss = 0.56378425\n",
            "Iteration 35, loss = 0.56172040\n",
            "Iteration 36, loss = 0.56642424\n",
            "Iteration 37, loss = 0.57407243\n",
            "Iteration 38, loss = 0.57435221\n",
            "Iteration 39, loss = 0.57268431\n",
            "Iteration 40, loss = 0.57102143\n",
            "Iteration 41, loss = 0.57241714\n",
            "Iteration 42, loss = 0.56858683\n",
            "Iteration 43, loss = 0.56911508\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 44, loss = 0.54264904\n",
            "Iteration 45, loss = 0.53228921\n",
            "Iteration 46, loss = 0.52733684\n",
            "Iteration 47, loss = 0.52279309\n",
            "Iteration 48, loss = 0.51904065\n",
            "Iteration 49, loss = 0.51326768\n",
            "Iteration 50, loss = 0.50900811\n",
            "Iteration 51, loss = 0.50498748\n",
            "Iteration 52, loss = 0.50107458\n",
            "Iteration 53, loss = 0.50378597\n",
            "Iteration 54, loss = 0.50411105\n",
            "Iteration 55, loss = 0.50701706\n",
            "Iteration 56, loss = 0.51341467\n",
            "Iteration 57, loss = 0.51206760\n",
            "Iteration 58, loss = 0.51920550\n",
            "Iteration 59, loss = 0.50487345\n",
            "Iteration 60, loss = 0.51749721\n",
            "Iteration 61, loss = 0.51989937\n",
            "Iteration 62, loss = 0.51457527\n",
            "Iteration 63, loss = 0.51147758\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 64, loss = 0.45784527\n",
            "Iteration 65, loss = 0.45485624\n",
            "Iteration 66, loss = 0.45318177\n",
            "Iteration 67, loss = 0.45093658\n",
            "Iteration 68, loss = 0.44880935\n",
            "Iteration 69, loss = 0.44666638\n",
            "Iteration 70, loss = 0.44468750\n",
            "Iteration 71, loss = 0.44265026\n",
            "Iteration 72, loss = 0.44111485\n",
            "Iteration 73, loss = 0.43854824\n",
            "Iteration 74, loss = 0.43653740\n",
            "Iteration 75, loss = 0.43413618\n",
            "Iteration 76, loss = 0.43234143\n",
            "Iteration 77, loss = 0.43046550\n",
            "Iteration 78, loss = 0.42792107\n",
            "Iteration 79, loss = 0.42584541\n",
            "Iteration 80, loss = 0.42438804\n",
            "Iteration 81, loss = 0.42209972\n",
            "Iteration 82, loss = 0.42001277\n",
            "Iteration 83, loss = 0.41768180\n",
            "Iteration 84, loss = 0.41594042\n",
            "Iteration 85, loss = 0.41422349\n",
            "Iteration 86, loss = 0.41223416\n",
            "Iteration 87, loss = 0.40997577\n",
            "Iteration 88, loss = 0.40821876\n",
            "Iteration 89, loss = 0.40641649\n",
            "Iteration 90, loss = 0.40428072\n",
            "Iteration 91, loss = 0.40278766\n",
            "Iteration 92, loss = 0.40051517\n",
            "Iteration 93, loss = 0.39820611\n",
            "Iteration 94, loss = 0.39669433\n",
            "Iteration 95, loss = 0.39453572\n",
            "Iteration 96, loss = 0.39339229\n",
            "Iteration 97, loss = 0.39125108\n",
            "Iteration 98, loss = 0.38923994\n",
            "Iteration 99, loss = 0.38727944\n",
            "Iteration 100, loss = 0.38550050\n",
            "Iteration 101, loss = 0.38418905\n",
            "Iteration 102, loss = 0.38237187\n",
            "Iteration 103, loss = 0.37991534\n",
            "Iteration 104, loss = 0.37906208\n",
            "Iteration 105, loss = 0.37708081\n",
            "Iteration 106, loss = 0.37520907\n",
            "Iteration 107, loss = 0.37377703\n",
            "Iteration 108, loss = 0.37096329\n",
            "Iteration 109, loss = 0.37051730\n",
            "Iteration 110, loss = 0.36864179\n",
            "Iteration 111, loss = 0.36704344\n",
            "Iteration 112, loss = 0.36571657\n",
            "Iteration 113, loss = 0.36367056\n",
            "Iteration 114, loss = 0.36136780\n",
            "Iteration 115, loss = 0.36060903\n",
            "Iteration 116, loss = 0.35950951\n",
            "Iteration 117, loss = 0.35720199\n",
            "Iteration 118, loss = 0.35565605\n",
            "Iteration 119, loss = 0.35482701\n",
            "Iteration 120, loss = 0.35322467\n",
            "Iteration 121, loss = 0.35160413\n",
            "Iteration 122, loss = 0.34952028\n",
            "Iteration 123, loss = 0.34847385\n",
            "Iteration 124, loss = 0.34770606\n",
            "Iteration 125, loss = 0.34664248\n",
            "Iteration 126, loss = 0.34487387\n",
            "Iteration 127, loss = 0.34267362\n",
            "Iteration 128, loss = 0.34255665\n",
            "Iteration 129, loss = 0.33929100\n",
            "Iteration 130, loss = 0.34103057\n",
            "Iteration 131, loss = 0.33829976\n",
            "Iteration 132, loss = 0.33794892\n",
            "Iteration 133, loss = 0.33627062\n",
            "Iteration 134, loss = 0.33465979\n",
            "Iteration 135, loss = 0.33281843\n",
            "Iteration 136, loss = 0.33343940\n",
            "Iteration 137, loss = 0.33116896\n",
            "Iteration 138, loss = 0.33049668\n",
            "Iteration 139, loss = 0.32785213\n",
            "Iteration 140, loss = 0.32814663\n",
            "Iteration 141, loss = 0.32513211\n",
            "Iteration 142, loss = 0.32639268\n",
            "Iteration 143, loss = 0.32591968\n",
            "Iteration 144, loss = 0.32533614\n",
            "Iteration 145, loss = 0.32269084\n",
            "Iteration 146, loss = 0.32560983\n",
            "Iteration 147, loss = 0.32119508\n",
            "Iteration 148, loss = 0.32095980\n",
            "Iteration 149, loss = 0.32132784\n",
            "Iteration 150, loss = 0.31978791\n",
            "Iteration 151, loss = 0.31690632\n",
            "Iteration 152, loss = 0.31924606\n",
            "Iteration 153, loss = 0.31590465\n",
            "Iteration 154, loss = 0.31623041\n",
            "Iteration 155, loss = 0.31448420\n",
            "Iteration 156, loss = 0.31522254\n",
            "Iteration 157, loss = 0.31235391\n",
            "Iteration 158, loss = 0.31340287\n",
            "Iteration 159, loss = 0.31205907\n",
            "Iteration 160, loss = 0.31308073\n",
            "Iteration 161, loss = 0.31064590\n",
            "Iteration 162, loss = 0.31128731\n",
            "Iteration 163, loss = 0.31264770\n",
            "Iteration 164, loss = 0.31059960\n",
            "Iteration 165, loss = 0.31258475\n",
            "Iteration 166, loss = 0.30884592\n",
            "Iteration 167, loss = 0.31003417\n",
            "Iteration 168, loss = 0.30870100\n",
            "Iteration 169, loss = 0.30573536\n",
            "Iteration 170, loss = 0.30563120\n",
            "Iteration 171, loss = 0.30981929\n",
            "Iteration 172, loss = 0.30558948\n",
            "Iteration 173, loss = 0.30571944\n",
            "Iteration 174, loss = 0.31007148\n",
            "Iteration 175, loss = 0.30268426\n",
            "Iteration 176, loss = 0.30321053\n",
            "Iteration 177, loss = 0.30234850\n",
            "Iteration 178, loss = 0.30609038\n",
            "Iteration 179, loss = 0.30436783\n",
            "Iteration 180, loss = 0.30261255\n",
            "Iteration 181, loss = 0.30535238\n",
            "Iteration 182, loss = 0.29827105\n",
            "Iteration 183, loss = 0.30520962\n",
            "Iteration 184, loss = 0.30301545\n",
            "Iteration 185, loss = 0.29950296\n",
            "Iteration 186, loss = 0.30217005\n",
            "Iteration 187, loss = 0.30141933\n",
            "Iteration 188, loss = 0.30402947\n",
            "Iteration 189, loss = 0.30203221\n",
            "Iteration 190, loss = 0.30766412\n",
            "Iteration 191, loss = 0.30134359\n",
            "Iteration 192, loss = 0.29116620\n",
            "Iteration 193, loss = 0.30372656\n",
            "Iteration 194, loss = 0.30372243\n",
            "Iteration 195, loss = 0.30046911\n",
            "Iteration 196, loss = 0.29859146\n",
            "Iteration 197, loss = 0.30946905\n",
            "Iteration 198, loss = 0.30282530\n",
            "Iteration 199, loss = 0.30677898\n",
            "Iteration 200, loss = 0.29654839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72485506\n",
            "Iteration 3, loss = 0.70337870\n",
            "Iteration 4, loss = 0.68502674\n",
            "Iteration 5, loss = 0.67168017\n",
            "Iteration 6, loss = 0.65752450\n",
            "Iteration 7, loss = 0.64722943\n",
            "Iteration 8, loss = 0.63733029\n",
            "Iteration 9, loss = 0.62878364\n",
            "Iteration 10, loss = 0.62046088\n",
            "Iteration 11, loss = 0.61494164\n",
            "Iteration 12, loss = 0.60646360\n",
            "Iteration 13, loss = 0.60355548\n",
            "Iteration 14, loss = 0.59847015\n",
            "Iteration 15, loss = 0.59053091\n",
            "Iteration 16, loss = 0.59053054\n",
            "Iteration 17, loss = 0.58596707\n",
            "Iteration 18, loss = 0.58347373\n",
            "Iteration 19, loss = 0.57925056\n",
            "Iteration 20, loss = 0.58141712\n",
            "Iteration 21, loss = 0.57609577\n",
            "Iteration 22, loss = 0.57609106\n",
            "Iteration 23, loss = 0.57424125\n",
            "Iteration 24, loss = 0.57286080\n",
            "Iteration 25, loss = 0.56796300\n",
            "Iteration 26, loss = 0.57240411\n",
            "Iteration 27, loss = 0.56600417\n",
            "Iteration 28, loss = 0.57187904\n",
            "Iteration 29, loss = 0.56698921\n",
            "Iteration 30, loss = 0.56805305\n",
            "Iteration 31, loss = 0.56842526\n",
            "Iteration 32, loss = 0.56616981\n",
            "Iteration 33, loss = 0.56339787\n",
            "Iteration 34, loss = 0.56687922\n",
            "Iteration 35, loss = 0.56679595\n",
            "Iteration 36, loss = 0.56382768\n",
            "Iteration 37, loss = 0.56454221\n",
            "Iteration 38, loss = 0.56125743\n",
            "Iteration 39, loss = 0.56441488\n",
            "Iteration 40, loss = 0.56195408\n",
            "Iteration 41, loss = 0.56558004\n",
            "Iteration 42, loss = 0.56478716\n",
            "Iteration 43, loss = 0.56252046\n",
            "Iteration 44, loss = 0.56571837\n",
            "Iteration 45, loss = 0.56030616\n",
            "Iteration 46, loss = 0.56795320\n",
            "Iteration 47, loss = 0.56284955\n",
            "Iteration 48, loss = 0.56095474\n",
            "Iteration 49, loss = 0.56420345\n",
            "Iteration 50, loss = 0.56188158\n",
            "Iteration 51, loss = 0.56106754\n",
            "Iteration 52, loss = 0.56393996\n",
            "Iteration 53, loss = 0.56819885\n",
            "Iteration 54, loss = 0.56265146\n",
            "Iteration 55, loss = 0.56493783\n",
            "Iteration 56, loss = 0.56292867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.52320611\n",
            "Iteration 58, loss = 0.51103947\n",
            "Iteration 59, loss = 0.50421686\n",
            "Iteration 60, loss = 0.49758614\n",
            "Iteration 61, loss = 0.49181501\n",
            "Iteration 62, loss = 0.48727805\n",
            "Iteration 63, loss = 0.48476663\n",
            "Iteration 64, loss = 0.47720844\n",
            "Iteration 65, loss = 0.48578892\n",
            "Iteration 66, loss = 0.48857802\n",
            "Iteration 67, loss = 0.49934805\n",
            "Iteration 68, loss = 0.51480353\n",
            "Iteration 69, loss = 0.50929201\n",
            "Iteration 70, loss = 0.51043101\n",
            "Iteration 71, loss = 0.51037493\n",
            "Iteration 72, loss = 0.52140058\n",
            "Iteration 73, loss = 0.52945952\n",
            "Iteration 74, loss = 0.52276077\n",
            "Iteration 75, loss = 0.51231697\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.43943745\n",
            "Iteration 77, loss = 0.43566822\n",
            "Iteration 78, loss = 0.43273793\n",
            "Iteration 79, loss = 0.43039746\n",
            "Iteration 80, loss = 0.42702703\n",
            "Iteration 81, loss = 0.42467662\n",
            "Iteration 82, loss = 0.42183000\n",
            "Iteration 83, loss = 0.41890964\n",
            "Iteration 84, loss = 0.41647781\n",
            "Iteration 85, loss = 0.41375506\n",
            "Iteration 86, loss = 0.41084687\n",
            "Iteration 87, loss = 0.40833863\n",
            "Iteration 88, loss = 0.40560008\n",
            "Iteration 89, loss = 0.40356544\n",
            "Iteration 90, loss = 0.40080577\n",
            "Iteration 91, loss = 0.39857768\n",
            "Iteration 92, loss = 0.39577025\n",
            "Iteration 93, loss = 0.39352291\n",
            "Iteration 94, loss = 0.39087163\n",
            "Iteration 95, loss = 0.38879369\n",
            "Iteration 96, loss = 0.38621092\n",
            "Iteration 97, loss = 0.38394840\n",
            "Iteration 98, loss = 0.38189420\n",
            "Iteration 99, loss = 0.37939886\n",
            "Iteration 100, loss = 0.37736520\n",
            "Iteration 101, loss = 0.37514324\n",
            "Iteration 102, loss = 0.37297644\n",
            "Iteration 103, loss = 0.37091174\n",
            "Iteration 104, loss = 0.36903154\n",
            "Iteration 105, loss = 0.36657234\n",
            "Iteration 106, loss = 0.36495730\n",
            "Iteration 107, loss = 0.36259624\n",
            "Iteration 108, loss = 0.36079027\n",
            "Iteration 109, loss = 0.35951132\n",
            "Iteration 110, loss = 0.35744124\n",
            "Iteration 111, loss = 0.35526565\n",
            "Iteration 112, loss = 0.35358797\n",
            "Iteration 113, loss = 0.35153993\n",
            "Iteration 114, loss = 0.34976184\n",
            "Iteration 115, loss = 0.34848668\n",
            "Iteration 116, loss = 0.34668247\n",
            "Iteration 117, loss = 0.34616545\n",
            "Iteration 118, loss = 0.34421201\n",
            "Iteration 119, loss = 0.34193532\n",
            "Iteration 120, loss = 0.34063253\n",
            "Iteration 121, loss = 0.33839585\n",
            "Iteration 122, loss = 0.33688538\n",
            "Iteration 123, loss = 0.33624702\n",
            "Iteration 124, loss = 0.33412249\n",
            "Iteration 125, loss = 0.33302075\n",
            "Iteration 126, loss = 0.33227699\n",
            "Iteration 127, loss = 0.32999260\n",
            "Iteration 128, loss = 0.32774344\n",
            "Iteration 129, loss = 0.32703877\n",
            "Iteration 130, loss = 0.32628160\n",
            "Iteration 131, loss = 0.32452508\n",
            "Iteration 132, loss = 0.32339174\n",
            "Iteration 133, loss = 0.32270873\n",
            "Iteration 134, loss = 0.32093357\n",
            "Iteration 135, loss = 0.32086759\n",
            "Iteration 136, loss = 0.32010970\n",
            "Iteration 137, loss = 0.31973197\n",
            "Iteration 138, loss = 0.31642143\n",
            "Iteration 139, loss = 0.31521578\n",
            "Iteration 140, loss = 0.31539061\n",
            "Iteration 141, loss = 0.31357134\n",
            "Iteration 142, loss = 0.31236591\n",
            "Iteration 143, loss = 0.31275511\n",
            "Iteration 144, loss = 0.31051878\n",
            "Iteration 145, loss = 0.31128513\n",
            "Iteration 146, loss = 0.31025076\n",
            "Iteration 147, loss = 0.30827311\n",
            "Iteration 148, loss = 0.30664326\n",
            "Iteration 149, loss = 0.30561684\n",
            "Iteration 150, loss = 0.30454833\n",
            "Iteration 151, loss = 0.30547222\n",
            "Iteration 152, loss = 0.30317535\n",
            "Iteration 153, loss = 0.30049990\n",
            "Iteration 154, loss = 0.30076362\n",
            "Iteration 155, loss = 0.30028083\n",
            "Iteration 156, loss = 0.30144704\n",
            "Iteration 157, loss = 0.30093765\n",
            "Iteration 158, loss = 0.29957264\n",
            "Iteration 159, loss = 0.29926197\n",
            "Iteration 160, loss = 0.29793784\n",
            "Iteration 161, loss = 0.29767767\n",
            "Iteration 162, loss = 0.29596353\n",
            "Iteration 163, loss = 0.29757037\n",
            "Iteration 164, loss = 0.29456489\n",
            "Iteration 165, loss = 0.29524099\n",
            "Iteration 166, loss = 0.29648927\n",
            "Iteration 167, loss = 0.29361620\n",
            "Iteration 168, loss = 0.29200726\n",
            "Iteration 169, loss = 0.29448907\n",
            "Iteration 170, loss = 0.29244320\n",
            "Iteration 171, loss = 0.29493196\n",
            "Iteration 172, loss = 0.29155002\n",
            "Iteration 173, loss = 0.29239364\n",
            "Iteration 174, loss = 0.29425835\n",
            "Iteration 175, loss = 0.29438859\n",
            "Iteration 176, loss = 0.28756447\n",
            "Iteration 177, loss = 0.29010279\n",
            "Iteration 178, loss = 0.28971230\n",
            "Iteration 179, loss = 0.29020622\n",
            "Iteration 180, loss = 0.28994814\n",
            "Iteration 181, loss = 0.28995120\n",
            "Iteration 182, loss = 0.29243851\n",
            "Iteration 183, loss = 0.28833804\n",
            "Iteration 184, loss = 0.29012861\n",
            "Iteration 185, loss = 0.28726110\n",
            "Iteration 186, loss = 0.28827120\n",
            "Iteration 187, loss = 0.28909818\n",
            "Iteration 188, loss = 0.28916088\n",
            "Iteration 189, loss = 0.28857715\n",
            "Iteration 190, loss = 0.29224528\n",
            "Iteration 191, loss = 0.29180176\n",
            "Iteration 192, loss = 0.29243059\n",
            "Iteration 193, loss = 0.28700040\n",
            "Iteration 194, loss = 0.28613544\n",
            "Iteration 195, loss = 0.29109597\n",
            "Iteration 196, loss = 0.28432306\n",
            "Iteration 197, loss = 0.28584548\n",
            "Iteration 198, loss = 0.28062974\n",
            "Iteration 199, loss = 0.29089782\n",
            "Iteration 200, loss = 0.28546898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69154598\n",
            "Iteration 3, loss = 0.67409845\n",
            "Iteration 4, loss = 0.66326428\n",
            "Iteration 5, loss = 0.64720416\n",
            "Iteration 6, loss = 0.63841738\n",
            "Iteration 7, loss = 0.62798193\n",
            "Iteration 8, loss = 0.61940665\n",
            "Iteration 9, loss = 0.61377541\n",
            "Iteration 10, loss = 0.60681220\n",
            "Iteration 11, loss = 0.60192053\n",
            "Iteration 12, loss = 0.59769015\n",
            "Iteration 13, loss = 0.59224016\n",
            "Iteration 14, loss = 0.59110094\n",
            "Iteration 15, loss = 0.58557661\n",
            "Iteration 16, loss = 0.58385453\n",
            "Iteration 17, loss = 0.58171036\n",
            "Iteration 18, loss = 0.57795596\n",
            "Iteration 19, loss = 0.57730630\n",
            "Iteration 20, loss = 0.57582343\n",
            "Iteration 21, loss = 0.57594628\n",
            "Iteration 22, loss = 0.56982706\n",
            "Iteration 23, loss = 0.57037865\n",
            "Iteration 24, loss = 0.57227690\n",
            "Iteration 25, loss = 0.56736536\n",
            "Iteration 26, loss = 0.56515010\n",
            "Iteration 27, loss = 0.57452962\n",
            "Iteration 28, loss = 0.56993049\n",
            "Iteration 29, loss = 0.56562873\n",
            "Iteration 30, loss = 0.56540834\n",
            "Iteration 31, loss = 0.56461392\n",
            "Iteration 32, loss = 0.56742695\n",
            "Iteration 33, loss = 0.56563227\n",
            "Iteration 34, loss = 0.56212818\n",
            "Iteration 35, loss = 0.56683305\n",
            "Iteration 36, loss = 0.56181096\n",
            "Iteration 37, loss = 0.56424905\n",
            "Iteration 38, loss = 0.56507257\n",
            "Iteration 39, loss = 0.56335433\n",
            "Iteration 40, loss = 0.56360422\n",
            "Iteration 41, loss = 0.55792032\n",
            "Iteration 42, loss = 0.56518218\n",
            "Iteration 43, loss = 0.56666071\n",
            "Iteration 44, loss = 0.56382611\n",
            "Iteration 45, loss = 0.56565708\n",
            "Iteration 46, loss = 0.56008278\n",
            "Iteration 47, loss = 0.56036418\n",
            "Iteration 48, loss = 0.56527574\n",
            "Iteration 49, loss = 0.56580604\n",
            "Iteration 50, loss = 0.56139547\n",
            "Iteration 51, loss = 0.56307384\n",
            "Iteration 52, loss = 0.56334609\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.52040827\n",
            "Iteration 54, loss = 0.51087484\n",
            "Iteration 55, loss = 0.50394839\n",
            "Iteration 56, loss = 0.49797292\n",
            "Iteration 57, loss = 0.49238940\n",
            "Iteration 58, loss = 0.48709817\n",
            "Iteration 59, loss = 0.48181112\n",
            "Iteration 60, loss = 0.47870128\n",
            "Iteration 61, loss = 0.48282125\n",
            "Iteration 62, loss = 0.49704108\n",
            "Iteration 63, loss = 0.50316164\n",
            "Iteration 64, loss = 0.50725969\n",
            "Iteration 65, loss = 0.51033842\n",
            "Iteration 66, loss = 0.51467261\n",
            "Iteration 67, loss = 0.51673863\n",
            "Iteration 68, loss = 0.53657645\n",
            "Iteration 69, loss = 0.51394992\n",
            "Iteration 70, loss = 0.51531047\n",
            "Iteration 71, loss = 0.52711849\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.44160391\n",
            "Iteration 73, loss = 0.43861286\n",
            "Iteration 74, loss = 0.43599609\n",
            "Iteration 75, loss = 0.43271993\n",
            "Iteration 76, loss = 0.42986458\n",
            "Iteration 77, loss = 0.42718217\n",
            "Iteration 78, loss = 0.42485158\n",
            "Iteration 79, loss = 0.42202603\n",
            "Iteration 80, loss = 0.41948728\n",
            "Iteration 81, loss = 0.41647111\n",
            "Iteration 82, loss = 0.41419565\n",
            "Iteration 83, loss = 0.41129626\n",
            "Iteration 84, loss = 0.40859162\n",
            "Iteration 85, loss = 0.40635554\n",
            "Iteration 86, loss = 0.40342847\n",
            "Iteration 87, loss = 0.40123737\n",
            "Iteration 88, loss = 0.39879041\n",
            "Iteration 89, loss = 0.39633301\n",
            "Iteration 90, loss = 0.39381632\n",
            "Iteration 91, loss = 0.39132240\n",
            "Iteration 92, loss = 0.38926767\n",
            "Iteration 93, loss = 0.38682889\n",
            "Iteration 94, loss = 0.38437929\n",
            "Iteration 95, loss = 0.38232059\n",
            "Iteration 96, loss = 0.37975851\n",
            "Iteration 97, loss = 0.37840952\n",
            "Iteration 98, loss = 0.37605591\n",
            "Iteration 99, loss = 0.37352753\n",
            "Iteration 100, loss = 0.37137345\n",
            "Iteration 101, loss = 0.36941266\n",
            "Iteration 102, loss = 0.36748070\n",
            "Iteration 103, loss = 0.36547290\n",
            "Iteration 104, loss = 0.36294182\n",
            "Iteration 105, loss = 0.36148716\n",
            "Iteration 106, loss = 0.35911242\n",
            "Iteration 107, loss = 0.35750840\n",
            "Iteration 108, loss = 0.35569441\n",
            "Iteration 109, loss = 0.35408245\n",
            "Iteration 110, loss = 0.35201689\n",
            "Iteration 111, loss = 0.35076549\n",
            "Iteration 112, loss = 0.34839548\n",
            "Iteration 113, loss = 0.34738188\n",
            "Iteration 114, loss = 0.34575855\n",
            "Iteration 115, loss = 0.34456699\n",
            "Iteration 116, loss = 0.34360430\n",
            "Iteration 117, loss = 0.34073938\n",
            "Iteration 118, loss = 0.33940873\n",
            "Iteration 119, loss = 0.33774775\n",
            "Iteration 120, loss = 0.33637891\n",
            "Iteration 121, loss = 0.33465957\n",
            "Iteration 122, loss = 0.33401063\n",
            "Iteration 123, loss = 0.33236645\n",
            "Iteration 124, loss = 0.33069066\n",
            "Iteration 125, loss = 0.33006512\n",
            "Iteration 126, loss = 0.32889596\n",
            "Iteration 127, loss = 0.32786941\n",
            "Iteration 128, loss = 0.32546385\n",
            "Iteration 129, loss = 0.32399708\n",
            "Iteration 130, loss = 0.32356675\n",
            "Iteration 131, loss = 0.32164810\n",
            "Iteration 132, loss = 0.32124185\n",
            "Iteration 133, loss = 0.32149521\n",
            "Iteration 134, loss = 0.31818894\n",
            "Iteration 135, loss = 0.31838905\n",
            "Iteration 136, loss = 0.31601260\n",
            "Iteration 137, loss = 0.31485143\n",
            "Iteration 138, loss = 0.31451752\n",
            "Iteration 139, loss = 0.31251836\n",
            "Iteration 140, loss = 0.31230534\n",
            "Iteration 141, loss = 0.31256346\n",
            "Iteration 142, loss = 0.31136325\n",
            "Iteration 143, loss = 0.31023732\n",
            "Iteration 144, loss = 0.30916398\n",
            "Iteration 145, loss = 0.30750426\n",
            "Iteration 146, loss = 0.30728055\n",
            "Iteration 147, loss = 0.30651552\n",
            "Iteration 148, loss = 0.30506165\n",
            "Iteration 149, loss = 0.30504670\n",
            "Iteration 150, loss = 0.30479262\n",
            "Iteration 151, loss = 0.30145471\n",
            "Iteration 152, loss = 0.30150947\n",
            "Iteration 153, loss = 0.30232538\n",
            "Iteration 154, loss = 0.30132662\n",
            "Iteration 155, loss = 0.29928584\n",
            "Iteration 156, loss = 0.29906619\n",
            "Iteration 157, loss = 0.29831465\n",
            "Iteration 158, loss = 0.29869892\n",
            "Iteration 159, loss = 0.29909799\n",
            "Iteration 160, loss = 0.29809077\n",
            "Iteration 161, loss = 0.29739288\n",
            "Iteration 162, loss = 0.29772483\n",
            "Iteration 163, loss = 0.29648332\n",
            "Iteration 164, loss = 0.29355887\n",
            "Iteration 165, loss = 0.29319015\n",
            "Iteration 166, loss = 0.29478842\n",
            "Iteration 167, loss = 0.29281294\n",
            "Iteration 168, loss = 0.29202158\n",
            "Iteration 169, loss = 0.29552551\n",
            "Iteration 170, loss = 0.29687017\n",
            "Iteration 171, loss = 0.29027950\n",
            "Iteration 172, loss = 0.29260216\n",
            "Iteration 173, loss = 0.29025325\n",
            "Iteration 174, loss = 0.28889603\n",
            "Iteration 175, loss = 0.29084971\n",
            "Iteration 176, loss = 0.29307265\n",
            "Iteration 177, loss = 0.28960818\n",
            "Iteration 178, loss = 0.28964041\n",
            "Iteration 179, loss = 0.28801395\n",
            "Iteration 180, loss = 0.29246761\n",
            "Iteration 181, loss = 0.29532246\n",
            "Iteration 182, loss = 0.29191091\n",
            "Iteration 183, loss = 0.28649338\n",
            "Iteration 184, loss = 0.29335449\n",
            "Iteration 185, loss = 0.28913770\n",
            "Iteration 186, loss = 0.28868469\n",
            "Iteration 187, loss = 0.28950781\n",
            "Iteration 188, loss = 0.28606383\n",
            "Iteration 189, loss = 0.28913621\n",
            "Iteration 190, loss = 0.28748355\n",
            "Iteration 191, loss = 0.28381749\n",
            "Iteration 192, loss = 0.28724777\n",
            "Iteration 193, loss = 0.29929492\n",
            "Iteration 194, loss = 0.28551272\n",
            "Iteration 195, loss = 0.29144847\n",
            "Iteration 196, loss = 0.28908883\n",
            "Iteration 197, loss = 0.27973129\n",
            "Iteration 198, loss = 0.28462383\n",
            "Iteration 199, loss = 0.28940439\n",
            "Iteration 200, loss = 0.28335434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69503099\n",
            "Iteration 3, loss = 0.68777552\n",
            "Iteration 4, loss = 0.67359120\n",
            "Iteration 5, loss = 0.66009893\n",
            "Iteration 6, loss = 0.64755305\n",
            "Iteration 7, loss = 0.63671463\n",
            "Iteration 8, loss = 0.63279409\n",
            "Iteration 9, loss = 0.62547804\n",
            "Iteration 10, loss = 0.61865797\n",
            "Iteration 11, loss = 0.61108619\n",
            "Iteration 12, loss = 0.60623624\n",
            "Iteration 13, loss = 0.60206443\n",
            "Iteration 14, loss = 0.59833245\n",
            "Iteration 15, loss = 0.59418988\n",
            "Iteration 16, loss = 0.59262079\n",
            "Iteration 17, loss = 0.58706169\n",
            "Iteration 18, loss = 0.58652664\n",
            "Iteration 19, loss = 0.58491733\n",
            "Iteration 20, loss = 0.58110629\n",
            "Iteration 21, loss = 0.57856161\n",
            "Iteration 22, loss = 0.58388813\n",
            "Iteration 23, loss = 0.57643258\n",
            "Iteration 24, loss = 0.57742575\n",
            "Iteration 25, loss = 0.57632894\n",
            "Iteration 26, loss = 0.57112494\n",
            "Iteration 27, loss = 0.57392466\n",
            "Iteration 28, loss = 0.57096004\n",
            "Iteration 29, loss = 0.57217461\n",
            "Iteration 30, loss = 0.57407742\n",
            "Iteration 31, loss = 0.57533638\n",
            "Iteration 32, loss = 0.56919253\n",
            "Iteration 33, loss = 0.57118915\n",
            "Iteration 34, loss = 0.56677086\n",
            "Iteration 35, loss = 0.56962671\n",
            "Iteration 36, loss = 0.56856997\n",
            "Iteration 37, loss = 0.56825010\n",
            "Iteration 38, loss = 0.57043710\n",
            "Iteration 39, loss = 0.56903018\n",
            "Iteration 40, loss = 0.56922782\n",
            "Iteration 41, loss = 0.56935005\n",
            "Iteration 42, loss = 0.56913991\n",
            "Iteration 43, loss = 0.56565971\n",
            "Iteration 44, loss = 0.56657147\n",
            "Iteration 45, loss = 0.57090223\n",
            "Iteration 46, loss = 0.57044076\n",
            "Iteration 47, loss = 0.56832590\n",
            "Iteration 48, loss = 0.56886140\n",
            "Iteration 49, loss = 0.56960129\n",
            "Iteration 50, loss = 0.56723902\n",
            "Iteration 51, loss = 0.57152232\n",
            "Iteration 52, loss = 0.56802102\n",
            "Iteration 53, loss = 0.56620024\n",
            "Iteration 54, loss = 0.56942341\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 55, loss = 0.53329186\n",
            "Iteration 56, loss = 0.52313447\n",
            "Iteration 57, loss = 0.51655927\n",
            "Iteration 58, loss = 0.51082884\n",
            "Iteration 59, loss = 0.50590391\n",
            "Iteration 60, loss = 0.50213438\n",
            "Iteration 61, loss = 0.49749931\n",
            "Iteration 62, loss = 0.49954595\n",
            "Iteration 63, loss = 0.50121957\n",
            "Iteration 64, loss = 0.51021687\n",
            "Iteration 65, loss = 0.51233329\n",
            "Iteration 66, loss = 0.52013457\n",
            "Iteration 67, loss = 0.52138817\n",
            "Iteration 68, loss = 0.52258161\n",
            "Iteration 69, loss = 0.51960593\n",
            "Iteration 70, loss = 0.52635334\n",
            "Iteration 71, loss = 0.52055136\n",
            "Iteration 72, loss = 0.52164236\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.45807954\n",
            "Iteration 74, loss = 0.45576611\n",
            "Iteration 75, loss = 0.45353444\n",
            "Iteration 76, loss = 0.45103430\n",
            "Iteration 77, loss = 0.44896054\n",
            "Iteration 78, loss = 0.44679318\n",
            "Iteration 79, loss = 0.44461233\n",
            "Iteration 80, loss = 0.44198422\n",
            "Iteration 81, loss = 0.44012866\n",
            "Iteration 82, loss = 0.43778726\n",
            "Iteration 83, loss = 0.43560680\n",
            "Iteration 84, loss = 0.43331828\n",
            "Iteration 85, loss = 0.43173880\n",
            "Iteration 86, loss = 0.42923491\n",
            "Iteration 87, loss = 0.42749987\n",
            "Iteration 88, loss = 0.42503613\n",
            "Iteration 89, loss = 0.42336069\n",
            "Iteration 90, loss = 0.42166069\n",
            "Iteration 91, loss = 0.41937576\n",
            "Iteration 92, loss = 0.41751326\n",
            "Iteration 93, loss = 0.41576020\n",
            "Iteration 94, loss = 0.41387211\n",
            "Iteration 95, loss = 0.41140809\n",
            "Iteration 96, loss = 0.40989672\n",
            "Iteration 97, loss = 0.40813146\n",
            "Iteration 98, loss = 0.40622872\n",
            "Iteration 99, loss = 0.40483334\n",
            "Iteration 100, loss = 0.40321445\n",
            "Iteration 101, loss = 0.40086656\n",
            "Iteration 102, loss = 0.39952995\n",
            "Iteration 103, loss = 0.39792827\n",
            "Iteration 104, loss = 0.39682317\n",
            "Iteration 105, loss = 0.39453584\n",
            "Iteration 106, loss = 0.39321608\n",
            "Iteration 107, loss = 0.39192341\n",
            "Iteration 108, loss = 0.39042412\n",
            "Iteration 109, loss = 0.38883674\n",
            "Iteration 110, loss = 0.38831273\n",
            "Iteration 111, loss = 0.38542748\n",
            "Iteration 112, loss = 0.38491499\n",
            "Iteration 113, loss = 0.38327221\n",
            "Iteration 114, loss = 0.38220874\n",
            "Iteration 115, loss = 0.38209923\n",
            "Iteration 116, loss = 0.37958669\n",
            "Iteration 117, loss = 0.37903271\n",
            "Iteration 118, loss = 0.37671655\n",
            "Iteration 119, loss = 0.37640637\n",
            "Iteration 120, loss = 0.37510479\n",
            "Iteration 121, loss = 0.37348473\n",
            "Iteration 122, loss = 0.37304795\n",
            "Iteration 123, loss = 0.37126790\n",
            "Iteration 124, loss = 0.36986166\n",
            "Iteration 125, loss = 0.36977947\n",
            "Iteration 126, loss = 0.36956403\n",
            "Iteration 127, loss = 0.36917853\n",
            "Iteration 128, loss = 0.36672782\n",
            "Iteration 129, loss = 0.36590204\n",
            "Iteration 130, loss = 0.36619288\n",
            "Iteration 131, loss = 0.36416981\n",
            "Iteration 132, loss = 0.36739045\n",
            "Iteration 133, loss = 0.36396907\n",
            "Iteration 134, loss = 0.36278030\n",
            "Iteration 135, loss = 0.36307064\n",
            "Iteration 136, loss = 0.36347646\n",
            "Iteration 137, loss = 0.36211811\n",
            "Iteration 138, loss = 0.36069447\n",
            "Iteration 139, loss = 0.35955591\n",
            "Iteration 140, loss = 0.36300846\n",
            "Iteration 141, loss = 0.35974287\n",
            "Iteration 142, loss = 0.36131046\n",
            "Iteration 143, loss = 0.35973729\n",
            "Iteration 144, loss = 0.36355490\n",
            "Iteration 145, loss = 0.35962582\n",
            "Iteration 146, loss = 0.35857599\n",
            "Iteration 147, loss = 0.36109586\n",
            "Iteration 148, loss = 0.36049747\n",
            "Iteration 149, loss = 0.35492286\n",
            "Iteration 150, loss = 0.36177801\n",
            "Iteration 151, loss = 0.35666946\n",
            "Iteration 152, loss = 0.35868801\n",
            "Iteration 153, loss = 0.35958651\n",
            "Iteration 154, loss = 0.35579425\n",
            "Iteration 155, loss = 0.35634248\n",
            "Iteration 156, loss = 0.35770655\n",
            "Iteration 157, loss = 0.35769748\n",
            "Iteration 158, loss = 0.35217708\n",
            "Iteration 159, loss = 0.35593233\n",
            "Iteration 160, loss = 0.35709960\n",
            "Iteration 161, loss = 0.35401097\n",
            "Iteration 162, loss = 0.35467157\n",
            "Iteration 163, loss = 0.35231857\n",
            "Iteration 164, loss = 0.35866410\n",
            "Iteration 165, loss = 0.35451603\n",
            "Iteration 166, loss = 0.35734753\n",
            "Iteration 167, loss = 0.36343971\n",
            "Iteration 168, loss = 0.35571642\n",
            "Iteration 169, loss = 0.35126026\n",
            "Iteration 170, loss = 0.35636248\n",
            "Iteration 171, loss = 0.35918127\n",
            "Iteration 172, loss = 0.35498990\n",
            "Iteration 173, loss = 0.36119586\n",
            "Iteration 174, loss = 0.35451668\n",
            "Iteration 175, loss = 0.35466445\n",
            "Iteration 176, loss = 0.35031076\n",
            "Iteration 177, loss = 0.35783168\n",
            "Iteration 178, loss = 0.35199667\n",
            "Iteration 179, loss = 0.34847720\n",
            "Iteration 180, loss = 0.35539237\n",
            "Iteration 181, loss = 0.35960308\n",
            "Iteration 182, loss = 0.36049655\n",
            "Iteration 183, loss = 0.35317780\n",
            "Iteration 184, loss = 0.35600739\n",
            "Iteration 185, loss = 0.35461213\n",
            "Iteration 186, loss = 0.35786274\n",
            "Iteration 187, loss = 0.35150625\n",
            "Iteration 188, loss = 0.35681629\n",
            "Iteration 189, loss = 0.35566690\n",
            "Iteration 190, loss = 0.35159253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 191, loss = 0.31967739\n",
            "Iteration 192, loss = 0.31929728\n",
            "Iteration 193, loss = 0.31910194\n",
            "Iteration 194, loss = 0.31887114\n",
            "Iteration 195, loss = 0.31883623\n",
            "Iteration 196, loss = 0.31863232\n",
            "Iteration 197, loss = 0.31856039\n",
            "Iteration 198, loss = 0.31834277\n",
            "Iteration 199, loss = 0.31838438\n",
            "Iteration 200, loss = 0.31829460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71308698\n",
            "Iteration 3, loss = 0.69046261\n",
            "Iteration 4, loss = 0.67629959\n",
            "Iteration 5, loss = 0.66165665\n",
            "Iteration 6, loss = 0.65030140\n",
            "Iteration 7, loss = 0.64332737\n",
            "Iteration 8, loss = 0.63120566\n",
            "Iteration 9, loss = 0.62597864\n",
            "Iteration 10, loss = 0.61906093\n",
            "Iteration 11, loss = 0.61176351\n",
            "Iteration 12, loss = 0.60709373\n",
            "Iteration 13, loss = 0.60000935\n",
            "Iteration 14, loss = 0.59928737\n",
            "Iteration 15, loss = 0.59641989\n",
            "Iteration 16, loss = 0.58862505\n",
            "Iteration 17, loss = 0.59019716\n",
            "Iteration 18, loss = 0.58545387\n",
            "Iteration 19, loss = 0.58527171\n",
            "Iteration 20, loss = 0.57758463\n",
            "Iteration 21, loss = 0.58279200\n",
            "Iteration 22, loss = 0.63677569\n",
            "Iteration 23, loss = 0.62265704\n",
            "Iteration 24, loss = 0.60907523\n",
            "Iteration 25, loss = 0.59811625\n",
            "Iteration 26, loss = 0.59807095\n",
            "Iteration 27, loss = 0.58775003\n",
            "Iteration 28, loss = 0.58621959\n",
            "Iteration 29, loss = 0.58134154\n",
            "Iteration 30, loss = 0.58045437\n",
            "Iteration 31, loss = 0.57816084\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.56502658\n",
            "Iteration 33, loss = 0.56175563\n",
            "Iteration 34, loss = 0.56058021\n",
            "Iteration 35, loss = 0.55899898\n",
            "Iteration 36, loss = 0.55800049\n",
            "Iteration 37, loss = 0.55619531\n",
            "Iteration 38, loss = 0.55547579\n",
            "Iteration 39, loss = 0.55359440\n",
            "Iteration 40, loss = 0.55166931\n",
            "Iteration 41, loss = 0.54989816\n",
            "Iteration 42, loss = 0.54843589\n",
            "Iteration 43, loss = 0.54651105\n",
            "Iteration 44, loss = 0.54450455\n",
            "Iteration 45, loss = 0.54239862\n",
            "Iteration 46, loss = 0.53900850\n",
            "Iteration 47, loss = 0.53787193\n",
            "Iteration 48, loss = 0.53580797\n",
            "Iteration 49, loss = 0.53411873\n",
            "Iteration 50, loss = 0.53539797\n",
            "Iteration 51, loss = 0.53318800\n",
            "Iteration 52, loss = 0.53348189\n",
            "Iteration 53, loss = 0.53230421\n",
            "Iteration 54, loss = 0.53607921\n",
            "Iteration 55, loss = 0.53519221\n",
            "Iteration 56, loss = 0.53461823\n",
            "Iteration 57, loss = 0.54036592\n",
            "Iteration 58, loss = 0.53793651\n",
            "Iteration 59, loss = 0.53343788\n",
            "Iteration 60, loss = 0.52894773\n",
            "Iteration 61, loss = 0.53392585\n",
            "Iteration 62, loss = 0.53965157\n",
            "Iteration 63, loss = 0.54040933\n",
            "Iteration 64, loss = 0.53402229\n",
            "Iteration 65, loss = 0.53325988\n",
            "Iteration 66, loss = 0.53178586\n",
            "Iteration 67, loss = 0.53171970\n",
            "Iteration 68, loss = 0.53359660\n",
            "Iteration 69, loss = 0.53496411\n",
            "Iteration 70, loss = 0.53029165\n",
            "Iteration 71, loss = 0.52967993\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.48829204\n",
            "Iteration 73, loss = 0.48007404\n",
            "Iteration 74, loss = 0.47835073\n",
            "Iteration 75, loss = 0.47680817\n",
            "Iteration 76, loss = 0.47539332\n",
            "Iteration 77, loss = 0.47356798\n",
            "Iteration 78, loss = 0.47196385\n",
            "Iteration 79, loss = 0.47022045\n",
            "Iteration 80, loss = 0.46854190\n",
            "Iteration 81, loss = 0.46686512\n",
            "Iteration 82, loss = 0.46568910\n",
            "Iteration 83, loss = 0.46360345\n",
            "Iteration 84, loss = 0.46213383\n",
            "Iteration 85, loss = 0.46106048\n",
            "Iteration 86, loss = 0.45905669\n",
            "Iteration 87, loss = 0.45725615\n",
            "Iteration 88, loss = 0.45535914\n",
            "Iteration 89, loss = 0.45389653\n",
            "Iteration 90, loss = 0.45228330\n",
            "Iteration 91, loss = 0.45099902\n",
            "Iteration 92, loss = 0.44936548\n",
            "Iteration 93, loss = 0.44776476\n",
            "Iteration 94, loss = 0.44627742\n",
            "Iteration 95, loss = 0.44473545\n",
            "Iteration 96, loss = 0.44301330\n",
            "Iteration 97, loss = 0.44196538\n",
            "Iteration 98, loss = 0.43993319\n",
            "Iteration 99, loss = 0.43863974\n",
            "Iteration 100, loss = 0.43696460\n",
            "Iteration 101, loss = 0.43538161\n",
            "Iteration 102, loss = 0.43338216\n",
            "Iteration 103, loss = 0.43221690\n",
            "Iteration 104, loss = 0.43090366\n",
            "Iteration 105, loss = 0.42851561\n",
            "Iteration 106, loss = 0.42811873\n",
            "Iteration 107, loss = 0.42763684\n",
            "Iteration 108, loss = 0.42512018\n",
            "Iteration 109, loss = 0.42350007\n",
            "Iteration 110, loss = 0.42132717\n",
            "Iteration 111, loss = 0.42162473\n",
            "Iteration 112, loss = 0.41987676\n",
            "Iteration 113, loss = 0.41818859\n",
            "Iteration 114, loss = 0.41761014\n",
            "Iteration 115, loss = 0.41536166\n",
            "Iteration 116, loss = 0.41494434\n",
            "Iteration 117, loss = 0.41341858\n",
            "Iteration 118, loss = 0.41254142\n",
            "Iteration 119, loss = 0.41055158\n",
            "Iteration 120, loss = 0.41058855\n",
            "Iteration 121, loss = 0.40887863\n",
            "Iteration 122, loss = 0.40812886\n",
            "Iteration 123, loss = 0.40822810\n",
            "Iteration 124, loss = 0.40491383\n",
            "Iteration 125, loss = 0.40258108\n",
            "Iteration 126, loss = 0.40355296\n",
            "Iteration 127, loss = 0.40041017\n",
            "Iteration 128, loss = 0.40215470\n",
            "Iteration 129, loss = 0.40207748\n",
            "Iteration 130, loss = 0.40053381\n",
            "Iteration 131, loss = 0.39745848\n",
            "Iteration 132, loss = 0.39723402\n",
            "Iteration 133, loss = 0.39764721\n",
            "Iteration 134, loss = 0.39839719\n",
            "Iteration 135, loss = 0.39784875\n",
            "Iteration 136, loss = 0.39437034\n",
            "Iteration 137, loss = 0.39769557\n",
            "Iteration 138, loss = 0.39359741\n",
            "Iteration 139, loss = 0.39187577\n",
            "Iteration 140, loss = 0.39192420\n",
            "Iteration 141, loss = 0.39022453\n",
            "Iteration 142, loss = 0.38982364\n",
            "Iteration 143, loss = 0.39480146\n",
            "Iteration 144, loss = 0.39044692\n",
            "Iteration 145, loss = 0.39011302\n",
            "Iteration 146, loss = 0.39282703\n",
            "Iteration 147, loss = 0.38726642\n",
            "Iteration 148, loss = 0.38457706\n",
            "Iteration 149, loss = 0.38943231\n",
            "Iteration 150, loss = 0.39191841\n",
            "Iteration 151, loss = 0.38737370\n",
            "Iteration 152, loss = 0.38974058\n",
            "Iteration 153, loss = 0.39045373\n",
            "Iteration 154, loss = 0.38948446\n",
            "Iteration 155, loss = 0.38564293\n",
            "Iteration 156, loss = 0.39085540\n",
            "Iteration 157, loss = 0.38932397\n",
            "Iteration 158, loss = 0.39139894\n",
            "Iteration 159, loss = 0.38436219\n",
            "Iteration 160, loss = 0.39125389\n",
            "Iteration 161, loss = 0.39413969\n",
            "Iteration 162, loss = 0.38057750\n",
            "Iteration 163, loss = 0.38282473\n",
            "Iteration 164, loss = 0.38278335\n",
            "Iteration 165, loss = 0.38649005\n",
            "Iteration 166, loss = 0.38178245\n",
            "Iteration 167, loss = 0.38791577\n",
            "Iteration 168, loss = 0.38146753\n",
            "Iteration 169, loss = 0.37798026\n",
            "Iteration 170, loss = 0.38262673\n",
            "Iteration 171, loss = 0.37879592\n",
            "Iteration 172, loss = 0.38859609\n",
            "Iteration 173, loss = 0.38181662\n",
            "Iteration 174, loss = 0.37957669\n",
            "Iteration 175, loss = 0.38674816\n",
            "Iteration 176, loss = 0.37906215\n",
            "Iteration 177, loss = 0.38596135\n",
            "Iteration 178, loss = 0.37781560\n",
            "Iteration 179, loss = 0.38622650\n",
            "Iteration 180, loss = 0.38668460\n",
            "Iteration 181, loss = 0.37997923\n",
            "Iteration 182, loss = 0.38223177\n",
            "Iteration 183, loss = 0.38332613\n",
            "Iteration 184, loss = 0.38164477\n",
            "Iteration 185, loss = 0.38054360\n",
            "Iteration 186, loss = 0.38097615\n",
            "Iteration 187, loss = 0.37790702\n",
            "Iteration 188, loss = 0.38303937\n",
            "Iteration 189, loss = 0.38216433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 190, loss = 0.34402359\n",
            "Iteration 191, loss = 0.34367802\n",
            "Iteration 192, loss = 0.34351053\n",
            "Iteration 193, loss = 0.34340417\n",
            "Iteration 194, loss = 0.34318027\n",
            "Iteration 195, loss = 0.34314609\n",
            "Iteration 196, loss = 0.34280546\n",
            "Iteration 197, loss = 0.34266647\n",
            "Iteration 198, loss = 0.34248835\n",
            "Iteration 199, loss = 0.34229043\n",
            "Iteration 200, loss = 0.34250328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69193842\n",
            "Iteration 3, loss = 0.67486514\n",
            "Iteration 4, loss = 0.65838164\n",
            "Iteration 5, loss = 0.64841321\n",
            "Iteration 6, loss = 0.63693795\n",
            "Iteration 7, loss = 0.62819418\n",
            "Iteration 8, loss = 0.61914088\n",
            "Iteration 9, loss = 0.61404387\n",
            "Iteration 10, loss = 0.60728483\n",
            "Iteration 11, loss = 0.60249383\n",
            "Iteration 12, loss = 0.59675363\n",
            "Iteration 13, loss = 0.59498650\n",
            "Iteration 14, loss = 0.59502511\n",
            "Iteration 15, loss = 0.65012027\n",
            "Iteration 16, loss = 0.61998322\n",
            "Iteration 17, loss = 0.60995896\n",
            "Iteration 18, loss = 0.60300950\n",
            "Iteration 19, loss = 0.59809483\n",
            "Iteration 20, loss = 0.59191195\n",
            "Iteration 21, loss = 0.58874424\n",
            "Iteration 22, loss = 0.59010685\n",
            "Iteration 23, loss = 0.58300161\n",
            "Iteration 24, loss = 0.58245537\n",
            "Iteration 25, loss = 0.57881460\n",
            "Iteration 26, loss = 0.57521465\n",
            "Iteration 27, loss = 0.58125536\n",
            "Iteration 28, loss = 0.57541076\n",
            "Iteration 29, loss = 0.57350265\n",
            "Iteration 30, loss = 0.56971080\n",
            "Iteration 31, loss = 0.57326787\n",
            "Iteration 32, loss = 0.57498741\n",
            "Iteration 33, loss = 0.56884451\n",
            "Iteration 34, loss = 0.57129547\n",
            "Iteration 35, loss = 0.57012559\n",
            "Iteration 36, loss = 0.56860809\n",
            "Iteration 37, loss = 0.57090742\n",
            "Iteration 38, loss = 0.57081848\n",
            "Iteration 39, loss = 0.56876306\n",
            "Iteration 40, loss = 0.65318229\n",
            "Iteration 41, loss = 0.61847164\n",
            "Iteration 42, loss = 0.60283438\n",
            "Iteration 43, loss = 0.59864114\n",
            "Iteration 44, loss = 0.59245813\n",
            "Iteration 45, loss = 0.58657545\n",
            "Iteration 46, loss = 0.58260439\n",
            "Iteration 47, loss = 0.58009932\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.56558361\n",
            "Iteration 49, loss = 0.56328947\n",
            "Iteration 50, loss = 0.56193815\n",
            "Iteration 51, loss = 0.56044787\n",
            "Iteration 52, loss = 0.55866417\n",
            "Iteration 53, loss = 0.55753906\n",
            "Iteration 54, loss = 0.55581013\n",
            "Iteration 55, loss = 0.55366600\n",
            "Iteration 56, loss = 0.55204846\n",
            "Iteration 57, loss = 0.55078747\n",
            "Iteration 58, loss = 0.54877160\n",
            "Iteration 59, loss = 0.54677809\n",
            "Iteration 60, loss = 0.54370976\n",
            "Iteration 61, loss = 0.54336362\n",
            "Iteration 62, loss = 0.54067303\n",
            "Iteration 63, loss = 0.53875748\n",
            "Iteration 64, loss = 0.53731820\n",
            "Iteration 65, loss = 0.53420738\n",
            "Iteration 66, loss = 0.53277034\n",
            "Iteration 67, loss = 0.53174201\n",
            "Iteration 68, loss = 0.53540717\n",
            "Iteration 69, loss = 0.53540391\n",
            "Iteration 70, loss = 0.53164035\n",
            "Iteration 71, loss = 0.53560958\n",
            "Iteration 72, loss = 0.53830820\n",
            "Iteration 73, loss = 0.53802466\n",
            "Iteration 74, loss = 0.53445961\n",
            "Iteration 75, loss = 0.54238653\n",
            "Iteration 76, loss = 0.53134801\n",
            "Iteration 77, loss = 0.53798691\n",
            "Iteration 78, loss = 0.53765241\n",
            "Iteration 79, loss = 0.53309935\n",
            "Iteration 80, loss = 0.53496178\n",
            "Iteration 81, loss = 0.54098700\n",
            "Iteration 82, loss = 0.53083609\n",
            "Iteration 83, loss = 0.53828204\n",
            "Iteration 84, loss = 0.53280222\n",
            "Iteration 85, loss = 0.52862448\n",
            "Iteration 86, loss = 0.53117799\n",
            "Iteration 87, loss = 0.53152779\n",
            "Iteration 88, loss = 0.52990094\n",
            "Iteration 89, loss = 0.53610045\n",
            "Iteration 90, loss = 0.53329626\n",
            "Iteration 91, loss = 0.52883386\n",
            "Iteration 92, loss = 0.53150317\n",
            "Iteration 93, loss = 0.52214246\n",
            "Iteration 94, loss = 0.52237444\n",
            "Iteration 95, loss = 0.52999755\n",
            "Iteration 96, loss = 0.53265018\n",
            "Iteration 97, loss = 0.52523209\n",
            "Iteration 98, loss = 0.53523954\n",
            "Iteration 99, loss = 0.52042091\n",
            "Iteration 100, loss = 0.52207714\n",
            "Iteration 101, loss = 0.52979392\n",
            "Iteration 102, loss = 0.53144865\n",
            "Iteration 103, loss = 0.52690359\n",
            "Iteration 104, loss = 0.52739507\n",
            "Iteration 105, loss = 0.53030870\n",
            "Iteration 106, loss = 0.51943931\n",
            "Iteration 107, loss = 0.52223910\n",
            "Iteration 108, loss = 0.52764503\n",
            "Iteration 109, loss = 0.52731714\n",
            "Iteration 110, loss = 0.52900699\n",
            "Iteration 111, loss = 0.52371930\n",
            "Iteration 112, loss = 0.52194142\n",
            "Iteration 113, loss = 0.53133866\n",
            "Iteration 114, loss = 0.51874117\n",
            "Iteration 115, loss = 0.51946883\n",
            "Iteration 116, loss = 0.52433421\n",
            "Iteration 117, loss = 0.52135210\n",
            "Iteration 118, loss = 0.53052810\n",
            "Iteration 119, loss = 0.51650435\n",
            "Iteration 120, loss = 0.52913276\n",
            "Iteration 121, loss = 0.51846995\n",
            "Iteration 122, loss = 0.51832463\n",
            "Iteration 123, loss = 0.51967519\n",
            "Iteration 124, loss = 0.51981058\n",
            "Iteration 125, loss = 0.51870558\n",
            "Iteration 126, loss = 0.52876730\n",
            "Iteration 127, loss = 0.52439359\n",
            "Iteration 128, loss = 0.51979191\n",
            "Iteration 129, loss = 0.52328497\n",
            "Iteration 130, loss = 0.51100238\n",
            "Iteration 131, loss = 0.52512956\n",
            "Iteration 132, loss = 0.51961481\n",
            "Iteration 133, loss = 0.52143140\n",
            "Iteration 134, loss = 0.51587477\n",
            "Iteration 135, loss = 0.52769489\n",
            "Iteration 136, loss = 0.51579590\n",
            "Iteration 137, loss = 0.51857043\n",
            "Iteration 138, loss = 0.51493742\n",
            "Iteration 139, loss = 0.52359419\n",
            "Iteration 140, loss = 0.51705685\n",
            "Iteration 141, loss = 0.52790148\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 142, loss = 0.44442690\n",
            "Iteration 143, loss = 0.44080745\n",
            "Iteration 144, loss = 0.43761220\n",
            "Iteration 145, loss = 0.43466668\n",
            "Iteration 146, loss = 0.43217949\n",
            "Iteration 147, loss = 0.42892797\n",
            "Iteration 148, loss = 0.42622199\n",
            "Iteration 149, loss = 0.42325781\n",
            "Iteration 150, loss = 0.42083096\n",
            "Iteration 151, loss = 0.41798851\n",
            "Iteration 152, loss = 0.41540875\n",
            "Iteration 153, loss = 0.41297220\n",
            "Iteration 154, loss = 0.41107319\n",
            "Iteration 155, loss = 0.40782911\n",
            "Iteration 156, loss = 0.40561478\n",
            "Iteration 157, loss = 0.40312284\n",
            "Iteration 158, loss = 0.40076460\n",
            "Iteration 159, loss = 0.39871783\n",
            "Iteration 160, loss = 0.39610332\n",
            "Iteration 161, loss = 0.39374338\n",
            "Iteration 162, loss = 0.39224146\n",
            "Iteration 163, loss = 0.38985521\n",
            "Iteration 164, loss = 0.38820067\n",
            "Iteration 165, loss = 0.38619608\n",
            "Iteration 166, loss = 0.38433205\n",
            "Iteration 167, loss = 0.38168452\n",
            "Iteration 168, loss = 0.38036597\n",
            "Iteration 169, loss = 0.37850252\n",
            "Iteration 170, loss = 0.37675752\n",
            "Iteration 171, loss = 0.37469648\n",
            "Iteration 172, loss = 0.37292610\n",
            "Iteration 173, loss = 0.37188904\n",
            "Iteration 174, loss = 0.37028316\n",
            "Iteration 175, loss = 0.36794908\n",
            "Iteration 176, loss = 0.36718500\n",
            "Iteration 177, loss = 0.36541091\n",
            "Iteration 178, loss = 0.36395994\n",
            "Iteration 179, loss = 0.36294288\n",
            "Iteration 180, loss = 0.36193608\n",
            "Iteration 181, loss = 0.36020681\n",
            "Iteration 182, loss = 0.35920864\n",
            "Iteration 183, loss = 0.35859303\n",
            "Iteration 184, loss = 0.35639892\n",
            "Iteration 185, loss = 0.35631543\n",
            "Iteration 186, loss = 0.35547991\n",
            "Iteration 187, loss = 0.35226649\n",
            "Iteration 188, loss = 0.35120504\n",
            "Iteration 189, loss = 0.35174948\n",
            "Iteration 190, loss = 0.35026765\n",
            "Iteration 191, loss = 0.34925944\n",
            "Iteration 192, loss = 0.34815717\n",
            "Iteration 193, loss = 0.34695346\n",
            "Iteration 194, loss = 0.34900079\n",
            "Iteration 195, loss = 0.34666408\n",
            "Iteration 196, loss = 0.34526343\n",
            "Iteration 197, loss = 0.34480747\n",
            "Iteration 198, loss = 0.34314550\n",
            "Iteration 199, loss = 0.34493483\n",
            "Iteration 200, loss = 0.34360796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71660303\n",
            "Iteration 3, loss = 0.69556953\n",
            "Iteration 4, loss = 0.67476859\n",
            "Iteration 5, loss = 0.66337838\n",
            "Iteration 6, loss = 0.64839754\n",
            "Iteration 7, loss = 0.63965164\n",
            "Iteration 8, loss = 0.63221546\n",
            "Iteration 9, loss = 0.62644338\n",
            "Iteration 10, loss = 0.61961498\n",
            "Iteration 11, loss = 0.61057266\n",
            "Iteration 12, loss = 0.60848367\n",
            "Iteration 13, loss = 0.60488825\n",
            "Iteration 14, loss = 0.60016899\n",
            "Iteration 15, loss = 0.59646273\n",
            "Iteration 16, loss = 0.59145647\n",
            "Iteration 17, loss = 0.58668857\n",
            "Iteration 18, loss = 0.58691564\n",
            "Iteration 19, loss = 0.58237910\n",
            "Iteration 20, loss = 0.58539951\n",
            "Iteration 21, loss = 0.58288073\n",
            "Iteration 22, loss = 0.58017153\n",
            "Iteration 23, loss = 0.57911623\n",
            "Iteration 24, loss = 0.57670202\n",
            "Iteration 25, loss = 0.57685335\n",
            "Iteration 26, loss = 0.57715848\n",
            "Iteration 27, loss = 0.57512643\n",
            "Iteration 28, loss = 0.57252994\n",
            "Iteration 29, loss = 0.57561459\n",
            "Iteration 30, loss = 0.56747771\n",
            "Iteration 31, loss = 0.57312172\n",
            "Iteration 32, loss = 0.57260097\n",
            "Iteration 33, loss = 0.57006017\n",
            "Iteration 34, loss = 0.56883757\n",
            "Iteration 35, loss = 0.57059365\n",
            "Iteration 36, loss = 0.56822350\n",
            "Iteration 37, loss = 0.56959416\n",
            "Iteration 38, loss = 0.56984337\n",
            "Iteration 39, loss = 0.56879952\n",
            "Iteration 40, loss = 0.56747158\n",
            "Iteration 41, loss = 0.56889435\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 42, loss = 0.53467104\n",
            "Iteration 43, loss = 0.52698735\n",
            "Iteration 44, loss = 0.52174256\n",
            "Iteration 45, loss = 0.51677166\n",
            "Iteration 46, loss = 0.51097520\n",
            "Iteration 47, loss = 0.50749985\n",
            "Iteration 48, loss = 0.50443852\n",
            "Iteration 49, loss = 0.50177051\n",
            "Iteration 50, loss = 0.51001087\n",
            "Iteration 51, loss = 0.51274930\n",
            "Iteration 52, loss = 0.51206158\n",
            "Iteration 53, loss = 0.52647908\n",
            "Iteration 54, loss = 0.51912753\n",
            "Iteration 55, loss = 0.52620634\n",
            "Iteration 56, loss = 0.52714091\n",
            "Iteration 57, loss = 0.52513669\n",
            "Iteration 58, loss = 0.52260103\n",
            "Iteration 59, loss = 0.52570251\n",
            "Iteration 60, loss = 0.52511424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 61, loss = 0.46502750\n",
            "Iteration 62, loss = 0.46225300\n",
            "Iteration 63, loss = 0.45958470\n",
            "Iteration 64, loss = 0.45736262\n",
            "Iteration 65, loss = 0.45512507\n",
            "Iteration 66, loss = 0.45308566\n",
            "Iteration 67, loss = 0.45107677\n",
            "Iteration 68, loss = 0.44918052\n",
            "Iteration 69, loss = 0.44720543\n",
            "Iteration 70, loss = 0.44489517\n",
            "Iteration 71, loss = 0.44245205\n",
            "Iteration 72, loss = 0.44063346\n",
            "Iteration 73, loss = 0.43859354\n",
            "Iteration 74, loss = 0.43651891\n",
            "Iteration 75, loss = 0.43511825\n",
            "Iteration 76, loss = 0.43273732\n",
            "Iteration 77, loss = 0.43109347\n",
            "Iteration 78, loss = 0.42889963\n",
            "Iteration 79, loss = 0.42667547\n",
            "Iteration 80, loss = 0.42502043\n",
            "Iteration 81, loss = 0.42355707\n",
            "Iteration 82, loss = 0.42139819\n",
            "Iteration 83, loss = 0.41923583\n",
            "Iteration 84, loss = 0.41742691\n",
            "Iteration 85, loss = 0.41621114\n",
            "Iteration 86, loss = 0.41428600\n",
            "Iteration 87, loss = 0.41284209\n",
            "Iteration 88, loss = 0.41043355\n",
            "Iteration 89, loss = 0.40891131\n",
            "Iteration 90, loss = 0.40776716\n",
            "Iteration 91, loss = 0.40611581\n",
            "Iteration 92, loss = 0.40448797\n",
            "Iteration 93, loss = 0.40326462\n",
            "Iteration 94, loss = 0.40166824\n",
            "Iteration 95, loss = 0.40029281\n",
            "Iteration 96, loss = 0.39867485\n",
            "Iteration 97, loss = 0.39641216\n",
            "Iteration 98, loss = 0.39720378\n",
            "Iteration 99, loss = 0.39407429\n",
            "Iteration 100, loss = 0.39311176\n",
            "Iteration 101, loss = 0.39119659\n",
            "Iteration 102, loss = 0.38948213\n",
            "Iteration 103, loss = 0.38946590\n",
            "Iteration 104, loss = 0.38828502\n",
            "Iteration 105, loss = 0.38660272\n",
            "Iteration 106, loss = 0.38539005\n",
            "Iteration 107, loss = 0.38371576\n",
            "Iteration 108, loss = 0.38386606\n",
            "Iteration 109, loss = 0.38303126\n",
            "Iteration 110, loss = 0.38111171\n",
            "Iteration 111, loss = 0.37982180\n",
            "Iteration 112, loss = 0.38015612\n",
            "Iteration 113, loss = 0.37784599\n",
            "Iteration 114, loss = 0.37801800\n",
            "Iteration 115, loss = 0.37724979\n",
            "Iteration 116, loss = 0.37668060\n",
            "Iteration 117, loss = 0.37626137\n",
            "Iteration 118, loss = 0.37183075\n",
            "Iteration 119, loss = 0.37318037\n",
            "Iteration 120, loss = 0.37116185\n",
            "Iteration 121, loss = 0.37405946\n",
            "Iteration 122, loss = 0.37154777\n",
            "Iteration 123, loss = 0.37126629\n",
            "Iteration 124, loss = 0.37134933\n",
            "Iteration 125, loss = 0.37026957\n",
            "Iteration 126, loss = 0.36740768\n",
            "Iteration 127, loss = 0.36946520\n",
            "Iteration 128, loss = 0.36776884\n",
            "Iteration 129, loss = 0.36819558\n",
            "Iteration 130, loss = 0.36759985\n",
            "Iteration 131, loss = 0.36401122\n",
            "Iteration 132, loss = 0.36550028\n",
            "Iteration 133, loss = 0.37136962\n",
            "Iteration 134, loss = 0.36486964\n",
            "Iteration 135, loss = 0.36783512\n",
            "Iteration 136, loss = 0.36420816\n",
            "Iteration 137, loss = 0.36315670\n",
            "Iteration 138, loss = 0.36434119\n",
            "Iteration 139, loss = 0.36256760\n",
            "Iteration 140, loss = 0.36526897\n",
            "Iteration 141, loss = 0.36326572\n",
            "Iteration 142, loss = 0.36620348\n",
            "Iteration 143, loss = 0.36296599\n",
            "Iteration 144, loss = 0.35225416\n",
            "Iteration 145, loss = 0.33957830\n",
            "Iteration 146, loss = 0.33428102\n",
            "Iteration 147, loss = 0.33367409\n",
            "Iteration 148, loss = 0.33125309\n",
            "Iteration 149, loss = 0.32913561\n",
            "Iteration 150, loss = 0.32683922\n",
            "Iteration 151, loss = 0.32540594\n",
            "Iteration 152, loss = 0.32510300\n",
            "Iteration 153, loss = 0.32443345\n",
            "Iteration 154, loss = 0.32170462\n",
            "Iteration 155, loss = 0.32224641\n",
            "Iteration 156, loss = 0.32283732\n",
            "Iteration 157, loss = 0.32190436\n",
            "Iteration 158, loss = 0.32182851\n",
            "Iteration 159, loss = 0.31837180\n",
            "Iteration 160, loss = 0.31875409\n",
            "Iteration 161, loss = 0.31811837\n",
            "Iteration 162, loss = 0.31727799\n",
            "Iteration 163, loss = 0.31657895\n",
            "Iteration 164, loss = 0.31595750\n",
            "Iteration 165, loss = 0.31429388\n",
            "Iteration 166, loss = 0.31150973\n",
            "Iteration 167, loss = 0.31576163\n",
            "Iteration 168, loss = 0.31254197\n",
            "Iteration 169, loss = 0.31223910\n",
            "Iteration 170, loss = 0.31031220\n",
            "Iteration 171, loss = 0.30895694\n",
            "Iteration 172, loss = 0.30918059\n",
            "Iteration 173, loss = 0.31430713\n",
            "Iteration 174, loss = 0.30728348\n",
            "Iteration 175, loss = 0.30552850\n",
            "Iteration 176, loss = 0.30659735\n",
            "Iteration 177, loss = 0.31008248\n",
            "Iteration 178, loss = 0.30506850\n",
            "Iteration 179, loss = 0.31503371\n",
            "Iteration 180, loss = 0.30530881\n",
            "Iteration 181, loss = 0.30526029\n",
            "Iteration 182, loss = 0.31016008\n",
            "Iteration 183, loss = 0.31290087\n",
            "Iteration 184, loss = 0.30808056\n",
            "Iteration 185, loss = 0.32317287\n",
            "Iteration 186, loss = 0.31032058\n",
            "Iteration 187, loss = 0.30538341\n",
            "Iteration 188, loss = 0.30506898\n",
            "Iteration 189, loss = 0.29946811\n",
            "Iteration 190, loss = 0.31398592\n",
            "Iteration 191, loss = 0.30236638\n",
            "Iteration 192, loss = 0.30699925\n",
            "Iteration 193, loss = 0.30712642\n",
            "Iteration 194, loss = 0.30217677\n",
            "Iteration 195, loss = 0.30422411\n",
            "Iteration 196, loss = 0.30373497\n",
            "Iteration 197, loss = 0.30227057\n",
            "Iteration 198, loss = 0.30123488\n",
            "Iteration 199, loss = 0.30726323\n",
            "Iteration 200, loss = 0.29665638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.75860435\n",
            "Iteration 2, loss = 0.69690306\n",
            "Iteration 3, loss = 0.67853887\n",
            "Iteration 4, loss = 0.66459293\n",
            "Iteration 5, loss = 0.65010433\n",
            "Iteration 6, loss = 0.64162381\n",
            "Iteration 7, loss = 0.63233483\n",
            "Iteration 8, loss = 0.62283295\n",
            "Iteration 9, loss = 0.61720164\n",
            "Iteration 10, loss = 0.61168623\n",
            "Iteration 11, loss = 0.60386979\n",
            "Iteration 12, loss = 0.60102926\n",
            "Iteration 13, loss = 0.59569507\n",
            "Iteration 14, loss = 0.59280317\n",
            "Iteration 15, loss = 0.58783337\n",
            "Iteration 16, loss = 0.58621405\n",
            "Iteration 17, loss = 0.58122587\n",
            "Iteration 18, loss = 0.58131043\n",
            "Iteration 19, loss = 0.57704208\n",
            "Iteration 20, loss = 0.57689536\n",
            "Iteration 21, loss = 0.57711335\n",
            "Iteration 22, loss = 0.57382429\n",
            "Iteration 23, loss = 0.57200787\n",
            "Iteration 24, loss = 0.57491035\n",
            "Iteration 25, loss = 0.56961195\n",
            "Iteration 26, loss = 0.56957993\n",
            "Iteration 27, loss = 0.57101584\n",
            "Iteration 28, loss = 0.56845512\n",
            "Iteration 29, loss = 0.56745496\n",
            "Iteration 30, loss = 0.56789438\n",
            "Iteration 31, loss = 0.56495690\n",
            "Iteration 32, loss = 0.56803324\n",
            "Iteration 33, loss = 0.56718616\n",
            "Iteration 34, loss = 0.56673495\n",
            "Iteration 35, loss = 0.56511832\n",
            "Iteration 36, loss = 0.56761223\n",
            "Iteration 37, loss = 0.56545863\n",
            "Iteration 38, loss = 0.56536247\n",
            "Iteration 39, loss = 0.56744702\n",
            "Iteration 40, loss = 0.56503423\n",
            "Iteration 41, loss = 0.56559557\n",
            "Iteration 42, loss = 0.56316083\n",
            "Iteration 43, loss = 0.57068015\n",
            "Iteration 44, loss = 0.56223699\n",
            "Iteration 45, loss = 0.56743547\n",
            "Iteration 46, loss = 0.56622396\n",
            "Iteration 47, loss = 0.56530374\n",
            "Iteration 48, loss = 0.57338303\n",
            "Iteration 49, loss = 0.57462608\n",
            "Iteration 50, loss = 0.57002193\n",
            "Iteration 51, loss = 0.57174309\n",
            "Iteration 52, loss = 0.57294767\n",
            "Iteration 53, loss = 0.57384321\n",
            "Iteration 54, loss = 0.57263749\n",
            "Iteration 55, loss = 0.56989591\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 56, loss = 0.53274283\n",
            "Iteration 57, loss = 0.52483524\n",
            "Iteration 58, loss = 0.51823671\n",
            "Iteration 59, loss = 0.51328846\n",
            "Iteration 60, loss = 0.50802500\n",
            "Iteration 61, loss = 0.50451244\n",
            "Iteration 62, loss = 0.49791988\n",
            "Iteration 63, loss = 0.50140449\n",
            "Iteration 64, loss = 0.50037275\n",
            "Iteration 65, loss = 0.50793690\n",
            "Iteration 66, loss = 0.52297522\n",
            "Iteration 67, loss = 0.51914985\n",
            "Iteration 68, loss = 0.51821654\n",
            "Iteration 69, loss = 0.52703352\n",
            "Iteration 70, loss = 0.52624676\n",
            "Iteration 71, loss = 0.52253679\n",
            "Iteration 72, loss = 0.52798755\n",
            "Iteration 73, loss = 0.52023270\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.46066599\n",
            "Iteration 75, loss = 0.45789176\n",
            "Iteration 76, loss = 0.45545712\n",
            "Iteration 77, loss = 0.45343134\n",
            "Iteration 78, loss = 0.45070572\n",
            "Iteration 79, loss = 0.44888923\n",
            "Iteration 80, loss = 0.44663198\n",
            "Iteration 81, loss = 0.44409496\n",
            "Iteration 82, loss = 0.44181460\n",
            "Iteration 83, loss = 0.43973443\n",
            "Iteration 84, loss = 0.43764352\n",
            "Iteration 85, loss = 0.43521401\n",
            "Iteration 86, loss = 0.43326160\n",
            "Iteration 87, loss = 0.43132464\n",
            "Iteration 88, loss = 0.42936200\n",
            "Iteration 89, loss = 0.42693003\n",
            "Iteration 90, loss = 0.42511737\n",
            "Iteration 91, loss = 0.42309370\n",
            "Iteration 92, loss = 0.42127513\n",
            "Iteration 93, loss = 0.41939668\n",
            "Iteration 94, loss = 0.41686440\n",
            "Iteration 95, loss = 0.41515916\n",
            "Iteration 96, loss = 0.41305973\n",
            "Iteration 97, loss = 0.41144721\n",
            "Iteration 98, loss = 0.40957640\n",
            "Iteration 99, loss = 0.40747824\n",
            "Iteration 100, loss = 0.40658091\n",
            "Iteration 101, loss = 0.40386575\n",
            "Iteration 102, loss = 0.40296653\n",
            "Iteration 103, loss = 0.40144220\n",
            "Iteration 104, loss = 0.39942755\n",
            "Iteration 105, loss = 0.39832635\n",
            "Iteration 106, loss = 0.39605394\n",
            "Iteration 107, loss = 0.39534464\n",
            "Iteration 108, loss = 0.39303591\n",
            "Iteration 109, loss = 0.39175922\n",
            "Iteration 110, loss = 0.39038495\n",
            "Iteration 111, loss = 0.38840174\n",
            "Iteration 112, loss = 0.38731846\n",
            "Iteration 113, loss = 0.38631692\n",
            "Iteration 114, loss = 0.38436507\n",
            "Iteration 115, loss = 0.38289189\n",
            "Iteration 116, loss = 0.38213306\n",
            "Iteration 117, loss = 0.38120893\n",
            "Iteration 118, loss = 0.37945271\n",
            "Iteration 119, loss = 0.37761056\n",
            "Iteration 120, loss = 0.37746651\n",
            "Iteration 121, loss = 0.37626850\n",
            "Iteration 122, loss = 0.37486627\n",
            "Iteration 123, loss = 0.37373646\n",
            "Iteration 124, loss = 0.37390015\n",
            "Iteration 125, loss = 0.37294041\n",
            "Iteration 126, loss = 0.37267467\n",
            "Iteration 127, loss = 0.37000870\n",
            "Iteration 128, loss = 0.36918827\n",
            "Iteration 129, loss = 0.36925279\n",
            "Iteration 130, loss = 0.36707686\n",
            "Iteration 131, loss = 0.36545787\n",
            "Iteration 132, loss = 0.36695592\n",
            "Iteration 133, loss = 0.36610541\n",
            "Iteration 134, loss = 0.36518893\n",
            "Iteration 135, loss = 0.36659399\n",
            "Iteration 136, loss = 0.36237111\n",
            "Iteration 137, loss = 0.36296808\n",
            "Iteration 138, loss = 0.36292692\n",
            "Iteration 139, loss = 0.36085031\n",
            "Iteration 140, loss = 0.36147669\n",
            "Iteration 141, loss = 0.35968460\n",
            "Iteration 142, loss = 0.36379671\n",
            "Iteration 143, loss = 0.36165593\n",
            "Iteration 144, loss = 0.36244758\n",
            "Iteration 145, loss = 0.36115288\n",
            "Iteration 146, loss = 0.35780624\n",
            "Iteration 147, loss = 0.36352049\n",
            "Iteration 148, loss = 0.35922827\n",
            "Iteration 149, loss = 0.35680459\n",
            "Iteration 150, loss = 0.35793199\n",
            "Iteration 151, loss = 0.35641891\n",
            "Iteration 152, loss = 0.36038200\n",
            "Iteration 153, loss = 0.36123603\n",
            "Iteration 154, loss = 0.35606088\n",
            "Iteration 155, loss = 0.35982058\n",
            "Iteration 156, loss = 0.35617276\n",
            "Iteration 157, loss = 0.35195521\n",
            "Iteration 158, loss = 0.35753752\n",
            "Iteration 159, loss = 0.35880628\n",
            "Iteration 160, loss = 0.35528045\n",
            "Iteration 161, loss = 0.35730239\n",
            "Iteration 162, loss = 0.35193179\n",
            "Iteration 163, loss = 0.35947830\n",
            "Iteration 164, loss = 0.36052925\n",
            "Iteration 165, loss = 0.35185827\n",
            "Iteration 166, loss = 0.35827742\n",
            "Iteration 167, loss = 0.36016941\n",
            "Iteration 168, loss = 0.36302622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 169, loss = 0.33133843\n",
            "Iteration 170, loss = 0.33122724\n",
            "Iteration 171, loss = 0.33093268\n",
            "Iteration 172, loss = 0.33072029\n",
            "Iteration 173, loss = 0.33068578\n",
            "Iteration 174, loss = 0.33053847\n",
            "Iteration 175, loss = 0.33040056\n",
            "Iteration 176, loss = 0.33020843\n",
            "Iteration 177, loss = 0.33027703\n",
            "Iteration 178, loss = 0.33007184\n",
            "Iteration 179, loss = 0.32972598\n",
            "Iteration 180, loss = 0.32970920\n",
            "Iteration 181, loss = 0.32960650\n",
            "Iteration 182, loss = 0.32951532\n",
            "Iteration 183, loss = 0.32931682\n",
            "Iteration 184, loss = 0.32906914\n",
            "Iteration 185, loss = 0.32913456\n",
            "Iteration 186, loss = 0.32886550\n",
            "Iteration 187, loss = 0.32866342\n",
            "Iteration 188, loss = 0.32866052\n",
            "Iteration 189, loss = 0.32848143\n",
            "Iteration 190, loss = 0.32824050\n",
            "Iteration 191, loss = 0.32828059\n",
            "Iteration 192, loss = 0.32809018\n",
            "Iteration 193, loss = 0.32806225\n",
            "Iteration 194, loss = 0.32800859\n",
            "Iteration 195, loss = 0.32774580\n",
            "Iteration 196, loss = 0.32774360\n",
            "Iteration 197, loss = 0.32747800\n",
            "Iteration 198, loss = 0.32724115\n",
            "Iteration 199, loss = 0.32717573\n",
            "Iteration 200, loss = 0.32700496\n",
            "----------------------------------\n",
            "[[33419   308]\n",
            " [ 1218 15701]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 11.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     33727\n",
            "           1       0.98      0.93      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.97     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.681% (9.124%)\n",
            "Prediction Accuracy (Shuffle Split) : 99.065% (4.639%)\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66228798\n",
            "Iteration 3, loss = 0.62496313\n",
            "Iteration 4, loss = 0.60634311\n",
            "Iteration 5, loss = 0.59365438\n",
            "Iteration 6, loss = 0.58811944\n",
            "Iteration 7, loss = 0.58477415\n",
            "Iteration 8, loss = 0.58438747\n",
            "Iteration 9, loss = 0.58407649\n",
            "Iteration 10, loss = 0.58289942\n",
            "Iteration 11, loss = 0.58273136\n",
            "Iteration 12, loss = 0.58260960\n",
            "Iteration 13, loss = 0.58360581\n",
            "Iteration 14, loss = 0.58296848\n",
            "Iteration 15, loss = 0.58240468\n",
            "Iteration 16, loss = 0.58240591\n",
            "Iteration 17, loss = 0.58226914\n",
            "Iteration 18, loss = 0.58147970\n",
            "Iteration 19, loss = 0.58103221\n",
            "Iteration 20, loss = 0.58086683\n",
            "Iteration 21, loss = 0.59237660\n",
            "Iteration 22, loss = 0.64901384\n",
            "Iteration 23, loss = 0.61994957\n",
            "Iteration 24, loss = 0.60181895\n",
            "Iteration 25, loss = 0.59261715\n",
            "Iteration 26, loss = 0.58841262\n",
            "Iteration 27, loss = 0.58672803\n",
            "Iteration 28, loss = 0.59848698\n",
            "Iteration 29, loss = 0.60434653\n",
            "Iteration 30, loss = 0.59312943\n",
            "Iteration 31, loss = 0.60648499\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.59597630\n",
            "Iteration 33, loss = 0.58745515\n",
            "Iteration 34, loss = 0.58292642\n",
            "Iteration 35, loss = 0.57866642\n",
            "Iteration 36, loss = 0.57309544\n",
            "Iteration 37, loss = 0.56788396\n",
            "Iteration 38, loss = 0.56381345\n",
            "Iteration 39, loss = 0.55925422\n",
            "Iteration 40, loss = 0.56008877\n",
            "Iteration 41, loss = 0.56219404\n",
            "Iteration 42, loss = 0.56143093\n",
            "Iteration 43, loss = 0.56007508\n",
            "Iteration 44, loss = 0.56148690\n",
            "Iteration 45, loss = 0.55948258\n",
            "Iteration 46, loss = 0.55773280\n",
            "Iteration 47, loss = 0.55596571\n",
            "Iteration 48, loss = 0.55325545\n",
            "Iteration 49, loss = 0.55261170\n",
            "Iteration 50, loss = 0.55265700\n",
            "Iteration 51, loss = 0.54933693\n",
            "Iteration 52, loss = 0.55058061\n",
            "Iteration 53, loss = 0.54735183\n",
            "Iteration 54, loss = 0.54996869\n",
            "Iteration 55, loss = 0.54443318\n",
            "Iteration 56, loss = 0.54984982\n",
            "Iteration 57, loss = 0.54610874\n",
            "Iteration 58, loss = 0.54977100\n",
            "Iteration 59, loss = 0.54442111\n",
            "Iteration 60, loss = 0.54589598\n",
            "Iteration 61, loss = 0.54557015\n",
            "Iteration 62, loss = 0.54447037\n",
            "Iteration 63, loss = 0.54581188\n",
            "Iteration 64, loss = 0.54580062\n",
            "Iteration 65, loss = 0.54495863\n",
            "Iteration 66, loss = 0.54536030\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.46510161\n",
            "Iteration 68, loss = 0.45185286\n",
            "Iteration 69, loss = 0.44010535\n",
            "Iteration 70, loss = 0.42975570\n",
            "Iteration 71, loss = 0.42076168\n",
            "Iteration 72, loss = 0.41310283\n",
            "Iteration 73, loss = 0.40617771\n",
            "Iteration 74, loss = 0.40075148\n",
            "Iteration 75, loss = 0.39651136\n",
            "Iteration 76, loss = 0.39199119\n",
            "Iteration 77, loss = 0.38857441\n",
            "Iteration 78, loss = 0.38545229\n",
            "Iteration 79, loss = 0.38262651\n",
            "Iteration 80, loss = 0.38075427\n",
            "Iteration 81, loss = 0.37966429\n",
            "Iteration 82, loss = 0.37838706\n",
            "Iteration 83, loss = 0.37802675\n",
            "Iteration 84, loss = 0.37462960\n",
            "Iteration 85, loss = 0.37530492\n",
            "Iteration 86, loss = 0.37304996\n",
            "Iteration 87, loss = 0.37468684\n",
            "Iteration 88, loss = 0.37375441\n",
            "Iteration 89, loss = 0.37272957\n",
            "Iteration 90, loss = 0.37289361\n",
            "Iteration 91, loss = 0.37613967\n",
            "Iteration 92, loss = 0.37279140\n",
            "Iteration 93, loss = 0.37670632\n",
            "Iteration 94, loss = 0.37415225\n",
            "Iteration 95, loss = 0.37849176\n",
            "Iteration 96, loss = 0.37458579\n",
            "Iteration 97, loss = 0.37784727\n",
            "Iteration 98, loss = 0.37557963\n",
            "Iteration 99, loss = 0.37722293\n",
            "Iteration 100, loss = 0.38045050\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.34972382\n",
            "Iteration 102, loss = 0.34949382\n",
            "Iteration 103, loss = 0.34924071\n",
            "Iteration 104, loss = 0.34906644\n",
            "Iteration 105, loss = 0.34889878\n",
            "Iteration 106, loss = 0.34869319\n",
            "Iteration 107, loss = 0.34854164\n",
            "Iteration 108, loss = 0.34842424\n",
            "Iteration 109, loss = 0.34824914\n",
            "Iteration 110, loss = 0.34809922\n",
            "Iteration 111, loss = 0.34800656\n",
            "Iteration 112, loss = 0.34778200\n",
            "Iteration 113, loss = 0.34768414\n",
            "Iteration 114, loss = 0.34750666\n",
            "Iteration 115, loss = 0.34744498\n",
            "Iteration 116, loss = 0.34719701\n",
            "Iteration 117, loss = 0.34703985\n",
            "Iteration 118, loss = 0.34693687\n",
            "Iteration 119, loss = 0.34678955\n",
            "Iteration 120, loss = 0.34661698\n",
            "Iteration 121, loss = 0.34654535\n",
            "Iteration 122, loss = 0.34637483\n",
            "Iteration 123, loss = 0.34617480\n",
            "Iteration 124, loss = 0.34614473\n",
            "Iteration 125, loss = 0.34602558\n",
            "Iteration 126, loss = 0.34580571\n",
            "Iteration 127, loss = 0.34570254\n",
            "Iteration 128, loss = 0.34559301\n",
            "Iteration 129, loss = 0.34544252\n",
            "Iteration 130, loss = 0.34530776\n",
            "Iteration 131, loss = 0.34521677\n",
            "Iteration 132, loss = 0.34504621\n",
            "Iteration 133, loss = 0.34497444\n",
            "Iteration 134, loss = 0.34478883\n",
            "Iteration 135, loss = 0.34470109\n",
            "Iteration 136, loss = 0.34468985\n",
            "Iteration 137, loss = 0.34446266\n",
            "Iteration 138, loss = 0.34440388\n",
            "Iteration 139, loss = 0.34422261\n",
            "Iteration 140, loss = 0.34413359\n",
            "Iteration 141, loss = 0.34400179\n",
            "Iteration 142, loss = 0.34388588\n",
            "Iteration 143, loss = 0.34381201\n",
            "Iteration 144, loss = 0.34376305\n",
            "Iteration 145, loss = 0.34357938\n",
            "Iteration 146, loss = 0.34337511\n",
            "Iteration 147, loss = 0.34344976\n",
            "Iteration 148, loss = 0.34317897\n",
            "Iteration 149, loss = 0.34313371\n",
            "Iteration 150, loss = 0.34295541\n",
            "Iteration 151, loss = 0.34297894\n",
            "Iteration 152, loss = 0.34283177\n",
            "Iteration 153, loss = 0.34259565\n",
            "Iteration 154, loss = 0.34254744\n",
            "Iteration 155, loss = 0.34245913\n",
            "Iteration 156, loss = 0.34228952\n",
            "Iteration 157, loss = 0.34232937\n",
            "Iteration 158, loss = 0.34218718\n",
            "Iteration 159, loss = 0.34207195\n",
            "Iteration 160, loss = 0.34202116\n",
            "Iteration 161, loss = 0.34184252\n",
            "Iteration 162, loss = 0.34173069\n",
            "Iteration 163, loss = 0.34173129\n",
            "Iteration 164, loss = 0.34157976\n",
            "Iteration 165, loss = 0.34146899\n",
            "Iteration 166, loss = 0.34136664\n",
            "Iteration 167, loss = 0.34131177\n",
            "Iteration 168, loss = 0.34117812\n",
            "Iteration 169, loss = 0.34113864\n",
            "Iteration 170, loss = 0.34100542\n",
            "Iteration 171, loss = 0.34098536\n",
            "Iteration 172, loss = 0.34088423\n",
            "Iteration 173, loss = 0.34078820\n",
            "Iteration 174, loss = 0.34072440\n",
            "Iteration 175, loss = 0.34061545\n",
            "Iteration 176, loss = 0.34049577\n",
            "Iteration 177, loss = 0.34043137\n",
            "Iteration 178, loss = 0.34032585\n",
            "Iteration 179, loss = 0.34026063\n",
            "Iteration 180, loss = 0.34010522\n",
            "Iteration 181, loss = 0.33999888\n",
            "Iteration 182, loss = 0.33991423\n",
            "Iteration 183, loss = 0.33984633\n",
            "Iteration 184, loss = 0.33984337\n",
            "Iteration 185, loss = 0.33968460\n",
            "Iteration 186, loss = 0.33962975\n",
            "Iteration 187, loss = 0.33953926\n",
            "Iteration 188, loss = 0.33941291\n",
            "Iteration 189, loss = 0.33942777\n",
            "Iteration 190, loss = 0.33928216\n",
            "Iteration 191, loss = 0.33929264\n",
            "Iteration 192, loss = 0.33910240\n",
            "Iteration 193, loss = 0.33913285\n",
            "Iteration 194, loss = 0.33897803\n",
            "Iteration 195, loss = 0.33894370\n",
            "Iteration 196, loss = 0.33878894\n",
            "Iteration 197, loss = 0.33877259\n",
            "Iteration 198, loss = 0.33869945\n",
            "Iteration 199, loss = 0.33856441\n",
            "Iteration 200, loss = 0.33852217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62716055\n",
            "Iteration 3, loss = 0.60365089\n",
            "Iteration 4, loss = 0.59053169\n",
            "Iteration 5, loss = 0.57897387\n",
            "Iteration 6, loss = 0.57734145\n",
            "Iteration 7, loss = 0.57582105\n",
            "Iteration 8, loss = 0.57057976\n",
            "Iteration 9, loss = 0.57210131\n",
            "Iteration 10, loss = 0.57259470\n",
            "Iteration 11, loss = 0.57182867\n",
            "Iteration 12, loss = 0.57026717\n",
            "Iteration 13, loss = 0.56929101\n",
            "Iteration 14, loss = 0.56651879\n",
            "Iteration 15, loss = 0.56670313\n",
            "Iteration 16, loss = 0.56620893\n",
            "Iteration 17, loss = 0.56591266\n",
            "Iteration 18, loss = 0.56568933\n",
            "Iteration 19, loss = 0.56589946\n",
            "Iteration 20, loss = 0.56654983\n",
            "Iteration 21, loss = 0.56517872\n",
            "Iteration 22, loss = 0.56690190\n",
            "Iteration 23, loss = 0.56494131\n",
            "Iteration 24, loss = 0.56608552\n",
            "Iteration 25, loss = 0.56620137\n",
            "Iteration 26, loss = 0.56561010\n",
            "Iteration 27, loss = 0.56654050\n",
            "Iteration 28, loss = 0.56562782\n",
            "Iteration 29, loss = 0.56667416\n",
            "Iteration 30, loss = 0.56506774\n",
            "Iteration 31, loss = 0.58084892\n",
            "Iteration 32, loss = 0.59551085\n",
            "Iteration 33, loss = 0.57774007\n",
            "Iteration 34, loss = 0.57116895\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.53545809\n",
            "Iteration 36, loss = 0.52006439\n",
            "Iteration 37, loss = 0.50741089\n",
            "Iteration 38, loss = 0.51278514\n",
            "Iteration 39, loss = 0.51895570\n",
            "Iteration 40, loss = 0.51720614\n",
            "Iteration 41, loss = 0.51501449\n",
            "Iteration 42, loss = 0.51438132\n",
            "Iteration 43, loss = 0.51129890\n",
            "Iteration 44, loss = 0.51206262\n",
            "Iteration 45, loss = 0.51058746\n",
            "Iteration 46, loss = 0.50649430\n",
            "Iteration 47, loss = 0.50774496\n",
            "Iteration 48, loss = 0.51284416\n",
            "Iteration 49, loss = 0.50398060\n",
            "Iteration 50, loss = 0.50895093\n",
            "Iteration 51, loss = 0.50867816\n",
            "Iteration 52, loss = 0.50287496\n",
            "Iteration 53, loss = 0.51271889\n",
            "Iteration 54, loss = 0.50520637\n",
            "Iteration 55, loss = 0.50930988\n",
            "Iteration 56, loss = 0.50612067\n",
            "Iteration 57, loss = 0.50861770\n",
            "Iteration 58, loss = 0.50647890\n",
            "Iteration 59, loss = 0.50839519\n",
            "Iteration 60, loss = 0.50936606\n",
            "Iteration 61, loss = 0.50677809\n",
            "Iteration 62, loss = 0.50908167\n",
            "Iteration 63, loss = 0.50766170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 64, loss = 0.39603830\n",
            "Iteration 65, loss = 0.38122332\n",
            "Iteration 66, loss = 0.36802434\n",
            "Iteration 67, loss = 0.35653801\n",
            "Iteration 68, loss = 0.34626863\n",
            "Iteration 69, loss = 0.33732921\n",
            "Iteration 70, loss = 0.32913424\n",
            "Iteration 71, loss = 0.32203781\n",
            "Iteration 72, loss = 0.31564963\n",
            "Iteration 73, loss = 0.30990556\n",
            "Iteration 74, loss = 0.30493914\n",
            "Iteration 75, loss = 0.30037013\n",
            "Iteration 76, loss = 0.29635822\n",
            "Iteration 77, loss = 0.29268348\n",
            "Iteration 78, loss = 0.28970745\n",
            "Iteration 79, loss = 0.28627063\n",
            "Iteration 80, loss = 0.28367872\n",
            "Iteration 81, loss = 0.28144353\n",
            "Iteration 82, loss = 0.27918074\n",
            "Iteration 83, loss = 0.27706246\n",
            "Iteration 84, loss = 0.27505722\n",
            "Iteration 85, loss = 0.27352752\n",
            "Iteration 86, loss = 0.27213277\n",
            "Iteration 87, loss = 0.27032228\n",
            "Iteration 88, loss = 0.26909679\n",
            "Iteration 89, loss = 0.26829095\n",
            "Iteration 90, loss = 0.26689196\n",
            "Iteration 91, loss = 0.26537580\n",
            "Iteration 92, loss = 0.26441628\n",
            "Iteration 93, loss = 0.26405209\n",
            "Iteration 94, loss = 0.26312453\n",
            "Iteration 95, loss = 0.26249362\n",
            "Iteration 96, loss = 0.26211556\n",
            "Iteration 97, loss = 0.26120857\n",
            "Iteration 98, loss = 0.26047959\n",
            "Iteration 99, loss = 0.25949781\n",
            "Iteration 100, loss = 0.25915139\n",
            "Iteration 101, loss = 0.25923143\n",
            "Iteration 102, loss = 0.25853599\n",
            "Iteration 103, loss = 0.25854049\n",
            "Iteration 104, loss = 0.25850880\n",
            "Iteration 105, loss = 0.25799011\n",
            "Iteration 106, loss = 0.25698839\n",
            "Iteration 107, loss = 0.25742881\n",
            "Iteration 108, loss = 0.25769812\n",
            "Iteration 109, loss = 0.25623625\n",
            "Iteration 110, loss = 0.25593220\n",
            "Iteration 111, loss = 0.25661767\n",
            "Iteration 112, loss = 0.25634801\n",
            "Iteration 113, loss = 0.25549969\n",
            "Iteration 114, loss = 0.25530176\n",
            "Iteration 115, loss = 0.25573990\n",
            "Iteration 116, loss = 0.25563839\n",
            "Iteration 117, loss = 0.25618552\n",
            "Iteration 118, loss = 0.25463865\n",
            "Iteration 119, loss = 0.25461431\n",
            "Iteration 120, loss = 0.25510182\n",
            "Iteration 121, loss = 0.25613726\n",
            "Iteration 122, loss = 0.25448477\n",
            "Iteration 123, loss = 0.25612708\n",
            "Iteration 124, loss = 0.25473154\n",
            "Iteration 125, loss = 0.25527429\n",
            "Iteration 126, loss = 0.26081466\n",
            "Iteration 127, loss = 0.25977724\n",
            "Iteration 128, loss = 0.25483525\n",
            "Iteration 129, loss = 0.25600743\n",
            "Iteration 130, loss = 0.25750838\n",
            "Iteration 131, loss = 0.25806939\n",
            "Iteration 132, loss = 0.25596283\n",
            "Iteration 133, loss = 0.25747470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 134, loss = 0.24771243\n",
            "Iteration 135, loss = 0.24770367\n",
            "Iteration 136, loss = 0.24764243\n",
            "Iteration 137, loss = 0.24765485\n",
            "Iteration 138, loss = 0.24759139\n",
            "Iteration 139, loss = 0.24755856\n",
            "Iteration 140, loss = 0.24757464\n",
            "Iteration 141, loss = 0.24756900\n",
            "Iteration 142, loss = 0.24750568\n",
            "Iteration 143, loss = 0.24751863\n",
            "Iteration 144, loss = 0.24743373\n",
            "Iteration 145, loss = 0.24745322\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 146, loss = 0.24717474\n",
            "Iteration 147, loss = 0.24717032\n",
            "Iteration 148, loss = 0.24716944\n",
            "Iteration 149, loss = 0.24716686\n",
            "Iteration 150, loss = 0.24716186\n",
            "Iteration 151, loss = 0.24715653\n",
            "Iteration 152, loss = 0.24715278\n",
            "Iteration 153, loss = 0.24713822\n",
            "Iteration 154, loss = 0.24713715\n",
            "Iteration 155, loss = 0.24712640\n",
            "Iteration 156, loss = 0.24710478\n",
            "Iteration 157, loss = 0.24711833\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 158, loss = 0.24704010\n",
            "Iteration 159, loss = 0.24704203\n",
            "Iteration 160, loss = 0.24703524\n",
            "Iteration 161, loss = 0.24704314\n",
            "Iteration 162, loss = 0.24703164\n",
            "Iteration 163, loss = 0.24703363\n",
            "Iteration 164, loss = 0.24702879\n",
            "Iteration 165, loss = 0.24702734\n",
            "Iteration 166, loss = 0.24703127\n",
            "Iteration 167, loss = 0.24703373\n",
            "Iteration 168, loss = 0.24703479\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66281253\n",
            "Iteration 3, loss = 0.62195451\n",
            "Iteration 4, loss = 0.60154308\n",
            "Iteration 5, loss = 0.59066789\n",
            "Iteration 6, loss = 0.58246892\n",
            "Iteration 7, loss = 0.57947445\n",
            "Iteration 8, loss = 0.57575249\n",
            "Iteration 9, loss = 0.57236742\n",
            "Iteration 10, loss = 0.57382887\n",
            "Iteration 11, loss = 0.57471575\n",
            "Iteration 12, loss = 0.57237837\n",
            "Iteration 13, loss = 0.57263171\n",
            "Iteration 14, loss = 0.57213417\n",
            "Iteration 15, loss = 0.57332169\n",
            "Iteration 16, loss = 0.57441574\n",
            "Iteration 17, loss = 0.57413020\n",
            "Iteration 18, loss = 0.57340308\n",
            "Iteration 19, loss = 0.57352600\n",
            "Iteration 20, loss = 0.57346698\n",
            "Iteration 21, loss = 0.57356680\n",
            "Iteration 22, loss = 0.57421982\n",
            "Iteration 23, loss = 0.57371239\n",
            "Iteration 24, loss = 0.57306813\n",
            "Iteration 25, loss = 0.57530453\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.52013866\n",
            "Iteration 27, loss = 0.49910701\n",
            "Iteration 28, loss = 0.52066689\n",
            "Iteration 29, loss = 0.52276612\n",
            "Iteration 30, loss = 0.52403605\n",
            "Iteration 31, loss = 0.52188086\n",
            "Iteration 32, loss = 0.52319646\n",
            "Iteration 33, loss = 0.52229496\n",
            "Iteration 34, loss = 0.52249540\n",
            "Iteration 35, loss = 0.52463454\n",
            "Iteration 36, loss = 0.52168144\n",
            "Iteration 37, loss = 0.52208319\n",
            "Iteration 38, loss = 0.52587410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.43729935\n",
            "Iteration 40, loss = 0.42378804\n",
            "Iteration 41, loss = 0.41168669\n",
            "Iteration 42, loss = 0.40077589\n",
            "Iteration 43, loss = 0.39068887\n",
            "Iteration 44, loss = 0.38150491\n",
            "Iteration 45, loss = 0.37340319\n",
            "Iteration 46, loss = 0.36594849\n",
            "Iteration 47, loss = 0.35938771\n",
            "Iteration 48, loss = 0.35405935\n",
            "Iteration 49, loss = 0.34884355\n",
            "Iteration 50, loss = 0.34422125\n",
            "Iteration 51, loss = 0.34063692\n",
            "Iteration 52, loss = 0.33745122\n",
            "Iteration 53, loss = 0.33374559\n",
            "Iteration 54, loss = 0.33215277\n",
            "Iteration 55, loss = 0.32951084\n",
            "Iteration 56, loss = 0.32685652\n",
            "Iteration 57, loss = 0.32558322\n",
            "Iteration 58, loss = 0.32525969\n",
            "Iteration 59, loss = 0.32379560\n",
            "Iteration 60, loss = 0.32259361\n",
            "Iteration 61, loss = 0.32135350\n",
            "Iteration 62, loss = 0.31845898\n",
            "Iteration 63, loss = 0.32458289\n",
            "Iteration 64, loss = 0.31997669\n",
            "Iteration 65, loss = 0.32111169\n",
            "Iteration 66, loss = 0.32227755\n",
            "Iteration 67, loss = 0.31895226\n",
            "Iteration 68, loss = 0.32206087\n",
            "Iteration 69, loss = 0.31960821\n",
            "Iteration 70, loss = 0.32346659\n",
            "Iteration 71, loss = 0.32139503\n",
            "Iteration 72, loss = 0.32171098\n",
            "Iteration 73, loss = 0.32190290\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 74, loss = 0.29410783\n",
            "Iteration 75, loss = 0.29373105\n",
            "Iteration 76, loss = 0.29355188\n",
            "Iteration 77, loss = 0.29330522\n",
            "Iteration 78, loss = 0.29310510\n",
            "Iteration 79, loss = 0.29286176\n",
            "Iteration 80, loss = 0.29263702\n",
            "Iteration 81, loss = 0.29247680\n",
            "Iteration 82, loss = 0.29221510\n",
            "Iteration 83, loss = 0.29209350\n",
            "Iteration 84, loss = 0.29191015\n",
            "Iteration 85, loss = 0.29169437\n",
            "Iteration 86, loss = 0.29151540\n",
            "Iteration 87, loss = 0.29131168\n",
            "Iteration 88, loss = 0.29113987\n",
            "Iteration 89, loss = 0.29098369\n",
            "Iteration 90, loss = 0.29078852\n",
            "Iteration 91, loss = 0.29058324\n",
            "Iteration 92, loss = 0.29039580\n",
            "Iteration 93, loss = 0.29022804\n",
            "Iteration 94, loss = 0.29006260\n",
            "Iteration 95, loss = 0.28983547\n",
            "Iteration 96, loss = 0.28970526\n",
            "Iteration 97, loss = 0.28950556\n",
            "Iteration 98, loss = 0.28931544\n",
            "Iteration 99, loss = 0.28912813\n",
            "Iteration 100, loss = 0.28899717\n",
            "Iteration 101, loss = 0.28879032\n",
            "Iteration 102, loss = 0.28868835\n",
            "Iteration 103, loss = 0.28856238\n",
            "Iteration 104, loss = 0.28836950\n",
            "Iteration 105, loss = 0.28819404\n",
            "Iteration 106, loss = 0.28804448\n",
            "Iteration 107, loss = 0.28789141\n",
            "Iteration 108, loss = 0.28770550\n",
            "Iteration 109, loss = 0.28739352\n",
            "Iteration 110, loss = 0.28673768\n",
            "Iteration 111, loss = 0.28503944\n",
            "Iteration 112, loss = 0.28145648\n",
            "Iteration 113, loss = 0.27702386\n",
            "Iteration 114, loss = 0.27391513\n",
            "Iteration 115, loss = 0.27215525\n",
            "Iteration 116, loss = 0.27122497\n",
            "Iteration 117, loss = 0.27058017\n",
            "Iteration 118, loss = 0.27009861\n",
            "Iteration 119, loss = 0.26977874\n",
            "Iteration 120, loss = 0.26945552\n",
            "Iteration 121, loss = 0.26913615\n",
            "Iteration 122, loss = 0.26887787\n",
            "Iteration 123, loss = 0.26870729\n",
            "Iteration 124, loss = 0.26839850\n",
            "Iteration 125, loss = 0.26823295\n",
            "Iteration 126, loss = 0.26801412\n",
            "Iteration 127, loss = 0.26782809\n",
            "Iteration 128, loss = 0.26758132\n",
            "Iteration 129, loss = 0.26734985\n",
            "Iteration 130, loss = 0.26705692\n",
            "Iteration 131, loss = 0.26685865\n",
            "Iteration 132, loss = 0.26669828\n",
            "Iteration 133, loss = 0.26651505\n",
            "Iteration 134, loss = 0.26637279\n",
            "Iteration 135, loss = 0.26606466\n",
            "Iteration 136, loss = 0.26593382\n",
            "Iteration 137, loss = 0.26567081\n",
            "Iteration 138, loss = 0.26554515\n",
            "Iteration 139, loss = 0.26536938\n",
            "Iteration 140, loss = 0.26513140\n",
            "Iteration 141, loss = 0.26498405\n",
            "Iteration 142, loss = 0.26484305\n",
            "Iteration 143, loss = 0.26468607\n",
            "Iteration 144, loss = 0.26446325\n",
            "Iteration 145, loss = 0.26429478\n",
            "Iteration 146, loss = 0.26416715\n",
            "Iteration 147, loss = 0.26400926\n",
            "Iteration 148, loss = 0.26376800\n",
            "Iteration 149, loss = 0.26364873\n",
            "Iteration 150, loss = 0.26354660\n",
            "Iteration 151, loss = 0.26338389\n",
            "Iteration 152, loss = 0.26326450\n",
            "Iteration 153, loss = 0.26308543\n",
            "Iteration 154, loss = 0.26290353\n",
            "Iteration 155, loss = 0.26278308\n",
            "Iteration 156, loss = 0.26258489\n",
            "Iteration 157, loss = 0.26241894\n",
            "Iteration 158, loss = 0.26239004\n",
            "Iteration 159, loss = 0.26223084\n",
            "Iteration 160, loss = 0.26207182\n",
            "Iteration 161, loss = 0.26191389\n",
            "Iteration 162, loss = 0.26184564\n",
            "Iteration 163, loss = 0.26170904\n",
            "Iteration 164, loss = 0.26151640\n",
            "Iteration 165, loss = 0.26148541\n",
            "Iteration 166, loss = 0.26136832\n",
            "Iteration 167, loss = 0.26119296\n",
            "Iteration 168, loss = 0.26106029\n",
            "Iteration 169, loss = 0.26107121\n",
            "Iteration 170, loss = 0.26084355\n",
            "Iteration 171, loss = 0.26068869\n",
            "Iteration 172, loss = 0.26066567\n",
            "Iteration 173, loss = 0.26051347\n",
            "Iteration 174, loss = 0.26047593\n",
            "Iteration 175, loss = 0.26031744\n",
            "Iteration 176, loss = 0.26028157\n",
            "Iteration 177, loss = 0.26012014\n",
            "Iteration 178, loss = 0.25994097\n",
            "Iteration 179, loss = 0.25987715\n",
            "Iteration 180, loss = 0.25978228\n",
            "Iteration 181, loss = 0.25969515\n",
            "Iteration 182, loss = 0.25955137\n",
            "Iteration 183, loss = 0.25951689\n",
            "Iteration 184, loss = 0.25938478\n",
            "Iteration 185, loss = 0.25930753\n",
            "Iteration 186, loss = 0.25919878\n",
            "Iteration 187, loss = 0.25910653\n",
            "Iteration 188, loss = 0.25904053\n",
            "Iteration 189, loss = 0.25892370\n",
            "Iteration 190, loss = 0.25889277\n",
            "Iteration 191, loss = 0.25874231\n",
            "Iteration 192, loss = 0.25865030\n",
            "Iteration 193, loss = 0.25862615\n",
            "Iteration 194, loss = 0.25851941\n",
            "Iteration 195, loss = 0.25840033\n",
            "Iteration 196, loss = 0.25837068\n",
            "Iteration 197, loss = 0.25822977\n",
            "Iteration 198, loss = 0.25824049\n",
            "Iteration 199, loss = 0.25809319\n",
            "Iteration 200, loss = 0.25807639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63690925\n",
            "Iteration 3, loss = 0.60881327\n",
            "Iteration 4, loss = 0.59320766\n",
            "Iteration 5, loss = 0.58297498\n",
            "Iteration 6, loss = 0.57726901\n",
            "Iteration 7, loss = 0.57204280\n",
            "Iteration 8, loss = 0.57228038\n",
            "Iteration 9, loss = 0.57066409\n",
            "Iteration 10, loss = 0.56994070\n",
            "Iteration 11, loss = 0.56898757\n",
            "Iteration 12, loss = 0.56893900\n",
            "Iteration 13, loss = 0.56853109\n",
            "Iteration 14, loss = 0.56959936\n",
            "Iteration 15, loss = 0.56987326\n",
            "Iteration 16, loss = 0.56977486\n",
            "Iteration 17, loss = 0.57191417\n",
            "Iteration 18, loss = 0.57526497\n",
            "Iteration 19, loss = 0.57303046\n",
            "Iteration 20, loss = 0.57203677\n",
            "Iteration 21, loss = 0.57031492\n",
            "Iteration 22, loss = 0.57156473\n",
            "Iteration 23, loss = 0.58714091\n",
            "Iteration 24, loss = 0.60443535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.61874436\n",
            "Iteration 26, loss = 0.61113163\n",
            "Iteration 27, loss = 0.60046480\n",
            "Iteration 28, loss = 0.58229820\n",
            "Iteration 29, loss = 0.57208791\n",
            "Iteration 30, loss = 0.56326721\n",
            "Iteration 31, loss = 0.55383903\n",
            "Iteration 32, loss = 0.54535874\n",
            "Iteration 33, loss = 0.54364925\n",
            "Iteration 34, loss = 0.54133443\n",
            "Iteration 35, loss = 0.53578276\n",
            "Iteration 36, loss = 0.53145523\n",
            "Iteration 37, loss = 0.53313668\n",
            "Iteration 38, loss = 0.52801183\n",
            "Iteration 39, loss = 0.52524607\n",
            "Iteration 40, loss = 0.52167612\n",
            "Iteration 41, loss = 0.52042719\n",
            "Iteration 42, loss = 0.51787509\n",
            "Iteration 43, loss = 0.51995901\n",
            "Iteration 44, loss = 0.51785443\n",
            "Iteration 45, loss = 0.51194841\n",
            "Iteration 46, loss = 0.51766404\n",
            "Iteration 47, loss = 0.51421600\n",
            "Iteration 48, loss = 0.51570830\n",
            "Iteration 49, loss = 0.51460366\n",
            "Iteration 50, loss = 0.51648803\n",
            "Iteration 51, loss = 0.51223619\n",
            "Iteration 52, loss = 0.51173190\n",
            "Iteration 53, loss = 0.51402944\n",
            "Iteration 54, loss = 0.51495157\n",
            "Iteration 55, loss = 0.51463395\n",
            "Iteration 56, loss = 0.51502014\n",
            "Iteration 57, loss = 0.51087306\n",
            "Iteration 58, loss = 0.51432894\n",
            "Iteration 59, loss = 0.51432049\n",
            "Iteration 60, loss = 0.51052984\n",
            "Iteration 61, loss = 0.51621697\n",
            "Iteration 62, loss = 0.51674986\n",
            "Iteration 63, loss = 0.51229023\n",
            "Iteration 64, loss = 0.51417511\n",
            "Iteration 65, loss = 0.51351808\n",
            "Iteration 66, loss = 0.51657634\n",
            "Iteration 67, loss = 0.52124612\n",
            "Iteration 68, loss = 0.51030315\n",
            "Iteration 69, loss = 0.51950936\n",
            "Iteration 70, loss = 0.51752992\n",
            "Iteration 71, loss = 0.51189547\n",
            "Iteration 72, loss = 0.52159186\n",
            "Iteration 73, loss = 0.51512049\n",
            "Iteration 74, loss = 0.51994815\n",
            "Iteration 75, loss = 0.51528272\n",
            "Iteration 76, loss = 0.53747009\n",
            "Iteration 77, loss = 0.60335642\n",
            "Iteration 78, loss = 0.58890104\n",
            "Iteration 79, loss = 0.59117940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.52633393\n",
            "Iteration 81, loss = 0.51652183\n",
            "Iteration 82, loss = 0.50679772\n",
            "Iteration 83, loss = 0.49738625\n",
            "Iteration 84, loss = 0.48800291\n",
            "Iteration 85, loss = 0.47841063\n",
            "Iteration 86, loss = 0.46926179\n",
            "Iteration 87, loss = 0.46001036\n",
            "Iteration 88, loss = 0.45144498\n",
            "Iteration 89, loss = 0.44262574\n",
            "Iteration 90, loss = 0.43481541\n",
            "Iteration 91, loss = 0.42632023\n",
            "Iteration 92, loss = 0.41861628\n",
            "Iteration 93, loss = 0.41134080\n",
            "Iteration 94, loss = 0.40434995\n",
            "Iteration 95, loss = 0.39776654\n",
            "Iteration 96, loss = 0.39214591\n",
            "Iteration 97, loss = 0.38611751\n",
            "Iteration 98, loss = 0.37838457\n",
            "Iteration 99, loss = 0.37389509\n",
            "Iteration 100, loss = 0.36865283\n",
            "Iteration 101, loss = 0.36397158\n",
            "Iteration 102, loss = 0.36281321\n",
            "Iteration 103, loss = 0.35770174\n",
            "Iteration 104, loss = 0.35358441\n",
            "Iteration 105, loss = 0.35174647\n",
            "Iteration 106, loss = 0.35012340\n",
            "Iteration 107, loss = 0.34573078\n",
            "Iteration 108, loss = 0.34495161\n",
            "Iteration 109, loss = 0.34805616\n",
            "Iteration 110, loss = 0.34018407\n",
            "Iteration 111, loss = 0.34787361\n",
            "Iteration 112, loss = 0.33718651\n",
            "Iteration 113, loss = 0.33667300\n",
            "Iteration 114, loss = 0.34220810\n",
            "Iteration 115, loss = 0.34034546\n",
            "Iteration 116, loss = 0.32783359\n",
            "Iteration 117, loss = 0.33748359\n",
            "Iteration 118, loss = 0.33378956\n",
            "Iteration 119, loss = 0.33974401\n",
            "Iteration 120, loss = 0.33353299\n",
            "Iteration 121, loss = 0.32592948\n",
            "Iteration 122, loss = 0.32573867\n",
            "Iteration 123, loss = 0.33308570\n",
            "Iteration 124, loss = 0.33485886\n",
            "Iteration 125, loss = 0.33775268\n",
            "Iteration 126, loss = 0.31799762\n",
            "Iteration 127, loss = 0.32521438\n",
            "Iteration 128, loss = 0.32496612\n",
            "Iteration 129, loss = 0.31988422\n",
            "Iteration 130, loss = 0.32177743\n",
            "Iteration 131, loss = 0.30610391\n",
            "Iteration 132, loss = 0.31944792\n",
            "Iteration 133, loss = 0.32805629\n",
            "Iteration 134, loss = 0.30612249\n",
            "Iteration 135, loss = 0.31250510\n",
            "Iteration 136, loss = 0.31402901\n",
            "Iteration 137, loss = 0.31456832\n",
            "Iteration 138, loss = 0.31275375\n",
            "Iteration 139, loss = 0.30461688\n",
            "Iteration 140, loss = 0.30338023\n",
            "Iteration 141, loss = 0.29904106\n",
            "Iteration 142, loss = 0.29560768\n",
            "Iteration 143, loss = 0.30494580\n",
            "Iteration 144, loss = 0.30055060\n",
            "Iteration 145, loss = 0.30038378\n",
            "Iteration 146, loss = 0.29294013\n",
            "Iteration 147, loss = 0.29679702\n",
            "Iteration 148, loss = 0.30016014\n",
            "Iteration 149, loss = 0.30014585\n",
            "Iteration 150, loss = 0.29788726\n",
            "Iteration 151, loss = 0.30755169\n",
            "Iteration 152, loss = 0.29251380\n",
            "Iteration 153, loss = 0.29973172\n",
            "Iteration 154, loss = 0.30214419\n",
            "Iteration 155, loss = 0.28677017\n",
            "Iteration 156, loss = 0.30168923\n",
            "Iteration 157, loss = 0.31547876\n",
            "Iteration 158, loss = 0.29932243\n",
            "Iteration 159, loss = 0.28594772\n",
            "Iteration 160, loss = 0.28367444\n",
            "Iteration 161, loss = 0.30137690\n",
            "Iteration 162, loss = 0.29666330\n",
            "Iteration 163, loss = 0.29158713\n",
            "Iteration 164, loss = 0.28878272\n",
            "Iteration 165, loss = 0.27890063\n",
            "Iteration 166, loss = 0.29233841\n",
            "Iteration 167, loss = 0.29378982\n",
            "Iteration 168, loss = 0.29339713\n",
            "Iteration 169, loss = 0.29418292\n",
            "Iteration 170, loss = 0.28001948\n",
            "Iteration 171, loss = 0.29121624\n",
            "Iteration 172, loss = 0.28727163\n",
            "Iteration 173, loss = 0.27510529\n",
            "Iteration 174, loss = 0.26955549\n",
            "Iteration 175, loss = 0.28683844\n",
            "Iteration 176, loss = 0.27761797\n",
            "Iteration 177, loss = 0.28825304\n",
            "Iteration 178, loss = 0.28677054\n",
            "Iteration 179, loss = 0.29271941\n",
            "Iteration 180, loss = 0.28117349\n",
            "Iteration 181, loss = 0.27940291\n",
            "Iteration 182, loss = 0.28890670\n",
            "Iteration 183, loss = 0.28428023\n",
            "Iteration 184, loss = 0.28846611\n",
            "Iteration 185, loss = 0.26838251\n",
            "Iteration 186, loss = 0.27609046\n",
            "Iteration 187, loss = 0.28500685\n",
            "Iteration 188, loss = 0.28459862\n",
            "Iteration 189, loss = 0.28382750\n",
            "Iteration 190, loss = 0.27816033\n",
            "Iteration 191, loss = 0.29497910\n",
            "Iteration 192, loss = 0.27244144\n",
            "Iteration 193, loss = 0.27354853\n",
            "Iteration 194, loss = 0.27634288\n",
            "Iteration 195, loss = 0.27397564\n",
            "Iteration 196, loss = 0.28155582\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 197, loss = 0.25626869\n",
            "Iteration 198, loss = 0.25617760\n",
            "Iteration 199, loss = 0.25616252\n",
            "Iteration 200, loss = 0.25608009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63693414\n",
            "Iteration 3, loss = 0.61063344\n",
            "Iteration 4, loss = 0.59439891\n",
            "Iteration 5, loss = 0.58349842\n",
            "Iteration 6, loss = 0.59789214\n",
            "Iteration 7, loss = 0.59328057\n",
            "Iteration 8, loss = 0.58484000\n",
            "Iteration 9, loss = 0.57854315\n",
            "Iteration 10, loss = 0.57613816\n",
            "Iteration 11, loss = 0.57368927\n",
            "Iteration 12, loss = 0.57324893\n",
            "Iteration 13, loss = 0.57255443\n",
            "Iteration 14, loss = 0.60939853\n",
            "Iteration 15, loss = 0.59112572\n",
            "Iteration 16, loss = 0.58007251\n",
            "Iteration 17, loss = 0.57682023\n",
            "Iteration 18, loss = 0.57364688\n",
            "Iteration 19, loss = 0.57206759\n",
            "Iteration 20, loss = 0.57265446\n",
            "Iteration 21, loss = 0.57247856\n",
            "Iteration 22, loss = 0.57169964\n",
            "Iteration 23, loss = 0.57200668\n",
            "Iteration 24, loss = 0.57404697\n",
            "Iteration 25, loss = 0.57415205\n",
            "Iteration 26, loss = 0.57322822\n",
            "Iteration 27, loss = 0.57316159\n",
            "Iteration 28, loss = 0.57374483\n",
            "Iteration 29, loss = 0.57548130\n",
            "Iteration 30, loss = 0.57354256\n",
            "Iteration 31, loss = 0.57446665\n",
            "Iteration 32, loss = 0.57423702\n",
            "Iteration 33, loss = 0.57784214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 34, loss = 0.52743557\n",
            "Iteration 35, loss = 0.50776097\n",
            "Iteration 36, loss = 0.52331258\n",
            "Iteration 37, loss = 0.52830436\n",
            "Iteration 38, loss = 0.53187210\n",
            "Iteration 39, loss = 0.52907938\n",
            "Iteration 40, loss = 0.52961561\n",
            "Iteration 41, loss = 0.52807594\n",
            "Iteration 42, loss = 0.52647386\n",
            "Iteration 43, loss = 0.52318060\n",
            "Iteration 44, loss = 0.52832345\n",
            "Iteration 45, loss = 0.52511408\n",
            "Iteration 46, loss = 0.52748281\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 47, loss = 0.44430309\n",
            "Iteration 48, loss = 0.43182645\n",
            "Iteration 49, loss = 0.42052519\n",
            "Iteration 50, loss = 0.41024731\n",
            "Iteration 51, loss = 0.40055124\n",
            "Iteration 52, loss = 0.39194744\n",
            "Iteration 53, loss = 0.38408179\n",
            "Iteration 54, loss = 0.37701968\n",
            "Iteration 55, loss = 0.37119631\n",
            "Iteration 56, loss = 0.36578751\n",
            "Iteration 57, loss = 0.36069196\n",
            "Iteration 58, loss = 0.35619600\n",
            "Iteration 59, loss = 0.35286535\n",
            "Iteration 60, loss = 0.34863440\n",
            "Iteration 61, loss = 0.34600422\n",
            "Iteration 62, loss = 0.34491103\n",
            "Iteration 63, loss = 0.34236225\n",
            "Iteration 64, loss = 0.33911146\n",
            "Iteration 65, loss = 0.33743469\n",
            "Iteration 66, loss = 0.33614320\n",
            "Iteration 67, loss = 0.33495358\n",
            "Iteration 68, loss = 0.33494663\n",
            "Iteration 69, loss = 0.33340658\n",
            "Iteration 70, loss = 0.33235765\n",
            "Iteration 71, loss = 0.33209541\n",
            "Iteration 72, loss = 0.33466938\n",
            "Iteration 73, loss = 0.33145942\n",
            "Iteration 74, loss = 0.33310053\n",
            "Iteration 75, loss = 0.33265779\n",
            "Iteration 76, loss = 0.33092947\n",
            "Iteration 77, loss = 0.33405944\n",
            "Iteration 78, loss = 0.33518794\n",
            "Iteration 79, loss = 0.33067795\n",
            "Iteration 80, loss = 0.33238434\n",
            "Iteration 81, loss = 0.33085436\n",
            "Iteration 82, loss = 0.33205070\n",
            "Iteration 83, loss = 0.33046321\n",
            "Iteration 84, loss = 0.33346802\n",
            "Iteration 85, loss = 0.33705435\n",
            "Iteration 86, loss = 0.33153335\n",
            "Iteration 87, loss = 0.33040242\n",
            "Iteration 88, loss = 0.33264194\n",
            "Iteration 89, loss = 0.33461410\n",
            "Iteration 90, loss = 0.33226288\n",
            "Iteration 91, loss = 0.32994574\n",
            "Iteration 92, loss = 0.32728087\n",
            "Iteration 93, loss = 0.33334401\n",
            "Iteration 94, loss = 0.32793377\n",
            "Iteration 95, loss = 0.33627067\n",
            "Iteration 96, loss = 0.33071459\n",
            "Iteration 97, loss = 0.33006467\n",
            "Iteration 98, loss = 0.32297970\n",
            "Iteration 99, loss = 0.33167807\n",
            "Iteration 100, loss = 0.32606355\n",
            "Iteration 101, loss = 0.33084931\n",
            "Iteration 102, loss = 0.32410416\n",
            "Iteration 103, loss = 0.32772686\n",
            "Iteration 104, loss = 0.33028273\n",
            "Iteration 105, loss = 0.32263798\n",
            "Iteration 106, loss = 0.33330950\n",
            "Iteration 107, loss = 0.32195783\n",
            "Iteration 108, loss = 0.32315641\n",
            "Iteration 109, loss = 0.32476980\n",
            "Iteration 110, loss = 0.31949132\n",
            "Iteration 111, loss = 0.32234760\n",
            "Iteration 112, loss = 0.32305954\n",
            "Iteration 113, loss = 0.31641005\n",
            "Iteration 114, loss = 0.32770627\n",
            "Iteration 115, loss = 0.32519985\n",
            "Iteration 116, loss = 0.32361613\n",
            "Iteration 117, loss = 0.32240498\n",
            "Iteration 118, loss = 0.32615345\n",
            "Iteration 119, loss = 0.31717492\n",
            "Iteration 120, loss = 0.32399306\n",
            "Iteration 121, loss = 0.32641123\n",
            "Iteration 122, loss = 0.32414234\n",
            "Iteration 123, loss = 0.32623937\n",
            "Iteration 124, loss = 0.31811178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 125, loss = 0.28025983\n",
            "Iteration 126, loss = 0.27977549\n",
            "Iteration 127, loss = 0.27961565\n",
            "Iteration 128, loss = 0.27947452\n",
            "Iteration 129, loss = 0.27938279\n",
            "Iteration 130, loss = 0.27926360\n",
            "Iteration 131, loss = 0.27915570\n",
            "Iteration 132, loss = 0.27909070\n",
            "Iteration 133, loss = 0.27898131\n",
            "Iteration 134, loss = 0.27891994\n",
            "Iteration 135, loss = 0.27887817\n",
            "Iteration 136, loss = 0.27871299\n",
            "Iteration 137, loss = 0.27864340\n",
            "Iteration 138, loss = 0.27855146\n",
            "Iteration 139, loss = 0.27848891\n",
            "Iteration 140, loss = 0.27840434\n",
            "Iteration 141, loss = 0.27833934\n",
            "Iteration 142, loss = 0.27822916\n",
            "Iteration 143, loss = 0.27811371\n",
            "Iteration 144, loss = 0.27805901\n",
            "Iteration 145, loss = 0.27798576\n",
            "Iteration 146, loss = 0.27783845\n",
            "Iteration 147, loss = 0.27781074\n",
            "Iteration 148, loss = 0.27776624\n",
            "Iteration 149, loss = 0.27763670\n",
            "Iteration 150, loss = 0.27750909\n",
            "Iteration 151, loss = 0.27742244\n",
            "Iteration 152, loss = 0.27735619\n",
            "Iteration 153, loss = 0.27731259\n",
            "Iteration 154, loss = 0.27724925\n",
            "Iteration 155, loss = 0.27714469\n",
            "Iteration 156, loss = 0.27706573\n",
            "Iteration 157, loss = 0.27700014\n",
            "Iteration 158, loss = 0.27690275\n",
            "Iteration 159, loss = 0.27689931\n",
            "Iteration 160, loss = 0.27679527\n",
            "Iteration 161, loss = 0.27667724\n",
            "Iteration 162, loss = 0.27662396\n",
            "Iteration 163, loss = 0.27652192\n",
            "Iteration 164, loss = 0.27646265\n",
            "Iteration 165, loss = 0.27639339\n",
            "Iteration 166, loss = 0.27631497\n",
            "Iteration 167, loss = 0.27627236\n",
            "Iteration 168, loss = 0.27620659\n",
            "Iteration 169, loss = 0.27606347\n",
            "Iteration 170, loss = 0.27600395\n",
            "Iteration 171, loss = 0.27599221\n",
            "Iteration 172, loss = 0.27590719\n",
            "Iteration 173, loss = 0.27580986\n",
            "Iteration 174, loss = 0.27571396\n",
            "Iteration 175, loss = 0.27569198\n",
            "Iteration 176, loss = 0.27563723\n",
            "Iteration 177, loss = 0.27555057\n",
            "Iteration 178, loss = 0.27550750\n",
            "Iteration 179, loss = 0.27546329\n",
            "Iteration 180, loss = 0.27531544\n",
            "Iteration 181, loss = 0.27528249\n",
            "Iteration 182, loss = 0.27521971\n",
            "Iteration 183, loss = 0.27513524\n",
            "Iteration 184, loss = 0.27508544\n",
            "Iteration 185, loss = 0.27501696\n",
            "Iteration 186, loss = 0.27495044\n",
            "Iteration 187, loss = 0.27494490\n",
            "Iteration 188, loss = 0.27485808\n",
            "Iteration 189, loss = 0.27471649\n",
            "Iteration 190, loss = 0.27471875\n",
            "Iteration 191, loss = 0.27464503\n",
            "Iteration 192, loss = 0.27452821\n",
            "Iteration 193, loss = 0.27448507\n",
            "Iteration 194, loss = 0.27441986\n",
            "Iteration 195, loss = 0.27434507\n",
            "Iteration 196, loss = 0.27432967\n",
            "Iteration 197, loss = 0.27426032\n",
            "Iteration 198, loss = 0.27416105\n",
            "Iteration 199, loss = 0.27417684\n",
            "Iteration 200, loss = 0.27411514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64591382\n",
            "Iteration 3, loss = 0.61680310\n",
            "Iteration 4, loss = 0.59872230\n",
            "Iteration 5, loss = 0.60853227\n",
            "Iteration 6, loss = 0.59698232\n",
            "Iteration 7, loss = 0.58407271\n",
            "Iteration 8, loss = 0.59618580\n",
            "Iteration 9, loss = 0.59111547\n",
            "Iteration 10, loss = 0.58035507\n",
            "Iteration 11, loss = 0.57605957\n",
            "Iteration 12, loss = 0.57359385\n",
            "Iteration 13, loss = 0.57243929\n",
            "Iteration 14, loss = 0.57095670\n",
            "Iteration 15, loss = 0.57357140\n",
            "Iteration 16, loss = 0.57318314\n",
            "Iteration 17, loss = 0.57440597\n",
            "Iteration 18, loss = 0.57258808\n",
            "Iteration 19, loss = 0.57268615\n",
            "Iteration 20, loss = 0.57322900\n",
            "Iteration 21, loss = 0.58368459\n",
            "Iteration 22, loss = 0.61513476\n",
            "Iteration 23, loss = 0.58856739\n",
            "Iteration 24, loss = 0.57951493\n",
            "Iteration 25, loss = 0.57379412\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.54345887\n",
            "Iteration 27, loss = 0.52812678\n",
            "Iteration 28, loss = 0.51594238\n",
            "Iteration 29, loss = 0.52967569\n",
            "Iteration 30, loss = 0.53042854\n",
            "Iteration 31, loss = 0.52419847\n",
            "Iteration 32, loss = 0.52907158\n",
            "Iteration 33, loss = 0.52498053\n",
            "Iteration 34, loss = 0.52476222\n",
            "Iteration 35, loss = 0.52299244\n",
            "Iteration 36, loss = 0.51940024\n",
            "Iteration 37, loss = 0.52439706\n",
            "Iteration 38, loss = 0.51903883\n",
            "Iteration 39, loss = 0.52190968\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 40, loss = 0.43061447\n",
            "Iteration 41, loss = 0.41778408\n",
            "Iteration 42, loss = 0.40550112\n",
            "Iteration 43, loss = 0.39392596\n",
            "Iteration 44, loss = 0.38338848\n",
            "Iteration 45, loss = 0.37348773\n",
            "Iteration 46, loss = 0.36431255\n",
            "Iteration 47, loss = 0.35572817\n",
            "Iteration 48, loss = 0.34819526\n",
            "Iteration 49, loss = 0.34117813\n",
            "Iteration 50, loss = 0.33484214\n",
            "Iteration 51, loss = 0.32902325\n",
            "Iteration 52, loss = 0.32338748\n",
            "Iteration 53, loss = 0.31824224\n",
            "Iteration 54, loss = 0.31398136\n",
            "Iteration 55, loss = 0.31010755\n",
            "Iteration 56, loss = 0.30600960\n",
            "Iteration 57, loss = 0.30207844\n",
            "Iteration 58, loss = 0.29995102\n",
            "Iteration 59, loss = 0.29746118\n",
            "Iteration 60, loss = 0.29452547\n",
            "Iteration 61, loss = 0.29162291\n",
            "Iteration 62, loss = 0.29048268\n",
            "Iteration 63, loss = 0.28829060\n",
            "Iteration 64, loss = 0.28738851\n",
            "Iteration 65, loss = 0.28642069\n",
            "Iteration 66, loss = 0.28456681\n",
            "Iteration 67, loss = 0.28191085\n",
            "Iteration 68, loss = 0.28499096\n",
            "Iteration 69, loss = 0.28010138\n",
            "Iteration 70, loss = 0.28393785\n",
            "Iteration 71, loss = 0.27839403\n",
            "Iteration 72, loss = 0.28042473\n",
            "Iteration 73, loss = 0.28360166\n",
            "Iteration 74, loss = 0.28507091\n",
            "Iteration 75, loss = 0.28985399\n",
            "Iteration 76, loss = 0.27830391\n",
            "Iteration 77, loss = 0.28474915\n",
            "Iteration 78, loss = 0.28264447\n",
            "Iteration 79, loss = 0.28071224\n",
            "Iteration 80, loss = 0.28367842\n",
            "Iteration 81, loss = 0.28541883\n",
            "Iteration 82, loss = 0.28398867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 83, loss = 0.26001595\n",
            "Iteration 84, loss = 0.25974100\n",
            "Iteration 85, loss = 0.25958894\n",
            "Iteration 86, loss = 0.25950404\n",
            "Iteration 87, loss = 0.25940597\n",
            "Iteration 88, loss = 0.25926885\n",
            "Iteration 89, loss = 0.25916516\n",
            "Iteration 90, loss = 0.25904167\n",
            "Iteration 91, loss = 0.25898177\n",
            "Iteration 92, loss = 0.25886369\n",
            "Iteration 93, loss = 0.25867031\n",
            "Iteration 94, loss = 0.25857990\n",
            "Iteration 95, loss = 0.25851527\n",
            "Iteration 96, loss = 0.25834959\n",
            "Iteration 97, loss = 0.25835771\n",
            "Iteration 98, loss = 0.25822232\n",
            "Iteration 99, loss = 0.25818309\n",
            "Iteration 100, loss = 0.25801172\n",
            "Iteration 101, loss = 0.25789734\n",
            "Iteration 102, loss = 0.25786591\n",
            "Iteration 103, loss = 0.25771286\n",
            "Iteration 104, loss = 0.25768127\n",
            "Iteration 105, loss = 0.25755637\n",
            "Iteration 106, loss = 0.25747281\n",
            "Iteration 107, loss = 0.25737865\n",
            "Iteration 108, loss = 0.25728639\n",
            "Iteration 109, loss = 0.25724032\n",
            "Iteration 110, loss = 0.25714183\n",
            "Iteration 111, loss = 0.25705955\n",
            "Iteration 112, loss = 0.25695592\n",
            "Iteration 113, loss = 0.25686755\n",
            "Iteration 114, loss = 0.25682065\n",
            "Iteration 115, loss = 0.25667717\n",
            "Iteration 116, loss = 0.25668072\n",
            "Iteration 117, loss = 0.25659199\n",
            "Iteration 118, loss = 0.25653963\n",
            "Iteration 119, loss = 0.25639388\n",
            "Iteration 120, loss = 0.25632646\n",
            "Iteration 121, loss = 0.25628741\n",
            "Iteration 122, loss = 0.25618346\n",
            "Iteration 123, loss = 0.25612701\n",
            "Iteration 124, loss = 0.25611129\n",
            "Iteration 125, loss = 0.25600101\n",
            "Iteration 126, loss = 0.25594216\n",
            "Iteration 127, loss = 0.25583443\n",
            "Iteration 128, loss = 0.25584398\n",
            "Iteration 129, loss = 0.25575174\n",
            "Iteration 130, loss = 0.25568235\n",
            "Iteration 131, loss = 0.25560249\n",
            "Iteration 132, loss = 0.25554578\n",
            "Iteration 133, loss = 0.25546392\n",
            "Iteration 134, loss = 0.25545737\n",
            "Iteration 135, loss = 0.25538174\n",
            "Iteration 136, loss = 0.25529648\n",
            "Iteration 137, loss = 0.25526632\n",
            "Iteration 138, loss = 0.25520688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 139, loss = 0.25487579\n",
            "Iteration 140, loss = 0.25484563\n",
            "Iteration 141, loss = 0.25480212\n",
            "Iteration 142, loss = 0.25479604\n",
            "Iteration 143, loss = 0.25479518\n",
            "Iteration 144, loss = 0.25478607\n",
            "Iteration 145, loss = 0.25478934\n",
            "Iteration 146, loss = 0.25475735\n",
            "Iteration 147, loss = 0.25471781\n",
            "Iteration 148, loss = 0.25473463\n",
            "Iteration 149, loss = 0.25472925\n",
            "Iteration 150, loss = 0.25471909\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 151, loss = 0.25461856\n",
            "Iteration 152, loss = 0.25461972\n",
            "Iteration 153, loss = 0.25460678\n",
            "Iteration 154, loss = 0.25460553\n",
            "Iteration 155, loss = 0.25460794\n",
            "Iteration 156, loss = 0.25460069\n",
            "Iteration 157, loss = 0.25460307\n",
            "Iteration 158, loss = 0.25460523\n",
            "Iteration 159, loss = 0.25460906\n",
            "Iteration 160, loss = 0.25460576\n",
            "Iteration 161, loss = 0.25459664\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64711022\n",
            "Iteration 3, loss = 0.61591944\n",
            "Iteration 4, loss = 0.59599244\n",
            "Iteration 5, loss = 0.58649951\n",
            "Iteration 6, loss = 0.57827223\n",
            "Iteration 7, loss = 0.57526345\n",
            "Iteration 8, loss = 0.58235053\n",
            "Iteration 9, loss = 0.60259878\n",
            "Iteration 10, loss = 0.58503762\n",
            "Iteration 11, loss = 0.57701698\n",
            "Iteration 12, loss = 0.57408249\n",
            "Iteration 13, loss = 0.59655021\n",
            "Iteration 14, loss = 0.60295683\n",
            "Iteration 15, loss = 0.58531452\n",
            "Iteration 16, loss = 0.57816694\n",
            "Iteration 17, loss = 0.57459544\n",
            "Iteration 18, loss = 0.57496573\n",
            "Iteration 19, loss = 0.57352068\n",
            "Iteration 20, loss = 0.57364804\n",
            "Iteration 21, loss = 0.57247878\n",
            "Iteration 22, loss = 0.57274676\n",
            "Iteration 23, loss = 0.57484148\n",
            "Iteration 24, loss = 0.57223233\n",
            "Iteration 25, loss = 0.57350875\n",
            "Iteration 26, loss = 0.57393482\n",
            "Iteration 27, loss = 0.57535233\n",
            "Iteration 28, loss = 0.57489961\n",
            "Iteration 29, loss = 0.57456483\n",
            "Iteration 30, loss = 0.57511023\n",
            "Iteration 31, loss = 0.57465532\n",
            "Iteration 32, loss = 0.57545320\n",
            "Iteration 33, loss = 0.57624883\n",
            "Iteration 34, loss = 0.57506000\n",
            "Iteration 35, loss = 0.57454365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.51296727\n",
            "Iteration 37, loss = 0.49493376\n",
            "Iteration 38, loss = 0.51686734\n",
            "Iteration 39, loss = 0.51843932\n",
            "Iteration 40, loss = 0.51723159\n",
            "Iteration 41, loss = 0.52164809\n",
            "Iteration 42, loss = 0.51979923\n",
            "Iteration 43, loss = 0.52236722\n",
            "Iteration 44, loss = 0.51785821\n",
            "Iteration 45, loss = 0.51690574\n",
            "Iteration 46, loss = 0.52180649\n",
            "Iteration 47, loss = 0.51732175\n",
            "Iteration 48, loss = 0.52224426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.41836619\n",
            "Iteration 50, loss = 0.40349954\n",
            "Iteration 51, loss = 0.39016102\n",
            "Iteration 52, loss = 0.37784694\n",
            "Iteration 53, loss = 0.36657264\n",
            "Iteration 54, loss = 0.35679475\n",
            "Iteration 55, loss = 0.34771436\n",
            "Iteration 56, loss = 0.33965601\n",
            "Iteration 57, loss = 0.33217871\n",
            "Iteration 58, loss = 0.32562684\n",
            "Iteration 59, loss = 0.31995610\n",
            "Iteration 60, loss = 0.31437507\n",
            "Iteration 61, loss = 0.30946063\n",
            "Iteration 62, loss = 0.30533987\n",
            "Iteration 63, loss = 0.30168173\n",
            "Iteration 64, loss = 0.29750692\n",
            "Iteration 65, loss = 0.29465146\n",
            "Iteration 66, loss = 0.29147408\n",
            "Iteration 67, loss = 0.28852750\n",
            "Iteration 68, loss = 0.28590167\n",
            "Iteration 69, loss = 0.28478913\n",
            "Iteration 70, loss = 0.28212062\n",
            "Iteration 71, loss = 0.28030098\n",
            "Iteration 72, loss = 0.27956425\n",
            "Iteration 73, loss = 0.27734992\n",
            "Iteration 74, loss = 0.27633856\n",
            "Iteration 75, loss = 0.27482008\n",
            "Iteration 76, loss = 0.27470125\n",
            "Iteration 77, loss = 0.27334259\n",
            "Iteration 78, loss = 0.27246498\n",
            "Iteration 79, loss = 0.27103667\n",
            "Iteration 80, loss = 0.27126379\n",
            "Iteration 81, loss = 0.26992265\n",
            "Iteration 82, loss = 0.26960872\n",
            "Iteration 83, loss = 0.26887627\n",
            "Iteration 84, loss = 0.26983161\n",
            "Iteration 85, loss = 0.26984459\n",
            "Iteration 86, loss = 0.27028514\n",
            "Iteration 87, loss = 0.27082153\n",
            "Iteration 88, loss = 0.26802552\n",
            "Iteration 89, loss = 0.27039329\n",
            "Iteration 90, loss = 0.27090509\n",
            "Iteration 91, loss = 0.26766775\n",
            "Iteration 92, loss = 0.26770795\n",
            "Iteration 93, loss = 0.26675365\n",
            "Iteration 94, loss = 0.27210145\n",
            "Iteration 95, loss = 0.26586831\n",
            "Iteration 96, loss = 0.28104251\n",
            "Iteration 97, loss = 0.27259911\n",
            "Iteration 98, loss = 0.26793695\n",
            "Iteration 99, loss = 0.27114664\n",
            "Iteration 100, loss = 0.26793133\n",
            "Iteration 101, loss = 0.26588177\n",
            "Iteration 102, loss = 0.27271608\n",
            "Iteration 103, loss = 0.29630310\n",
            "Iteration 104, loss = 0.27150975\n",
            "Iteration 105, loss = 0.27965952\n",
            "Iteration 106, loss = 0.27830354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 107, loss = 0.25354324\n",
            "Iteration 108, loss = 0.25344576\n",
            "Iteration 109, loss = 0.25338311\n",
            "Iteration 110, loss = 0.25334728\n",
            "Iteration 111, loss = 0.25330311\n",
            "Iteration 112, loss = 0.25327322\n",
            "Iteration 113, loss = 0.25319292\n",
            "Iteration 114, loss = 0.25320268\n",
            "Iteration 115, loss = 0.25310856\n",
            "Iteration 116, loss = 0.25302124\n",
            "Iteration 117, loss = 0.25307144\n",
            "Iteration 118, loss = 0.25294103\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 119, loss = 0.25272538\n",
            "Iteration 120, loss = 0.25267062\n",
            "Iteration 121, loss = 0.25265448\n",
            "Iteration 122, loss = 0.25265842\n",
            "Iteration 123, loss = 0.25263352\n",
            "Iteration 124, loss = 0.25262325\n",
            "Iteration 125, loss = 0.25263979\n",
            "Iteration 126, loss = 0.25263675\n",
            "Iteration 127, loss = 0.25263762\n",
            "Iteration 128, loss = 0.25261065\n",
            "Iteration 129, loss = 0.25261420\n",
            "Iteration 130, loss = 0.25258586\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.25250240\n",
            "Iteration 132, loss = 0.25250103\n",
            "Iteration 133, loss = 0.25249407\n",
            "Iteration 134, loss = 0.25250048\n",
            "Iteration 135, loss = 0.25249258\n",
            "Iteration 136, loss = 0.25249646\n",
            "Iteration 137, loss = 0.25250147\n",
            "Iteration 138, loss = 0.25249151\n",
            "Iteration 139, loss = 0.25248660\n",
            "Iteration 140, loss = 0.25248558\n",
            "Iteration 141, loss = 0.25249360\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65590486\n",
            "Iteration 3, loss = 0.62227133\n",
            "Iteration 4, loss = 0.60184204\n",
            "Iteration 5, loss = 0.59005678\n",
            "Iteration 6, loss = 0.58514120\n",
            "Iteration 7, loss = 0.57745849\n",
            "Iteration 8, loss = 0.57702483\n",
            "Iteration 9, loss = 0.57876873\n",
            "Iteration 10, loss = 0.61151634\n",
            "Iteration 11, loss = 0.58733651\n",
            "Iteration 12, loss = 0.57971773\n",
            "Iteration 13, loss = 0.57534436\n",
            "Iteration 14, loss = 0.57399598\n",
            "Iteration 15, loss = 0.57147520\n",
            "Iteration 16, loss = 0.57310824\n",
            "Iteration 17, loss = 0.57239165\n",
            "Iteration 18, loss = 0.57142536\n",
            "Iteration 19, loss = 0.57442251\n",
            "Iteration 20, loss = 0.57242226\n",
            "Iteration 21, loss = 0.57471366\n",
            "Iteration 22, loss = 0.57256776\n",
            "Iteration 23, loss = 0.57385887\n",
            "Iteration 24, loss = 0.57496750\n",
            "Iteration 25, loss = 0.57453478\n",
            "Iteration 26, loss = 0.57555593\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.51780036\n",
            "Iteration 28, loss = 0.49312668\n",
            "Iteration 29, loss = 0.51612387\n",
            "Iteration 30, loss = 0.52220903\n",
            "Iteration 31, loss = 0.51901463\n",
            "Iteration 32, loss = 0.52602507\n",
            "Iteration 33, loss = 0.51910104\n",
            "Iteration 34, loss = 0.51734275\n",
            "Iteration 35, loss = 0.52244801\n",
            "Iteration 36, loss = 0.51889715\n",
            "Iteration 37, loss = 0.52227595\n",
            "Iteration 38, loss = 0.52405896\n",
            "Iteration 39, loss = 0.51811865\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 40, loss = 0.41966393\n",
            "Iteration 41, loss = 0.40495285\n",
            "Iteration 42, loss = 0.39162063\n",
            "Iteration 43, loss = 0.37940644\n",
            "Iteration 44, loss = 0.36884001\n",
            "Iteration 45, loss = 0.35865135\n",
            "Iteration 46, loss = 0.34969954\n",
            "Iteration 47, loss = 0.34135174\n",
            "Iteration 48, loss = 0.33441888\n",
            "Iteration 49, loss = 0.32770196\n",
            "Iteration 50, loss = 0.32189701\n",
            "Iteration 51, loss = 0.31664171\n",
            "Iteration 52, loss = 0.31182282\n",
            "Iteration 53, loss = 0.30707399\n",
            "Iteration 54, loss = 0.30309065\n",
            "Iteration 55, loss = 0.29959442\n",
            "Iteration 56, loss = 0.29668822\n",
            "Iteration 57, loss = 0.29382819\n",
            "Iteration 58, loss = 0.29097628\n",
            "Iteration 59, loss = 0.28848872\n",
            "Iteration 60, loss = 0.28659231\n",
            "Iteration 61, loss = 0.28418919\n",
            "Iteration 62, loss = 0.28210795\n",
            "Iteration 63, loss = 0.28053011\n",
            "Iteration 64, loss = 0.28039086\n",
            "Iteration 65, loss = 0.27835317\n",
            "Iteration 66, loss = 0.27687041\n",
            "Iteration 67, loss = 0.27507142\n",
            "Iteration 68, loss = 0.27480617\n",
            "Iteration 69, loss = 0.27298855\n",
            "Iteration 70, loss = 0.27323678\n",
            "Iteration 71, loss = 0.27311964\n",
            "Iteration 72, loss = 0.27198054\n",
            "Iteration 73, loss = 0.27225818\n",
            "Iteration 74, loss = 0.27278593\n",
            "Iteration 75, loss = 0.27201672\n",
            "Iteration 76, loss = 0.27148918\n",
            "Iteration 77, loss = 0.26854142\n",
            "Iteration 78, loss = 0.27082525\n",
            "Iteration 79, loss = 0.27442909\n",
            "Iteration 80, loss = 0.27369790\n",
            "Iteration 81, loss = 0.26850538\n",
            "Iteration 82, loss = 0.27039885\n",
            "Iteration 83, loss = 0.27032245\n",
            "Iteration 84, loss = 0.27181326\n",
            "Iteration 85, loss = 0.27607596\n",
            "Iteration 86, loss = 0.27479320\n",
            "Iteration 87, loss = 0.27347109\n",
            "Iteration 88, loss = 0.27567726\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 89, loss = 0.25659344\n",
            "Iteration 90, loss = 0.25649063\n",
            "Iteration 91, loss = 0.25641184\n",
            "Iteration 92, loss = 0.25634562\n",
            "Iteration 93, loss = 0.25634784\n",
            "Iteration 94, loss = 0.25627500\n",
            "Iteration 95, loss = 0.25614051\n",
            "Iteration 96, loss = 0.25610675\n",
            "Iteration 97, loss = 0.25598482\n",
            "Iteration 98, loss = 0.25596362\n",
            "Iteration 99, loss = 0.25592575\n",
            "Iteration 100, loss = 0.25585567\n",
            "Iteration 101, loss = 0.25576360\n",
            "Iteration 102, loss = 0.25576281\n",
            "Iteration 103, loss = 0.25570381\n",
            "Iteration 104, loss = 0.25561734\n",
            "Iteration 105, loss = 0.25561023\n",
            "Iteration 106, loss = 0.25554938\n",
            "Iteration 107, loss = 0.25550486\n",
            "Iteration 108, loss = 0.25544743\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.25513803\n",
            "Iteration 110, loss = 0.25509418\n",
            "Iteration 111, loss = 0.25508788\n",
            "Iteration 112, loss = 0.25509184\n",
            "Iteration 113, loss = 0.25507270\n",
            "Iteration 114, loss = 0.25504643\n",
            "Iteration 115, loss = 0.25507836\n",
            "Iteration 116, loss = 0.25503953\n",
            "Iteration 117, loss = 0.25503509\n",
            "Iteration 118, loss = 0.25502467\n",
            "Iteration 119, loss = 0.25501063\n",
            "Iteration 120, loss = 0.25503002\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.25490302\n",
            "Iteration 122, loss = 0.25490308\n",
            "Iteration 123, loss = 0.25489723\n",
            "Iteration 124, loss = 0.25490220\n",
            "Iteration 125, loss = 0.25489388\n",
            "Iteration 126, loss = 0.25488378\n",
            "Iteration 127, loss = 0.25488345\n",
            "Iteration 128, loss = 0.25488362\n",
            "Iteration 129, loss = 0.25488750\n",
            "Iteration 130, loss = 0.25489062\n",
            "Iteration 131, loss = 0.25487757\n",
            "Iteration 132, loss = 0.25487709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63712271\n",
            "Iteration 3, loss = 0.61164133\n",
            "Iteration 4, loss = 0.59343919\n",
            "Iteration 5, loss = 0.58846513\n",
            "Iteration 6, loss = 0.58185214\n",
            "Iteration 7, loss = 0.57640688\n",
            "Iteration 8, loss = 0.57288919\n",
            "Iteration 9, loss = 0.57172009\n",
            "Iteration 10, loss = 0.56891744\n",
            "Iteration 11, loss = 0.56903255\n",
            "Iteration 12, loss = 0.56935641\n",
            "Iteration 13, loss = 0.56906176\n",
            "Iteration 14, loss = 0.57339846\n",
            "Iteration 15, loss = 0.57163355\n",
            "Iteration 16, loss = 0.57173865\n",
            "Iteration 17, loss = 0.57195125\n",
            "Iteration 18, loss = 0.57143546\n",
            "Iteration 19, loss = 0.57273707\n",
            "Iteration 20, loss = 0.57108113\n",
            "Iteration 21, loss = 0.57217923\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 22, loss = 0.51094717\n",
            "Iteration 23, loss = 0.48721387\n",
            "Iteration 24, loss = 0.51077099\n",
            "Iteration 25, loss = 0.51751031\n",
            "Iteration 26, loss = 0.51240527\n",
            "Iteration 27, loss = 0.51588161\n",
            "Iteration 28, loss = 0.51446494\n",
            "Iteration 29, loss = 0.50999342\n",
            "Iteration 30, loss = 0.51944290\n",
            "Iteration 31, loss = 0.51352856\n",
            "Iteration 32, loss = 0.51395799\n",
            "Iteration 33, loss = 0.51134429\n",
            "Iteration 34, loss = 0.51337352\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 35, loss = 0.41025177\n",
            "Iteration 36, loss = 0.39506000\n",
            "Iteration 37, loss = 0.38180793\n",
            "Iteration 38, loss = 0.36995662\n",
            "Iteration 39, loss = 0.35899745\n",
            "Iteration 40, loss = 0.34933735\n",
            "Iteration 41, loss = 0.34078195\n",
            "Iteration 42, loss = 0.33248941\n",
            "Iteration 43, loss = 0.32585732\n",
            "Iteration 44, loss = 0.31962296\n",
            "Iteration 45, loss = 0.31407712\n",
            "Iteration 46, loss = 0.30864704\n",
            "Iteration 47, loss = 0.30413538\n",
            "Iteration 48, loss = 0.29990364\n",
            "Iteration 49, loss = 0.29635152\n",
            "Iteration 50, loss = 0.29257517\n",
            "Iteration 51, loss = 0.28941979\n",
            "Iteration 52, loss = 0.28680274\n",
            "Iteration 53, loss = 0.28381401\n",
            "Iteration 54, loss = 0.28207219\n",
            "Iteration 55, loss = 0.28003272\n",
            "Iteration 56, loss = 0.27799411\n",
            "Iteration 57, loss = 0.27575591\n",
            "Iteration 58, loss = 0.27447172\n",
            "Iteration 59, loss = 0.27307470\n",
            "Iteration 60, loss = 0.27203621\n",
            "Iteration 61, loss = 0.27068586\n",
            "Iteration 62, loss = 0.26983363\n",
            "Iteration 63, loss = 0.26844344\n",
            "Iteration 64, loss = 0.26923152\n",
            "Iteration 65, loss = 0.26667046\n",
            "Iteration 66, loss = 0.26647475\n",
            "Iteration 67, loss = 0.26668911\n",
            "Iteration 68, loss = 0.26508230\n",
            "Iteration 69, loss = 0.26569531\n",
            "Iteration 70, loss = 0.26517332\n",
            "Iteration 71, loss = 0.26526497\n",
            "Iteration 72, loss = 0.26348950\n",
            "Iteration 73, loss = 0.26386886\n",
            "Iteration 74, loss = 0.26385651\n",
            "Iteration 75, loss = 0.26270532\n",
            "Iteration 76, loss = 0.26372143\n",
            "Iteration 77, loss = 0.26311273\n",
            "Iteration 78, loss = 0.26252755\n",
            "Iteration 79, loss = 0.26316194\n",
            "Iteration 80, loss = 0.26362115\n",
            "Iteration 81, loss = 0.26200340\n",
            "Iteration 82, loss = 0.26047936\n",
            "Iteration 83, loss = 0.26183158\n",
            "Iteration 84, loss = 0.26907594\n",
            "Iteration 85, loss = 0.26924731\n",
            "Iteration 86, loss = 0.26225419\n",
            "Iteration 87, loss = 0.27837530\n",
            "Iteration 88, loss = 0.27348520\n",
            "Iteration 89, loss = 0.26904165\n",
            "Iteration 90, loss = 0.26083412\n",
            "Iteration 91, loss = 0.27036389\n",
            "Iteration 92, loss = 0.27868260\n",
            "Iteration 93, loss = 0.26811543\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.25054422\n",
            "Iteration 95, loss = 0.25036359\n",
            "Iteration 96, loss = 0.25033262\n",
            "Iteration 97, loss = 0.25030421\n",
            "Iteration 98, loss = 0.25024873\n",
            "Iteration 99, loss = 0.25023400\n",
            "Iteration 100, loss = 0.25016055\n",
            "Iteration 101, loss = 0.25013209\n",
            "Iteration 102, loss = 0.25010384\n",
            "Iteration 103, loss = 0.25007337\n",
            "Iteration 104, loss = 0.25003713\n",
            "Iteration 105, loss = 0.24998977\n",
            "Iteration 106, loss = 0.24992554\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 107, loss = 0.24971796\n",
            "Iteration 108, loss = 0.24967255\n",
            "Iteration 109, loss = 0.24966408\n",
            "Iteration 110, loss = 0.24965578\n",
            "Iteration 111, loss = 0.24968907\n",
            "Iteration 112, loss = 0.24964211\n",
            "Iteration 113, loss = 0.24963339\n",
            "Iteration 114, loss = 0.24965929\n",
            "Iteration 115, loss = 0.24960737\n",
            "Iteration 116, loss = 0.24961983\n",
            "Iteration 117, loss = 0.24962428\n",
            "Iteration 118, loss = 0.24961206\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 119, loss = 0.24952102\n",
            "Iteration 120, loss = 0.24951612\n",
            "Iteration 121, loss = 0.24950399\n",
            "Iteration 122, loss = 0.24951613\n",
            "Iteration 123, loss = 0.24950716\n",
            "Iteration 124, loss = 0.24951030\n",
            "Iteration 125, loss = 0.24951256\n",
            "Iteration 126, loss = 0.24951434\n",
            "Iteration 127, loss = 0.24950127\n",
            "Iteration 128, loss = 0.24950764\n",
            "Iteration 129, loss = 0.24949373\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63097448\n",
            "Iteration 3, loss = 0.60613201\n",
            "Iteration 4, loss = 0.58972829\n",
            "Iteration 5, loss = 0.58129623\n",
            "Iteration 6, loss = 0.57543201\n",
            "Iteration 7, loss = 0.57321718\n",
            "Iteration 8, loss = 0.57091318\n",
            "Iteration 9, loss = 0.56946484\n",
            "Iteration 10, loss = 0.56837986\n",
            "Iteration 11, loss = 0.56919841\n",
            "Iteration 12, loss = 0.57050151\n",
            "Iteration 13, loss = 0.58294687\n",
            "Iteration 14, loss = 0.57585270\n",
            "Iteration 15, loss = 0.57236647\n",
            "Iteration 16, loss = 0.57021148\n",
            "Iteration 17, loss = 0.56694887\n",
            "Iteration 18, loss = 0.57526707\n",
            "Iteration 19, loss = 0.58325669\n",
            "Iteration 20, loss = 0.57822516\n",
            "Iteration 21, loss = 0.57731169\n",
            "Iteration 22, loss = 0.57387758\n",
            "Iteration 23, loss = 0.57193733\n",
            "Iteration 24, loss = 0.57349735\n",
            "Iteration 25, loss = 0.57702607\n",
            "Iteration 26, loss = 0.59621083\n",
            "Iteration 27, loss = 0.58023704\n",
            "Iteration 28, loss = 0.57374368\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.53558301\n",
            "Iteration 30, loss = 0.51736889\n",
            "Iteration 31, loss = 0.51119949\n",
            "Iteration 32, loss = 0.52496421\n",
            "Iteration 33, loss = 0.52389971\n",
            "Iteration 34, loss = 0.52418004\n",
            "Iteration 35, loss = 0.52037453\n",
            "Iteration 36, loss = 0.52073394\n",
            "Iteration 37, loss = 0.51906651\n",
            "Iteration 38, loss = 0.51695774\n",
            "Iteration 39, loss = 0.51549649\n",
            "Iteration 40, loss = 0.51921433\n",
            "Iteration 41, loss = 0.51738064\n",
            "Iteration 42, loss = 0.51315412\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 43, loss = 0.42351053\n",
            "Iteration 44, loss = 0.40971826\n",
            "Iteration 45, loss = 0.39722419\n",
            "Iteration 46, loss = 0.38567396\n",
            "Iteration 47, loss = 0.37518772\n",
            "Iteration 48, loss = 0.36535100\n",
            "Iteration 49, loss = 0.35631093\n",
            "Iteration 50, loss = 0.34828493\n",
            "Iteration 51, loss = 0.34043422\n",
            "Iteration 52, loss = 0.33365527\n",
            "Iteration 53, loss = 0.32726910\n",
            "Iteration 54, loss = 0.32149047\n",
            "Iteration 55, loss = 0.31623900\n",
            "Iteration 56, loss = 0.31194962\n",
            "Iteration 57, loss = 0.30725654\n",
            "Iteration 58, loss = 0.30372596\n",
            "Iteration 59, loss = 0.29936579\n",
            "Iteration 60, loss = 0.29649087\n",
            "Iteration 61, loss = 0.29422523\n",
            "Iteration 62, loss = 0.29158574\n",
            "Iteration 63, loss = 0.28841075\n",
            "Iteration 64, loss = 0.28633832\n",
            "Iteration 65, loss = 0.28477293\n",
            "Iteration 66, loss = 0.28312592\n",
            "Iteration 67, loss = 0.28061098\n",
            "Iteration 68, loss = 0.28058209\n",
            "Iteration 69, loss = 0.27824394\n",
            "Iteration 70, loss = 0.27664616\n",
            "Iteration 71, loss = 0.27513192\n",
            "Iteration 72, loss = 0.27791040\n",
            "Iteration 73, loss = 0.27534748\n",
            "Iteration 74, loss = 0.27480110\n",
            "Iteration 75, loss = 0.27235785\n",
            "Iteration 76, loss = 0.27397024\n",
            "Iteration 77, loss = 0.27447719\n",
            "Iteration 78, loss = 0.27198700\n",
            "Iteration 79, loss = 0.27220977\n",
            "Iteration 80, loss = 0.27521736\n",
            "Iteration 81, loss = 0.27239652\n",
            "Iteration 82, loss = 0.27660089\n",
            "Iteration 83, loss = 0.26910209\n",
            "Iteration 84, loss = 0.27523571\n",
            "Iteration 85, loss = 0.27046863\n",
            "Iteration 86, loss = 0.27723478\n",
            "Iteration 87, loss = 0.27549882\n",
            "Iteration 88, loss = 0.27450540\n",
            "Iteration 89, loss = 0.27168865\n",
            "Iteration 90, loss = 0.27666377\n",
            "Iteration 91, loss = 0.27550471\n",
            "Iteration 92, loss = 0.27483703\n",
            "Iteration 93, loss = 0.27092567\n",
            "Iteration 94, loss = 0.27701750\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.25384052\n",
            "Iteration 96, loss = 0.25360444\n",
            "Iteration 97, loss = 0.25357366\n",
            "Iteration 98, loss = 0.25349400\n",
            "Iteration 99, loss = 0.25340380\n",
            "Iteration 100, loss = 0.25333002\n",
            "Iteration 101, loss = 0.25328200\n",
            "Iteration 102, loss = 0.25323644\n",
            "Iteration 103, loss = 0.25318897\n",
            "Iteration 104, loss = 0.25318746\n",
            "Iteration 105, loss = 0.25302802\n",
            "Iteration 106, loss = 0.25304390\n",
            "Iteration 107, loss = 0.25291208\n",
            "Iteration 108, loss = 0.25280353\n",
            "Iteration 109, loss = 0.25283443\n",
            "Iteration 110, loss = 0.25276782\n",
            "Iteration 111, loss = 0.25274642\n",
            "Iteration 112, loss = 0.25267256\n",
            "Iteration 113, loss = 0.25259714\n",
            "Iteration 114, loss = 0.25262115\n",
            "Iteration 115, loss = 0.25249320\n",
            "Iteration 116, loss = 0.25247389\n",
            "Iteration 117, loss = 0.25241821\n",
            "Iteration 118, loss = 0.25235071\n",
            "Iteration 119, loss = 0.25232818\n",
            "Iteration 120, loss = 0.25226435\n",
            "Iteration 121, loss = 0.25220216\n",
            "Iteration 122, loss = 0.25216599\n",
            "Iteration 123, loss = 0.25209643\n",
            "Iteration 124, loss = 0.25205480\n",
            "Iteration 125, loss = 0.25202013\n",
            "Iteration 126, loss = 0.25197550\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 127, loss = 0.25168619\n",
            "Iteration 128, loss = 0.25166287\n",
            "Iteration 129, loss = 0.25165955\n",
            "Iteration 130, loss = 0.25164254\n",
            "Iteration 131, loss = 0.25162239\n",
            "Iteration 132, loss = 0.25161818\n",
            "Iteration 133, loss = 0.25161507\n",
            "Iteration 134, loss = 0.25159700\n",
            "Iteration 135, loss = 0.25159901\n",
            "Iteration 136, loss = 0.25158928\n",
            "Iteration 137, loss = 0.25157959\n",
            "Iteration 138, loss = 0.25157211\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 139, loss = 0.25147270\n",
            "Iteration 140, loss = 0.25147606\n",
            "Iteration 141, loss = 0.25147005\n",
            "Iteration 142, loss = 0.25146291\n",
            "Iteration 143, loss = 0.25146257\n",
            "Iteration 144, loss = 0.25146266\n",
            "Iteration 145, loss = 0.25146089\n",
            "Iteration 146, loss = 0.25145605\n",
            "Iteration 147, loss = 0.25146191\n",
            "Iteration 148, loss = 0.25145769\n",
            "Iteration 149, loss = 0.25145530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63299123\n",
            "Iteration 3, loss = 0.60681006\n",
            "Iteration 4, loss = 0.59150874\n",
            "Iteration 5, loss = 0.58278120\n",
            "Iteration 6, loss = 0.57732215\n",
            "Iteration 7, loss = 0.57278731\n",
            "Iteration 8, loss = 0.57083339\n",
            "Iteration 9, loss = 0.57191214\n",
            "Iteration 10, loss = 0.57693723\n",
            "Iteration 11, loss = 0.57356552\n",
            "Iteration 12, loss = 0.57119744\n",
            "Iteration 13, loss = 0.57129096\n",
            "Iteration 14, loss = 0.57115605\n",
            "Iteration 15, loss = 0.57001027\n",
            "Iteration 16, loss = 0.57038862\n",
            "Iteration 17, loss = 0.57122461\n",
            "Iteration 18, loss = 0.56886427\n",
            "Iteration 19, loss = 0.57048555\n",
            "Iteration 20, loss = 0.56903259\n",
            "Iteration 21, loss = 0.57000282\n",
            "Iteration 22, loss = 0.56992057\n",
            "Iteration 23, loss = 0.57019218\n",
            "Iteration 24, loss = 0.56916397\n",
            "Iteration 25, loss = 0.57020984\n",
            "Iteration 26, loss = 0.57004849\n",
            "Iteration 27, loss = 0.62252149\n",
            "Iteration 28, loss = 0.59561960\n",
            "Iteration 29, loss = 0.57880569\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.55201842\n",
            "Iteration 31, loss = 0.54157101\n",
            "Iteration 32, loss = 0.53059201\n",
            "Iteration 33, loss = 0.52183969\n",
            "Iteration 34, loss = 0.52729013\n",
            "Iteration 35, loss = 0.52604668\n",
            "Iteration 36, loss = 0.52611050\n",
            "Iteration 37, loss = 0.52743752\n",
            "Iteration 38, loss = 0.52089499\n",
            "Iteration 39, loss = 0.52015853\n",
            "Iteration 40, loss = 0.51705718\n",
            "Iteration 41, loss = 0.51610937\n",
            "Iteration 42, loss = 0.51722000\n",
            "Iteration 43, loss = 0.51292313\n",
            "Iteration 44, loss = 0.51433148\n",
            "Iteration 45, loss = 0.51349714\n",
            "Iteration 46, loss = 0.50961608\n",
            "Iteration 47, loss = 0.51050958\n",
            "Iteration 48, loss = 0.51492984\n",
            "Iteration 49, loss = 0.50930664\n",
            "Iteration 50, loss = 0.50929526\n",
            "Iteration 51, loss = 0.51162734\n",
            "Iteration 52, loss = 0.50917908\n",
            "Iteration 53, loss = 0.50809736\n",
            "Iteration 54, loss = 0.51401794\n",
            "Iteration 55, loss = 0.50816523\n",
            "Iteration 56, loss = 0.51092894\n",
            "Iteration 57, loss = 0.50954135\n",
            "Iteration 58, loss = 0.50904860\n",
            "Iteration 59, loss = 0.51838836\n",
            "Iteration 60, loss = 0.50605420\n",
            "Iteration 61, loss = 0.51168523\n",
            "Iteration 62, loss = 0.51146692\n",
            "Iteration 63, loss = 0.51062386\n",
            "Iteration 64, loss = 0.51425678\n",
            "Iteration 65, loss = 0.51145864\n",
            "Iteration 66, loss = 0.50680595\n",
            "Iteration 67, loss = 0.51953588\n",
            "Iteration 68, loss = 0.51390100\n",
            "Iteration 69, loss = 0.51370009\n",
            "Iteration 70, loss = 0.51094209\n",
            "Iteration 71, loss = 0.51648957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.39621541\n",
            "Iteration 73, loss = 0.38045745\n",
            "Iteration 74, loss = 0.36658322\n",
            "Iteration 75, loss = 0.35460849\n",
            "Iteration 76, loss = 0.34416649\n",
            "Iteration 77, loss = 0.33497057\n",
            "Iteration 78, loss = 0.32717744\n",
            "Iteration 79, loss = 0.31997718\n",
            "Iteration 80, loss = 0.31366665\n",
            "Iteration 81, loss = 0.30840070\n",
            "Iteration 82, loss = 0.30337895\n",
            "Iteration 83, loss = 0.29914697\n",
            "Iteration 84, loss = 0.29511895\n",
            "Iteration 85, loss = 0.29184854\n",
            "Iteration 86, loss = 0.28868424\n",
            "Iteration 87, loss = 0.28589959\n",
            "Iteration 88, loss = 0.28320059\n",
            "Iteration 89, loss = 0.28092749\n",
            "Iteration 90, loss = 0.27880434\n",
            "Iteration 91, loss = 0.27688629\n",
            "Iteration 92, loss = 0.27520868\n",
            "Iteration 93, loss = 0.27355251\n",
            "Iteration 94, loss = 0.27220488\n",
            "Iteration 95, loss = 0.27083050\n",
            "Iteration 96, loss = 0.26945929\n",
            "Iteration 97, loss = 0.26818774\n",
            "Iteration 98, loss = 0.26717071\n",
            "Iteration 99, loss = 0.26623797\n",
            "Iteration 100, loss = 0.26525268\n",
            "Iteration 101, loss = 0.26471988\n",
            "Iteration 102, loss = 0.26359650\n",
            "Iteration 103, loss = 0.26298474\n",
            "Iteration 104, loss = 0.26229525\n",
            "Iteration 105, loss = 0.26157424\n",
            "Iteration 106, loss = 0.26070425\n",
            "Iteration 107, loss = 0.26061640\n",
            "Iteration 108, loss = 0.25962521\n",
            "Iteration 109, loss = 0.25964197\n",
            "Iteration 110, loss = 0.25948055\n",
            "Iteration 111, loss = 0.25862877\n",
            "Iteration 112, loss = 0.25890881\n",
            "Iteration 113, loss = 0.25837200\n",
            "Iteration 114, loss = 0.25807308\n",
            "Iteration 115, loss = 0.25744857\n",
            "Iteration 116, loss = 0.25713382\n",
            "Iteration 117, loss = 0.25670008\n",
            "Iteration 118, loss = 0.25694877\n",
            "Iteration 119, loss = 0.25628802\n",
            "Iteration 120, loss = 0.25621931\n",
            "Iteration 121, loss = 0.25617515\n",
            "Iteration 122, loss = 0.25656960\n",
            "Iteration 123, loss = 0.25620186\n",
            "Iteration 124, loss = 0.25532590\n",
            "Iteration 125, loss = 0.25615912\n",
            "Iteration 126, loss = 0.25544743\n",
            "Iteration 127, loss = 0.25556555\n",
            "Iteration 128, loss = 0.25498229\n",
            "Iteration 129, loss = 0.25524109\n",
            "Iteration 130, loss = 0.25484676\n",
            "Iteration 131, loss = 0.25564702\n",
            "Iteration 132, loss = 0.25463551\n",
            "Iteration 133, loss = 0.25471671\n",
            "Iteration 134, loss = 0.25503563\n",
            "Iteration 135, loss = 0.25546244\n",
            "Iteration 136, loss = 0.25556863\n",
            "Iteration 137, loss = 0.25641614\n",
            "Iteration 138, loss = 0.25608081\n",
            "Iteration 139, loss = 0.25389132\n",
            "Iteration 140, loss = 0.25625779\n",
            "Iteration 141, loss = 0.25445779\n",
            "Iteration 142, loss = 0.25774080\n",
            "Iteration 143, loss = 0.26066971\n",
            "Iteration 144, loss = 0.25704063\n",
            "Iteration 145, loss = 0.25740738\n",
            "Iteration 146, loss = 0.25536383\n",
            "Iteration 147, loss = 0.25703169\n",
            "Iteration 148, loss = 0.25433548\n",
            "Iteration 149, loss = 0.25931347\n",
            "Iteration 150, loss = 0.27962764\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 151, loss = 0.24787476\n",
            "Iteration 152, loss = 0.24773245\n",
            "Iteration 153, loss = 0.24771980\n",
            "Iteration 154, loss = 0.24772071\n",
            "Iteration 155, loss = 0.24773989\n",
            "Iteration 156, loss = 0.24771500\n",
            "Iteration 157, loss = 0.24767788\n",
            "Iteration 158, loss = 0.24761777\n",
            "Iteration 159, loss = 0.24762384\n",
            "Iteration 160, loss = 0.24764215\n",
            "Iteration 161, loss = 0.24755018\n",
            "Iteration 162, loss = 0.24759158\n",
            "Iteration 163, loss = 0.24751880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 164, loss = 0.24727647\n",
            "Iteration 165, loss = 0.24726707\n",
            "Iteration 166, loss = 0.24727388\n",
            "Iteration 167, loss = 0.24724661\n",
            "Iteration 168, loss = 0.24724663\n",
            "Iteration 169, loss = 0.24724005\n",
            "Iteration 170, loss = 0.24723738\n",
            "Iteration 171, loss = 0.24726909\n",
            "Iteration 172, loss = 0.24723983\n",
            "Iteration 173, loss = 0.24724807\n",
            "Iteration 174, loss = 0.24723645\n",
            "Iteration 175, loss = 0.24722826\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 176, loss = 0.24716361\n",
            "Iteration 177, loss = 0.24714921\n",
            "Iteration 178, loss = 0.24714506\n",
            "Iteration 179, loss = 0.24716049\n",
            "Iteration 180, loss = 0.24714930\n",
            "Iteration 181, loss = 0.24714653\n",
            "Iteration 182, loss = 0.24715378\n",
            "Iteration 183, loss = 0.24713792\n",
            "Iteration 184, loss = 0.24714503\n",
            "Iteration 185, loss = 0.24714664\n",
            "Iteration 186, loss = 0.24713945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 15.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69707132\n",
            "Iteration 3, loss = 0.67935017\n",
            "Iteration 4, loss = 0.66277870\n",
            "Iteration 5, loss = 0.65074610\n",
            "Iteration 6, loss = 0.63982999\n",
            "Iteration 7, loss = 0.63329649\n",
            "Iteration 8, loss = 0.62456803\n",
            "Iteration 9, loss = 0.61955590\n",
            "Iteration 10, loss = 0.61208162\n",
            "Iteration 11, loss = 0.60780911\n",
            "Iteration 12, loss = 0.59994235\n",
            "Iteration 13, loss = 0.59861775\n",
            "Iteration 14, loss = 0.59456359\n",
            "Iteration 15, loss = 0.58874111\n",
            "Iteration 16, loss = 0.58761864\n",
            "Iteration 17, loss = 0.58399915\n",
            "Iteration 18, loss = 0.58530153\n",
            "Iteration 19, loss = 0.57927266\n",
            "Iteration 20, loss = 0.58150687\n",
            "Iteration 21, loss = 0.57786307\n",
            "Iteration 22, loss = 0.57838554\n",
            "Iteration 23, loss = 0.57848220\n",
            "Iteration 24, loss = 0.57488868\n",
            "Iteration 25, loss = 0.57217472\n",
            "Iteration 26, loss = 0.57202967\n",
            "Iteration 27, loss = 0.57337524\n",
            "Iteration 28, loss = 0.57071027\n",
            "Iteration 29, loss = 0.57030371\n",
            "Iteration 30, loss = 0.57447664\n",
            "Iteration 31, loss = 0.57089232\n",
            "Iteration 32, loss = 0.57248430\n",
            "Iteration 33, loss = 0.57190064\n",
            "Iteration 34, loss = 0.56995411\n",
            "Iteration 35, loss = 0.56967642\n",
            "Iteration 36, loss = 0.57047674\n",
            "Iteration 37, loss = 0.56938257\n",
            "Iteration 38, loss = 0.57621164\n",
            "Iteration 39, loss = 0.56831533\n",
            "Iteration 40, loss = 0.56809949\n",
            "Iteration 41, loss = 0.56799735\n",
            "Iteration 42, loss = 0.57000875\n",
            "Iteration 43, loss = 0.57244040\n",
            "Iteration 44, loss = 0.56671805\n",
            "Iteration 45, loss = 0.56960985\n",
            "Iteration 46, loss = 0.57158683\n",
            "Iteration 47, loss = 0.56641281\n",
            "Iteration 48, loss = 0.57108064\n",
            "Iteration 49, loss = 0.56869553\n",
            "Iteration 50, loss = 0.57211707\n",
            "Iteration 51, loss = 0.56996062\n",
            "Iteration 52, loss = 0.56861591\n",
            "Iteration 53, loss = 0.56798463\n",
            "Iteration 54, loss = 0.57028153\n",
            "Iteration 55, loss = 0.56810435\n",
            "Iteration 56, loss = 0.59090993\n",
            "Iteration 57, loss = 0.61712801\n",
            "Iteration 58, loss = 0.62873856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 59, loss = 0.62083651\n",
            "Iteration 60, loss = 0.61660358\n",
            "Iteration 61, loss = 0.61224824\n",
            "Iteration 62, loss = 0.60781765\n",
            "Iteration 63, loss = 0.60360039\n",
            "Iteration 64, loss = 0.60016978\n",
            "Iteration 65, loss = 0.59762909\n",
            "Iteration 66, loss = 0.59526565\n",
            "Iteration 67, loss = 0.59362751\n",
            "Iteration 68, loss = 0.59168504\n",
            "Iteration 69, loss = 0.59044516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 70, loss = 0.58713589\n",
            "Iteration 71, loss = 0.58652069\n",
            "Iteration 72, loss = 0.58607420\n",
            "Iteration 73, loss = 0.58557136\n",
            "Iteration 74, loss = 0.58583147\n",
            "Iteration 75, loss = 0.58512232\n",
            "Iteration 76, loss = 0.58494370\n",
            "Iteration 77, loss = 0.58460721\n",
            "Iteration 78, loss = 0.58409161\n",
            "Iteration 79, loss = 0.58382280\n",
            "Iteration 80, loss = 0.58350419\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 81, loss = 0.58259126\n",
            "Iteration 82, loss = 0.58262379\n",
            "Iteration 83, loss = 0.58245407\n",
            "Iteration 84, loss = 0.58235626\n",
            "Iteration 85, loss = 0.58227035\n",
            "Iteration 86, loss = 0.58217817\n",
            "Iteration 87, loss = 0.58216791\n",
            "Iteration 88, loss = 0.58205728\n",
            "Iteration 89, loss = 0.58196195\n",
            "Iteration 90, loss = 0.58187278\n",
            "Iteration 91, loss = 0.58184888\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 92, loss = 0.58169580\n",
            "Iteration 93, loss = 0.58165115\n",
            "Iteration 94, loss = 0.58162501\n",
            "Iteration 95, loss = 0.58160371\n",
            "Iteration 96, loss = 0.58156691\n",
            "Iteration 97, loss = 0.58159177\n",
            "Iteration 98, loss = 0.58155287\n",
            "Iteration 99, loss = 0.58152198\n",
            "Iteration 100, loss = 0.58153004\n",
            "Iteration 101, loss = 0.58152816\n",
            "Iteration 102, loss = 0.58149484\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 103, loss = 0.58144760\n",
            "Iteration 104, loss = 0.58142984\n",
            "Iteration 105, loss = 0.58143959\n",
            "Iteration 106, loss = 0.58142033\n",
            "Iteration 107, loss = 0.58143081\n",
            "Iteration 108, loss = 0.58141865\n",
            "Iteration 109, loss = 0.58142170\n",
            "Iteration 110, loss = 0.58142281\n",
            "Iteration 111, loss = 0.58141190\n",
            "Iteration 112, loss = 0.58140764\n",
            "Iteration 113, loss = 0.58140805\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72030127\n",
            "Iteration 3, loss = 0.70717656\n",
            "Iteration 4, loss = 0.69544518\n",
            "Iteration 5, loss = 0.68442120\n",
            "Iteration 6, loss = 0.67359938\n",
            "Iteration 7, loss = 0.66104364\n",
            "Iteration 8, loss = 0.64322298\n",
            "Iteration 9, loss = 0.63317857\n",
            "Iteration 10, loss = 0.62389981\n",
            "Iteration 11, loss = 0.61810626\n",
            "Iteration 12, loss = 0.61072457\n",
            "Iteration 13, loss = 0.60775689\n",
            "Iteration 14, loss = 0.60182208\n",
            "Iteration 15, loss = 0.60074090\n",
            "Iteration 16, loss = 0.59318373\n",
            "Iteration 17, loss = 0.59307369\n",
            "Iteration 18, loss = 0.58676987\n",
            "Iteration 19, loss = 0.58936275\n",
            "Iteration 20, loss = 0.58510292\n",
            "Iteration 21, loss = 0.58177723\n",
            "Iteration 22, loss = 0.58518027\n",
            "Iteration 23, loss = 0.57957909\n",
            "Iteration 24, loss = 0.57957777\n",
            "Iteration 25, loss = 0.57933963\n",
            "Iteration 26, loss = 0.57836242\n",
            "Iteration 27, loss = 0.57989720\n",
            "Iteration 28, loss = 0.57684587\n",
            "Iteration 29, loss = 0.57707959\n",
            "Iteration 30, loss = 0.57518108\n",
            "Iteration 31, loss = 0.57639425\n",
            "Iteration 32, loss = 0.57538969\n",
            "Iteration 33, loss = 0.57302708\n",
            "Iteration 34, loss = 0.57432838\n",
            "Iteration 35, loss = 0.57452579\n",
            "Iteration 36, loss = 0.57370727\n",
            "Iteration 37, loss = 0.57544917\n",
            "Iteration 38, loss = 0.57173394\n",
            "Iteration 39, loss = 0.57549419\n",
            "Iteration 40, loss = 0.57226374\n",
            "Iteration 41, loss = 0.57264509\n",
            "Iteration 42, loss = 0.57697831\n",
            "Iteration 43, loss = 0.57090163\n",
            "Iteration 44, loss = 0.57601950\n",
            "Iteration 45, loss = 0.57283522\n",
            "Iteration 46, loss = 0.57081670\n",
            "Iteration 47, loss = 0.57199170\n",
            "Iteration 48, loss = 0.57455604\n",
            "Iteration 49, loss = 0.57126044\n",
            "Iteration 50, loss = 0.57312131\n",
            "Iteration 51, loss = 0.57305876\n",
            "Iteration 52, loss = 0.57225220\n",
            "Iteration 53, loss = 0.57204219\n",
            "Iteration 54, loss = 0.57485125\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 55, loss = 0.53968968\n",
            "Iteration 56, loss = 0.52801193\n",
            "Iteration 57, loss = 0.52243692\n",
            "Iteration 58, loss = 0.51781430\n",
            "Iteration 59, loss = 0.51290159\n",
            "Iteration 60, loss = 0.50647589\n",
            "Iteration 61, loss = 0.50390334\n",
            "Iteration 62, loss = 0.50193576\n",
            "Iteration 63, loss = 0.50551424\n",
            "Iteration 64, loss = 0.52004614\n",
            "Iteration 65, loss = 0.52567905\n",
            "Iteration 66, loss = 0.52152152\n",
            "Iteration 67, loss = 0.52112520\n",
            "Iteration 68, loss = 0.53279405\n",
            "Iteration 69, loss = 0.53111129\n",
            "Iteration 70, loss = 0.52455103\n",
            "Iteration 71, loss = 0.53505370\n",
            "Iteration 72, loss = 0.52738212\n",
            "Iteration 73, loss = 0.52024890\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.46372050\n",
            "Iteration 75, loss = 0.46129091\n",
            "Iteration 76, loss = 0.45878089\n",
            "Iteration 77, loss = 0.45671134\n",
            "Iteration 78, loss = 0.45411614\n",
            "Iteration 79, loss = 0.45196547\n",
            "Iteration 80, loss = 0.44955221\n",
            "Iteration 81, loss = 0.44742041\n",
            "Iteration 82, loss = 0.44490709\n",
            "Iteration 83, loss = 0.44271449\n",
            "Iteration 84, loss = 0.44033140\n",
            "Iteration 85, loss = 0.43858442\n",
            "Iteration 86, loss = 0.43597920\n",
            "Iteration 87, loss = 0.43432293\n",
            "Iteration 88, loss = 0.43245908\n",
            "Iteration 89, loss = 0.42988607\n",
            "Iteration 90, loss = 0.42778531\n",
            "Iteration 91, loss = 0.42589918\n",
            "Iteration 92, loss = 0.42409574\n",
            "Iteration 93, loss = 0.42199305\n",
            "Iteration 94, loss = 0.42015640\n",
            "Iteration 95, loss = 0.41794122\n",
            "Iteration 96, loss = 0.41632007\n",
            "Iteration 97, loss = 0.41449221\n",
            "Iteration 98, loss = 0.41279671\n",
            "Iteration 99, loss = 0.41105801\n",
            "Iteration 100, loss = 0.40920604\n",
            "Iteration 101, loss = 0.40754507\n",
            "Iteration 102, loss = 0.40538633\n",
            "Iteration 103, loss = 0.40355080\n",
            "Iteration 104, loss = 0.40189891\n",
            "Iteration 105, loss = 0.40093728\n",
            "Iteration 106, loss = 0.39915930\n",
            "Iteration 107, loss = 0.39801382\n",
            "Iteration 108, loss = 0.39576174\n",
            "Iteration 109, loss = 0.39436771\n",
            "Iteration 110, loss = 0.39296300\n",
            "Iteration 111, loss = 0.39123307\n",
            "Iteration 112, loss = 0.39048176\n",
            "Iteration 113, loss = 0.38825724\n",
            "Iteration 114, loss = 0.38816588\n",
            "Iteration 115, loss = 0.38645459\n",
            "Iteration 116, loss = 0.38494868\n",
            "Iteration 117, loss = 0.38346860\n",
            "Iteration 118, loss = 0.38276224\n",
            "Iteration 119, loss = 0.38155097\n",
            "Iteration 120, loss = 0.37956173\n",
            "Iteration 121, loss = 0.37863390\n",
            "Iteration 122, loss = 0.37938659\n",
            "Iteration 123, loss = 0.37800742\n",
            "Iteration 124, loss = 0.37775976\n",
            "Iteration 125, loss = 0.37553777\n",
            "Iteration 126, loss = 0.37284958\n",
            "Iteration 127, loss = 0.37454530\n",
            "Iteration 128, loss = 0.37394149\n",
            "Iteration 129, loss = 0.37198922\n",
            "Iteration 130, loss = 0.36988257\n",
            "Iteration 131, loss = 0.36953166\n",
            "Iteration 132, loss = 0.37011448\n",
            "Iteration 133, loss = 0.37122569\n",
            "Iteration 134, loss = 0.36938994\n",
            "Iteration 135, loss = 0.36735682\n",
            "Iteration 136, loss = 0.36511136\n",
            "Iteration 137, loss = 0.36692844\n",
            "Iteration 138, loss = 0.36344619\n",
            "Iteration 139, loss = 0.36550367\n",
            "Iteration 140, loss = 0.36086068\n",
            "Iteration 141, loss = 0.36416219\n",
            "Iteration 142, loss = 0.36373223\n",
            "Iteration 143, loss = 0.36334372\n",
            "Iteration 144, loss = 0.36578805\n",
            "Iteration 145, loss = 0.36415920\n",
            "Iteration 146, loss = 0.36211241\n",
            "Iteration 147, loss = 0.36321857\n",
            "Iteration 148, loss = 0.36433140\n",
            "Iteration 149, loss = 0.36348652\n",
            "Iteration 150, loss = 0.36230342\n",
            "Iteration 151, loss = 0.36247344\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 152, loss = 0.34574934\n",
            "Iteration 153, loss = 0.34402202\n",
            "Iteration 154, loss = 0.34398644\n",
            "Iteration 155, loss = 0.34383536\n",
            "Iteration 156, loss = 0.34363659\n",
            "Iteration 157, loss = 0.34333340\n",
            "Iteration 158, loss = 0.34329634\n",
            "Iteration 159, loss = 0.34296529\n",
            "Iteration 160, loss = 0.34288456\n",
            "Iteration 161, loss = 0.34270274\n",
            "Iteration 162, loss = 0.34268736\n",
            "Iteration 163, loss = 0.34239657\n",
            "Iteration 164, loss = 0.34227722\n",
            "Iteration 165, loss = 0.34214342\n",
            "Iteration 166, loss = 0.34183986\n",
            "Iteration 167, loss = 0.34189323\n",
            "Iteration 168, loss = 0.34165043\n",
            "Iteration 169, loss = 0.34146593\n",
            "Iteration 170, loss = 0.34136774\n",
            "Iteration 171, loss = 0.34116503\n",
            "Iteration 172, loss = 0.34088926\n",
            "Iteration 173, loss = 0.34084712\n",
            "Iteration 174, loss = 0.34063739\n",
            "Iteration 175, loss = 0.34056920\n",
            "Iteration 176, loss = 0.34022354\n",
            "Iteration 177, loss = 0.34033720\n",
            "Iteration 178, loss = 0.34003652\n",
            "Iteration 179, loss = 0.33981650\n",
            "Iteration 180, loss = 0.33974676\n",
            "Iteration 181, loss = 0.33965585\n",
            "Iteration 182, loss = 0.33947433\n",
            "Iteration 183, loss = 0.33920927\n",
            "Iteration 184, loss = 0.33900024\n",
            "Iteration 185, loss = 0.33896565\n",
            "Iteration 186, loss = 0.33886047\n",
            "Iteration 187, loss = 0.33856507\n",
            "Iteration 188, loss = 0.33858708\n",
            "Iteration 189, loss = 0.33847040\n",
            "Iteration 190, loss = 0.33834602\n",
            "Iteration 191, loss = 0.33796240\n",
            "Iteration 192, loss = 0.33798984\n",
            "Iteration 193, loss = 0.33778739\n",
            "Iteration 194, loss = 0.33758147\n",
            "Iteration 195, loss = 0.33752642\n",
            "Iteration 196, loss = 0.33740401\n",
            "Iteration 197, loss = 0.33724635\n",
            "Iteration 198, loss = 0.33729072\n",
            "Iteration 199, loss = 0.33697762\n",
            "Iteration 200, loss = 0.33679373\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.72653357\n",
            "Iteration 3, loss = 0.71145228\n",
            "Iteration 4, loss = 0.69487902\n",
            "Iteration 5, loss = 0.67387624\n",
            "Iteration 6, loss = 0.68571030\n",
            "Iteration 7, loss = 0.66738195\n",
            "Iteration 8, loss = 0.65034494\n",
            "Iteration 9, loss = 0.64122624\n",
            "Iteration 10, loss = 0.63001432\n",
            "Iteration 11, loss = 0.62705917\n",
            "Iteration 12, loss = 0.62004786\n",
            "Iteration 13, loss = 0.61327684\n",
            "Iteration 14, loss = 0.60901560\n",
            "Iteration 15, loss = 0.60559508\n",
            "Iteration 16, loss = 0.59968969\n",
            "Iteration 17, loss = 0.59671768\n",
            "Iteration 18, loss = 0.59186957\n",
            "Iteration 19, loss = 0.59350945\n",
            "Iteration 20, loss = 0.58999488\n",
            "Iteration 21, loss = 0.59013758\n",
            "Iteration 22, loss = 0.58657536\n",
            "Iteration 23, loss = 0.58267683\n",
            "Iteration 24, loss = 0.58232779\n",
            "Iteration 25, loss = 0.58095543\n",
            "Iteration 26, loss = 0.57748981\n",
            "Iteration 27, loss = 0.58068476\n",
            "Iteration 28, loss = 0.57857211\n",
            "Iteration 29, loss = 0.57917251\n",
            "Iteration 30, loss = 0.57694098\n",
            "Iteration 31, loss = 0.57613600\n",
            "Iteration 32, loss = 0.57818708\n",
            "Iteration 33, loss = 0.57628628\n",
            "Iteration 34, loss = 0.57465366\n",
            "Iteration 35, loss = 0.57403665\n",
            "Iteration 36, loss = 0.57763424\n",
            "Iteration 37, loss = 0.57601042\n",
            "Iteration 38, loss = 0.57281960\n",
            "Iteration 39, loss = 0.57319808\n",
            "Iteration 40, loss = 0.57495056\n",
            "Iteration 41, loss = 0.57411546\n",
            "Iteration 42, loss = 0.57332549\n",
            "Iteration 43, loss = 0.57546036\n",
            "Iteration 44, loss = 0.57424912\n",
            "Iteration 45, loss = 0.57542817\n",
            "Iteration 46, loss = 0.57310840\n",
            "Iteration 47, loss = 0.57058071\n",
            "Iteration 48, loss = 0.57779215\n",
            "Iteration 49, loss = 0.57153371\n",
            "Iteration 50, loss = 0.57438464\n",
            "Iteration 51, loss = 0.57356846\n",
            "Iteration 52, loss = 0.57211601\n",
            "Iteration 53, loss = 0.57450863\n",
            "Iteration 54, loss = 0.56855827\n",
            "Iteration 55, loss = 0.57375323\n",
            "Iteration 56, loss = 0.57774843\n",
            "Iteration 57, loss = 0.57297532\n",
            "Iteration 58, loss = 0.57400046\n",
            "Iteration 59, loss = 0.57378545\n",
            "Iteration 60, loss = 0.57344501\n",
            "Iteration 61, loss = 0.57324890\n",
            "Iteration 62, loss = 0.57258871\n",
            "Iteration 63, loss = 0.57713384\n",
            "Iteration 64, loss = 0.57532252\n",
            "Iteration 65, loss = 0.57363496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 66, loss = 0.53488069\n",
            "Iteration 67, loss = 0.52676574\n",
            "Iteration 68, loss = 0.52041223\n",
            "Iteration 69, loss = 0.51533812\n",
            "Iteration 70, loss = 0.51185242\n",
            "Iteration 71, loss = 0.50279656\n",
            "Iteration 72, loss = 0.50705807\n",
            "Iteration 73, loss = 0.50582172\n",
            "Iteration 74, loss = 0.51134547\n",
            "Iteration 75, loss = 0.52417004\n",
            "Iteration 76, loss = 0.51606208\n",
            "Iteration 77, loss = 0.53019300\n",
            "Iteration 78, loss = 0.52579480\n",
            "Iteration 79, loss = 0.52535788\n",
            "Iteration 80, loss = 0.53031100\n",
            "Iteration 81, loss = 0.51981319\n",
            "Iteration 82, loss = 0.52987859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 83, loss = 0.47086241\n",
            "Iteration 84, loss = 0.46182799\n",
            "Iteration 85, loss = 0.46003328\n",
            "Iteration 86, loss = 0.45741500\n",
            "Iteration 87, loss = 0.45507339\n",
            "Iteration 88, loss = 0.45232172\n",
            "Iteration 89, loss = 0.45015259\n",
            "Iteration 90, loss = 0.44789187\n",
            "Iteration 91, loss = 0.44586076\n",
            "Iteration 92, loss = 0.44331688\n",
            "Iteration 93, loss = 0.44159278\n",
            "Iteration 94, loss = 0.43899454\n",
            "Iteration 95, loss = 0.43739325\n",
            "Iteration 96, loss = 0.43485029\n",
            "Iteration 97, loss = 0.43305679\n",
            "Iteration 98, loss = 0.43079682\n",
            "Iteration 99, loss = 0.42867233\n",
            "Iteration 100, loss = 0.42685405\n",
            "Iteration 101, loss = 0.42438307\n",
            "Iteration 102, loss = 0.42256368\n",
            "Iteration 103, loss = 0.42085734\n",
            "Iteration 104, loss = 0.41872578\n",
            "Iteration 105, loss = 0.41653978\n",
            "Iteration 106, loss = 0.41573522\n",
            "Iteration 107, loss = 0.41361088\n",
            "Iteration 108, loss = 0.41155492\n",
            "Iteration 109, loss = 0.40938477\n",
            "Iteration 110, loss = 0.40816430\n",
            "Iteration 111, loss = 0.40649582\n",
            "Iteration 112, loss = 0.40450055\n",
            "Iteration 113, loss = 0.40229853\n",
            "Iteration 114, loss = 0.40089610\n",
            "Iteration 115, loss = 0.39967581\n",
            "Iteration 116, loss = 0.39815401\n",
            "Iteration 117, loss = 0.39708537\n",
            "Iteration 118, loss = 0.39616410\n",
            "Iteration 119, loss = 0.39387499\n",
            "Iteration 120, loss = 0.39210322\n",
            "Iteration 121, loss = 0.39090678\n",
            "Iteration 122, loss = 0.38911028\n",
            "Iteration 123, loss = 0.38779761\n",
            "Iteration 124, loss = 0.38737222\n",
            "Iteration 125, loss = 0.38484808\n",
            "Iteration 126, loss = 0.38512225\n",
            "Iteration 127, loss = 0.38321056\n",
            "Iteration 128, loss = 0.38251648\n",
            "Iteration 129, loss = 0.38072278\n",
            "Iteration 130, loss = 0.37943477\n",
            "Iteration 131, loss = 0.37844970\n",
            "Iteration 132, loss = 0.37711506\n",
            "Iteration 133, loss = 0.37568473\n",
            "Iteration 134, loss = 0.37763741\n",
            "Iteration 135, loss = 0.37598773\n",
            "Iteration 136, loss = 0.37229454\n",
            "Iteration 137, loss = 0.37258499\n",
            "Iteration 138, loss = 0.37269187\n",
            "Iteration 139, loss = 0.37276435\n",
            "Iteration 140, loss = 0.36886503\n",
            "Iteration 141, loss = 0.36862875\n",
            "Iteration 142, loss = 0.37219846\n",
            "Iteration 143, loss = 0.36823836\n",
            "Iteration 144, loss = 0.36977523\n",
            "Iteration 145, loss = 0.36630590\n",
            "Iteration 146, loss = 0.36509678\n",
            "Iteration 147, loss = 0.36623087\n",
            "Iteration 148, loss = 0.36237294\n",
            "Iteration 149, loss = 0.36489738\n",
            "Iteration 150, loss = 0.36349765\n",
            "Iteration 151, loss = 0.36505957\n",
            "Iteration 152, loss = 0.36688572\n",
            "Iteration 153, loss = 0.36589403\n",
            "Iteration 154, loss = 0.36207868\n",
            "Iteration 155, loss = 0.36410499\n",
            "Iteration 156, loss = 0.36528587\n",
            "Iteration 157, loss = 0.36746303\n",
            "Iteration 158, loss = 0.36096590\n",
            "Iteration 159, loss = 0.36443008\n",
            "Iteration 160, loss = 0.36445579\n",
            "Iteration 161, loss = 0.36078780\n",
            "Iteration 162, loss = 0.36504813\n",
            "Iteration 163, loss = 0.36305213\n",
            "Iteration 164, loss = 0.36754499\n",
            "Iteration 165, loss = 0.35885286\n",
            "Iteration 166, loss = 0.36117040\n",
            "Iteration 167, loss = 0.35965774\n",
            "Iteration 168, loss = 0.36613200\n",
            "Iteration 169, loss = 0.36615006\n",
            "Iteration 170, loss = 0.36073719\n",
            "Iteration 171, loss = 0.35880386\n",
            "Iteration 172, loss = 0.35535336\n",
            "Iteration 173, loss = 0.36261923\n",
            "Iteration 174, loss = 0.36178866\n",
            "Iteration 175, loss = 0.35971826\n",
            "Iteration 176, loss = 0.35962619\n",
            "Iteration 177, loss = 0.36051204\n",
            "Iteration 178, loss = 0.35369040\n",
            "Iteration 179, loss = 0.36782256\n",
            "Iteration 180, loss = 0.36154994\n",
            "Iteration 181, loss = 0.36286657\n",
            "Iteration 182, loss = 0.36320003\n",
            "Iteration 183, loss = 0.35865339\n",
            "Iteration 184, loss = 0.36730926\n",
            "Iteration 185, loss = 0.35905778\n",
            "Iteration 186, loss = 0.36205871\n",
            "Iteration 187, loss = 0.36148579\n",
            "Iteration 188, loss = 0.35845170\n",
            "Iteration 189, loss = 0.35194364\n",
            "Iteration 190, loss = 0.35326666\n",
            "Iteration 191, loss = 0.35975793\n",
            "Iteration 192, loss = 0.35570895\n",
            "Iteration 193, loss = 0.35612852\n",
            "Iteration 194, loss = 0.36519680\n",
            "Iteration 195, loss = 0.35502087\n",
            "Iteration 196, loss = 0.35393379\n",
            "Iteration 197, loss = 0.36051126\n",
            "Iteration 198, loss = 0.36257384\n",
            "Iteration 199, loss = 0.35868288\n",
            "Iteration 200, loss = 0.35842522\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.72170779\n",
            "Iteration 3, loss = 0.70727510\n",
            "Iteration 4, loss = 0.69205931\n",
            "Iteration 5, loss = 0.67330306\n",
            "Iteration 6, loss = 0.65876470\n",
            "Iteration 7, loss = 0.64703396\n",
            "Iteration 8, loss = 0.63841861\n",
            "Iteration 9, loss = 0.63153962\n",
            "Iteration 10, loss = 0.62210940\n",
            "Iteration 11, loss = 0.61629397\n",
            "Iteration 12, loss = 0.61175164\n",
            "Iteration 13, loss = 0.60738288\n",
            "Iteration 14, loss = 0.60185283\n",
            "Iteration 15, loss = 0.59822909\n",
            "Iteration 16, loss = 0.59612564\n",
            "Iteration 17, loss = 0.59360565\n",
            "Iteration 18, loss = 0.59082331\n",
            "Iteration 19, loss = 0.58929224\n",
            "Iteration 20, loss = 0.58376258\n",
            "Iteration 21, loss = 0.58482347\n",
            "Iteration 22, loss = 0.59130084\n",
            "Iteration 23, loss = 0.58895035\n",
            "Iteration 24, loss = 0.58704210\n",
            "Iteration 25, loss = 0.58635249\n",
            "Iteration 26, loss = 0.59071501\n",
            "Iteration 27, loss = 0.58080486\n",
            "Iteration 28, loss = 0.58344059\n",
            "Iteration 29, loss = 0.58376922\n",
            "Iteration 30, loss = 0.57945920\n",
            "Iteration 31, loss = 0.58109977\n",
            "Iteration 32, loss = 0.58121341\n",
            "Iteration 33, loss = 0.58209728\n",
            "Iteration 34, loss = 0.58040373\n",
            "Iteration 35, loss = 0.58086014\n",
            "Iteration 36, loss = 0.57816596\n",
            "Iteration 37, loss = 0.58002940\n",
            "Iteration 38, loss = 0.57688905\n",
            "Iteration 39, loss = 0.57935721\n",
            "Iteration 40, loss = 0.57662281\n",
            "Iteration 41, loss = 0.57921788\n",
            "Iteration 42, loss = 0.74520281\n",
            "Iteration 43, loss = 0.68110165\n",
            "Iteration 44, loss = 0.65815325\n",
            "Iteration 45, loss = 0.64895973\n",
            "Iteration 46, loss = 0.64461560\n",
            "Iteration 47, loss = 0.64210937\n",
            "Iteration 48, loss = 0.64034752\n",
            "Iteration 49, loss = 0.63894368\n",
            "Iteration 50, loss = 0.63776029\n",
            "Iteration 51, loss = 0.63672124\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.63611833\n",
            "Iteration 53, loss = 0.63593735\n",
            "Iteration 54, loss = 0.63576264\n",
            "Iteration 55, loss = 0.63559233\n",
            "Iteration 56, loss = 0.63542575\n",
            "Iteration 57, loss = 0.63526227\n",
            "Iteration 58, loss = 0.63510281\n",
            "Iteration 59, loss = 0.63494694\n",
            "Iteration 60, loss = 0.63479424\n",
            "Iteration 61, loss = 0.63464485\n",
            "Iteration 62, loss = 0.63449906\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 63, loss = 0.63440733\n",
            "Iteration 64, loss = 0.63437863\n",
            "Iteration 65, loss = 0.63435032\n",
            "Iteration 66, loss = 0.63432222\n",
            "Iteration 67, loss = 0.63429422\n",
            "Iteration 68, loss = 0.63426639\n",
            "Iteration 69, loss = 0.63423860\n",
            "Iteration 70, loss = 0.63421087\n",
            "Iteration 71, loss = 0.63418345\n",
            "Iteration 72, loss = 0.63415614\n",
            "Iteration 73, loss = 0.63412875\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 74, loss = 0.63411131\n",
            "Iteration 75, loss = 0.63410583\n",
            "Iteration 76, loss = 0.63410042\n",
            "Iteration 77, loss = 0.63409502\n",
            "Iteration 78, loss = 0.63408958\n",
            "Iteration 79, loss = 0.63408418\n",
            "Iteration 80, loss = 0.63407877\n",
            "Iteration 81, loss = 0.63407340\n",
            "Iteration 82, loss = 0.63406800\n",
            "Iteration 83, loss = 0.63406262\n",
            "Iteration 84, loss = 0.63405722\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 85, loss = 0.63405377\n",
            "Iteration 86, loss = 0.63405269\n",
            "Iteration 87, loss = 0.63405162\n",
            "Iteration 88, loss = 0.63405054\n",
            "Iteration 89, loss = 0.63404947\n",
            "Iteration 90, loss = 0.63404839\n",
            "Iteration 91, loss = 0.63404731\n",
            "Iteration 92, loss = 0.63404624\n",
            "Iteration 93, loss = 0.63404517\n",
            "Iteration 94, loss = 0.63404409\n",
            "Iteration 95, loss = 0.63404302\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 96, loss = 0.63404233\n",
            "Iteration 97, loss = 0.63404211\n",
            "Iteration 98, loss = 0.63404190\n",
            "Iteration 99, loss = 0.63404168\n",
            "Iteration 100, loss = 0.63404147\n",
            "Iteration 101, loss = 0.63404126\n",
            "Iteration 102, loss = 0.63404104\n",
            "Iteration 103, loss = 0.63404083\n",
            "Iteration 104, loss = 0.63404061\n",
            "Iteration 105, loss = 0.63404040\n",
            "Iteration 106, loss = 0.63404018\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69614159\n",
            "Iteration 3, loss = 0.67901288\n",
            "Iteration 4, loss = 0.66921194\n",
            "Iteration 5, loss = 0.65455726\n",
            "Iteration 6, loss = 0.64365377\n",
            "Iteration 7, loss = 0.63674238\n",
            "Iteration 8, loss = 0.62602784\n",
            "Iteration 9, loss = 0.62055599\n",
            "Iteration 10, loss = 0.61416811\n",
            "Iteration 11, loss = 0.60987833\n",
            "Iteration 12, loss = 0.60363788\n",
            "Iteration 13, loss = 0.59908406\n",
            "Iteration 14, loss = 0.59835829\n",
            "Iteration 15, loss = 0.59187277\n",
            "Iteration 16, loss = 0.59117297\n",
            "Iteration 17, loss = 0.58847139\n",
            "Iteration 18, loss = 0.58887152\n",
            "Iteration 19, loss = 0.58165826\n",
            "Iteration 20, loss = 0.58478560\n",
            "Iteration 21, loss = 0.58047711\n",
            "Iteration 22, loss = 0.58066086\n",
            "Iteration 23, loss = 0.57572160\n",
            "Iteration 24, loss = 0.57918495\n",
            "Iteration 25, loss = 0.57377823\n",
            "Iteration 26, loss = 0.58150889\n",
            "Iteration 27, loss = 0.57529930\n",
            "Iteration 28, loss = 0.57266853\n",
            "Iteration 29, loss = 0.57488505\n",
            "Iteration 30, loss = 0.57317267\n",
            "Iteration 31, loss = 0.57187627\n",
            "Iteration 32, loss = 0.57494856\n",
            "Iteration 33, loss = 0.57088037\n",
            "Iteration 34, loss = 0.57275013\n",
            "Iteration 35, loss = 0.57253290\n",
            "Iteration 36, loss = 0.57108231\n",
            "Iteration 37, loss = 0.57051600\n",
            "Iteration 38, loss = 0.56759562\n",
            "Iteration 39, loss = 0.56917110\n",
            "Iteration 40, loss = 0.57205370\n",
            "Iteration 41, loss = 0.57255100\n",
            "Iteration 42, loss = 0.57011328\n",
            "Iteration 43, loss = 0.56797734\n",
            "Iteration 44, loss = 0.57300771\n",
            "Iteration 45, loss = 0.56824127\n",
            "Iteration 46, loss = 0.56939941\n",
            "Iteration 47, loss = 0.56993451\n",
            "Iteration 48, loss = 0.57154138\n",
            "Iteration 49, loss = 0.56761874\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 50, loss = 0.52735428\n",
            "Iteration 51, loss = 0.51544338\n",
            "Iteration 52, loss = 0.50791982\n",
            "Iteration 53, loss = 0.50274921\n",
            "Iteration 54, loss = 0.49474288\n",
            "Iteration 55, loss = 0.48953787\n",
            "Iteration 56, loss = 0.48403224\n",
            "Iteration 57, loss = 0.48964316\n",
            "Iteration 58, loss = 0.49746881\n",
            "Iteration 59, loss = 0.49895672\n",
            "Iteration 60, loss = 0.51005619\n",
            "Iteration 61, loss = 0.52264834\n",
            "Iteration 62, loss = 0.51533527\n",
            "Iteration 63, loss = 0.51178715\n",
            "Iteration 64, loss = 0.51813276\n",
            "Iteration 65, loss = 0.52891552\n",
            "Iteration 66, loss = 0.50915166\n",
            "Iteration 67, loss = 0.51621854\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.47117744\n",
            "Iteration 69, loss = 0.44282603\n",
            "Iteration 70, loss = 0.43571224\n",
            "Iteration 71, loss = 0.43218155\n",
            "Iteration 72, loss = 0.42936551\n",
            "Iteration 73, loss = 0.42624881\n",
            "Iteration 74, loss = 0.42351250\n",
            "Iteration 75, loss = 0.42093835\n",
            "Iteration 76, loss = 0.41848914\n",
            "Iteration 77, loss = 0.41609704\n",
            "Iteration 78, loss = 0.41315594\n",
            "Iteration 79, loss = 0.41046695\n",
            "Iteration 80, loss = 0.40835091\n",
            "Iteration 81, loss = 0.40549066\n",
            "Iteration 82, loss = 0.40363996\n",
            "Iteration 83, loss = 0.40077343\n",
            "Iteration 84, loss = 0.39830196\n",
            "Iteration 85, loss = 0.39636786\n",
            "Iteration 86, loss = 0.39353270\n",
            "Iteration 87, loss = 0.39136643\n",
            "Iteration 88, loss = 0.38933233\n",
            "Iteration 89, loss = 0.38679661\n",
            "Iteration 90, loss = 0.38430233\n",
            "Iteration 91, loss = 0.38227519\n",
            "Iteration 92, loss = 0.38005745\n",
            "Iteration 93, loss = 0.37837442\n",
            "Iteration 94, loss = 0.37578430\n",
            "Iteration 95, loss = 0.37395993\n",
            "Iteration 96, loss = 0.37212903\n",
            "Iteration 97, loss = 0.37005628\n",
            "Iteration 98, loss = 0.36805640\n",
            "Iteration 99, loss = 0.36699957\n",
            "Iteration 100, loss = 0.36414147\n",
            "Iteration 101, loss = 0.36214294\n",
            "Iteration 102, loss = 0.36119794\n",
            "Iteration 103, loss = 0.35860262\n",
            "Iteration 104, loss = 0.35701598\n",
            "Iteration 105, loss = 0.35506389\n",
            "Iteration 106, loss = 0.35389837\n",
            "Iteration 107, loss = 0.35172533\n",
            "Iteration 108, loss = 0.34956185\n",
            "Iteration 109, loss = 0.34861417\n",
            "Iteration 110, loss = 0.34672479\n",
            "Iteration 111, loss = 0.34520041\n",
            "Iteration 112, loss = 0.34318471\n",
            "Iteration 113, loss = 0.34288097\n",
            "Iteration 114, loss = 0.34037726\n",
            "Iteration 115, loss = 0.33955051\n",
            "Iteration 116, loss = 0.33767376\n",
            "Iteration 117, loss = 0.33600394\n",
            "Iteration 118, loss = 0.33468474\n",
            "Iteration 119, loss = 0.33271731\n",
            "Iteration 120, loss = 0.33216301\n",
            "Iteration 121, loss = 0.33112429\n",
            "Iteration 122, loss = 0.32943321\n",
            "Iteration 123, loss = 0.32829665\n",
            "Iteration 124, loss = 0.32762356\n",
            "Iteration 125, loss = 0.32561882\n",
            "Iteration 126, loss = 0.32509788\n",
            "Iteration 127, loss = 0.32428961\n",
            "Iteration 128, loss = 0.32322419\n",
            "Iteration 129, loss = 0.32185899\n",
            "Iteration 130, loss = 0.32014337\n",
            "Iteration 131, loss = 0.31821185\n",
            "Iteration 132, loss = 0.31788774\n",
            "Iteration 133, loss = 0.31757821\n",
            "Iteration 134, loss = 0.31630482\n",
            "Iteration 135, loss = 0.31397536\n",
            "Iteration 136, loss = 0.31364790\n",
            "Iteration 137, loss = 0.31372217\n",
            "Iteration 138, loss = 0.31103281\n",
            "Iteration 139, loss = 0.31326086\n",
            "Iteration 140, loss = 0.30959115\n",
            "Iteration 141, loss = 0.30970803\n",
            "Iteration 142, loss = 0.30836999\n",
            "Iteration 143, loss = 0.30781143\n",
            "Iteration 144, loss = 0.30646795\n",
            "Iteration 145, loss = 0.30728909\n",
            "Iteration 146, loss = 0.30606424\n",
            "Iteration 147, loss = 0.30464480\n",
            "Iteration 148, loss = 0.30371488\n",
            "Iteration 149, loss = 0.30475206\n",
            "Iteration 150, loss = 0.30115027\n",
            "Iteration 151, loss = 0.30195724\n",
            "Iteration 152, loss = 0.30085101\n",
            "Iteration 153, loss = 0.30231202\n",
            "Iteration 154, loss = 0.29846004\n",
            "Iteration 155, loss = 0.30092832\n",
            "Iteration 156, loss = 0.29872774\n",
            "Iteration 157, loss = 0.30008426\n",
            "Iteration 158, loss = 0.29737251\n",
            "Iteration 159, loss = 0.29609084\n",
            "Iteration 160, loss = 0.29764551\n",
            "Iteration 161, loss = 0.29683758\n",
            "Iteration 162, loss = 0.29818092\n",
            "Iteration 163, loss = 0.29340034\n",
            "Iteration 164, loss = 0.29331576\n",
            "Iteration 165, loss = 0.29519674\n",
            "Iteration 166, loss = 0.29396278\n",
            "Iteration 167, loss = 0.29388880\n",
            "Iteration 168, loss = 0.29051330\n",
            "Iteration 169, loss = 0.29201803\n",
            "Iteration 170, loss = 0.28880685\n",
            "Iteration 171, loss = 0.29029768\n",
            "Iteration 172, loss = 0.29390257\n",
            "Iteration 173, loss = 0.29128516\n",
            "Iteration 174, loss = 0.29058873\n",
            "Iteration 175, loss = 0.28970672\n",
            "Iteration 176, loss = 0.28809139\n",
            "Iteration 177, loss = 0.28937788\n",
            "Iteration 178, loss = 0.28969152\n",
            "Iteration 179, loss = 0.29297478\n",
            "Iteration 180, loss = 0.29359107\n",
            "Iteration 181, loss = 0.28631914\n",
            "Iteration 182, loss = 0.28647290\n",
            "Iteration 183, loss = 0.29006458\n",
            "Iteration 184, loss = 0.29271573\n",
            "Iteration 185, loss = 0.28727855\n",
            "Iteration 186, loss = 0.28606840\n",
            "Iteration 187, loss = 0.28454640\n",
            "Iteration 188, loss = 0.28562573\n",
            "Iteration 189, loss = 0.28403227\n",
            "Iteration 190, loss = 0.28496940\n",
            "Iteration 191, loss = 0.28509955\n",
            "Iteration 192, loss = 0.28874681\n",
            "Iteration 193, loss = 0.28226966\n",
            "Iteration 194, loss = 0.28943964\n",
            "Iteration 195, loss = 0.28654041\n",
            "Iteration 196, loss = 0.28758597\n",
            "Iteration 197, loss = 0.28343478\n",
            "Iteration 198, loss = 0.28641470\n",
            "Iteration 199, loss = 0.29655710\n",
            "Iteration 200, loss = 0.28119153\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.72877736\n",
            "Iteration 3, loss = 0.70960867\n",
            "Iteration 4, loss = 0.68514613\n",
            "Iteration 5, loss = 0.66863919\n",
            "Iteration 6, loss = 0.65679228\n",
            "Iteration 7, loss = 0.64803448\n",
            "Iteration 8, loss = 0.63746458\n",
            "Iteration 9, loss = 0.62858798\n",
            "Iteration 10, loss = 0.62446962\n",
            "Iteration 11, loss = 0.61559235\n",
            "Iteration 12, loss = 0.61146890\n",
            "Iteration 13, loss = 0.60401482\n",
            "Iteration 14, loss = 0.60461746\n",
            "Iteration 15, loss = 0.59875935\n",
            "Iteration 16, loss = 0.59746732\n",
            "Iteration 17, loss = 0.59429606\n",
            "Iteration 18, loss = 0.59017575\n",
            "Iteration 19, loss = 0.58809026\n",
            "Iteration 20, loss = 0.58524579\n",
            "Iteration 21, loss = 0.59109227\n",
            "Iteration 22, loss = 0.58931916\n",
            "Iteration 23, loss = 0.58646743\n",
            "Iteration 24, loss = 0.58017841\n",
            "Iteration 25, loss = 0.58001025\n",
            "Iteration 26, loss = 0.58177251\n",
            "Iteration 27, loss = 0.57819345\n",
            "Iteration 28, loss = 0.57980370\n",
            "Iteration 29, loss = 0.57779483\n",
            "Iteration 30, loss = 0.57780236\n",
            "Iteration 31, loss = 0.57579281\n",
            "Iteration 32, loss = 0.57258719\n",
            "Iteration 33, loss = 0.57853865\n",
            "Iteration 34, loss = 0.57637079\n",
            "Iteration 35, loss = 0.57857957\n",
            "Iteration 36, loss = 0.57586458\n",
            "Iteration 37, loss = 0.57410389\n",
            "Iteration 38, loss = 0.57344949\n",
            "Iteration 39, loss = 0.57551679\n",
            "Iteration 40, loss = 0.57206586\n",
            "Iteration 41, loss = 0.57671295\n",
            "Iteration 42, loss = 0.57449361\n",
            "Iteration 43, loss = 0.57062557\n",
            "Iteration 44, loss = 0.57083458\n",
            "Iteration 45, loss = 0.57799574\n",
            "Iteration 46, loss = 0.57469259\n",
            "Iteration 47, loss = 0.57267285\n",
            "Iteration 48, loss = 0.57283299\n",
            "Iteration 49, loss = 0.57329126\n",
            "Iteration 50, loss = 0.57086105\n",
            "Iteration 51, loss = 0.57770380\n",
            "Iteration 52, loss = 0.56975821\n",
            "Iteration 53, loss = 0.57291310\n",
            "Iteration 54, loss = 0.57284238\n",
            "Iteration 55, loss = 0.57194672\n",
            "Iteration 56, loss = 0.57317008\n",
            "Iteration 57, loss = 0.57579428\n",
            "Iteration 58, loss = 0.57334852\n",
            "Iteration 59, loss = 0.57290171\n",
            "Iteration 60, loss = 0.57408472\n",
            "Iteration 61, loss = 0.57384788\n",
            "Iteration 62, loss = 0.57413018\n",
            "Iteration 63, loss = 0.66095738\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 64, loss = 0.63051016\n",
            "Iteration 65, loss = 0.62664063\n",
            "Iteration 66, loss = 0.62166032\n",
            "Iteration 67, loss = 0.61652871\n",
            "Iteration 68, loss = 0.61064382\n",
            "Iteration 69, loss = 0.60547031\n",
            "Iteration 70, loss = 0.60135537\n",
            "Iteration 71, loss = 0.59765406\n",
            "Iteration 72, loss = 0.59429344\n",
            "Iteration 73, loss = 0.59178414\n",
            "Iteration 74, loss = 0.58966770\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 75, loss = 0.58702989\n",
            "Iteration 76, loss = 0.58589194\n",
            "Iteration 77, loss = 0.58564290\n",
            "Iteration 78, loss = 0.58492854\n",
            "Iteration 79, loss = 0.58480692\n",
            "Iteration 80, loss = 0.58427785\n",
            "Iteration 81, loss = 0.58417698\n",
            "Iteration 82, loss = 0.58336174\n",
            "Iteration 83, loss = 0.58303126\n",
            "Iteration 84, loss = 0.58252597\n",
            "Iteration 85, loss = 0.58211458\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 86, loss = 0.58124834\n",
            "Iteration 87, loss = 0.58100007\n",
            "Iteration 88, loss = 0.58112305\n",
            "Iteration 89, loss = 0.58095200\n",
            "Iteration 90, loss = 0.58075597\n",
            "Iteration 91, loss = 0.58094602\n",
            "Iteration 92, loss = 0.58073101\n",
            "Iteration 93, loss = 0.58062121\n",
            "Iteration 94, loss = 0.58052882\n",
            "Iteration 95, loss = 0.58054438\n",
            "Iteration 96, loss = 0.58035398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 97, loss = 0.58009961\n",
            "Iteration 98, loss = 0.58009454\n",
            "Iteration 99, loss = 0.58006847\n",
            "Iteration 100, loss = 0.58004001\n",
            "Iteration 101, loss = 0.57997783\n",
            "Iteration 102, loss = 0.58005749\n",
            "Iteration 103, loss = 0.58001041\n",
            "Iteration 104, loss = 0.58001693\n",
            "Iteration 105, loss = 0.57988859\n",
            "Iteration 106, loss = 0.57996288\n",
            "Iteration 107, loss = 0.57994319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 108, loss = 0.57991531\n",
            "Iteration 109, loss = 0.57987948\n",
            "Iteration 110, loss = 0.57986459\n",
            "Iteration 111, loss = 0.57985414\n",
            "Iteration 112, loss = 0.57986322\n",
            "Iteration 113, loss = 0.57985998\n",
            "Iteration 114, loss = 0.57984963\n",
            "Iteration 115, loss = 0.57984548\n",
            "Iteration 116, loss = 0.57982871\n",
            "Iteration 117, loss = 0.57982871\n",
            "Iteration 118, loss = 0.57983418\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70715782\n",
            "Iteration 3, loss = 0.68915595\n",
            "Iteration 4, loss = 0.67054923\n",
            "Iteration 5, loss = 0.65937423\n",
            "Iteration 6, loss = 0.64980652\n",
            "Iteration 7, loss = 0.64038418\n",
            "Iteration 8, loss = 0.63244717\n",
            "Iteration 9, loss = 0.62448994\n",
            "Iteration 10, loss = 0.61945725\n",
            "Iteration 11, loss = 0.61318187\n",
            "Iteration 12, loss = 0.60942493\n",
            "Iteration 13, loss = 0.60787118\n",
            "Iteration 14, loss = 0.60035955\n",
            "Iteration 15, loss = 0.59500056\n",
            "Iteration 16, loss = 0.59575990\n",
            "Iteration 17, loss = 0.59243014\n",
            "Iteration 18, loss = 0.59024054\n",
            "Iteration 19, loss = 0.58840058\n",
            "Iteration 20, loss = 0.58499397\n",
            "Iteration 21, loss = 0.58616650\n",
            "Iteration 22, loss = 0.57912315\n",
            "Iteration 23, loss = 0.58133162\n",
            "Iteration 24, loss = 0.58125255\n",
            "Iteration 25, loss = 0.58101779\n",
            "Iteration 26, loss = 0.57552882\n",
            "Iteration 27, loss = 0.57554949\n",
            "Iteration 28, loss = 0.57974871\n",
            "Iteration 29, loss = 0.57535388\n",
            "Iteration 30, loss = 0.58030870\n",
            "Iteration 31, loss = 0.57609694\n",
            "Iteration 32, loss = 0.57429237\n",
            "Iteration 33, loss = 0.57351883\n",
            "Iteration 34, loss = 0.58280221\n",
            "Iteration 35, loss = 0.57614235\n",
            "Iteration 36, loss = 0.57431960\n",
            "Iteration 37, loss = 0.61705882\n",
            "Iteration 38, loss = 0.62278570\n",
            "Iteration 39, loss = 0.60778643\n",
            "Iteration 40, loss = 0.60234218\n",
            "Iteration 41, loss = 0.59350723\n",
            "Iteration 42, loss = 0.59269301\n",
            "Iteration 43, loss = 0.63451856\n",
            "Iteration 44, loss = 0.61821000\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.60698940\n",
            "Iteration 46, loss = 0.60185228\n",
            "Iteration 47, loss = 0.59603152\n",
            "Iteration 48, loss = 0.59127395\n",
            "Iteration 49, loss = 0.58655621\n",
            "Iteration 50, loss = 0.58321205\n",
            "Iteration 51, loss = 0.58042025\n",
            "Iteration 52, loss = 0.57834409\n",
            "Iteration 53, loss = 0.57639655\n",
            "Iteration 54, loss = 0.57472657\n",
            "Iteration 55, loss = 0.57313853\n",
            "Iteration 56, loss = 0.57170608\n",
            "Iteration 57, loss = 0.57042227\n",
            "Iteration 58, loss = 0.56920306\n",
            "Iteration 59, loss = 0.56772826\n",
            "Iteration 60, loss = 0.56539375\n",
            "Iteration 61, loss = 0.56430638\n",
            "Iteration 62, loss = 0.56256857\n",
            "Iteration 63, loss = 0.55913980\n",
            "Iteration 64, loss = 0.55773954\n",
            "Iteration 65, loss = 0.55614389\n",
            "Iteration 66, loss = 0.55619646\n",
            "Iteration 67, loss = 0.55344911\n",
            "Iteration 68, loss = 0.55466708\n",
            "Iteration 69, loss = 0.55011345\n",
            "Iteration 70, loss = 0.55003154\n",
            "Iteration 71, loss = 0.54866698\n",
            "Iteration 72, loss = 0.55256117\n",
            "Iteration 73, loss = 0.54730035\n",
            "Iteration 74, loss = 0.55025588\n",
            "Iteration 75, loss = 0.54876403\n",
            "Iteration 76, loss = 0.54369868\n",
            "Iteration 77, loss = 0.54467436\n",
            "Iteration 78, loss = 0.54453339\n",
            "Iteration 79, loss = 0.54962511\n",
            "Iteration 80, loss = 0.54525497\n",
            "Iteration 81, loss = 0.54811466\n",
            "Iteration 82, loss = 0.54423517\n",
            "Iteration 83, loss = 0.53894606\n",
            "Iteration 84, loss = 0.54014285\n",
            "Iteration 85, loss = 0.54958789\n",
            "Iteration 86, loss = 0.53464368\n",
            "Iteration 87, loss = 0.54764377\n",
            "Iteration 88, loss = 0.54254933\n",
            "Iteration 89, loss = 0.54712933\n",
            "Iteration 90, loss = 0.54250231\n",
            "Iteration 91, loss = 0.54376394\n",
            "Iteration 92, loss = 0.53656288\n",
            "Iteration 93, loss = 0.54093464\n",
            "Iteration 94, loss = 0.53967122\n",
            "Iteration 95, loss = 0.53743477\n",
            "Iteration 96, loss = 0.53865034\n",
            "Iteration 97, loss = 0.53456387\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 98, loss = 0.48745808\n",
            "Iteration 99, loss = 0.48530871\n",
            "Iteration 100, loss = 0.48364970\n",
            "Iteration 101, loss = 0.48203786\n",
            "Iteration 102, loss = 0.48022019\n",
            "Iteration 103, loss = 0.47809739\n",
            "Iteration 104, loss = 0.47671077\n",
            "Iteration 105, loss = 0.47521296\n",
            "Iteration 106, loss = 0.47322704\n",
            "Iteration 107, loss = 0.47161020\n",
            "Iteration 108, loss = 0.46973752\n",
            "Iteration 109, loss = 0.46826616\n",
            "Iteration 110, loss = 0.46627105\n",
            "Iteration 111, loss = 0.46454683\n",
            "Iteration 112, loss = 0.46287646\n",
            "Iteration 113, loss = 0.46113094\n",
            "Iteration 114, loss = 0.45916184\n",
            "Iteration 115, loss = 0.45740658\n",
            "Iteration 116, loss = 0.45628546\n",
            "Iteration 117, loss = 0.45456698\n",
            "Iteration 118, loss = 0.45258553\n",
            "Iteration 119, loss = 0.45106363\n",
            "Iteration 120, loss = 0.44958438\n",
            "Iteration 121, loss = 0.44739852\n",
            "Iteration 122, loss = 0.44564826\n",
            "Iteration 123, loss = 0.44415356\n",
            "Iteration 124, loss = 0.44346759\n",
            "Iteration 125, loss = 0.44142201\n",
            "Iteration 126, loss = 0.43946115\n",
            "Iteration 127, loss = 0.43751227\n",
            "Iteration 128, loss = 0.43639953\n",
            "Iteration 129, loss = 0.43455355\n",
            "Iteration 130, loss = 0.43351004\n",
            "Iteration 131, loss = 0.43220912\n",
            "Iteration 132, loss = 0.43041779\n",
            "Iteration 133, loss = 0.42914978\n",
            "Iteration 134, loss = 0.42740134\n",
            "Iteration 135, loss = 0.42658689\n",
            "Iteration 136, loss = 0.42429771\n",
            "Iteration 137, loss = 0.42311701\n",
            "Iteration 138, loss = 0.42220899\n",
            "Iteration 139, loss = 0.42065273\n",
            "Iteration 140, loss = 0.41874871\n",
            "Iteration 141, loss = 0.41782825\n",
            "Iteration 142, loss = 0.41541752\n",
            "Iteration 143, loss = 0.41430072\n",
            "Iteration 144, loss = 0.41413079\n",
            "Iteration 145, loss = 0.41219088\n",
            "Iteration 146, loss = 0.41128967\n",
            "Iteration 147, loss = 0.41005550\n",
            "Iteration 148, loss = 0.40932997\n",
            "Iteration 149, loss = 0.40751582\n",
            "Iteration 150, loss = 0.40565470\n",
            "Iteration 151, loss = 0.40678361\n",
            "Iteration 152, loss = 0.40495306\n",
            "Iteration 153, loss = 0.40362317\n",
            "Iteration 154, loss = 0.40182614\n",
            "Iteration 155, loss = 0.40202635\n",
            "Iteration 156, loss = 0.40203851\n",
            "Iteration 157, loss = 0.40068391\n",
            "Iteration 158, loss = 0.39957109\n",
            "Iteration 159, loss = 0.40129706\n",
            "Iteration 160, loss = 0.39812281\n",
            "Iteration 161, loss = 0.39924085\n",
            "Iteration 162, loss = 0.39538234\n",
            "Iteration 163, loss = 0.39668022\n",
            "Iteration 164, loss = 0.39517664\n",
            "Iteration 165, loss = 0.39387357\n",
            "Iteration 166, loss = 0.39595796\n",
            "Iteration 167, loss = 0.39138219\n",
            "Iteration 168, loss = 0.39428352\n",
            "Iteration 169, loss = 0.39138802\n",
            "Iteration 170, loss = 0.39279610\n",
            "Iteration 171, loss = 0.39463771\n",
            "Iteration 172, loss = 0.39435383\n",
            "Iteration 173, loss = 0.38788785\n",
            "Iteration 174, loss = 0.38843065\n",
            "Iteration 175, loss = 0.39591562\n",
            "Iteration 176, loss = 0.38819455\n",
            "Iteration 177, loss = 0.39143183\n",
            "Iteration 178, loss = 0.38893401\n",
            "Iteration 179, loss = 0.39240347\n",
            "Iteration 180, loss = 0.39314386\n",
            "Iteration 181, loss = 0.39181812\n",
            "Iteration 182, loss = 0.39097903\n",
            "Iteration 183, loss = 0.38994427\n",
            "Iteration 184, loss = 0.38841434\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 185, loss = 0.36585022\n",
            "Iteration 186, loss = 0.36466642\n",
            "Iteration 187, loss = 0.36432908\n",
            "Iteration 188, loss = 0.36437263\n",
            "Iteration 189, loss = 0.36396384\n",
            "Iteration 190, loss = 0.36371886\n",
            "Iteration 191, loss = 0.36360742\n",
            "Iteration 192, loss = 0.36331160\n",
            "Iteration 193, loss = 0.36319269\n",
            "Iteration 194, loss = 0.36298238\n",
            "Iteration 195, loss = 0.36279405\n",
            "Iteration 196, loss = 0.36264505\n",
            "Iteration 197, loss = 0.36261810\n",
            "Iteration 198, loss = 0.36215561\n",
            "Iteration 199, loss = 0.36214320\n",
            "Iteration 200, loss = 0.36183436\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.72123696\n",
            "Iteration 3, loss = 0.69932807\n",
            "Iteration 4, loss = 0.68171015\n",
            "Iteration 5, loss = 0.67005183\n",
            "Iteration 6, loss = 0.65684663\n",
            "Iteration 7, loss = 0.64349873\n",
            "Iteration 8, loss = 0.63964469\n",
            "Iteration 9, loss = 0.63091134\n",
            "Iteration 10, loss = 0.62115443\n",
            "Iteration 11, loss = 0.61707667\n",
            "Iteration 12, loss = 0.61227076\n",
            "Iteration 13, loss = 0.60441492\n",
            "Iteration 14, loss = 0.59869404\n",
            "Iteration 15, loss = 0.59785413\n",
            "Iteration 16, loss = 0.59337631\n",
            "Iteration 17, loss = 0.58741215\n",
            "Iteration 18, loss = 0.59131788\n",
            "Iteration 19, loss = 0.63305094\n",
            "Iteration 20, loss = 0.64059608\n",
            "Iteration 21, loss = 0.63439835\n",
            "Iteration 22, loss = 0.62592213\n",
            "Iteration 23, loss = 0.61537870\n",
            "Iteration 24, loss = 0.60756748\n",
            "Iteration 25, loss = 0.60768201\n",
            "Iteration 26, loss = 0.59868221\n",
            "Iteration 27, loss = 0.59685330\n",
            "Iteration 28, loss = 0.58978127\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.57962329\n",
            "Iteration 30, loss = 0.57679402\n",
            "Iteration 31, loss = 0.57516951\n",
            "Iteration 32, loss = 0.57424721\n",
            "Iteration 33, loss = 0.57326008\n",
            "Iteration 34, loss = 0.57103476\n",
            "Iteration 35, loss = 0.56964509\n",
            "Iteration 36, loss = 0.56829947\n",
            "Iteration 37, loss = 0.56677167\n",
            "Iteration 38, loss = 0.56550497\n",
            "Iteration 39, loss = 0.56338837\n",
            "Iteration 40, loss = 0.56147152\n",
            "Iteration 41, loss = 0.55968046\n",
            "Iteration 42, loss = 0.55713808\n",
            "Iteration 43, loss = 0.55574932\n",
            "Iteration 44, loss = 0.55385786\n",
            "Iteration 45, loss = 0.55235623\n",
            "Iteration 46, loss = 0.54817651\n",
            "Iteration 47, loss = 0.54781269\n",
            "Iteration 48, loss = 0.54898334\n",
            "Iteration 49, loss = 0.54630873\n",
            "Iteration 50, loss = 0.54743442\n",
            "Iteration 51, loss = 0.54260569\n",
            "Iteration 52, loss = 0.54392100\n",
            "Iteration 53, loss = 0.55209431\n",
            "Iteration 54, loss = 0.54614847\n",
            "Iteration 55, loss = 0.54682620\n",
            "Iteration 56, loss = 0.54687172\n",
            "Iteration 57, loss = 0.54761253\n",
            "Iteration 58, loss = 0.54206230\n",
            "Iteration 59, loss = 0.54610288\n",
            "Iteration 60, loss = 0.55019639\n",
            "Iteration 61, loss = 0.54468679\n",
            "Iteration 62, loss = 0.54504875\n",
            "Iteration 63, loss = 0.54562966\n",
            "Iteration 64, loss = 0.54904816\n",
            "Iteration 65, loss = 0.54505788\n",
            "Iteration 66, loss = 0.54002436\n",
            "Iteration 67, loss = 0.54544894\n",
            "Iteration 68, loss = 0.54108899\n",
            "Iteration 69, loss = 0.70522163\n",
            "Iteration 70, loss = 0.65425966\n",
            "Iteration 71, loss = 0.63303076\n",
            "Iteration 72, loss = 0.63052171\n",
            "Iteration 73, loss = 0.62761286\n",
            "Iteration 74, loss = 0.62426439\n",
            "Iteration 75, loss = 0.62018137\n",
            "Iteration 76, loss = 0.61510452\n",
            "Iteration 77, loss = 0.60998130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 78, loss = 0.60673013\n",
            "Iteration 79, loss = 0.60477504\n",
            "Iteration 80, loss = 0.60392080\n",
            "Iteration 81, loss = 0.60277942\n",
            "Iteration 82, loss = 0.60181059\n",
            "Iteration 83, loss = 0.60090605\n",
            "Iteration 84, loss = 0.59982183\n",
            "Iteration 85, loss = 0.59885647\n",
            "Iteration 86, loss = 0.59803916\n",
            "Iteration 87, loss = 0.59746012\n",
            "Iteration 88, loss = 0.59675378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 89, loss = 0.59570355\n",
            "Iteration 90, loss = 0.59566151\n",
            "Iteration 91, loss = 0.59551821\n",
            "Iteration 92, loss = 0.59525622\n",
            "Iteration 93, loss = 0.59501013\n",
            "Iteration 94, loss = 0.59492324\n",
            "Iteration 95, loss = 0.59487328\n",
            "Iteration 96, loss = 0.59469463\n",
            "Iteration 97, loss = 0.59465567\n",
            "Iteration 98, loss = 0.59450562\n",
            "Iteration 99, loss = 0.59416582\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 100, loss = 0.59409657\n",
            "Iteration 101, loss = 0.59404351\n",
            "Iteration 102, loss = 0.59403170\n",
            "Iteration 103, loss = 0.59395649\n",
            "Iteration 104, loss = 0.59394004\n",
            "Iteration 105, loss = 0.59389405\n",
            "Iteration 106, loss = 0.59387020\n",
            "Iteration 107, loss = 0.59386282\n",
            "Iteration 108, loss = 0.59382403\n",
            "Iteration 109, loss = 0.59378196\n",
            "Iteration 110, loss = 0.59376130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 111, loss = 0.59371917\n",
            "Iteration 112, loss = 0.59370632\n",
            "Iteration 113, loss = 0.59370116\n",
            "Iteration 114, loss = 0.59369843\n",
            "Iteration 115, loss = 0.59369515\n",
            "Iteration 116, loss = 0.59368692\n",
            "Iteration 117, loss = 0.59368295\n",
            "Iteration 118, loss = 0.59367734\n",
            "Iteration 119, loss = 0.59366468\n",
            "Iteration 120, loss = 0.59366078\n",
            "Iteration 121, loss = 0.59365962\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69502169\n",
            "Iteration 3, loss = 0.68007011\n",
            "Iteration 4, loss = 0.66399426\n",
            "Iteration 5, loss = 0.65131283\n",
            "Iteration 6, loss = 0.64340141\n",
            "Iteration 7, loss = 0.63420644\n",
            "Iteration 8, loss = 0.62929325\n",
            "Iteration 9, loss = 0.61696985\n",
            "Iteration 10, loss = 0.61842897\n",
            "Iteration 11, loss = 0.60942141\n",
            "Iteration 12, loss = 0.60434466\n",
            "Iteration 13, loss = 0.60391398\n",
            "Iteration 14, loss = 0.60161992\n",
            "Iteration 15, loss = 0.59659237\n",
            "Iteration 16, loss = 0.59416045\n",
            "Iteration 17, loss = 0.59531248\n",
            "Iteration 18, loss = 0.59302857\n",
            "Iteration 19, loss = 0.58769522\n",
            "Iteration 20, loss = 0.58519269\n",
            "Iteration 21, loss = 0.58539268\n",
            "Iteration 22, loss = 0.58224600\n",
            "Iteration 23, loss = 0.58190951\n",
            "Iteration 24, loss = 0.57784618\n",
            "Iteration 25, loss = 0.57772135\n",
            "Iteration 26, loss = 0.57637399\n",
            "Iteration 27, loss = 0.57751072\n",
            "Iteration 28, loss = 0.57670754\n",
            "Iteration 29, loss = 0.57410063\n",
            "Iteration 30, loss = 0.57362519\n",
            "Iteration 31, loss = 0.57604574\n",
            "Iteration 32, loss = 0.57526468\n",
            "Iteration 33, loss = 0.57235223\n",
            "Iteration 34, loss = 0.57277213\n",
            "Iteration 35, loss = 0.57084329\n",
            "Iteration 36, loss = 0.57314541\n",
            "Iteration 37, loss = 0.57100239\n",
            "Iteration 38, loss = 0.57246269\n",
            "Iteration 39, loss = 0.57007140\n",
            "Iteration 40, loss = 0.57001824\n",
            "Iteration 41, loss = 0.57178654\n",
            "Iteration 42, loss = 0.57073076\n",
            "Iteration 43, loss = 0.57187854\n",
            "Iteration 44, loss = 0.57093322\n",
            "Iteration 45, loss = 0.56739520\n",
            "Iteration 46, loss = 0.57137853\n",
            "Iteration 47, loss = 0.57048221\n",
            "Iteration 48, loss = 0.56822861\n",
            "Iteration 49, loss = 0.57612537\n",
            "Iteration 50, loss = 0.57722111\n",
            "Iteration 51, loss = 0.57420047\n",
            "Iteration 52, loss = 0.57428281\n",
            "Iteration 53, loss = 0.57578758\n",
            "Iteration 54, loss = 0.57318801\n",
            "Iteration 55, loss = 0.57320073\n",
            "Iteration 56, loss = 0.57067893\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.52667577\n",
            "Iteration 58, loss = 0.51584739\n",
            "Iteration 59, loss = 0.50753325\n",
            "Iteration 60, loss = 0.50022857\n",
            "Iteration 61, loss = 0.49352764\n",
            "Iteration 62, loss = 0.48605176\n",
            "Iteration 63, loss = 0.48918653\n",
            "Iteration 64, loss = 0.50020266\n",
            "Iteration 65, loss = 0.50950584\n",
            "Iteration 66, loss = 0.51634354\n",
            "Iteration 67, loss = 0.52226749\n",
            "Iteration 68, loss = 0.51598218\n",
            "Iteration 69, loss = 0.51412775\n",
            "Iteration 70, loss = 0.52830756\n",
            "Iteration 71, loss = 0.50925664\n",
            "Iteration 72, loss = 0.52591025\n",
            "Iteration 73, loss = 0.50715977\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.45526002\n",
            "Iteration 75, loss = 0.44054032\n",
            "Iteration 76, loss = 0.43761194\n",
            "Iteration 77, loss = 0.43504471\n",
            "Iteration 78, loss = 0.43206785\n",
            "Iteration 79, loss = 0.42895459\n",
            "Iteration 80, loss = 0.42608648\n",
            "Iteration 81, loss = 0.42338465\n",
            "Iteration 82, loss = 0.42064925\n",
            "Iteration 83, loss = 0.41783281\n",
            "Iteration 84, loss = 0.41524849\n",
            "Iteration 85, loss = 0.41284686\n",
            "Iteration 86, loss = 0.40984670\n",
            "Iteration 87, loss = 0.40759804\n",
            "Iteration 88, loss = 0.40493318\n",
            "Iteration 89, loss = 0.40299878\n",
            "Iteration 90, loss = 0.40009702\n",
            "Iteration 91, loss = 0.39698381\n",
            "Iteration 92, loss = 0.39515333\n",
            "Iteration 93, loss = 0.39285787\n",
            "Iteration 94, loss = 0.39023202\n",
            "Iteration 95, loss = 0.38788064\n",
            "Iteration 96, loss = 0.38566939\n",
            "Iteration 97, loss = 0.38335031\n",
            "Iteration 98, loss = 0.38130380\n",
            "Iteration 99, loss = 0.37865521\n",
            "Iteration 100, loss = 0.37697749\n",
            "Iteration 101, loss = 0.37437122\n",
            "Iteration 102, loss = 0.37263137\n",
            "Iteration 103, loss = 0.37067666\n",
            "Iteration 104, loss = 0.36851875\n",
            "Iteration 105, loss = 0.36702785\n",
            "Iteration 106, loss = 0.36504259\n",
            "Iteration 107, loss = 0.36245267\n",
            "Iteration 108, loss = 0.36073277\n",
            "Iteration 109, loss = 0.35908946\n",
            "Iteration 110, loss = 0.35703029\n",
            "Iteration 111, loss = 0.35494757\n",
            "Iteration 112, loss = 0.35366288\n",
            "Iteration 113, loss = 0.35160561\n",
            "Iteration 114, loss = 0.34995106\n",
            "Iteration 115, loss = 0.34881615\n",
            "Iteration 116, loss = 0.34604225\n",
            "Iteration 117, loss = 0.34513446\n",
            "Iteration 118, loss = 0.34341216\n",
            "Iteration 119, loss = 0.34179283\n",
            "Iteration 120, loss = 0.33994318\n",
            "Iteration 121, loss = 0.33972768\n",
            "Iteration 122, loss = 0.33779170\n",
            "Iteration 123, loss = 0.33672403\n",
            "Iteration 124, loss = 0.33393521\n",
            "Iteration 125, loss = 0.33324921\n",
            "Iteration 126, loss = 0.33153027\n",
            "Iteration 127, loss = 0.33086547\n",
            "Iteration 128, loss = 0.32888438\n",
            "Iteration 129, loss = 0.32767286\n",
            "Iteration 130, loss = 0.32608687\n",
            "Iteration 131, loss = 0.32529538\n",
            "Iteration 132, loss = 0.32345293\n",
            "Iteration 133, loss = 0.32349424\n",
            "Iteration 134, loss = 0.32197774\n",
            "Iteration 135, loss = 0.32085895\n",
            "Iteration 136, loss = 0.31936872\n",
            "Iteration 137, loss = 0.31697233\n",
            "Iteration 138, loss = 0.31798269\n",
            "Iteration 139, loss = 0.31643250\n",
            "Iteration 140, loss = 0.31539965\n",
            "Iteration 141, loss = 0.31637021\n",
            "Iteration 142, loss = 0.31416657\n",
            "Iteration 143, loss = 0.31210070\n",
            "Iteration 144, loss = 0.31347374\n",
            "Iteration 145, loss = 0.31147156\n",
            "Iteration 146, loss = 0.30997950\n",
            "Iteration 147, loss = 0.31137278\n",
            "Iteration 148, loss = 0.30792409\n",
            "Iteration 149, loss = 0.30649462\n",
            "Iteration 150, loss = 0.30768254\n",
            "Iteration 151, loss = 0.30910262\n",
            "Iteration 152, loss = 0.30495619\n",
            "Iteration 153, loss = 0.30387484\n",
            "Iteration 154, loss = 0.30542019\n",
            "Iteration 155, loss = 0.30469219\n",
            "Iteration 156, loss = 0.30046232\n",
            "Iteration 157, loss = 0.30019016\n",
            "Iteration 158, loss = 0.30259517\n",
            "Iteration 159, loss = 0.30001009\n",
            "Iteration 160, loss = 0.29863028\n",
            "Iteration 161, loss = 0.30321389\n",
            "Iteration 162, loss = 0.29937592\n",
            "Iteration 163, loss = 0.29910197\n",
            "Iteration 164, loss = 0.29763794\n",
            "Iteration 165, loss = 0.29896279\n",
            "Iteration 166, loss = 0.29838927\n",
            "Iteration 167, loss = 0.29502894\n",
            "Iteration 168, loss = 0.29392070\n",
            "Iteration 169, loss = 0.29495875\n",
            "Iteration 170, loss = 0.29752071\n",
            "Iteration 171, loss = 0.29520831\n",
            "Iteration 172, loss = 0.29408741\n",
            "Iteration 173, loss = 0.29204486\n",
            "Iteration 174, loss = 0.29375962\n",
            "Iteration 175, loss = 0.29281647\n",
            "Iteration 176, loss = 0.29214276\n",
            "Iteration 177, loss = 0.29212520\n",
            "Iteration 178, loss = 0.29296974\n",
            "Iteration 179, loss = 0.29160829\n",
            "Iteration 180, loss = 0.28980477\n",
            "Iteration 181, loss = 0.28896884\n",
            "Iteration 182, loss = 0.28675850\n",
            "Iteration 183, loss = 0.29745318\n",
            "Iteration 184, loss = 0.29229192\n",
            "Iteration 185, loss = 0.29243224\n",
            "Iteration 186, loss = 0.28835087\n",
            "Iteration 187, loss = 0.30307653\n",
            "Iteration 188, loss = 0.29835847\n",
            "Iteration 189, loss = 0.29740150\n",
            "Iteration 190, loss = 0.29743400\n",
            "Iteration 191, loss = 0.28970476\n",
            "Iteration 192, loss = 0.28827656\n",
            "Iteration 193, loss = 0.29755742\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 194, loss = 0.27432070\n",
            "Iteration 195, loss = 0.27396460\n",
            "Iteration 196, loss = 0.27412306\n",
            "Iteration 197, loss = 0.27396569\n",
            "Iteration 198, loss = 0.27399112\n",
            "Iteration 199, loss = 0.27383717\n",
            "Iteration 200, loss = 0.27366452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72134655\n",
            "Iteration 3, loss = 0.69165494\n",
            "Iteration 4, loss = 0.67763912\n",
            "Iteration 5, loss = 0.66174027\n",
            "Iteration 6, loss = 0.65708667\n",
            "Iteration 7, loss = 0.64625301\n",
            "Iteration 8, loss = 0.63578732\n",
            "Iteration 9, loss = 0.62656944\n",
            "Iteration 10, loss = 0.62076831\n",
            "Iteration 11, loss = 0.61565993\n",
            "Iteration 12, loss = 0.60655936\n",
            "Iteration 13, loss = 0.60187943\n",
            "Iteration 14, loss = 0.60164147\n",
            "Iteration 15, loss = 0.59493057\n",
            "Iteration 16, loss = 0.59343557\n",
            "Iteration 17, loss = 0.59083737\n",
            "Iteration 18, loss = 0.58835170\n",
            "Iteration 19, loss = 0.58395136\n",
            "Iteration 20, loss = 0.58328247\n",
            "Iteration 21, loss = 0.57912769\n",
            "Iteration 22, loss = 0.63588548\n",
            "Iteration 23, loss = 0.61985149\n",
            "Iteration 24, loss = 0.61287272\n",
            "Iteration 25, loss = 0.60752255\n",
            "Iteration 26, loss = 0.60358577\n",
            "Iteration 27, loss = 0.62453370\n",
            "Iteration 28, loss = 0.61293724\n",
            "Iteration 29, loss = 0.60582740\n",
            "Iteration 30, loss = 0.60057204\n",
            "Iteration 31, loss = 0.59506173\n",
            "Iteration 32, loss = 0.59399296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 33, loss = 0.58023593\n",
            "Iteration 34, loss = 0.57831950\n",
            "Iteration 35, loss = 0.57734909\n",
            "Iteration 36, loss = 0.57619040\n",
            "Iteration 37, loss = 0.57534422\n",
            "Iteration 38, loss = 0.57362997\n",
            "Iteration 39, loss = 0.57256047\n",
            "Iteration 40, loss = 0.57136894\n",
            "Iteration 41, loss = 0.56960070\n",
            "Iteration 42, loss = 0.56849558\n",
            "Iteration 43, loss = 0.56779903\n",
            "Iteration 44, loss = 0.56547609\n",
            "Iteration 45, loss = 0.56460177\n",
            "Iteration 46, loss = 0.56287831\n",
            "Iteration 47, loss = 0.55994441\n",
            "Iteration 48, loss = 0.56013401\n",
            "Iteration 49, loss = 0.55752827\n",
            "Iteration 50, loss = 0.55647922\n",
            "Iteration 51, loss = 0.55322811\n",
            "Iteration 52, loss = 0.55152003\n",
            "Iteration 53, loss = 0.55128899\n",
            "Iteration 54, loss = 0.54987542\n",
            "Iteration 55, loss = 0.54821761\n",
            "Iteration 56, loss = 0.54706033\n",
            "Iteration 57, loss = 0.54573191\n",
            "Iteration 58, loss = 0.54861268\n",
            "Iteration 59, loss = 0.54511406\n",
            "Iteration 60, loss = 0.54502512\n",
            "Iteration 61, loss = 0.54847376\n",
            "Iteration 62, loss = 0.54189742\n",
            "Iteration 63, loss = 0.55060091\n",
            "Iteration 64, loss = 0.54413746\n",
            "Iteration 65, loss = 0.54353238\n",
            "Iteration 66, loss = 0.54550448\n",
            "Iteration 67, loss = 0.54503855\n",
            "Iteration 68, loss = 0.64049686\n",
            "Iteration 69, loss = 0.63618370\n",
            "Iteration 70, loss = 0.62256912\n",
            "Iteration 71, loss = 0.61877393\n",
            "Iteration 72, loss = 0.61447654\n",
            "Iteration 73, loss = 0.60902625\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.60472007\n",
            "Iteration 75, loss = 0.60390805\n",
            "Iteration 76, loss = 0.60266575\n",
            "Iteration 77, loss = 0.60139791\n",
            "Iteration 78, loss = 0.60032471\n",
            "Iteration 79, loss = 0.59929169\n",
            "Iteration 80, loss = 0.59802543\n",
            "Iteration 81, loss = 0.59725045\n",
            "Iteration 82, loss = 0.59608519\n",
            "Iteration 83, loss = 0.59512143\n",
            "Iteration 84, loss = 0.59410398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 85, loss = 0.59303921\n",
            "Iteration 86, loss = 0.59296570\n",
            "Iteration 87, loss = 0.59274899\n",
            "Iteration 88, loss = 0.59247500\n",
            "Iteration 89, loss = 0.59236756\n",
            "Iteration 90, loss = 0.59216320\n",
            "Iteration 91, loss = 0.59196754\n",
            "Iteration 92, loss = 0.59195280\n",
            "Iteration 93, loss = 0.59168506\n",
            "Iteration 94, loss = 0.59149654\n",
            "Iteration 95, loss = 0.59124900\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 96, loss = 0.59107457\n",
            "Iteration 97, loss = 0.59100632\n",
            "Iteration 98, loss = 0.59095426\n",
            "Iteration 99, loss = 0.59098404\n",
            "Iteration 100, loss = 0.59095313\n",
            "Iteration 101, loss = 0.59090893\n",
            "Iteration 102, loss = 0.59087190\n",
            "Iteration 103, loss = 0.59085029\n",
            "Iteration 104, loss = 0.59080379\n",
            "Iteration 105, loss = 0.59076782\n",
            "Iteration 106, loss = 0.59073013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 107, loss = 0.59068890\n",
            "Iteration 108, loss = 0.59067082\n",
            "Iteration 109, loss = 0.59066993\n",
            "Iteration 110, loss = 0.59066351\n",
            "Iteration 111, loss = 0.59065786\n",
            "Iteration 112, loss = 0.59065048\n",
            "Iteration 113, loss = 0.59064305\n",
            "Iteration 114, loss = 0.59063492\n",
            "Iteration 115, loss = 0.59062804\n",
            "Iteration 116, loss = 0.59062044\n",
            "Iteration 117, loss = 0.59061463\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33271   456]\n",
            " [ 1097 15822]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     33727\n",
            "           1       0.97      0.94      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.97     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.707% (0.151%)\n",
            "Prediction Accuracy (Shuffle Split) : 99.105% (13.703%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.89816350\n",
            "Iteration 2, loss = 0.64641640\n",
            "Iteration 3, loss = 0.61865972\n",
            "Iteration 4, loss = 0.60336593\n",
            "Iteration 5, loss = 0.59355854\n",
            "Iteration 6, loss = 0.58810451\n",
            "Iteration 7, loss = 0.58581107\n",
            "Iteration 8, loss = 0.58422790\n",
            "Iteration 9, loss = 0.58245948\n",
            "Iteration 10, loss = 0.58223722\n",
            "Iteration 11, loss = 0.58232948\n",
            "Iteration 12, loss = 0.58195507\n",
            "Iteration 13, loss = 0.58246870\n",
            "Iteration 14, loss = 0.58175991\n",
            "Iteration 15, loss = 0.58187391\n",
            "Iteration 16, loss = 0.58085104\n",
            "Iteration 17, loss = 0.58220950\n",
            "Iteration 18, loss = 0.58167643\n",
            "Iteration 19, loss = 0.58188749\n",
            "Iteration 20, loss = 0.58182286\n",
            "Iteration 21, loss = 0.58111130\n",
            "Iteration 22, loss = 0.58219408\n",
            "Iteration 23, loss = 0.58137680\n",
            "Iteration 24, loss = 0.57941800\n",
            "Iteration 25, loss = 0.57980065\n",
            "Iteration 26, loss = 0.58106648\n",
            "Iteration 27, loss = 0.60757333\n",
            "Iteration 28, loss = 0.60629704\n",
            "Iteration 29, loss = 0.59454460\n",
            "Iteration 30, loss = 0.58743661\n",
            "Iteration 31, loss = 0.58359200\n",
            "Iteration 32, loss = 0.58172651\n",
            "Iteration 33, loss = 0.58538359\n",
            "Iteration 34, loss = 0.60616794\n",
            "Iteration 35, loss = 0.60420576\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.58573973\n",
            "Iteration 37, loss = 0.58130288\n",
            "Iteration 38, loss = 0.57533756\n",
            "Iteration 39, loss = 0.56940696\n",
            "Iteration 40, loss = 0.56240527\n",
            "Iteration 41, loss = 0.55917712\n",
            "Iteration 42, loss = 0.58495474\n",
            "Iteration 43, loss = 0.58067131\n",
            "Iteration 44, loss = 0.57381934\n",
            "Iteration 45, loss = 0.56859451\n",
            "Iteration 46, loss = 0.56521402\n",
            "Iteration 47, loss = 0.56548898\n",
            "Iteration 48, loss = 0.56505062\n",
            "Iteration 49, loss = 0.56592958\n",
            "Iteration 50, loss = 0.56298865\n",
            "Iteration 51, loss = 0.56103225\n",
            "Iteration 52, loss = 0.55920979\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 53, loss = 0.50936684\n",
            "Iteration 54, loss = 0.50200725\n",
            "Iteration 55, loss = 0.49498747\n",
            "Iteration 56, loss = 0.48793400\n",
            "Iteration 57, loss = 0.48118043\n",
            "Iteration 58, loss = 0.47463498\n",
            "Iteration 59, loss = 0.46864349\n",
            "Iteration 60, loss = 0.46272602\n",
            "Iteration 61, loss = 0.45743076\n",
            "Iteration 62, loss = 0.45238236\n",
            "Iteration 63, loss = 0.44780756\n",
            "Iteration 64, loss = 0.44338024\n",
            "Iteration 65, loss = 0.44080258\n",
            "Iteration 66, loss = 0.43697711\n",
            "Iteration 67, loss = 0.43444677\n",
            "Iteration 68, loss = 0.43307971\n",
            "Iteration 69, loss = 0.42951815\n",
            "Iteration 70, loss = 0.43018579\n",
            "Iteration 71, loss = 0.42737928\n",
            "Iteration 72, loss = 0.43087550\n",
            "Iteration 73, loss = 0.42907607\n",
            "Iteration 74, loss = 0.42844617\n",
            "Iteration 75, loss = 0.42865795\n",
            "Iteration 76, loss = 0.42738076\n",
            "Iteration 77, loss = 0.42399487\n",
            "Iteration 78, loss = 0.42777532\n",
            "Iteration 79, loss = 0.42411657\n",
            "Iteration 80, loss = 0.42345615\n",
            "Iteration 81, loss = 0.42508495\n",
            "Iteration 82, loss = 0.42050139\n",
            "Iteration 83, loss = 0.42439545\n",
            "Iteration 84, loss = 0.41848404\n",
            "Iteration 85, loss = 0.42324143\n",
            "Iteration 86, loss = 0.41470036\n",
            "Iteration 87, loss = 0.41681708\n",
            "Iteration 88, loss = 0.41712439\n",
            "Iteration 89, loss = 0.41794593\n",
            "Iteration 90, loss = 0.41562720\n",
            "Iteration 91, loss = 0.41674518\n",
            "Iteration 92, loss = 0.41399773\n",
            "Iteration 93, loss = 0.41223198\n",
            "Iteration 94, loss = 0.41118929\n",
            "Iteration 95, loss = 0.40824543\n",
            "Iteration 96, loss = 0.40824825\n",
            "Iteration 97, loss = 0.40970878\n",
            "Iteration 98, loss = 0.40360687\n",
            "Iteration 99, loss = 0.40703340\n",
            "Iteration 100, loss = 0.40663880\n",
            "Iteration 101, loss = 0.40510668\n",
            "Iteration 102, loss = 0.40389598\n",
            "Iteration 103, loss = 0.40158000\n",
            "Iteration 104, loss = 0.40021667\n",
            "Iteration 105, loss = 0.40179908\n",
            "Iteration 106, loss = 0.40090781\n",
            "Iteration 107, loss = 0.40083444\n",
            "Iteration 108, loss = 0.39941196\n",
            "Iteration 109, loss = 0.40033559\n",
            "Iteration 110, loss = 0.39730755\n",
            "Iteration 111, loss = 0.39594373\n",
            "Iteration 112, loss = 0.39874397\n",
            "Iteration 113, loss = 0.39422619\n",
            "Iteration 114, loss = 0.39601713\n",
            "Iteration 115, loss = 0.39432047\n",
            "Iteration 116, loss = 0.39781642\n",
            "Iteration 117, loss = 0.39283271\n",
            "Iteration 118, loss = 0.38593179\n",
            "Iteration 119, loss = 0.38748860\n",
            "Iteration 120, loss = 0.39216678\n",
            "Iteration 121, loss = 0.38988059\n",
            "Iteration 122, loss = 0.39166265\n",
            "Iteration 123, loss = 0.38586675\n",
            "Iteration 124, loss = 0.39223698\n",
            "Iteration 125, loss = 0.39214304\n",
            "Iteration 126, loss = 0.38691347\n",
            "Iteration 127, loss = 0.38565753\n",
            "Iteration 128, loss = 0.39054670\n",
            "Iteration 129, loss = 0.38772338\n",
            "Iteration 130, loss = 0.38989561\n",
            "Iteration 131, loss = 0.38465599\n",
            "Iteration 132, loss = 0.38481224\n",
            "Iteration 133, loss = 0.38379516\n",
            "Iteration 134, loss = 0.38912452\n",
            "Iteration 135, loss = 0.38593017\n",
            "Iteration 136, loss = 0.38427199\n",
            "Iteration 137, loss = 0.38442048\n",
            "Iteration 138, loss = 0.38162817\n",
            "Iteration 139, loss = 0.38900790\n",
            "Iteration 140, loss = 0.38347367\n",
            "Iteration 141, loss = 0.38229835\n",
            "Iteration 142, loss = 0.38632849\n",
            "Iteration 143, loss = 0.37844945\n",
            "Iteration 144, loss = 0.37747801\n",
            "Iteration 145, loss = 0.38251221\n",
            "Iteration 146, loss = 0.38379517\n",
            "Iteration 147, loss = 0.37859829\n",
            "Iteration 148, loss = 0.37918494\n",
            "Iteration 149, loss = 0.37837962\n",
            "Iteration 150, loss = 0.37734156\n",
            "Iteration 151, loss = 0.37976549\n",
            "Iteration 152, loss = 0.38336577\n",
            "Iteration 153, loss = 0.38526924\n",
            "Iteration 154, loss = 0.38107923\n",
            "Iteration 155, loss = 0.37933460\n",
            "Iteration 156, loss = 0.37901808\n",
            "Iteration 157, loss = 0.37725750\n",
            "Iteration 158, loss = 0.37522654\n",
            "Iteration 159, loss = 0.37643364\n",
            "Iteration 160, loss = 0.37589218\n",
            "Iteration 161, loss = 0.37324318\n",
            "Iteration 162, loss = 0.37693054\n",
            "Iteration 163, loss = 0.37771073\n",
            "Iteration 164, loss = 0.38222287\n",
            "Iteration 165, loss = 0.37379740\n",
            "Iteration 166, loss = 0.37541660\n",
            "Iteration 167, loss = 0.37662776\n",
            "Iteration 168, loss = 0.37528687\n",
            "Iteration 169, loss = 0.38099787\n",
            "Iteration 170, loss = 0.38173749\n",
            "Iteration 171, loss = 0.37854925\n",
            "Iteration 172, loss = 0.37729787\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 173, loss = 0.33734836\n",
            "Iteration 174, loss = 0.33715250\n",
            "Iteration 175, loss = 0.33700187\n",
            "Iteration 176, loss = 0.33691211\n",
            "Iteration 177, loss = 0.33685813\n",
            "Iteration 178, loss = 0.33671518\n",
            "Iteration 179, loss = 0.33658669\n",
            "Iteration 180, loss = 0.33660374\n",
            "Iteration 181, loss = 0.33644756\n",
            "Iteration 182, loss = 0.33640476\n",
            "Iteration 183, loss = 0.33628957\n",
            "Iteration 184, loss = 0.33623724\n",
            "Iteration 185, loss = 0.33609085\n",
            "Iteration 186, loss = 0.33614657\n",
            "Iteration 187, loss = 0.33598088\n",
            "Iteration 188, loss = 0.33594468\n",
            "Iteration 189, loss = 0.33582840\n",
            "Iteration 190, loss = 0.33581405\n",
            "Iteration 191, loss = 0.33569005\n",
            "Iteration 192, loss = 0.33565543\n",
            "Iteration 193, loss = 0.33552808\n",
            "Iteration 194, loss = 0.33544141\n",
            "Iteration 195, loss = 0.33535291\n",
            "Iteration 196, loss = 0.33543363\n",
            "Iteration 197, loss = 0.33521161\n",
            "Iteration 198, loss = 0.33522760\n",
            "Iteration 199, loss = 0.33518501\n",
            "Iteration 200, loss = 0.33500307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64928959\n",
            "Iteration 3, loss = 0.61798904\n",
            "Iteration 4, loss = 0.60145915\n",
            "Iteration 5, loss = 0.59021092\n",
            "Iteration 6, loss = 0.60511466\n",
            "Iteration 7, loss = 0.59454644\n",
            "Iteration 8, loss = 0.58260288\n",
            "Iteration 9, loss = 0.57618147\n",
            "Iteration 10, loss = 0.57396196\n",
            "Iteration 11, loss = 0.57244902\n",
            "Iteration 12, loss = 0.57140217\n",
            "Iteration 13, loss = 0.57284257\n",
            "Iteration 14, loss = 0.57077639\n",
            "Iteration 15, loss = 0.57193684\n",
            "Iteration 16, loss = 0.57273841\n",
            "Iteration 17, loss = 0.57344932\n",
            "Iteration 18, loss = 0.57184194\n",
            "Iteration 19, loss = 0.57265791\n",
            "Iteration 20, loss = 0.57375376\n",
            "Iteration 21, loss = 0.57320622\n",
            "Iteration 22, loss = 0.57225603\n",
            "Iteration 23, loss = 0.57276185\n",
            "Iteration 24, loss = 0.57334664\n",
            "Iteration 25, loss = 0.57407091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.51998696\n",
            "Iteration 27, loss = 0.50092325\n",
            "Iteration 28, loss = 0.51768242\n",
            "Iteration 29, loss = 0.52688278\n",
            "Iteration 30, loss = 0.52212341\n",
            "Iteration 31, loss = 0.52426914\n",
            "Iteration 32, loss = 0.52408411\n",
            "Iteration 33, loss = 0.52220529\n",
            "Iteration 34, loss = 0.52198659\n",
            "Iteration 35, loss = 0.52155309\n",
            "Iteration 36, loss = 0.51945884\n",
            "Iteration 37, loss = 0.52432104\n",
            "Iteration 38, loss = 0.52129682\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.43584941\n",
            "Iteration 40, loss = 0.42275072\n",
            "Iteration 41, loss = 0.41095584\n",
            "Iteration 42, loss = 0.40016595\n",
            "Iteration 43, loss = 0.39048435\n",
            "Iteration 44, loss = 0.38132197\n",
            "Iteration 45, loss = 0.37329275\n",
            "Iteration 46, loss = 0.36631017\n",
            "Iteration 47, loss = 0.35981826\n",
            "Iteration 48, loss = 0.35413623\n",
            "Iteration 49, loss = 0.35020432\n",
            "Iteration 50, loss = 0.34491477\n",
            "Iteration 51, loss = 0.34114566\n",
            "Iteration 52, loss = 0.33861619\n",
            "Iteration 53, loss = 0.33589528\n",
            "Iteration 54, loss = 0.33282505\n",
            "Iteration 55, loss = 0.33022083\n",
            "Iteration 56, loss = 0.32850036\n",
            "Iteration 57, loss = 0.31767565\n",
            "Iteration 58, loss = 0.30551938\n",
            "Iteration 59, loss = 0.30149715\n",
            "Iteration 60, loss = 0.29838381\n",
            "Iteration 61, loss = 0.29594561\n",
            "Iteration 62, loss = 0.29307002\n",
            "Iteration 63, loss = 0.29179227\n",
            "Iteration 64, loss = 0.28913649\n",
            "Iteration 65, loss = 0.28896884\n",
            "Iteration 66, loss = 0.28391067\n",
            "Iteration 67, loss = 0.28408256\n",
            "Iteration 68, loss = 0.28309919\n",
            "Iteration 69, loss = 0.28163330\n",
            "Iteration 70, loss = 0.28183673\n",
            "Iteration 71, loss = 0.27945504\n",
            "Iteration 72, loss = 0.28199524\n",
            "Iteration 73, loss = 0.28011107\n",
            "Iteration 74, loss = 0.27991040\n",
            "Iteration 75, loss = 0.27899361\n",
            "Iteration 76, loss = 0.27986416\n",
            "Iteration 77, loss = 0.28022586\n",
            "Iteration 78, loss = 0.28304623\n",
            "Iteration 79, loss = 0.29041203\n",
            "Iteration 80, loss = 0.27897649\n",
            "Iteration 81, loss = 0.28186386\n",
            "Iteration 82, loss = 0.27702861\n",
            "Iteration 83, loss = 0.29279032\n",
            "Iteration 84, loss = 0.28986426\n",
            "Iteration 85, loss = 0.28900204\n",
            "Iteration 86, loss = 0.27934593\n",
            "Iteration 87, loss = 0.28564170\n",
            "Iteration 88, loss = 0.29153340\n",
            "Iteration 89, loss = 0.29528456\n",
            "Iteration 90, loss = 0.28440798\n",
            "Iteration 91, loss = 0.30185789\n",
            "Iteration 92, loss = 0.28312128\n",
            "Iteration 93, loss = 0.28347170\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.25692867\n",
            "Iteration 95, loss = 0.25682361\n",
            "Iteration 96, loss = 0.25672418\n",
            "Iteration 97, loss = 0.25670758\n",
            "Iteration 98, loss = 0.25655055\n",
            "Iteration 99, loss = 0.25647263\n",
            "Iteration 100, loss = 0.25646382\n",
            "Iteration 101, loss = 0.25635883\n",
            "Iteration 102, loss = 0.25629248\n",
            "Iteration 103, loss = 0.25624413\n",
            "Iteration 104, loss = 0.25614979\n",
            "Iteration 105, loss = 0.25611084\n",
            "Iteration 106, loss = 0.25605768\n",
            "Iteration 107, loss = 0.25597192\n",
            "Iteration 108, loss = 0.25588277\n",
            "Iteration 109, loss = 0.25586783\n",
            "Iteration 110, loss = 0.25577710\n",
            "Iteration 111, loss = 0.25572993\n",
            "Iteration 112, loss = 0.25567527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 113, loss = 0.25531341\n",
            "Iteration 114, loss = 0.25529921\n",
            "Iteration 115, loss = 0.25530542\n",
            "Iteration 116, loss = 0.25525812\n",
            "Iteration 117, loss = 0.25525893\n",
            "Iteration 118, loss = 0.25524436\n",
            "Iteration 119, loss = 0.25523521\n",
            "Iteration 120, loss = 0.25523291\n",
            "Iteration 121, loss = 0.25521243\n",
            "Iteration 122, loss = 0.25521355\n",
            "Iteration 123, loss = 0.25520362\n",
            "Iteration 124, loss = 0.25518684\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 125, loss = 0.25509060\n",
            "Iteration 126, loss = 0.25508279\n",
            "Iteration 127, loss = 0.25508815\n",
            "Iteration 128, loss = 0.25508296\n",
            "Iteration 129, loss = 0.25508101\n",
            "Iteration 130, loss = 0.25508011\n",
            "Iteration 131, loss = 0.25507035\n",
            "Iteration 132, loss = 0.25507473\n",
            "Iteration 133, loss = 0.25507218\n",
            "Iteration 134, loss = 0.25507110\n",
            "Iteration 135, loss = 0.25506999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63549089\n",
            "Iteration 3, loss = 0.60787309\n",
            "Iteration 4, loss = 0.59327434\n",
            "Iteration 5, loss = 0.58234994\n",
            "Iteration 6, loss = 0.57601339\n",
            "Iteration 7, loss = 0.57141406\n",
            "Iteration 8, loss = 0.56946269\n",
            "Iteration 9, loss = 0.56937230\n",
            "Iteration 10, loss = 0.57431275\n",
            "Iteration 11, loss = 0.57191042\n",
            "Iteration 12, loss = 0.57295695\n",
            "Iteration 13, loss = 0.57261786\n",
            "Iteration 14, loss = 0.58824218\n",
            "Iteration 15, loss = 0.58957708\n",
            "Iteration 16, loss = 0.57886820\n",
            "Iteration 17, loss = 0.57320487\n",
            "Iteration 18, loss = 0.58926775\n",
            "Iteration 19, loss = 0.61737669\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 20, loss = 0.61128720\n",
            "Iteration 21, loss = 0.59847103\n",
            "Iteration 22, loss = 0.58825927\n",
            "Iteration 23, loss = 0.58192557\n",
            "Iteration 24, loss = 0.57637078\n",
            "Iteration 25, loss = 0.57104064\n",
            "Iteration 26, loss = 0.56511207\n",
            "Iteration 27, loss = 0.55797501\n",
            "Iteration 28, loss = 0.55298845\n",
            "Iteration 29, loss = 0.55061732\n",
            "Iteration 30, loss = 0.55010602\n",
            "Iteration 31, loss = 0.57142832\n",
            "Iteration 32, loss = 0.55783821\n",
            "Iteration 33, loss = 0.55391648\n",
            "Iteration 34, loss = 0.55332781\n",
            "Iteration 35, loss = 0.54708296\n",
            "Iteration 36, loss = 0.54960851\n",
            "Iteration 37, loss = 0.54561357\n",
            "Iteration 38, loss = 0.54385784\n",
            "Iteration 39, loss = 0.54371548\n",
            "Iteration 40, loss = 0.54026238\n",
            "Iteration 41, loss = 0.53870954\n",
            "Iteration 42, loss = 0.53475237\n",
            "Iteration 43, loss = 0.53270669\n",
            "Iteration 44, loss = 0.53330360\n",
            "Iteration 45, loss = 0.53052643\n",
            "Iteration 46, loss = 0.53156502\n",
            "Iteration 47, loss = 0.52714019\n",
            "Iteration 48, loss = 0.52835186\n",
            "Iteration 49, loss = 0.52808851\n",
            "Iteration 50, loss = 0.52420378\n",
            "Iteration 51, loss = 0.52792422\n",
            "Iteration 52, loss = 0.52508844\n",
            "Iteration 53, loss = 0.52192790\n",
            "Iteration 54, loss = 0.52703017\n",
            "Iteration 55, loss = 0.52348368\n",
            "Iteration 56, loss = 0.52451883\n",
            "Iteration 57, loss = 0.52573258\n",
            "Iteration 58, loss = 0.52357844\n",
            "Iteration 59, loss = 0.52277887\n",
            "Iteration 60, loss = 0.52651429\n",
            "Iteration 61, loss = 0.52093096\n",
            "Iteration 62, loss = 0.52451936\n",
            "Iteration 63, loss = 0.52365579\n",
            "Iteration 64, loss = 0.52349473\n",
            "Iteration 65, loss = 0.52524967\n",
            "Iteration 66, loss = 0.52234223\n",
            "Iteration 67, loss = 0.52598275\n",
            "Iteration 68, loss = 0.52337049\n",
            "Iteration 69, loss = 0.52347771\n",
            "Iteration 70, loss = 0.52652578\n",
            "Iteration 71, loss = 0.52349598\n",
            "Iteration 72, loss = 0.52248328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.42271614\n",
            "Iteration 74, loss = 0.40626682\n",
            "Iteration 75, loss = 0.39304146\n",
            "Iteration 76, loss = 0.38139191\n",
            "Iteration 77, loss = 0.37096160\n",
            "Iteration 78, loss = 0.36177746\n",
            "Iteration 79, loss = 0.35385912\n",
            "Iteration 80, loss = 0.34720005\n",
            "Iteration 81, loss = 0.34088365\n",
            "Iteration 82, loss = 0.33558186\n",
            "Iteration 83, loss = 0.33125526\n",
            "Iteration 84, loss = 0.32734017\n",
            "Iteration 85, loss = 0.32379257\n",
            "Iteration 86, loss = 0.32081615\n",
            "Iteration 87, loss = 0.31769773\n",
            "Iteration 88, loss = 0.31536538\n",
            "Iteration 89, loss = 0.31353455\n",
            "Iteration 90, loss = 0.31124894\n",
            "Iteration 91, loss = 0.30992890\n",
            "Iteration 92, loss = 0.30872721\n",
            "Iteration 93, loss = 0.30649500\n",
            "Iteration 94, loss = 0.30562904\n",
            "Iteration 95, loss = 0.30385849\n",
            "Iteration 96, loss = 0.29150290\n",
            "Iteration 97, loss = 0.28576042\n",
            "Iteration 98, loss = 0.28355161\n",
            "Iteration 99, loss = 0.28269082\n",
            "Iteration 100, loss = 0.28059784\n",
            "Iteration 101, loss = 0.27914722\n",
            "Iteration 102, loss = 0.27810363\n",
            "Iteration 103, loss = 0.27715615\n",
            "Iteration 104, loss = 0.27542804\n",
            "Iteration 105, loss = 0.27530077\n",
            "Iteration 106, loss = 0.27396480\n",
            "Iteration 107, loss = 0.27313069\n",
            "Iteration 108, loss = 0.27313073\n",
            "Iteration 109, loss = 0.27312209\n",
            "Iteration 110, loss = 0.27146279\n",
            "Iteration 111, loss = 0.27122839\n",
            "Iteration 112, loss = 0.27079967\n",
            "Iteration 113, loss = 0.27374062\n",
            "Iteration 114, loss = 0.27106737\n",
            "Iteration 115, loss = 0.27102973\n",
            "Iteration 116, loss = 0.26970308\n",
            "Iteration 117, loss = 0.27024952\n",
            "Iteration 118, loss = 0.26896363\n",
            "Iteration 119, loss = 0.27101497\n",
            "Iteration 120, loss = 0.26891847\n",
            "Iteration 121, loss = 0.26901775\n",
            "Iteration 122, loss = 0.27555128\n",
            "Iteration 123, loss = 0.27036903\n",
            "Iteration 124, loss = 0.27984094\n",
            "Iteration 125, loss = 0.27068696\n",
            "Iteration 126, loss = 0.26996773\n",
            "Iteration 127, loss = 0.27077641\n",
            "Iteration 128, loss = 0.27064678\n",
            "Iteration 129, loss = 0.27994698\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 130, loss = 0.25726009\n",
            "Iteration 131, loss = 0.25713711\n",
            "Iteration 132, loss = 0.25710862\n",
            "Iteration 133, loss = 0.25706430\n",
            "Iteration 134, loss = 0.25706783\n",
            "Iteration 135, loss = 0.25692756\n",
            "Iteration 136, loss = 0.25684841\n",
            "Iteration 137, loss = 0.25682864\n",
            "Iteration 138, loss = 0.25681875\n",
            "Iteration 139, loss = 0.25671323\n",
            "Iteration 140, loss = 0.25672798\n",
            "Iteration 141, loss = 0.25663873\n",
            "Iteration 142, loss = 0.25656968\n",
            "Iteration 143, loss = 0.25658465\n",
            "Iteration 144, loss = 0.25652244\n",
            "Iteration 145, loss = 0.25646011\n",
            "Iteration 146, loss = 0.25642538\n",
            "Iteration 147, loss = 0.25635250\n",
            "Iteration 148, loss = 0.25628555\n",
            "Iteration 149, loss = 0.25632272\n",
            "Iteration 150, loss = 0.25616296\n",
            "Iteration 151, loss = 0.25622550\n",
            "Iteration 152, loss = 0.25619411\n",
            "Iteration 153, loss = 0.25612026\n",
            "Iteration 154, loss = 0.25602488\n",
            "Iteration 155, loss = 0.25602455\n",
            "Iteration 156, loss = 0.25595259\n",
            "Iteration 157, loss = 0.25594258\n",
            "Iteration 158, loss = 0.25590921\n",
            "Iteration 159, loss = 0.25588436\n",
            "Iteration 160, loss = 0.25579755\n",
            "Iteration 161, loss = 0.25577740\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 162, loss = 0.25548683\n",
            "Iteration 163, loss = 0.25548398\n",
            "Iteration 164, loss = 0.25543784\n",
            "Iteration 165, loss = 0.25544803\n",
            "Iteration 166, loss = 0.25542374\n",
            "Iteration 167, loss = 0.25540588\n",
            "Iteration 168, loss = 0.25541016\n",
            "Iteration 169, loss = 0.25540832\n",
            "Iteration 170, loss = 0.25539752\n",
            "Iteration 171, loss = 0.25542886\n",
            "Iteration 172, loss = 0.25539007\n",
            "Iteration 173, loss = 0.25535752\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 174, loss = 0.25528146\n",
            "Iteration 175, loss = 0.25528447\n",
            "Iteration 176, loss = 0.25527623\n",
            "Iteration 177, loss = 0.25527099\n",
            "Iteration 178, loss = 0.25527715\n",
            "Iteration 179, loss = 0.25526378\n",
            "Iteration 180, loss = 0.25526391\n",
            "Iteration 181, loss = 0.25526538\n",
            "Iteration 182, loss = 0.25526101\n",
            "Iteration 183, loss = 0.25526466\n",
            "Iteration 184, loss = 0.25527089\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63276732\n",
            "Iteration 3, loss = 0.60577415\n",
            "Iteration 4, loss = 0.58928480\n",
            "Iteration 5, loss = 0.58000394\n",
            "Iteration 6, loss = 0.57417174\n",
            "Iteration 7, loss = 0.57101679\n",
            "Iteration 8, loss = 0.56803738\n",
            "Iteration 9, loss = 0.60593975\n",
            "Iteration 10, loss = 0.60178314\n",
            "Iteration 11, loss = 0.58784010\n",
            "Iteration 12, loss = 0.57919292\n",
            "Iteration 13, loss = 0.57596970\n",
            "Iteration 14, loss = 0.57297066\n",
            "Iteration 15, loss = 0.57212559\n",
            "Iteration 16, loss = 0.57033480\n",
            "Iteration 17, loss = 0.57236076\n",
            "Iteration 18, loss = 0.57220390\n",
            "Iteration 19, loss = 0.57117328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 20, loss = 0.52938245\n",
            "Iteration 21, loss = 0.51067774\n",
            "Iteration 22, loss = 0.51626115\n",
            "Iteration 23, loss = 0.52759442\n",
            "Iteration 24, loss = 0.52745762\n",
            "Iteration 25, loss = 0.52548768\n",
            "Iteration 26, loss = 0.52645938\n",
            "Iteration 27, loss = 0.51557427\n",
            "Iteration 28, loss = 0.52375069\n",
            "Iteration 29, loss = 0.51819610\n",
            "Iteration 30, loss = 0.52375357\n",
            "Iteration 31, loss = 0.52097079\n",
            "Iteration 32, loss = 0.52006345\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 33, loss = 0.42927160\n",
            "Iteration 34, loss = 0.41516294\n",
            "Iteration 35, loss = 0.40267089\n",
            "Iteration 36, loss = 0.39089866\n",
            "Iteration 37, loss = 0.38000232\n",
            "Iteration 38, loss = 0.37011855\n",
            "Iteration 39, loss = 0.36113432\n",
            "Iteration 40, loss = 0.35266797\n",
            "Iteration 41, loss = 0.34466187\n",
            "Iteration 42, loss = 0.33778278\n",
            "Iteration 43, loss = 0.33144438\n",
            "Iteration 44, loss = 0.32555458\n",
            "Iteration 45, loss = 0.32012681\n",
            "Iteration 46, loss = 0.31518948\n",
            "Iteration 47, loss = 0.31068323\n",
            "Iteration 48, loss = 0.30639874\n",
            "Iteration 49, loss = 0.30323498\n",
            "Iteration 50, loss = 0.30011317\n",
            "Iteration 51, loss = 0.29654578\n",
            "Iteration 52, loss = 0.29434707\n",
            "Iteration 53, loss = 0.29100166\n",
            "Iteration 54, loss = 0.28955610\n",
            "Iteration 55, loss = 0.28794847\n",
            "Iteration 56, loss = 0.28538161\n",
            "Iteration 57, loss = 0.28447069\n",
            "Iteration 58, loss = 0.28345351\n",
            "Iteration 59, loss = 0.28241377\n",
            "Iteration 60, loss = 0.28130654\n",
            "Iteration 61, loss = 0.28069648\n",
            "Iteration 62, loss = 0.27768571\n",
            "Iteration 63, loss = 0.27854471\n",
            "Iteration 64, loss = 0.27751122\n",
            "Iteration 65, loss = 0.27762975\n",
            "Iteration 66, loss = 0.27828210\n",
            "Iteration 67, loss = 0.27643901\n",
            "Iteration 68, loss = 0.27789536\n",
            "Iteration 69, loss = 0.27348046\n",
            "Iteration 70, loss = 0.27890721\n",
            "Iteration 71, loss = 0.28102575\n",
            "Iteration 72, loss = 0.27788751\n",
            "Iteration 73, loss = 0.28050019\n",
            "Iteration 74, loss = 0.27934345\n",
            "Iteration 75, loss = 0.28287092\n",
            "Iteration 76, loss = 0.27826096\n",
            "Iteration 77, loss = 0.28483082\n",
            "Iteration 78, loss = 0.27857721\n",
            "Iteration 79, loss = 0.27857819\n",
            "Iteration 80, loss = 0.28106514\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 81, loss = 0.25644646\n",
            "Iteration 82, loss = 0.25624835\n",
            "Iteration 83, loss = 0.25619560\n",
            "Iteration 84, loss = 0.25609376\n",
            "Iteration 85, loss = 0.25600384\n",
            "Iteration 86, loss = 0.25586890\n",
            "Iteration 87, loss = 0.25583504\n",
            "Iteration 88, loss = 0.25582440\n",
            "Iteration 89, loss = 0.25566063\n",
            "Iteration 90, loss = 0.25560799\n",
            "Iteration 91, loss = 0.25554580\n",
            "Iteration 92, loss = 0.25548605\n",
            "Iteration 93, loss = 0.25540585\n",
            "Iteration 94, loss = 0.25531565\n",
            "Iteration 95, loss = 0.25529656\n",
            "Iteration 96, loss = 0.25517812\n",
            "Iteration 97, loss = 0.25517013\n",
            "Iteration 98, loss = 0.25505188\n",
            "Iteration 99, loss = 0.25498503\n",
            "Iteration 100, loss = 0.25498306\n",
            "Iteration 101, loss = 0.25483374\n",
            "Iteration 102, loss = 0.25475990\n",
            "Iteration 103, loss = 0.25472884\n",
            "Iteration 104, loss = 0.25464120\n",
            "Iteration 105, loss = 0.25462307\n",
            "Iteration 106, loss = 0.25456374\n",
            "Iteration 107, loss = 0.25445891\n",
            "Iteration 108, loss = 0.25444586\n",
            "Iteration 109, loss = 0.25435665\n",
            "Iteration 110, loss = 0.25436052\n",
            "Iteration 111, loss = 0.25418523\n",
            "Iteration 112, loss = 0.25418178\n",
            "Iteration 113, loss = 0.25413550\n",
            "Iteration 114, loss = 0.25410142\n",
            "Iteration 115, loss = 0.25405344\n",
            "Iteration 116, loss = 0.25394424\n",
            "Iteration 117, loss = 0.25391298\n",
            "Iteration 118, loss = 0.25382503\n",
            "Iteration 119, loss = 0.25381450\n",
            "Iteration 120, loss = 0.25377829\n",
            "Iteration 121, loss = 0.25370299\n",
            "Iteration 122, loss = 0.25367332\n",
            "Iteration 123, loss = 0.25363741\n",
            "Iteration 124, loss = 0.25356452\n",
            "Iteration 125, loss = 0.25351979\n",
            "Iteration 126, loss = 0.25342259\n",
            "Iteration 127, loss = 0.25345578\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 128, loss = 0.25310519\n",
            "Iteration 129, loss = 0.25306831\n",
            "Iteration 130, loss = 0.25306424\n",
            "Iteration 131, loss = 0.25308638\n",
            "Iteration 132, loss = 0.25305471\n",
            "Iteration 133, loss = 0.25304373\n",
            "Iteration 134, loss = 0.25302427\n",
            "Iteration 135, loss = 0.25305978\n",
            "Iteration 136, loss = 0.25305943\n",
            "Iteration 137, loss = 0.25300718\n",
            "Iteration 138, loss = 0.25300868\n",
            "Iteration 139, loss = 0.25300354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 140, loss = 0.25289970\n",
            "Iteration 141, loss = 0.25288309\n",
            "Iteration 142, loss = 0.25287733\n",
            "Iteration 143, loss = 0.25288820\n",
            "Iteration 144, loss = 0.25289148\n",
            "Iteration 145, loss = 0.25289221\n",
            "Iteration 146, loss = 0.25288414\n",
            "Iteration 147, loss = 0.25288241\n",
            "Iteration 148, loss = 0.25288328\n",
            "Iteration 149, loss = 0.25286587\n",
            "Iteration 150, loss = 0.25287500\n",
            "Iteration 151, loss = 0.25287139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64653436\n",
            "Iteration 3, loss = 0.62289810\n",
            "Iteration 4, loss = 0.60121543\n",
            "Iteration 5, loss = 0.58975873\n",
            "Iteration 6, loss = 0.58143961\n",
            "Iteration 7, loss = 0.57610372\n",
            "Iteration 8, loss = 0.57598443\n",
            "Iteration 9, loss = 0.57223348\n",
            "Iteration 10, loss = 0.57132880\n",
            "Iteration 11, loss = 0.57122504\n",
            "Iteration 12, loss = 0.57166961\n",
            "Iteration 13, loss = 0.57228755\n",
            "Iteration 14, loss = 0.57239348\n",
            "Iteration 15, loss = 0.57260316\n",
            "Iteration 16, loss = 0.57170656\n",
            "Iteration 17, loss = 0.57232159\n",
            "Iteration 18, loss = 0.57307482\n",
            "Iteration 19, loss = 0.57104191\n",
            "Iteration 20, loss = 0.57331609\n",
            "Iteration 21, loss = 0.57261806\n",
            "Iteration 22, loss = 0.57230965\n",
            "Iteration 23, loss = 0.57289789\n",
            "Iteration 24, loss = 0.57490646\n",
            "Iteration 25, loss = 0.57384473\n",
            "Iteration 26, loss = 0.57364055\n",
            "Iteration 27, loss = 0.57381238\n",
            "Iteration 28, loss = 0.57246657\n",
            "Iteration 29, loss = 0.57270896\n",
            "Iteration 30, loss = 0.57453102\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 31, loss = 0.51879478\n",
            "Iteration 32, loss = 0.49905238\n",
            "Iteration 33, loss = 0.51946297\n",
            "Iteration 34, loss = 0.52323676\n",
            "Iteration 35, loss = 0.52097737\n",
            "Iteration 36, loss = 0.52225708\n",
            "Iteration 37, loss = 0.52230252\n",
            "Iteration 38, loss = 0.52269349\n",
            "Iteration 39, loss = 0.52115767\n",
            "Iteration 40, loss = 0.51884356\n",
            "Iteration 41, loss = 0.52474635\n",
            "Iteration 42, loss = 0.52232362\n",
            "Iteration 43, loss = 0.51935033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 44, loss = 0.43364307\n",
            "Iteration 45, loss = 0.41992986\n",
            "Iteration 46, loss = 0.40797427\n",
            "Iteration 47, loss = 0.39724737\n",
            "Iteration 48, loss = 0.38719706\n",
            "Iteration 49, loss = 0.37828084\n",
            "Iteration 50, loss = 0.37030240\n",
            "Iteration 51, loss = 0.36341853\n",
            "Iteration 52, loss = 0.35696347\n",
            "Iteration 53, loss = 0.35147167\n",
            "Iteration 54, loss = 0.34642456\n",
            "Iteration 55, loss = 0.34198779\n",
            "Iteration 56, loss = 0.33868655\n",
            "Iteration 57, loss = 0.33526087\n",
            "Iteration 58, loss = 0.33205481\n",
            "Iteration 59, loss = 0.32971763\n",
            "Iteration 60, loss = 0.32748832\n",
            "Iteration 61, loss = 0.32643541\n",
            "Iteration 62, loss = 0.32386496\n",
            "Iteration 63, loss = 0.32372264\n",
            "Iteration 64, loss = 0.32261378\n",
            "Iteration 65, loss = 0.32083406\n",
            "Iteration 66, loss = 0.32130937\n",
            "Iteration 67, loss = 0.32056869\n",
            "Iteration 68, loss = 0.31915308\n",
            "Iteration 69, loss = 0.32181142\n",
            "Iteration 70, loss = 0.32002417\n",
            "Iteration 71, loss = 0.32044975\n",
            "Iteration 72, loss = 0.31919353\n",
            "Iteration 73, loss = 0.31903003\n",
            "Iteration 74, loss = 0.31886991\n",
            "Iteration 75, loss = 0.32021932\n",
            "Iteration 76, loss = 0.31990896\n",
            "Iteration 77, loss = 0.32190205\n",
            "Iteration 78, loss = 0.32205250\n",
            "Iteration 79, loss = 0.32210395\n",
            "Iteration 80, loss = 0.32170928\n",
            "Iteration 81, loss = 0.32068499\n",
            "Iteration 82, loss = 0.32128441\n",
            "Iteration 83, loss = 0.32325044\n",
            "Iteration 84, loss = 0.32066665\n",
            "Iteration 85, loss = 0.31658900\n",
            "Iteration 86, loss = 0.32663503\n",
            "Iteration 87, loss = 0.31985165\n",
            "Iteration 88, loss = 0.32614205\n",
            "Iteration 89, loss = 0.32026789\n",
            "Iteration 90, loss = 0.32139160\n",
            "Iteration 91, loss = 0.32350119\n",
            "Iteration 92, loss = 0.32173483\n",
            "Iteration 93, loss = 0.31865151\n",
            "Iteration 94, loss = 0.32202929\n",
            "Iteration 95, loss = 0.31859016\n",
            "Iteration 96, loss = 0.31706875\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.28202207\n",
            "Iteration 98, loss = 0.28157553\n",
            "Iteration 99, loss = 0.28139075\n",
            "Iteration 100, loss = 0.28116802\n",
            "Iteration 101, loss = 0.28105943\n",
            "Iteration 102, loss = 0.28092001\n",
            "Iteration 103, loss = 0.28078748\n",
            "Iteration 104, loss = 0.28073589\n",
            "Iteration 105, loss = 0.28051687\n",
            "Iteration 106, loss = 0.28045687\n",
            "Iteration 107, loss = 0.28031812\n",
            "Iteration 108, loss = 0.28019526\n",
            "Iteration 109, loss = 0.28011615\n",
            "Iteration 110, loss = 0.28000178\n",
            "Iteration 111, loss = 0.27990866\n",
            "Iteration 112, loss = 0.27977496\n",
            "Iteration 113, loss = 0.27962764\n",
            "Iteration 114, loss = 0.27951529\n",
            "Iteration 115, loss = 0.27942872\n",
            "Iteration 116, loss = 0.27931598\n",
            "Iteration 117, loss = 0.27920971\n",
            "Iteration 118, loss = 0.27912193\n",
            "Iteration 119, loss = 0.27903663\n",
            "Iteration 120, loss = 0.27889488\n",
            "Iteration 121, loss = 0.27877125\n",
            "Iteration 122, loss = 0.27865274\n",
            "Iteration 123, loss = 0.27854947\n",
            "Iteration 124, loss = 0.27847226\n",
            "Iteration 125, loss = 0.27831680\n",
            "Iteration 126, loss = 0.27825451\n",
            "Iteration 127, loss = 0.27814092\n",
            "Iteration 128, loss = 0.27805747\n",
            "Iteration 129, loss = 0.27793357\n",
            "Iteration 130, loss = 0.27785975\n",
            "Iteration 131, loss = 0.27773446\n",
            "Iteration 132, loss = 0.27764911\n",
            "Iteration 133, loss = 0.27752226\n",
            "Iteration 134, loss = 0.27742572\n",
            "Iteration 135, loss = 0.27736677\n",
            "Iteration 136, loss = 0.27721043\n",
            "Iteration 137, loss = 0.27712307\n",
            "Iteration 138, loss = 0.27704074\n",
            "Iteration 139, loss = 0.27693848\n",
            "Iteration 140, loss = 0.27688568\n",
            "Iteration 141, loss = 0.27680321\n",
            "Iteration 142, loss = 0.27667434\n",
            "Iteration 143, loss = 0.27660485\n",
            "Iteration 144, loss = 0.27651993\n",
            "Iteration 145, loss = 0.27640257\n",
            "Iteration 146, loss = 0.27635388\n",
            "Iteration 147, loss = 0.27623133\n",
            "Iteration 148, loss = 0.27611695\n",
            "Iteration 149, loss = 0.27608664\n",
            "Iteration 150, loss = 0.27593760\n",
            "Iteration 151, loss = 0.27590414\n",
            "Iteration 152, loss = 0.27584803\n",
            "Iteration 153, loss = 0.27571244\n",
            "Iteration 154, loss = 0.27561126\n",
            "Iteration 155, loss = 0.27549058\n",
            "Iteration 156, loss = 0.27548784\n",
            "Iteration 157, loss = 0.27535937\n",
            "Iteration 158, loss = 0.27531207\n",
            "Iteration 159, loss = 0.27524587\n",
            "Iteration 160, loss = 0.27509507\n",
            "Iteration 161, loss = 0.27502050\n",
            "Iteration 162, loss = 0.27503247\n",
            "Iteration 163, loss = 0.27488144\n",
            "Iteration 164, loss = 0.27483870\n",
            "Iteration 165, loss = 0.27472468\n",
            "Iteration 166, loss = 0.27463961\n",
            "Iteration 167, loss = 0.27457304\n",
            "Iteration 168, loss = 0.27441532\n",
            "Iteration 169, loss = 0.27441337\n",
            "Iteration 170, loss = 0.27437513\n",
            "Iteration 171, loss = 0.27419734\n",
            "Iteration 172, loss = 0.27420686\n",
            "Iteration 173, loss = 0.27408758\n",
            "Iteration 174, loss = 0.27406206\n",
            "Iteration 175, loss = 0.27392184\n",
            "Iteration 176, loss = 0.27398536\n",
            "Iteration 177, loss = 0.27378953\n",
            "Iteration 178, loss = 0.27370452\n",
            "Iteration 179, loss = 0.27364573\n",
            "Iteration 180, loss = 0.27359246\n",
            "Iteration 181, loss = 0.27353523\n",
            "Iteration 182, loss = 0.27340094\n",
            "Iteration 183, loss = 0.27337109\n",
            "Iteration 184, loss = 0.27328048\n",
            "Iteration 185, loss = 0.27323601\n",
            "Iteration 186, loss = 0.27315319\n",
            "Iteration 187, loss = 0.27306067\n",
            "Iteration 188, loss = 0.27298427\n",
            "Iteration 189, loss = 0.27294546\n",
            "Iteration 190, loss = 0.27285513\n",
            "Iteration 191, loss = 0.27281700\n",
            "Iteration 192, loss = 0.27270010\n",
            "Iteration 193, loss = 0.27268466\n",
            "Iteration 194, loss = 0.27264383\n",
            "Iteration 195, loss = 0.27252102\n",
            "Iteration 196, loss = 0.27248225\n",
            "Iteration 197, loss = 0.27235158\n",
            "Iteration 198, loss = 0.27232632\n",
            "Iteration 199, loss = 0.27224896\n",
            "Iteration 200, loss = 0.27226329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63021338\n",
            "Iteration 3, loss = 0.60490767\n",
            "Iteration 4, loss = 0.59365591\n",
            "Iteration 5, loss = 0.58462669\n",
            "Iteration 6, loss = 0.57655930\n",
            "Iteration 7, loss = 0.57220401\n",
            "Iteration 8, loss = 0.57080764\n",
            "Iteration 9, loss = 0.56877387\n",
            "Iteration 10, loss = 0.56757690\n",
            "Iteration 11, loss = 0.56859774\n",
            "Iteration 12, loss = 0.56850448\n",
            "Iteration 13, loss = 0.56756530\n",
            "Iteration 14, loss = 0.56759773\n",
            "Iteration 15, loss = 0.56942702\n",
            "Iteration 16, loss = 0.56797762\n",
            "Iteration 17, loss = 0.56901770\n",
            "Iteration 18, loss = 0.57186505\n",
            "Iteration 19, loss = 0.56995861\n",
            "Iteration 20, loss = 0.56732695\n",
            "Iteration 21, loss = 0.56695114\n",
            "Iteration 22, loss = 0.56900080\n",
            "Iteration 23, loss = 0.56811785\n",
            "Iteration 24, loss = 0.56799762\n",
            "Iteration 25, loss = 0.56604487\n",
            "Iteration 26, loss = 0.56707906\n",
            "Iteration 27, loss = 0.56764246\n",
            "Iteration 28, loss = 0.56929511\n",
            "Iteration 29, loss = 0.56705742\n",
            "Iteration 30, loss = 0.56645827\n",
            "Iteration 31, loss = 0.56832892\n",
            "Iteration 32, loss = 0.56793323\n",
            "Iteration 33, loss = 0.56785789\n",
            "Iteration 34, loss = 0.56665097\n",
            "Iteration 35, loss = 0.56988993\n",
            "Iteration 36, loss = 0.56750615\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 37, loss = 0.50773889\n",
            "Iteration 38, loss = 0.48638920\n",
            "Iteration 39, loss = 0.51556794\n",
            "Iteration 40, loss = 0.51474995\n",
            "Iteration 41, loss = 0.51309843\n",
            "Iteration 42, loss = 0.51480882\n",
            "Iteration 43, loss = 0.51590674\n",
            "Iteration 44, loss = 0.51620202\n",
            "Iteration 45, loss = 0.51432992\n",
            "Iteration 46, loss = 0.50685083\n",
            "Iteration 47, loss = 0.51268099\n",
            "Iteration 48, loss = 0.51200393\n",
            "Iteration 49, loss = 0.51517252\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 50, loss = 0.41177752\n",
            "Iteration 51, loss = 0.39652168\n",
            "Iteration 52, loss = 0.38308813\n",
            "Iteration 53, loss = 0.37099866\n",
            "Iteration 54, loss = 0.36004847\n",
            "Iteration 55, loss = 0.35016747\n",
            "Iteration 56, loss = 0.34147608\n",
            "Iteration 57, loss = 0.33344828\n",
            "Iteration 58, loss = 0.32630874\n",
            "Iteration 59, loss = 0.31970334\n",
            "Iteration 60, loss = 0.31403391\n",
            "Iteration 61, loss = 0.30923570\n",
            "Iteration 62, loss = 0.30389130\n",
            "Iteration 63, loss = 0.29951350\n",
            "Iteration 64, loss = 0.29578719\n",
            "Iteration 65, loss = 0.29271926\n",
            "Iteration 66, loss = 0.28941548\n",
            "Iteration 67, loss = 0.28631786\n",
            "Iteration 68, loss = 0.28377821\n",
            "Iteration 69, loss = 0.28139270\n",
            "Iteration 70, loss = 0.27941415\n",
            "Iteration 71, loss = 0.27762887\n",
            "Iteration 72, loss = 0.27534766\n",
            "Iteration 73, loss = 0.27393766\n",
            "Iteration 74, loss = 0.27245271\n",
            "Iteration 75, loss = 0.27124399\n",
            "Iteration 76, loss = 0.26989563\n",
            "Iteration 77, loss = 0.26855210\n",
            "Iteration 78, loss = 0.26801137\n",
            "Iteration 79, loss = 0.26705735\n",
            "Iteration 80, loss = 0.26625629\n",
            "Iteration 81, loss = 0.26541122\n",
            "Iteration 82, loss = 0.26614874\n",
            "Iteration 83, loss = 0.26444684\n",
            "Iteration 84, loss = 0.26294683\n",
            "Iteration 85, loss = 0.26297720\n",
            "Iteration 86, loss = 0.26289514\n",
            "Iteration 87, loss = 0.26457989\n",
            "Iteration 88, loss = 0.26310286\n",
            "Iteration 89, loss = 0.26282842\n",
            "Iteration 90, loss = 0.26263729\n",
            "Iteration 91, loss = 0.26090639\n",
            "Iteration 92, loss = 0.26227651\n",
            "Iteration 93, loss = 0.26202834\n",
            "Iteration 94, loss = 0.25983069\n",
            "Iteration 95, loss = 0.26158362\n",
            "Iteration 96, loss = 0.26328868\n",
            "Iteration 97, loss = 0.26401612\n",
            "Iteration 98, loss = 0.26318930\n",
            "Iteration 99, loss = 0.26024276\n",
            "Iteration 100, loss = 0.26372640\n",
            "Iteration 101, loss = 0.26462983\n",
            "Iteration 102, loss = 0.26670017\n",
            "Iteration 103, loss = 0.26213790\n",
            "Iteration 104, loss = 0.26579210\n",
            "Iteration 105, loss = 0.26799187\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 106, loss = 0.24969902\n",
            "Iteration 107, loss = 0.24952003\n",
            "Iteration 108, loss = 0.24946890\n",
            "Iteration 109, loss = 0.24938694\n",
            "Iteration 110, loss = 0.24936705\n",
            "Iteration 111, loss = 0.24937562\n",
            "Iteration 112, loss = 0.24931842\n",
            "Iteration 113, loss = 0.24924259\n",
            "Iteration 114, loss = 0.24923495\n",
            "Iteration 115, loss = 0.24916444\n",
            "Iteration 116, loss = 0.24914473\n",
            "Iteration 117, loss = 0.24910959\n",
            "Iteration 118, loss = 0.24911258\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 119, loss = 0.24880800\n",
            "Iteration 120, loss = 0.24877681\n",
            "Iteration 121, loss = 0.24877113\n",
            "Iteration 122, loss = 0.24873649\n",
            "Iteration 123, loss = 0.24874677\n",
            "Iteration 124, loss = 0.24875906\n",
            "Iteration 125, loss = 0.24872117\n",
            "Iteration 126, loss = 0.24872686\n",
            "Iteration 127, loss = 0.24870363\n",
            "Iteration 128, loss = 0.24874034\n",
            "Iteration 129, loss = 0.24871944\n",
            "Iteration 130, loss = 0.24868314\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 131, loss = 0.24860055\n",
            "Iteration 132, loss = 0.24860977\n",
            "Iteration 133, loss = 0.24860181\n",
            "Iteration 134, loss = 0.24860058\n",
            "Iteration 135, loss = 0.24860748\n",
            "Iteration 136, loss = 0.24859812\n",
            "Iteration 137, loss = 0.24859504\n",
            "Iteration 138, loss = 0.24858818\n",
            "Iteration 139, loss = 0.24859636\n",
            "Iteration 140, loss = 0.24859578\n",
            "Iteration 141, loss = 0.24860618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62598916\n",
            "Iteration 3, loss = 0.60074932\n",
            "Iteration 4, loss = 0.58674634\n",
            "Iteration 5, loss = 0.57691110\n",
            "Iteration 6, loss = 0.57333614\n",
            "Iteration 7, loss = 0.57045130\n",
            "Iteration 8, loss = 0.56788323\n",
            "Iteration 9, loss = 0.56653051\n",
            "Iteration 10, loss = 0.56554525\n",
            "Iteration 11, loss = 0.56650394\n",
            "Iteration 12, loss = 0.56610658\n",
            "Iteration 13, loss = 0.56595552\n",
            "Iteration 14, loss = 0.56689983\n",
            "Iteration 15, loss = 0.59272371\n",
            "Iteration 16, loss = 0.62132075\n",
            "Iteration 17, loss = 0.60970943\n",
            "Iteration 18, loss = 0.58619611\n",
            "Iteration 19, loss = 0.57465708\n",
            "Iteration 20, loss = 0.57136369\n",
            "Iteration 21, loss = 0.56689441\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 22, loss = 0.53122348\n",
            "Iteration 23, loss = 0.51317481\n",
            "Iteration 24, loss = 0.50790536\n",
            "Iteration 25, loss = 0.51914505\n",
            "Iteration 26, loss = 0.52320385\n",
            "Iteration 27, loss = 0.52103295\n",
            "Iteration 28, loss = 0.51939920\n",
            "Iteration 29, loss = 0.51657585\n",
            "Iteration 30, loss = 0.51792207\n",
            "Iteration 31, loss = 0.51359506\n",
            "Iteration 32, loss = 0.51443587\n",
            "Iteration 33, loss = 0.51539870\n",
            "Iteration 34, loss = 0.51544208\n",
            "Iteration 35, loss = 0.51869913\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.42405590\n",
            "Iteration 37, loss = 0.41036295\n",
            "Iteration 38, loss = 0.39792324\n",
            "Iteration 39, loss = 0.38604301\n",
            "Iteration 40, loss = 0.37531163\n",
            "Iteration 41, loss = 0.36529841\n",
            "Iteration 42, loss = 0.35633359\n",
            "Iteration 43, loss = 0.34787967\n",
            "Iteration 44, loss = 0.34001797\n",
            "Iteration 45, loss = 0.33314312\n",
            "Iteration 46, loss = 0.32711735\n",
            "Iteration 47, loss = 0.32092888\n",
            "Iteration 48, loss = 0.31605346\n",
            "Iteration 49, loss = 0.31120913\n",
            "Iteration 50, loss = 0.30675524\n",
            "Iteration 51, loss = 0.30276705\n",
            "Iteration 52, loss = 0.29892845\n",
            "Iteration 53, loss = 0.29631004\n",
            "Iteration 54, loss = 0.29285927\n",
            "Iteration 55, loss = 0.29038796\n",
            "Iteration 56, loss = 0.28796212\n",
            "Iteration 57, loss = 0.28532442\n",
            "Iteration 58, loss = 0.28383066\n",
            "Iteration 59, loss = 0.28168900\n",
            "Iteration 60, loss = 0.28002333\n",
            "Iteration 61, loss = 0.27924263\n",
            "Iteration 62, loss = 0.27618258\n",
            "Iteration 63, loss = 0.27631289\n",
            "Iteration 64, loss = 0.27418212\n",
            "Iteration 65, loss = 0.27488010\n",
            "Iteration 66, loss = 0.27442699\n",
            "Iteration 67, loss = 0.27343885\n",
            "Iteration 68, loss = 0.27303049\n",
            "Iteration 69, loss = 0.27337217\n",
            "Iteration 70, loss = 0.27267140\n",
            "Iteration 71, loss = 0.27200265\n",
            "Iteration 72, loss = 0.27075373\n",
            "Iteration 73, loss = 0.27089326\n",
            "Iteration 74, loss = 0.27495591\n",
            "Iteration 75, loss = 0.27010391\n",
            "Iteration 76, loss = 0.27220994\n",
            "Iteration 77, loss = 0.26822595\n",
            "Iteration 78, loss = 0.27247399\n",
            "Iteration 79, loss = 0.27882698\n",
            "Iteration 80, loss = 0.28417635\n",
            "Iteration 81, loss = 0.27041535\n",
            "Iteration 82, loss = 0.27584888\n",
            "Iteration 83, loss = 0.27333736\n",
            "Iteration 84, loss = 0.26999670\n",
            "Iteration 85, loss = 0.26691501\n",
            "Iteration 86, loss = 0.28647792\n",
            "Iteration 87, loss = 0.28803266\n",
            "Iteration 88, loss = 0.27257415\n",
            "Iteration 89, loss = 0.26603058\n",
            "Iteration 90, loss = 0.28952512\n",
            "Iteration 91, loss = 0.28566292\n",
            "Iteration 92, loss = 0.28014411\n",
            "Iteration 93, loss = 0.28406135\n",
            "Iteration 94, loss = 0.27141721\n",
            "Iteration 95, loss = 0.28326197\n",
            "Iteration 96, loss = 0.28690527\n",
            "Iteration 97, loss = 0.27826848\n",
            "Iteration 98, loss = 0.27381059\n",
            "Iteration 99, loss = 0.27810745\n",
            "Iteration 100, loss = 0.28088128\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.25118909\n",
            "Iteration 102, loss = 0.25111980\n",
            "Iteration 103, loss = 0.25104802\n",
            "Iteration 104, loss = 0.25099574\n",
            "Iteration 105, loss = 0.25097263\n",
            "Iteration 106, loss = 0.25094081\n",
            "Iteration 107, loss = 0.25090728\n",
            "Iteration 108, loss = 0.25080110\n",
            "Iteration 109, loss = 0.25083103\n",
            "Iteration 110, loss = 0.25081495\n",
            "Iteration 111, loss = 0.25074165\n",
            "Iteration 112, loss = 0.25069061\n",
            "Iteration 113, loss = 0.25071272\n",
            "Iteration 114, loss = 0.25056222\n",
            "Iteration 115, loss = 0.25063821\n",
            "Iteration 116, loss = 0.25059379\n",
            "Iteration 117, loss = 0.25053095\n",
            "Iteration 118, loss = 0.25049545\n",
            "Iteration 119, loss = 0.25044211\n",
            "Iteration 120, loss = 0.25047995\n",
            "Iteration 121, loss = 0.25040894\n",
            "Iteration 122, loss = 0.25033986\n",
            "Iteration 123, loss = 0.25029896\n",
            "Iteration 124, loss = 0.25024514\n",
            "Iteration 125, loss = 0.25026914\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 126, loss = 0.24999393\n",
            "Iteration 127, loss = 0.24994081\n",
            "Iteration 128, loss = 0.24994738\n",
            "Iteration 129, loss = 0.24993527\n",
            "Iteration 130, loss = 0.24995532\n",
            "Iteration 131, loss = 0.24991389\n",
            "Iteration 132, loss = 0.24994158\n",
            "Iteration 133, loss = 0.24993855\n",
            "Iteration 134, loss = 0.24992439\n",
            "Iteration 135, loss = 0.24992997\n",
            "Iteration 136, loss = 0.24988267\n",
            "Iteration 137, loss = 0.24990472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 138, loss = 0.24981253\n",
            "Iteration 139, loss = 0.24980212\n",
            "Iteration 140, loss = 0.24979925\n",
            "Iteration 141, loss = 0.24980348\n",
            "Iteration 142, loss = 0.24980005\n",
            "Iteration 143, loss = 0.24979489\n",
            "Iteration 144, loss = 0.24980958\n",
            "Iteration 145, loss = 0.24978770\n",
            "Iteration 146, loss = 0.24979689\n",
            "Iteration 147, loss = 0.24979986\n",
            "Iteration 148, loss = 0.24979660\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63150479\n",
            "Iteration 3, loss = 0.60686135\n",
            "Iteration 4, loss = 0.59036545\n",
            "Iteration 5, loss = 0.58127064\n",
            "Iteration 6, loss = 0.57593142\n",
            "Iteration 7, loss = 0.57571343\n",
            "Iteration 8, loss = 0.57192607\n",
            "Iteration 9, loss = 0.57025572\n",
            "Iteration 10, loss = 0.56883780\n",
            "Iteration 11, loss = 0.56794514\n",
            "Iteration 12, loss = 0.56795677\n",
            "Iteration 13, loss = 0.56803990\n",
            "Iteration 14, loss = 0.56718977\n",
            "Iteration 15, loss = 0.56716994\n",
            "Iteration 16, loss = 0.56794650\n",
            "Iteration 17, loss = 0.56702350\n",
            "Iteration 18, loss = 0.56881871\n",
            "Iteration 19, loss = 0.56773767\n",
            "Iteration 20, loss = 0.56917061\n",
            "Iteration 21, loss = 0.56752779\n",
            "Iteration 22, loss = 0.56993435\n",
            "Iteration 23, loss = 0.56853222\n",
            "Iteration 24, loss = 0.56983948\n",
            "Iteration 25, loss = 0.56706799\n",
            "Iteration 26, loss = 0.56894310\n",
            "Iteration 27, loss = 0.56937315\n",
            "Iteration 28, loss = 0.56799010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 29, loss = 0.50714696\n",
            "Iteration 30, loss = 0.48727694\n",
            "Iteration 31, loss = 0.51733081\n",
            "Iteration 32, loss = 0.51351050\n",
            "Iteration 33, loss = 0.51606269\n",
            "Iteration 34, loss = 0.51514480\n",
            "Iteration 35, loss = 0.51391127\n",
            "Iteration 36, loss = 0.51329601\n",
            "Iteration 37, loss = 0.52077397\n",
            "Iteration 38, loss = 0.50950541\n",
            "Iteration 39, loss = 0.51304420\n",
            "Iteration 40, loss = 0.51577697\n",
            "Iteration 41, loss = 0.51373940\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 42, loss = 0.41274177\n",
            "Iteration 43, loss = 0.39797977\n",
            "Iteration 44, loss = 0.38476332\n",
            "Iteration 45, loss = 0.37284606\n",
            "Iteration 46, loss = 0.36187218\n",
            "Iteration 47, loss = 0.35185504\n",
            "Iteration 48, loss = 0.34303770\n",
            "Iteration 49, loss = 0.33524479\n",
            "Iteration 50, loss = 0.32827708\n",
            "Iteration 51, loss = 0.32151858\n",
            "Iteration 52, loss = 0.31584782\n",
            "Iteration 53, loss = 0.31027372\n",
            "Iteration 54, loss = 0.30577385\n",
            "Iteration 55, loss = 0.30149829\n",
            "Iteration 56, loss = 0.29756180\n",
            "Iteration 57, loss = 0.29402826\n",
            "Iteration 58, loss = 0.29106868\n",
            "Iteration 59, loss = 0.28794669\n",
            "Iteration 60, loss = 0.28513538\n",
            "Iteration 61, loss = 0.28288445\n",
            "Iteration 62, loss = 0.28153213\n",
            "Iteration 63, loss = 0.27922423\n",
            "Iteration 64, loss = 0.27700339\n",
            "Iteration 65, loss = 0.27521785\n",
            "Iteration 66, loss = 0.27421525\n",
            "Iteration 67, loss = 0.27328589\n",
            "Iteration 68, loss = 0.27132855\n",
            "Iteration 69, loss = 0.27083128\n",
            "Iteration 70, loss = 0.26938737\n",
            "Iteration 71, loss = 0.26863922\n",
            "Iteration 72, loss = 0.26825934\n",
            "Iteration 73, loss = 0.26710768\n",
            "Iteration 74, loss = 0.26601315\n",
            "Iteration 75, loss = 0.26628022\n",
            "Iteration 76, loss = 0.26555398\n",
            "Iteration 77, loss = 0.26692557\n",
            "Iteration 78, loss = 0.26554713\n",
            "Iteration 79, loss = 0.26548089\n",
            "Iteration 80, loss = 0.26335693\n",
            "Iteration 81, loss = 0.26525391\n",
            "Iteration 82, loss = 0.26620186\n",
            "Iteration 83, loss = 0.26442354\n",
            "Iteration 84, loss = 0.26902091\n",
            "Iteration 85, loss = 0.26573427\n",
            "Iteration 86, loss = 0.26469333\n",
            "Iteration 87, loss = 0.26437254\n",
            "Iteration 88, loss = 0.26779042\n",
            "Iteration 89, loss = 0.26384471\n",
            "Iteration 90, loss = 0.26625047\n",
            "Iteration 91, loss = 0.26462971\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.25233174\n",
            "Iteration 93, loss = 0.25221363\n",
            "Iteration 94, loss = 0.25223036\n",
            "Iteration 95, loss = 0.25212449\n",
            "Iteration 96, loss = 0.25210978\n",
            "Iteration 97, loss = 0.25206157\n",
            "Iteration 98, loss = 0.25206129\n",
            "Iteration 99, loss = 0.25193003\n",
            "Iteration 100, loss = 0.25186779\n",
            "Iteration 101, loss = 0.25186354\n",
            "Iteration 102, loss = 0.25178194\n",
            "Iteration 103, loss = 0.25172357\n",
            "Iteration 104, loss = 0.25166832\n",
            "Iteration 105, loss = 0.25164065\n",
            "Iteration 106, loss = 0.25155034\n",
            "Iteration 107, loss = 0.25148212\n",
            "Iteration 108, loss = 0.25151548\n",
            "Iteration 109, loss = 0.25143559\n",
            "Iteration 110, loss = 0.25132849\n",
            "Iteration 111, loss = 0.25131451\n",
            "Iteration 112, loss = 0.25123416\n",
            "Iteration 113, loss = 0.25116836\n",
            "Iteration 114, loss = 0.25120309\n",
            "Iteration 115, loss = 0.25116446\n",
            "Iteration 116, loss = 0.25111088\n",
            "Iteration 117, loss = 0.25106445\n",
            "Iteration 118, loss = 0.25099346\n",
            "Iteration 119, loss = 0.25097323\n",
            "Iteration 120, loss = 0.25093655\n",
            "Iteration 121, loss = 0.25081831\n",
            "Iteration 122, loss = 0.25079184\n",
            "Iteration 123, loss = 0.25078364\n",
            "Iteration 124, loss = 0.25072660\n",
            "Iteration 125, loss = 0.25072655\n",
            "Iteration 126, loss = 0.25064762\n",
            "Iteration 127, loss = 0.25060500\n",
            "Iteration 128, loss = 0.25061031\n",
            "Iteration 129, loss = 0.25057051\n",
            "Iteration 130, loss = 0.25056636\n",
            "Iteration 131, loss = 0.25044007\n",
            "Iteration 132, loss = 0.25045727\n",
            "Iteration 133, loss = 0.25038875\n",
            "Iteration 134, loss = 0.25033661\n",
            "Iteration 135, loss = 0.25035512\n",
            "Iteration 136, loss = 0.25023797\n",
            "Iteration 137, loss = 0.25024969\n",
            "Iteration 138, loss = 0.25019956\n",
            "Iteration 139, loss = 0.25017214\n",
            "Iteration 140, loss = 0.25012439\n",
            "Iteration 141, loss = 0.25011187\n",
            "Iteration 142, loss = 0.25008995\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 143, loss = 0.24979200\n",
            "Iteration 144, loss = 0.24978460\n",
            "Iteration 145, loss = 0.24974873\n",
            "Iteration 146, loss = 0.24975103\n",
            "Iteration 147, loss = 0.24978217\n",
            "Iteration 148, loss = 0.24976610\n",
            "Iteration 149, loss = 0.24976529\n",
            "Iteration 150, loss = 0.24972980\n",
            "Iteration 151, loss = 0.24971299\n",
            "Iteration 152, loss = 0.24971600\n",
            "Iteration 153, loss = 0.24970597\n",
            "Iteration 154, loss = 0.24970733\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 155, loss = 0.24962392\n",
            "Iteration 156, loss = 0.24960488\n",
            "Iteration 157, loss = 0.24960766\n",
            "Iteration 158, loss = 0.24961471\n",
            "Iteration 159, loss = 0.24961455\n",
            "Iteration 160, loss = 0.24960860\n",
            "Iteration 161, loss = 0.24959846\n",
            "Iteration 162, loss = 0.24960771\n",
            "Iteration 163, loss = 0.24960146\n",
            "Iteration 164, loss = 0.24959138\n",
            "Iteration 165, loss = 0.24959444\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62996951\n",
            "Iteration 3, loss = 0.60398916\n",
            "Iteration 4, loss = 0.58742641\n",
            "Iteration 5, loss = 0.58022940\n",
            "Iteration 6, loss = 0.57346770\n",
            "Iteration 7, loss = 0.56972723\n",
            "Iteration 8, loss = 0.56688080\n",
            "Iteration 9, loss = 0.56639937\n",
            "Iteration 10, loss = 0.56569653\n",
            "Iteration 11, loss = 0.56730908\n",
            "Iteration 12, loss = 0.56480626\n",
            "Iteration 13, loss = 0.56499339\n",
            "Iteration 14, loss = 0.56558474\n",
            "Iteration 15, loss = 0.56469795\n",
            "Iteration 16, loss = 0.56491465\n",
            "Iteration 17, loss = 0.56570267\n",
            "Iteration 18, loss = 0.60353136\n",
            "Iteration 19, loss = 0.58925476\n",
            "Iteration 20, loss = 0.57574728\n",
            "Iteration 21, loss = 0.56857635\n",
            "Iteration 22, loss = 0.56529083\n",
            "Iteration 23, loss = 0.56292889\n",
            "Iteration 24, loss = 0.56194099\n",
            "Iteration 25, loss = 0.56255700\n",
            "Iteration 26, loss = 0.56073902\n",
            "Iteration 27, loss = 0.56236510\n",
            "Iteration 28, loss = 0.56226682\n",
            "Iteration 29, loss = 0.56087467\n",
            "Iteration 30, loss = 0.56355983\n",
            "Iteration 31, loss = 0.56304016\n",
            "Iteration 32, loss = 0.56471708\n",
            "Iteration 33, loss = 0.56304002\n",
            "Iteration 34, loss = 0.56355149\n",
            "Iteration 35, loss = 0.56207891\n",
            "Iteration 36, loss = 0.56324392\n",
            "Iteration 37, loss = 0.56406322\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.49956124\n",
            "Iteration 39, loss = 0.47542447\n",
            "Iteration 40, loss = 0.50304052\n",
            "Iteration 41, loss = 0.50372439\n",
            "Iteration 42, loss = 0.50783899\n",
            "Iteration 43, loss = 0.50288824\n",
            "Iteration 44, loss = 0.50295339\n",
            "Iteration 45, loss = 0.50167801\n",
            "Iteration 46, loss = 0.50339954\n",
            "Iteration 47, loss = 0.50369041\n",
            "Iteration 48, loss = 0.51070010\n",
            "Iteration 49, loss = 0.50191816\n",
            "Iteration 50, loss = 0.50506117\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 51, loss = 0.40493643\n",
            "Iteration 52, loss = 0.39070907\n",
            "Iteration 53, loss = 0.37780368\n",
            "Iteration 54, loss = 0.36625466\n",
            "Iteration 55, loss = 0.35558885\n",
            "Iteration 56, loss = 0.34631350\n",
            "Iteration 57, loss = 0.33784578\n",
            "Iteration 58, loss = 0.33031560\n",
            "Iteration 59, loss = 0.32316645\n",
            "Iteration 60, loss = 0.31734995\n",
            "Iteration 61, loss = 0.31122201\n",
            "Iteration 62, loss = 0.30648154\n",
            "Iteration 63, loss = 0.30187165\n",
            "Iteration 64, loss = 0.29789265\n",
            "Iteration 65, loss = 0.29419219\n",
            "Iteration 66, loss = 0.29057130\n",
            "Iteration 67, loss = 0.28807850\n",
            "Iteration 68, loss = 0.28471425\n",
            "Iteration 69, loss = 0.28226132\n",
            "Iteration 70, loss = 0.27997646\n",
            "Iteration 71, loss = 0.27736703\n",
            "Iteration 72, loss = 0.27574132\n",
            "Iteration 73, loss = 0.27401091\n",
            "Iteration 74, loss = 0.27258238\n",
            "Iteration 75, loss = 0.27048918\n",
            "Iteration 76, loss = 0.27006287\n",
            "Iteration 77, loss = 0.26870345\n",
            "Iteration 78, loss = 0.26738020\n",
            "Iteration 79, loss = 0.26596006\n",
            "Iteration 80, loss = 0.26625304\n",
            "Iteration 81, loss = 0.26504797\n",
            "Iteration 82, loss = 0.26469884\n",
            "Iteration 83, loss = 0.26308022\n",
            "Iteration 84, loss = 0.26202255\n",
            "Iteration 85, loss = 0.26204739\n",
            "Iteration 86, loss = 0.26133927\n",
            "Iteration 87, loss = 0.26139759\n",
            "Iteration 88, loss = 0.26109521\n",
            "Iteration 89, loss = 0.25989098\n",
            "Iteration 90, loss = 0.26007183\n",
            "Iteration 91, loss = 0.25896760\n",
            "Iteration 92, loss = 0.25821909\n",
            "Iteration 93, loss = 0.25817478\n",
            "Iteration 94, loss = 0.25776267\n",
            "Iteration 95, loss = 0.26009600\n",
            "Iteration 96, loss = 0.25965153\n",
            "Iteration 97, loss = 0.25715117\n",
            "Iteration 98, loss = 0.25804351\n",
            "Iteration 99, loss = 0.25613499\n",
            "Iteration 100, loss = 0.25718792\n",
            "Iteration 101, loss = 0.25921569\n",
            "Iteration 102, loss = 0.25816502\n",
            "Iteration 103, loss = 0.25717193\n",
            "Iteration 104, loss = 0.26178920\n",
            "Iteration 105, loss = 0.25633453\n",
            "Iteration 106, loss = 0.25783315\n",
            "Iteration 107, loss = 0.25610572\n",
            "Iteration 108, loss = 0.25716337\n",
            "Iteration 109, loss = 0.25678133\n",
            "Iteration 110, loss = 0.25901251\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 111, loss = 0.24831337\n",
            "Iteration 112, loss = 0.24824572\n",
            "Iteration 113, loss = 0.24825942\n",
            "Iteration 114, loss = 0.24815282\n",
            "Iteration 115, loss = 0.24815051\n",
            "Iteration 116, loss = 0.24812491\n",
            "Iteration 117, loss = 0.24804331\n",
            "Iteration 118, loss = 0.24800666\n",
            "Iteration 119, loss = 0.24799861\n",
            "Iteration 120, loss = 0.24802477\n",
            "Iteration 121, loss = 0.24795719\n",
            "Iteration 122, loss = 0.24789093\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 123, loss = 0.24764159\n",
            "Iteration 124, loss = 0.24765292\n",
            "Iteration 125, loss = 0.24764155\n",
            "Iteration 126, loss = 0.24764905\n",
            "Iteration 127, loss = 0.24762680\n",
            "Iteration 128, loss = 0.24762906\n",
            "Iteration 129, loss = 0.24762848\n",
            "Iteration 130, loss = 0.24760503\n",
            "Iteration 131, loss = 0.24760811\n",
            "Iteration 132, loss = 0.24758801\n",
            "Iteration 133, loss = 0.24758751\n",
            "Iteration 134, loss = 0.24759583\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 135, loss = 0.24748512\n",
            "Iteration 136, loss = 0.24748972\n",
            "Iteration 137, loss = 0.24749209\n",
            "Iteration 138, loss = 0.24749146\n",
            "Iteration 139, loss = 0.24748236\n",
            "Iteration 140, loss = 0.24749072\n",
            "Iteration 141, loss = 0.24749080\n",
            "Iteration 142, loss = 0.24748519\n",
            "Iteration 143, loss = 0.24749153\n",
            "Iteration 144, loss = 0.24748443\n",
            "Iteration 145, loss = 0.24747427\n",
            "Iteration 146, loss = 0.24747607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63713108\n",
            "Iteration 3, loss = 0.60920901\n",
            "Iteration 4, loss = 0.59381726\n",
            "Iteration 5, loss = 0.58435215\n",
            "Iteration 6, loss = 0.57754206\n",
            "Iteration 7, loss = 0.57365678\n",
            "Iteration 8, loss = 0.57150519\n",
            "Iteration 9, loss = 0.57127070\n",
            "Iteration 10, loss = 0.57192329\n",
            "Iteration 11, loss = 0.57004739\n",
            "Iteration 12, loss = 0.56919268\n",
            "Iteration 13, loss = 0.57065717\n",
            "Iteration 14, loss = 0.56838963\n",
            "Iteration 15, loss = 0.57387122\n",
            "Iteration 16, loss = 0.59624572\n",
            "Iteration 17, loss = 0.58114168\n",
            "Iteration 18, loss = 0.57503881\n",
            "Iteration 19, loss = 0.61347470\n",
            "Iteration 20, loss = 0.58510452\n",
            "Iteration 21, loss = 0.57373886\n",
            "Iteration 22, loss = 0.56926083\n",
            "Iteration 23, loss = 0.56596712\n",
            "Iteration 24, loss = 0.56354462\n",
            "Iteration 25, loss = 0.56363055\n",
            "Iteration 26, loss = 0.56553759\n",
            "Iteration 27, loss = 0.56464666\n",
            "Iteration 28, loss = 0.56407837\n",
            "Iteration 29, loss = 0.56563844\n",
            "Iteration 30, loss = 0.56512307\n",
            "Iteration 31, loss = 0.56623147\n",
            "Iteration 32, loss = 0.56575013\n",
            "Iteration 33, loss = 0.56558866\n",
            "Iteration 34, loss = 0.56631653\n",
            "Iteration 35, loss = 0.56646982\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.50499057\n",
            "Iteration 37, loss = 0.48314594\n",
            "Iteration 38, loss = 0.50495225\n",
            "Iteration 39, loss = 0.51044688\n",
            "Iteration 40, loss = 0.50806580\n",
            "Iteration 41, loss = 0.51092929\n",
            "Iteration 42, loss = 0.50871506\n",
            "Iteration 43, loss = 0.50497858\n",
            "Iteration 44, loss = 0.51366388\n",
            "Iteration 45, loss = 0.50887426\n",
            "Iteration 46, loss = 0.50643798\n",
            "Iteration 47, loss = 0.50790709\n",
            "Iteration 48, loss = 0.51276207\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 49, loss = 0.40945695\n",
            "Iteration 50, loss = 0.39479056\n",
            "Iteration 51, loss = 0.38137281\n",
            "Iteration 52, loss = 0.36948239\n",
            "Iteration 53, loss = 0.35852779\n",
            "Iteration 54, loss = 0.34871376\n",
            "Iteration 55, loss = 0.34003962\n",
            "Iteration 56, loss = 0.33223912\n",
            "Iteration 57, loss = 0.32507290\n",
            "Iteration 58, loss = 0.31846434\n",
            "Iteration 59, loss = 0.31291436\n",
            "Iteration 60, loss = 0.30803933\n",
            "Iteration 61, loss = 0.30313466\n",
            "Iteration 62, loss = 0.29886895\n",
            "Iteration 63, loss = 0.29499722\n",
            "Iteration 64, loss = 0.29152796\n",
            "Iteration 65, loss = 0.28819676\n",
            "Iteration 66, loss = 0.28585678\n",
            "Iteration 67, loss = 0.28316019\n",
            "Iteration 68, loss = 0.28060266\n",
            "Iteration 69, loss = 0.27905189\n",
            "Iteration 70, loss = 0.27683233\n",
            "Iteration 71, loss = 0.27477644\n",
            "Iteration 72, loss = 0.27287237\n",
            "Iteration 73, loss = 0.27227627\n",
            "Iteration 74, loss = 0.27074978\n",
            "Iteration 75, loss = 0.26980943\n",
            "Iteration 76, loss = 0.26831273\n",
            "Iteration 77, loss = 0.26763507\n",
            "Iteration 78, loss = 0.26695171\n",
            "Iteration 79, loss = 0.26515426\n",
            "Iteration 80, loss = 0.26530097\n",
            "Iteration 81, loss = 0.26542298\n",
            "Iteration 82, loss = 0.26526336\n",
            "Iteration 83, loss = 0.26415900\n",
            "Iteration 84, loss = 0.26310996\n",
            "Iteration 85, loss = 0.26266169\n",
            "Iteration 86, loss = 0.26361981\n",
            "Iteration 87, loss = 0.26179195\n",
            "Iteration 88, loss = 0.26043271\n",
            "Iteration 89, loss = 0.25849075\n",
            "Iteration 90, loss = 0.26015148\n",
            "Iteration 91, loss = 0.26231735\n",
            "Iteration 92, loss = 0.26172953\n",
            "Iteration 93, loss = 0.26065794\n",
            "Iteration 94, loss = 0.26174539\n",
            "Iteration 95, loss = 0.26053800\n",
            "Iteration 96, loss = 0.26027864\n",
            "Iteration 97, loss = 0.26312666\n",
            "Iteration 98, loss = 0.27317416\n",
            "Iteration 99, loss = 0.26864570\n",
            "Iteration 100, loss = 0.26371214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 101, loss = 0.25049003\n",
            "Iteration 102, loss = 0.25033098\n",
            "Iteration 103, loss = 0.25025060\n",
            "Iteration 104, loss = 0.25019719\n",
            "Iteration 105, loss = 0.25017322\n",
            "Iteration 106, loss = 0.25013545\n",
            "Iteration 107, loss = 0.25004725\n",
            "Iteration 108, loss = 0.25001698\n",
            "Iteration 109, loss = 0.24993641\n",
            "Iteration 110, loss = 0.24989124\n",
            "Iteration 111, loss = 0.24987311\n",
            "Iteration 112, loss = 0.24981583\n",
            "Iteration 113, loss = 0.24980321\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 114, loss = 0.24947531\n",
            "Iteration 115, loss = 0.24945921\n",
            "Iteration 116, loss = 0.24946897\n",
            "Iteration 117, loss = 0.24941945\n",
            "Iteration 118, loss = 0.24942819\n",
            "Iteration 119, loss = 0.24942341\n",
            "Iteration 120, loss = 0.24946195\n",
            "Iteration 121, loss = 0.24940636\n",
            "Iteration 122, loss = 0.24939074\n",
            "Iteration 123, loss = 0.24942042\n",
            "Iteration 124, loss = 0.24940291\n",
            "Iteration 125, loss = 0.24937987\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 126, loss = 0.24928661\n",
            "Iteration 127, loss = 0.24929002\n",
            "Iteration 128, loss = 0.24927957\n",
            "Iteration 129, loss = 0.24928743\n",
            "Iteration 130, loss = 0.24927659\n",
            "Iteration 131, loss = 0.24929548\n",
            "Iteration 132, loss = 0.24927596\n",
            "Iteration 133, loss = 0.24927162\n",
            "Iteration 134, loss = 0.24927549\n",
            "Iteration 135, loss = 0.24927563\n",
            "Iteration 136, loss = 0.24927720\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64822331\n",
            "Iteration 3, loss = 0.61684887\n",
            "Iteration 4, loss = 0.59581841\n",
            "Iteration 5, loss = 0.58400547\n",
            "Iteration 6, loss = 0.57750176\n",
            "Iteration 7, loss = 0.57296421\n",
            "Iteration 8, loss = 0.56994841\n",
            "Iteration 9, loss = 0.57034763\n",
            "Iteration 10, loss = 0.56918355\n",
            "Iteration 11, loss = 0.57169852\n",
            "Iteration 12, loss = 0.56876351\n",
            "Iteration 13, loss = 0.56762770\n",
            "Iteration 14, loss = 0.56847765\n",
            "Iteration 15, loss = 0.56729010\n",
            "Iteration 16, loss = 0.56705293\n",
            "Iteration 17, loss = 0.56897391\n",
            "Iteration 18, loss = 0.57193242\n",
            "Iteration 19, loss = 0.57228542\n",
            "Iteration 20, loss = 0.57033797\n",
            "Iteration 21, loss = 0.56877348\n",
            "Iteration 22, loss = 0.57049877\n",
            "Iteration 23, loss = 0.57049614\n",
            "Iteration 24, loss = 0.57056321\n",
            "Iteration 25, loss = 0.56947755\n",
            "Iteration 26, loss = 0.57046478\n",
            "Iteration 27, loss = 0.56908837\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 28, loss = 0.50248754\n",
            "Iteration 29, loss = 0.48096172\n",
            "Iteration 30, loss = 0.50624961\n",
            "Iteration 31, loss = 0.51041101\n",
            "Iteration 32, loss = 0.51320563\n",
            "Iteration 33, loss = 0.51030790\n",
            "Iteration 34, loss = 0.50852796\n",
            "Iteration 35, loss = 0.51535486\n",
            "Iteration 36, loss = 0.50538391\n",
            "Iteration 37, loss = 0.51031415\n",
            "Iteration 38, loss = 0.51175145\n",
            "Iteration 39, loss = 0.51131968\n",
            "Iteration 40, loss = 0.50939943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 41, loss = 0.40874925\n",
            "Iteration 42, loss = 0.39248580\n",
            "Iteration 43, loss = 0.37908833\n",
            "Iteration 44, loss = 0.36699480\n",
            "Iteration 45, loss = 0.35627058\n",
            "Iteration 46, loss = 0.34630085\n",
            "Iteration 47, loss = 0.33756558\n",
            "Iteration 48, loss = 0.32956081\n",
            "Iteration 49, loss = 0.32258058\n",
            "Iteration 50, loss = 0.31632191\n",
            "Iteration 51, loss = 0.31031650\n",
            "Iteration 52, loss = 0.30534410\n",
            "Iteration 53, loss = 0.30061434\n",
            "Iteration 54, loss = 0.29633802\n",
            "Iteration 55, loss = 0.29231528\n",
            "Iteration 56, loss = 0.28903164\n",
            "Iteration 57, loss = 0.28607770\n",
            "Iteration 58, loss = 0.28309614\n",
            "Iteration 59, loss = 0.28073295\n",
            "Iteration 60, loss = 0.27802840\n",
            "Iteration 61, loss = 0.27625130\n",
            "Iteration 62, loss = 0.27404932\n",
            "Iteration 63, loss = 0.27248140\n",
            "Iteration 64, loss = 0.27063812\n",
            "Iteration 65, loss = 0.26930098\n",
            "Iteration 66, loss = 0.26803953\n",
            "Iteration 67, loss = 0.26687701\n",
            "Iteration 68, loss = 0.26542540\n",
            "Iteration 69, loss = 0.26428109\n",
            "Iteration 70, loss = 0.26385738\n",
            "Iteration 71, loss = 0.26307716\n",
            "Iteration 72, loss = 0.26246807\n",
            "Iteration 73, loss = 0.26146373\n",
            "Iteration 74, loss = 0.26103540\n",
            "Iteration 75, loss = 0.26024893\n",
            "Iteration 76, loss = 0.25890151\n",
            "Iteration 77, loss = 0.25989798\n",
            "Iteration 78, loss = 0.25956627\n",
            "Iteration 79, loss = 0.25934615\n",
            "Iteration 80, loss = 0.25993120\n",
            "Iteration 81, loss = 0.25784709\n",
            "Iteration 82, loss = 0.25783610\n",
            "Iteration 83, loss = 0.25610761\n",
            "Iteration 84, loss = 0.25743785\n",
            "Iteration 85, loss = 0.25810049\n",
            "Iteration 86, loss = 0.25721232\n",
            "Iteration 87, loss = 0.25628950\n",
            "Iteration 88, loss = 0.25687555\n",
            "Iteration 89, loss = 0.25736531\n",
            "Iteration 90, loss = 0.25690290\n",
            "Iteration 91, loss = 0.26078625\n",
            "Iteration 92, loss = 0.25985560\n",
            "Iteration 93, loss = 0.25915487\n",
            "Iteration 94, loss = 0.25598920\n",
            "Iteration 95, loss = 0.25916782\n",
            "Iteration 96, loss = 0.26145574\n",
            "Iteration 97, loss = 0.25920534\n",
            "Iteration 98, loss = 0.25528188\n",
            "Iteration 99, loss = 0.25808205\n",
            "Iteration 100, loss = 0.25567396\n",
            "Iteration 101, loss = 0.25781166\n",
            "Iteration 102, loss = 0.26015492\n",
            "Iteration 103, loss = 0.25811620\n",
            "Iteration 104, loss = 0.26540147\n",
            "Iteration 105, loss = 0.25900378\n",
            "Iteration 106, loss = 0.25857606\n",
            "Iteration 107, loss = 0.25910235\n",
            "Iteration 108, loss = 0.26302067\n",
            "Iteration 109, loss = 0.26596532\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 110, loss = 0.24633691\n",
            "Iteration 111, loss = 0.24597162\n",
            "Iteration 112, loss = 0.24589792\n",
            "Iteration 113, loss = 0.24587702\n",
            "Iteration 114, loss = 0.24580999\n",
            "Iteration 115, loss = 0.24584072\n",
            "Iteration 116, loss = 0.24584913\n",
            "Iteration 117, loss = 0.24578767\n",
            "Iteration 118, loss = 0.24576688\n",
            "Iteration 119, loss = 0.24573110\n",
            "Iteration 120, loss = 0.24570160\n",
            "Iteration 121, loss = 0.24567372\n",
            "Iteration 122, loss = 0.24563652\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 123, loss = 0.24541151\n",
            "Iteration 124, loss = 0.24540608\n",
            "Iteration 125, loss = 0.24540624\n",
            "Iteration 126, loss = 0.24539524\n",
            "Iteration 127, loss = 0.24538139\n",
            "Iteration 128, loss = 0.24537983\n",
            "Iteration 129, loss = 0.24536655\n",
            "Iteration 130, loss = 0.24539232\n",
            "Iteration 131, loss = 0.24537649\n",
            "Iteration 132, loss = 0.24536472\n",
            "Iteration 133, loss = 0.24535048\n",
            "Iteration 134, loss = 0.24534144\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 135, loss = 0.24527505\n",
            "Iteration 136, loss = 0.24527575\n",
            "Iteration 137, loss = 0.24527091\n",
            "Iteration 138, loss = 0.24526545\n",
            "Iteration 139, loss = 0.24526890\n",
            "Iteration 140, loss = 0.24526521\n",
            "Iteration 141, loss = 0.24526425\n",
            "Iteration 142, loss = 0.24526948\n",
            "Iteration 143, loss = 0.24527983\n",
            "Iteration 144, loss = 0.24525832\n",
            "Iteration 145, loss = 0.24525965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 14.0min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68404077\n",
            "Iteration 3, loss = 0.66908137\n",
            "Iteration 4, loss = 0.65199275\n",
            "Iteration 5, loss = 0.64748591\n",
            "Iteration 6, loss = 0.63615188\n",
            "Iteration 7, loss = 0.62489950\n",
            "Iteration 8, loss = 0.62107838\n",
            "Iteration 9, loss = 0.61304952\n",
            "Iteration 10, loss = 0.60766311\n",
            "Iteration 11, loss = 0.60275165\n",
            "Iteration 12, loss = 0.59882496\n",
            "Iteration 13, loss = 0.59496407\n",
            "Iteration 14, loss = 0.59206508\n",
            "Iteration 15, loss = 0.58922988\n",
            "Iteration 16, loss = 0.58259741\n",
            "Iteration 17, loss = 0.58337317\n",
            "Iteration 18, loss = 0.58352165\n",
            "Iteration 19, loss = 0.57954926\n",
            "Iteration 20, loss = 0.57752482\n",
            "Iteration 21, loss = 0.57843100\n",
            "Iteration 22, loss = 0.57852770\n",
            "Iteration 23, loss = 0.57354632\n",
            "Iteration 24, loss = 0.57638790\n",
            "Iteration 25, loss = 0.57392791\n",
            "Iteration 26, loss = 0.57220339\n",
            "Iteration 27, loss = 0.57121724\n",
            "Iteration 28, loss = 0.56987997\n",
            "Iteration 29, loss = 0.57115989\n",
            "Iteration 30, loss = 0.57292842\n",
            "Iteration 31, loss = 0.56903147\n",
            "Iteration 32, loss = 0.56853932\n",
            "Iteration 33, loss = 0.56890377\n",
            "Iteration 34, loss = 0.57111778\n",
            "Iteration 35, loss = 0.57105163\n",
            "Iteration 36, loss = 0.56592109\n",
            "Iteration 37, loss = 0.56669614\n",
            "Iteration 38, loss = 0.56860050\n",
            "Iteration 39, loss = 0.56927088\n",
            "Iteration 40, loss = 0.56680174\n",
            "Iteration 41, loss = 0.56700692\n",
            "Iteration 42, loss = 0.56637434\n",
            "Iteration 43, loss = 0.56712059\n",
            "Iteration 44, loss = 0.56683804\n",
            "Iteration 45, loss = 0.56606016\n",
            "Iteration 46, loss = 0.56944149\n",
            "Iteration 47, loss = 0.56680483\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.52495593\n",
            "Iteration 49, loss = 0.50946508\n",
            "Iteration 50, loss = 0.50054439\n",
            "Iteration 51, loss = 0.49246558\n",
            "Iteration 52, loss = 0.48477081\n",
            "Iteration 53, loss = 0.47788149\n",
            "Iteration 54, loss = 0.47501341\n",
            "Iteration 55, loss = 0.47765970\n",
            "Iteration 56, loss = 0.50726629\n",
            "Iteration 57, loss = 0.51071168\n",
            "Iteration 58, loss = 0.50109406\n",
            "Iteration 59, loss = 0.50733145\n",
            "Iteration 60, loss = 0.51652637\n",
            "Iteration 61, loss = 0.50258139\n",
            "Iteration 62, loss = 0.50048175\n",
            "Iteration 63, loss = 0.52809501\n",
            "Iteration 64, loss = 0.51768185\n",
            "Iteration 65, loss = 0.49636957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.43351441\n",
            "Iteration 67, loss = 0.43021558\n",
            "Iteration 68, loss = 0.42743786\n",
            "Iteration 69, loss = 0.42454693\n",
            "Iteration 70, loss = 0.42159897\n",
            "Iteration 71, loss = 0.41945108\n",
            "Iteration 72, loss = 0.41685969\n",
            "Iteration 73, loss = 0.41417334\n",
            "Iteration 74, loss = 0.41129340\n",
            "Iteration 75, loss = 0.40856312\n",
            "Iteration 76, loss = 0.40634031\n",
            "Iteration 77, loss = 0.40387880\n",
            "Iteration 78, loss = 0.40149139\n",
            "Iteration 79, loss = 0.39889776\n",
            "Iteration 80, loss = 0.39654746\n",
            "Iteration 81, loss = 0.39422102\n",
            "Iteration 82, loss = 0.39224233\n",
            "Iteration 83, loss = 0.38956686\n",
            "Iteration 84, loss = 0.38761995\n",
            "Iteration 85, loss = 0.38552669\n",
            "Iteration 86, loss = 0.38282908\n",
            "Iteration 87, loss = 0.38097619\n",
            "Iteration 88, loss = 0.37908652\n",
            "Iteration 89, loss = 0.37675339\n",
            "Iteration 90, loss = 0.37466729\n",
            "Iteration 91, loss = 0.37236654\n",
            "Iteration 92, loss = 0.37070381\n",
            "Iteration 93, loss = 0.36818764\n",
            "Iteration 94, loss = 0.36662247\n",
            "Iteration 95, loss = 0.36470056\n",
            "Iteration 96, loss = 0.36243690\n",
            "Iteration 97, loss = 0.36142558\n",
            "Iteration 98, loss = 0.35896831\n",
            "Iteration 99, loss = 0.35714763\n",
            "Iteration 100, loss = 0.35556281\n",
            "Iteration 101, loss = 0.35385626\n",
            "Iteration 102, loss = 0.35153765\n",
            "Iteration 103, loss = 0.35009445\n",
            "Iteration 104, loss = 0.34857427\n",
            "Iteration 105, loss = 0.34760517\n",
            "Iteration 106, loss = 0.34553516\n",
            "Iteration 107, loss = 0.34388121\n",
            "Iteration 108, loss = 0.34181680\n",
            "Iteration 109, loss = 0.34100236\n",
            "Iteration 110, loss = 0.33898977\n",
            "Iteration 111, loss = 0.33794272\n",
            "Iteration 112, loss = 0.33630923\n",
            "Iteration 113, loss = 0.33470752\n",
            "Iteration 114, loss = 0.33358485\n",
            "Iteration 115, loss = 0.33301607\n",
            "Iteration 116, loss = 0.33087675\n",
            "Iteration 117, loss = 0.32981533\n",
            "Iteration 118, loss = 0.32894492\n",
            "Iteration 119, loss = 0.32734594\n",
            "Iteration 120, loss = 0.32624826\n",
            "Iteration 121, loss = 0.32422694\n",
            "Iteration 122, loss = 0.32320010\n",
            "Iteration 123, loss = 0.32221795\n",
            "Iteration 124, loss = 0.32233357\n",
            "Iteration 125, loss = 0.31983962\n",
            "Iteration 126, loss = 0.31902218\n",
            "Iteration 127, loss = 0.31772071\n",
            "Iteration 128, loss = 0.31592720\n",
            "Iteration 129, loss = 0.31523608\n",
            "Iteration 130, loss = 0.31385840\n",
            "Iteration 131, loss = 0.31481958\n",
            "Iteration 132, loss = 0.31323217\n",
            "Iteration 133, loss = 0.31168000\n",
            "Iteration 134, loss = 0.31062243\n",
            "Iteration 135, loss = 0.31121552\n",
            "Iteration 136, loss = 0.31005059\n",
            "Iteration 137, loss = 0.30780865\n",
            "Iteration 138, loss = 0.30756576\n",
            "Iteration 139, loss = 0.30608512\n",
            "Iteration 140, loss = 0.30535615\n",
            "Iteration 141, loss = 0.30504110\n",
            "Iteration 142, loss = 0.30634961\n",
            "Iteration 143, loss = 0.30298293\n",
            "Iteration 144, loss = 0.30559252\n",
            "Iteration 145, loss = 0.30162242\n",
            "Iteration 146, loss = 0.30121165\n",
            "Iteration 147, loss = 0.29999564\n",
            "Iteration 148, loss = 0.30033117\n",
            "Iteration 149, loss = 0.29979293\n",
            "Iteration 150, loss = 0.29730172\n",
            "Iteration 151, loss = 0.29737939\n",
            "Iteration 152, loss = 0.29718419\n",
            "Iteration 153, loss = 0.29541879\n",
            "Iteration 154, loss = 0.29701719\n",
            "Iteration 155, loss = 0.29455500\n",
            "Iteration 156, loss = 0.29551792\n",
            "Iteration 157, loss = 0.29581060\n",
            "Iteration 158, loss = 0.29311739\n",
            "Iteration 159, loss = 0.29168814\n",
            "Iteration 160, loss = 0.29136705\n",
            "Iteration 161, loss = 0.29167528\n",
            "Iteration 162, loss = 0.29008318\n",
            "Iteration 163, loss = 0.29215557\n",
            "Iteration 164, loss = 0.29080366\n",
            "Iteration 165, loss = 0.28958144\n",
            "Iteration 166, loss = 0.28903811\n",
            "Iteration 167, loss = 0.28774838\n",
            "Iteration 168, loss = 0.28863492\n",
            "Iteration 169, loss = 0.28807966\n",
            "Iteration 170, loss = 0.28725614\n",
            "Iteration 171, loss = 0.28497847\n",
            "Iteration 172, loss = 0.28496372\n",
            "Iteration 173, loss = 0.28553674\n",
            "Iteration 174, loss = 0.28661872\n",
            "Iteration 175, loss = 0.28507976\n",
            "Iteration 176, loss = 0.28426704\n",
            "Iteration 177, loss = 0.28360028\n",
            "Iteration 178, loss = 0.28440382\n",
            "Iteration 179, loss = 0.28555975\n",
            "Iteration 180, loss = 0.28432310\n",
            "Iteration 181, loss = 0.28241292\n",
            "Iteration 182, loss = 0.28324135\n",
            "Iteration 183, loss = 0.28292331\n",
            "Iteration 184, loss = 0.28172807\n",
            "Iteration 185, loss = 0.28197668\n",
            "Iteration 186, loss = 0.28526160\n",
            "Iteration 187, loss = 0.27867382\n",
            "Iteration 188, loss = 0.28111084\n",
            "Iteration 189, loss = 0.28049819\n",
            "Iteration 190, loss = 0.27650376\n",
            "Iteration 191, loss = 0.28194724\n",
            "Iteration 192, loss = 0.27737522\n",
            "Iteration 193, loss = 0.27865919\n",
            "Iteration 194, loss = 0.28044545\n",
            "Iteration 195, loss = 0.27928033\n",
            "Iteration 196, loss = 0.27871543\n",
            "Iteration 197, loss = 0.27887871\n",
            "Iteration 198, loss = 0.27737879\n",
            "Iteration 199, loss = 0.28198948\n",
            "Iteration 200, loss = 0.27917226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68715976\n",
            "Iteration 3, loss = 0.67504645\n",
            "Iteration 4, loss = 0.65853602\n",
            "Iteration 5, loss = 0.64655029\n",
            "Iteration 6, loss = 0.64078275\n",
            "Iteration 7, loss = 0.63148630\n",
            "Iteration 8, loss = 0.62292233\n",
            "Iteration 9, loss = 0.61495237\n",
            "Iteration 10, loss = 0.61156851\n",
            "Iteration 11, loss = 0.60524972\n",
            "Iteration 12, loss = 0.60464826\n",
            "Iteration 13, loss = 0.59595288\n",
            "Iteration 14, loss = 0.59307659\n",
            "Iteration 15, loss = 0.58790925\n",
            "Iteration 16, loss = 0.58765145\n",
            "Iteration 17, loss = 0.58859729\n",
            "Iteration 18, loss = 0.58268526\n",
            "Iteration 19, loss = 0.58487182\n",
            "Iteration 20, loss = 0.57967641\n",
            "Iteration 21, loss = 0.57850856\n",
            "Iteration 22, loss = 0.57683502\n",
            "Iteration 23, loss = 0.57404139\n",
            "Iteration 24, loss = 0.57894322\n",
            "Iteration 25, loss = 0.57099648\n",
            "Iteration 26, loss = 0.57279466\n",
            "Iteration 27, loss = 0.57201880\n",
            "Iteration 28, loss = 0.56971327\n",
            "Iteration 29, loss = 0.57717197\n",
            "Iteration 30, loss = 0.57207416\n",
            "Iteration 31, loss = 0.57082606\n",
            "Iteration 32, loss = 0.57291933\n",
            "Iteration 33, loss = 0.56797732\n",
            "Iteration 34, loss = 0.56897662\n",
            "Iteration 35, loss = 0.57051997\n",
            "Iteration 36, loss = 0.56799058\n",
            "Iteration 37, loss = 0.57210598\n",
            "Iteration 38, loss = 0.56968050\n",
            "Iteration 39, loss = 0.56712017\n",
            "Iteration 40, loss = 0.56935504\n",
            "Iteration 41, loss = 0.57183310\n",
            "Iteration 42, loss = 0.56807544\n",
            "Iteration 43, loss = 0.57216253\n",
            "Iteration 44, loss = 0.56716814\n",
            "Iteration 45, loss = 0.57429846\n",
            "Iteration 46, loss = 0.56463061\n",
            "Iteration 47, loss = 0.57353003\n",
            "Iteration 48, loss = 0.56743860\n",
            "Iteration 49, loss = 0.56950072\n",
            "Iteration 50, loss = 0.57074610\n",
            "Iteration 51, loss = 0.56848643\n",
            "Iteration 52, loss = 0.56993973\n",
            "Iteration 53, loss = 0.56792013\n",
            "Iteration 54, loss = 0.56844052\n",
            "Iteration 55, loss = 0.56923259\n",
            "Iteration 56, loss = 0.56774369\n",
            "Iteration 57, loss = 0.56718194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 58, loss = 0.52344473\n",
            "Iteration 59, loss = 0.51304760\n",
            "Iteration 60, loss = 0.50524380\n",
            "Iteration 61, loss = 0.49852978\n",
            "Iteration 62, loss = 0.49275248\n",
            "Iteration 63, loss = 0.48475283\n",
            "Iteration 64, loss = 0.48493880\n",
            "Iteration 65, loss = 0.47965400\n",
            "Iteration 66, loss = 0.50238543\n",
            "Iteration 67, loss = 0.50073568\n",
            "Iteration 68, loss = 0.50421440\n",
            "Iteration 69, loss = 0.50887519\n",
            "Iteration 70, loss = 0.51765654\n",
            "Iteration 71, loss = 0.50684924\n",
            "Iteration 72, loss = 0.51429790\n",
            "Iteration 73, loss = 0.51444009\n",
            "Iteration 74, loss = 0.51781125\n",
            "Iteration 75, loss = 0.50871441\n",
            "Iteration 76, loss = 0.52124261\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 77, loss = 0.44690470\n",
            "Iteration 78, loss = 0.43920477\n",
            "Iteration 79, loss = 0.43723803\n",
            "Iteration 80, loss = 0.43429773\n",
            "Iteration 81, loss = 0.43212709\n",
            "Iteration 82, loss = 0.42970236\n",
            "Iteration 83, loss = 0.42710269\n",
            "Iteration 84, loss = 0.42569013\n",
            "Iteration 85, loss = 0.42287562\n",
            "Iteration 86, loss = 0.42050999\n",
            "Iteration 87, loss = 0.41852883\n",
            "Iteration 88, loss = 0.41606934\n",
            "Iteration 89, loss = 0.41419804\n",
            "Iteration 90, loss = 0.41152130\n",
            "Iteration 91, loss = 0.40574585\n",
            "Iteration 92, loss = 0.39913623\n",
            "Iteration 93, loss = 0.39680854\n",
            "Iteration 94, loss = 0.39442667\n",
            "Iteration 95, loss = 0.39128962\n",
            "Iteration 96, loss = 0.38931811\n",
            "Iteration 97, loss = 0.38721498\n",
            "Iteration 98, loss = 0.38416458\n",
            "Iteration 99, loss = 0.38282200\n",
            "Iteration 100, loss = 0.37976911\n",
            "Iteration 101, loss = 0.37782457\n",
            "Iteration 102, loss = 0.37604871\n",
            "Iteration 103, loss = 0.37326189\n",
            "Iteration 104, loss = 0.37161115\n",
            "Iteration 105, loss = 0.36934654\n",
            "Iteration 106, loss = 0.36755005\n",
            "Iteration 107, loss = 0.36551771\n",
            "Iteration 108, loss = 0.36389147\n",
            "Iteration 109, loss = 0.36129982\n",
            "Iteration 110, loss = 0.36021836\n",
            "Iteration 111, loss = 0.35802997\n",
            "Iteration 112, loss = 0.35570863\n",
            "Iteration 113, loss = 0.35419583\n",
            "Iteration 114, loss = 0.35229100\n",
            "Iteration 115, loss = 0.34976643\n",
            "Iteration 116, loss = 0.34822840\n",
            "Iteration 117, loss = 0.34702086\n",
            "Iteration 118, loss = 0.34522339\n",
            "Iteration 119, loss = 0.34419456\n",
            "Iteration 120, loss = 0.34230412\n",
            "Iteration 121, loss = 0.34035454\n",
            "Iteration 122, loss = 0.33995882\n",
            "Iteration 123, loss = 0.33799954\n",
            "Iteration 124, loss = 0.33556444\n",
            "Iteration 125, loss = 0.33443599\n",
            "Iteration 126, loss = 0.33358321\n",
            "Iteration 127, loss = 0.33248008\n",
            "Iteration 128, loss = 0.33039024\n",
            "Iteration 129, loss = 0.32907052\n",
            "Iteration 130, loss = 0.32791158\n",
            "Iteration 131, loss = 0.32693109\n",
            "Iteration 132, loss = 0.32520204\n",
            "Iteration 133, loss = 0.32367036\n",
            "Iteration 134, loss = 0.32253326\n",
            "Iteration 135, loss = 0.32185828\n",
            "Iteration 136, loss = 0.32241571\n",
            "Iteration 137, loss = 0.31937836\n",
            "Iteration 138, loss = 0.31768788\n",
            "Iteration 139, loss = 0.31649562\n",
            "Iteration 140, loss = 0.31599933\n",
            "Iteration 141, loss = 0.31463214\n",
            "Iteration 142, loss = 0.31359729\n",
            "Iteration 143, loss = 0.31326284\n",
            "Iteration 144, loss = 0.31228354\n",
            "Iteration 145, loss = 0.31137868\n",
            "Iteration 146, loss = 0.31001671\n",
            "Iteration 147, loss = 0.30890446\n",
            "Iteration 148, loss = 0.30867793\n",
            "Iteration 149, loss = 0.30849259\n",
            "Iteration 150, loss = 0.30859204\n",
            "Iteration 151, loss = 0.30606229\n",
            "Iteration 152, loss = 0.30814009\n",
            "Iteration 153, loss = 0.30541862\n",
            "Iteration 154, loss = 0.30411927\n",
            "Iteration 155, loss = 0.30496441\n",
            "Iteration 156, loss = 0.30072810\n",
            "Iteration 157, loss = 0.30523523\n",
            "Iteration 158, loss = 0.30152570\n",
            "Iteration 159, loss = 0.30045285\n",
            "Iteration 160, loss = 0.29958792\n",
            "Iteration 161, loss = 0.29881496\n",
            "Iteration 162, loss = 0.29779427\n",
            "Iteration 163, loss = 0.30080015\n",
            "Iteration 164, loss = 0.29654597\n",
            "Iteration 165, loss = 0.29436702\n",
            "Iteration 166, loss = 0.29555371\n",
            "Iteration 167, loss = 0.29587012\n",
            "Iteration 168, loss = 0.29540925\n",
            "Iteration 169, loss = 0.29351809\n",
            "Iteration 170, loss = 0.29323130\n",
            "Iteration 171, loss = 0.29522185\n",
            "Iteration 172, loss = 0.29302668\n",
            "Iteration 173, loss = 0.29232749\n",
            "Iteration 174, loss = 0.29503017\n",
            "Iteration 175, loss = 0.29629813\n",
            "Iteration 176, loss = 0.28988727\n",
            "Iteration 177, loss = 0.29195447\n",
            "Iteration 178, loss = 0.29066865\n",
            "Iteration 179, loss = 0.28780034\n",
            "Iteration 180, loss = 0.28837428\n",
            "Iteration 181, loss = 0.29533166\n",
            "Iteration 182, loss = 0.29212545\n",
            "Iteration 183, loss = 0.29450865\n",
            "Iteration 184, loss = 0.29094656\n",
            "Iteration 185, loss = 0.28745308\n",
            "Iteration 186, loss = 0.29605768\n",
            "Iteration 187, loss = 0.29019267\n",
            "Iteration 188, loss = 0.28684827\n",
            "Iteration 189, loss = 0.29074300\n",
            "Iteration 190, loss = 0.28971807\n",
            "Iteration 191, loss = 0.28701699\n",
            "Iteration 192, loss = 0.28903852\n",
            "Iteration 193, loss = 0.29599411\n",
            "Iteration 194, loss = 0.29263587\n",
            "Iteration 195, loss = 0.28697510\n",
            "Iteration 196, loss = 0.29175697\n",
            "Iteration 197, loss = 0.28491064\n",
            "Iteration 198, loss = 0.29344851\n",
            "Iteration 199, loss = 0.29014203\n",
            "Iteration 200, loss = 0.28345372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70516045\n",
            "Iteration 3, loss = 0.68241551\n",
            "Iteration 4, loss = 0.66702080\n",
            "Iteration 5, loss = 0.64879615\n",
            "Iteration 6, loss = 0.64255048\n",
            "Iteration 7, loss = 0.62932348\n",
            "Iteration 8, loss = 0.62317975\n",
            "Iteration 9, loss = 0.61538717\n",
            "Iteration 10, loss = 0.60942452\n",
            "Iteration 11, loss = 0.60838720\n",
            "Iteration 12, loss = 0.60011078\n",
            "Iteration 13, loss = 0.59762788\n",
            "Iteration 14, loss = 0.59228215\n",
            "Iteration 15, loss = 0.59251139\n",
            "Iteration 16, loss = 0.58627612\n",
            "Iteration 17, loss = 0.58591971\n",
            "Iteration 18, loss = 0.58343043\n",
            "Iteration 19, loss = 0.57910472\n",
            "Iteration 20, loss = 0.57872167\n",
            "Iteration 21, loss = 0.57608352\n",
            "Iteration 22, loss = 0.57566783\n",
            "Iteration 23, loss = 0.57509199\n",
            "Iteration 24, loss = 0.57328301\n",
            "Iteration 25, loss = 0.57252630\n",
            "Iteration 26, loss = 0.57155326\n",
            "Iteration 27, loss = 0.56649442\n",
            "Iteration 28, loss = 0.57204862\n",
            "Iteration 29, loss = 0.56931258\n",
            "Iteration 30, loss = 0.57047216\n",
            "Iteration 31, loss = 0.56767486\n",
            "Iteration 32, loss = 0.56821635\n",
            "Iteration 33, loss = 0.57073416\n",
            "Iteration 34, loss = 0.56677623\n",
            "Iteration 35, loss = 0.56850105\n",
            "Iteration 36, loss = 0.56589883\n",
            "Iteration 37, loss = 0.56686825\n",
            "Iteration 38, loss = 0.56404596\n",
            "Iteration 39, loss = 0.56644059\n",
            "Iteration 40, loss = 0.56569015\n",
            "Iteration 41, loss = 0.56963336\n",
            "Iteration 42, loss = 0.56464103\n",
            "Iteration 43, loss = 0.56408807\n",
            "Iteration 44, loss = 0.56911736\n",
            "Iteration 45, loss = 0.56534027\n",
            "Iteration 46, loss = 0.56392103\n",
            "Iteration 47, loss = 0.56937070\n",
            "Iteration 48, loss = 0.56664751\n",
            "Iteration 49, loss = 0.56515254\n",
            "Iteration 50, loss = 0.56865102\n",
            "Iteration 51, loss = 0.56337314\n",
            "Iteration 52, loss = 0.56626303\n",
            "Iteration 53, loss = 0.56442396\n",
            "Iteration 54, loss = 0.56465466\n",
            "Iteration 55, loss = 0.56298264\n",
            "Iteration 56, loss = 0.56788402\n",
            "Iteration 57, loss = 0.56327304\n",
            "Iteration 58, loss = 0.56666420\n",
            "Iteration 59, loss = 0.56823244\n",
            "Iteration 60, loss = 0.56585010\n",
            "Iteration 61, loss = 0.56537841\n",
            "Iteration 62, loss = 0.56613446\n",
            "Iteration 63, loss = 0.56837882\n",
            "Iteration 64, loss = 0.56393902\n",
            "Iteration 65, loss = 0.56542794\n",
            "Iteration 66, loss = 0.56201250\n",
            "Iteration 67, loss = 0.56999257\n",
            "Iteration 68, loss = 0.56143357\n",
            "Iteration 69, loss = 0.57051587\n",
            "Iteration 70, loss = 0.56349660\n",
            "Iteration 71, loss = 0.56465910\n",
            "Iteration 72, loss = 0.56769768\n",
            "Iteration 73, loss = 0.56809372\n",
            "Iteration 74, loss = 0.56820103\n",
            "Iteration 75, loss = 0.56416235\n",
            "Iteration 76, loss = 0.56435568\n",
            "Iteration 77, loss = 0.57046153\n",
            "Iteration 78, loss = 0.56589439\n",
            "Iteration 79, loss = 0.56329674\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 80, loss = 0.51740194\n",
            "Iteration 81, loss = 0.50412159\n",
            "Iteration 82, loss = 0.49504717\n",
            "Iteration 83, loss = 0.48636018\n",
            "Iteration 84, loss = 0.47879696\n",
            "Iteration 85, loss = 0.47087030\n",
            "Iteration 86, loss = 0.47814434\n",
            "Iteration 87, loss = 0.48972150\n",
            "Iteration 88, loss = 0.50085657\n",
            "Iteration 89, loss = 0.50894563\n",
            "Iteration 90, loss = 0.50500305\n",
            "Iteration 91, loss = 0.50049280\n",
            "Iteration 92, loss = 0.51268789\n",
            "Iteration 93, loss = 0.51396363\n",
            "Iteration 94, loss = 0.51490221\n",
            "Iteration 95, loss = 0.49700641\n",
            "Iteration 96, loss = 0.50274817\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 97, loss = 0.43022089\n",
            "Iteration 98, loss = 0.42665039\n",
            "Iteration 99, loss = 0.42337365\n",
            "Iteration 100, loss = 0.42092230\n",
            "Iteration 101, loss = 0.41806319\n",
            "Iteration 102, loss = 0.41483317\n",
            "Iteration 103, loss = 0.41202108\n",
            "Iteration 104, loss = 0.40920142\n",
            "Iteration 105, loss = 0.40668602\n",
            "Iteration 106, loss = 0.40415404\n",
            "Iteration 107, loss = 0.40142278\n",
            "Iteration 108, loss = 0.39869751\n",
            "Iteration 109, loss = 0.39590716\n",
            "Iteration 110, loss = 0.39293666\n",
            "Iteration 111, loss = 0.39094918\n",
            "Iteration 112, loss = 0.38830297\n",
            "Iteration 113, loss = 0.38597644\n",
            "Iteration 114, loss = 0.38332697\n",
            "Iteration 115, loss = 0.38117815\n",
            "Iteration 116, loss = 0.37841047\n",
            "Iteration 117, loss = 0.37607569\n",
            "Iteration 118, loss = 0.37388371\n",
            "Iteration 119, loss = 0.37161515\n",
            "Iteration 120, loss = 0.36979422\n",
            "Iteration 121, loss = 0.36797138\n",
            "Iteration 122, loss = 0.36557289\n",
            "Iteration 123, loss = 0.36349113\n",
            "Iteration 124, loss = 0.36099767\n",
            "Iteration 125, loss = 0.35922917\n",
            "Iteration 126, loss = 0.35715618\n",
            "Iteration 127, loss = 0.35588786\n",
            "Iteration 128, loss = 0.35328464\n",
            "Iteration 129, loss = 0.35142698\n",
            "Iteration 130, loss = 0.34953476\n",
            "Iteration 131, loss = 0.34741821\n",
            "Iteration 132, loss = 0.34608500\n",
            "Iteration 133, loss = 0.34393039\n",
            "Iteration 134, loss = 0.34279926\n",
            "Iteration 135, loss = 0.34156902\n",
            "Iteration 136, loss = 0.33960629\n",
            "Iteration 137, loss = 0.33783293\n",
            "Iteration 138, loss = 0.33661424\n",
            "Iteration 139, loss = 0.33458654\n",
            "Iteration 140, loss = 0.33273403\n",
            "Iteration 141, loss = 0.33208138\n",
            "Iteration 142, loss = 0.33010919\n",
            "Iteration 143, loss = 0.32850899\n",
            "Iteration 144, loss = 0.32691508\n",
            "Iteration 145, loss = 0.32543520\n",
            "Iteration 146, loss = 0.32387957\n",
            "Iteration 147, loss = 0.32309936\n",
            "Iteration 148, loss = 0.32265074\n",
            "Iteration 149, loss = 0.32137855\n",
            "Iteration 150, loss = 0.31928583\n",
            "Iteration 151, loss = 0.31762295\n",
            "Iteration 152, loss = 0.31691267\n",
            "Iteration 153, loss = 0.31544651\n",
            "Iteration 154, loss = 0.31443848\n",
            "Iteration 155, loss = 0.31294151\n",
            "Iteration 156, loss = 0.31270818\n",
            "Iteration 157, loss = 0.31224196\n",
            "Iteration 158, loss = 0.31048701\n",
            "Iteration 159, loss = 0.30861096\n",
            "Iteration 160, loss = 0.30802511\n",
            "Iteration 161, loss = 0.30717612\n",
            "Iteration 162, loss = 0.30547681\n",
            "Iteration 163, loss = 0.30466907\n",
            "Iteration 164, loss = 0.30393711\n",
            "Iteration 165, loss = 0.30336770\n",
            "Iteration 166, loss = 0.30400948\n",
            "Iteration 167, loss = 0.30195235\n",
            "Iteration 168, loss = 0.30053514\n",
            "Iteration 169, loss = 0.30044676\n",
            "Iteration 170, loss = 0.29926215\n",
            "Iteration 171, loss = 0.29786709\n",
            "Iteration 172, loss = 0.29784537\n",
            "Iteration 173, loss = 0.29573439\n",
            "Iteration 174, loss = 0.29591927\n",
            "Iteration 175, loss = 0.29552716\n",
            "Iteration 176, loss = 0.29387058\n",
            "Iteration 177, loss = 0.29481108\n",
            "Iteration 178, loss = 0.29425590\n",
            "Iteration 179, loss = 0.29273613\n",
            "Iteration 180, loss = 0.29098002\n",
            "Iteration 181, loss = 0.29072456\n",
            "Iteration 182, loss = 0.29083932\n",
            "Iteration 183, loss = 0.29093237\n",
            "Iteration 184, loss = 0.28947368\n",
            "Iteration 185, loss = 0.28789196\n",
            "Iteration 186, loss = 0.28800014\n",
            "Iteration 187, loss = 0.28727648\n",
            "Iteration 188, loss = 0.28880306\n",
            "Iteration 189, loss = 0.28738045\n",
            "Iteration 190, loss = 0.28488969\n",
            "Iteration 191, loss = 0.28587115\n",
            "Iteration 192, loss = 0.28525525\n",
            "Iteration 193, loss = 0.28428468\n",
            "Iteration 194, loss = 0.28506586\n",
            "Iteration 195, loss = 0.28397572\n",
            "Iteration 196, loss = 0.28319198\n",
            "Iteration 197, loss = 0.28150997\n",
            "Iteration 198, loss = 0.28614191\n",
            "Iteration 199, loss = 0.28205926\n",
            "Iteration 200, loss = 0.28226548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71144901\n",
            "Iteration 3, loss = 0.69221901\n",
            "Iteration 4, loss = 0.67065242\n",
            "Iteration 5, loss = 0.65680882\n",
            "Iteration 6, loss = 0.64566594\n",
            "Iteration 7, loss = 0.63639542\n",
            "Iteration 8, loss = 0.62697610\n",
            "Iteration 9, loss = 0.61946562\n",
            "Iteration 10, loss = 0.61379076\n",
            "Iteration 11, loss = 0.60632399\n",
            "Iteration 12, loss = 0.60571933\n",
            "Iteration 13, loss = 0.59687682\n",
            "Iteration 14, loss = 0.59497970\n",
            "Iteration 15, loss = 0.58967163\n",
            "Iteration 16, loss = 0.58687454\n",
            "Iteration 17, loss = 0.58617106\n",
            "Iteration 18, loss = 0.58330637\n",
            "Iteration 19, loss = 0.58314943\n",
            "Iteration 20, loss = 0.57887783\n",
            "Iteration 21, loss = 0.57724587\n",
            "Iteration 22, loss = 0.57686927\n",
            "Iteration 23, loss = 0.57587262\n",
            "Iteration 24, loss = 0.57444154\n",
            "Iteration 25, loss = 0.56953171\n",
            "Iteration 26, loss = 0.57252431\n",
            "Iteration 27, loss = 0.57205739\n",
            "Iteration 28, loss = 0.57100263\n",
            "Iteration 29, loss = 0.57257842\n",
            "Iteration 30, loss = 0.56651216\n",
            "Iteration 31, loss = 0.57201960\n",
            "Iteration 32, loss = 0.56662633\n",
            "Iteration 33, loss = 0.57117460\n",
            "Iteration 34, loss = 0.56994149\n",
            "Iteration 35, loss = 0.56948703\n",
            "Iteration 36, loss = 0.56546495\n",
            "Iteration 37, loss = 0.56748644\n",
            "Iteration 38, loss = 0.57309065\n",
            "Iteration 39, loss = 0.57396092\n",
            "Iteration 40, loss = 0.57234256\n",
            "Iteration 41, loss = 0.57120918\n",
            "Iteration 42, loss = 0.56867875\n",
            "Iteration 43, loss = 0.57084080\n",
            "Iteration 44, loss = 0.56899377\n",
            "Iteration 45, loss = 0.57334569\n",
            "Iteration 46, loss = 0.56919883\n",
            "Iteration 47, loss = 0.57349688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 48, loss = 0.53433827\n",
            "Iteration 49, loss = 0.52042660\n",
            "Iteration 50, loss = 0.51129117\n",
            "Iteration 51, loss = 0.50468783\n",
            "Iteration 52, loss = 0.49639141\n",
            "Iteration 53, loss = 0.48913609\n",
            "Iteration 54, loss = 0.48445679\n",
            "Iteration 55, loss = 0.49201592\n",
            "Iteration 56, loss = 0.49471814\n",
            "Iteration 57, loss = 0.51229101\n",
            "Iteration 58, loss = 0.51023648\n",
            "Iteration 59, loss = 0.51043654\n",
            "Iteration 60, loss = 0.52179507\n",
            "Iteration 61, loss = 0.52765123\n",
            "Iteration 62, loss = 0.51218539\n",
            "Iteration 63, loss = 0.51861785\n",
            "Iteration 64, loss = 0.51938898\n",
            "Iteration 65, loss = 0.52211173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.44567602\n",
            "Iteration 67, loss = 0.44285903\n",
            "Iteration 68, loss = 0.43945786\n",
            "Iteration 69, loss = 0.43670963\n",
            "Iteration 70, loss = 0.43380603\n",
            "Iteration 71, loss = 0.43094519\n",
            "Iteration 72, loss = 0.42835991\n",
            "Iteration 73, loss = 0.42544703\n",
            "Iteration 74, loss = 0.42327776\n",
            "Iteration 75, loss = 0.41966127\n",
            "Iteration 76, loss = 0.41785352\n",
            "Iteration 77, loss = 0.41505499\n",
            "Iteration 78, loss = 0.41255650\n",
            "Iteration 79, loss = 0.40994475\n",
            "Iteration 80, loss = 0.40750049\n",
            "Iteration 81, loss = 0.40493001\n",
            "Iteration 82, loss = 0.40235734\n",
            "Iteration 83, loss = 0.39961704\n",
            "Iteration 84, loss = 0.39791654\n",
            "Iteration 85, loss = 0.39498238\n",
            "Iteration 86, loss = 0.39268353\n",
            "Iteration 87, loss = 0.39028702\n",
            "Iteration 88, loss = 0.38799021\n",
            "Iteration 89, loss = 0.38634711\n",
            "Iteration 90, loss = 0.38337639\n",
            "Iteration 91, loss = 0.38154587\n",
            "Iteration 92, loss = 0.37952359\n",
            "Iteration 93, loss = 0.37692518\n",
            "Iteration 94, loss = 0.37504946\n",
            "Iteration 95, loss = 0.37281172\n",
            "Iteration 96, loss = 0.37121452\n",
            "Iteration 97, loss = 0.36923488\n",
            "Iteration 98, loss = 0.36701372\n",
            "Iteration 99, loss = 0.36528902\n",
            "Iteration 100, loss = 0.36326414\n",
            "Iteration 101, loss = 0.36130683\n",
            "Iteration 102, loss = 0.35924414\n",
            "Iteration 103, loss = 0.35764255\n",
            "Iteration 104, loss = 0.35568950\n",
            "Iteration 105, loss = 0.35413923\n",
            "Iteration 106, loss = 0.35268877\n",
            "Iteration 107, loss = 0.35086144\n",
            "Iteration 108, loss = 0.34820699\n",
            "Iteration 109, loss = 0.34697901\n",
            "Iteration 110, loss = 0.34583365\n",
            "Iteration 111, loss = 0.34403728\n",
            "Iteration 112, loss = 0.34245628\n",
            "Iteration 113, loss = 0.34162878\n",
            "Iteration 114, loss = 0.33944654\n",
            "Iteration 115, loss = 0.33804820\n",
            "Iteration 116, loss = 0.33661087\n",
            "Iteration 117, loss = 0.33553978\n",
            "Iteration 118, loss = 0.33330838\n",
            "Iteration 119, loss = 0.33278760\n",
            "Iteration 120, loss = 0.33124513\n",
            "Iteration 121, loss = 0.32998144\n",
            "Iteration 122, loss = 0.32875598\n",
            "Iteration 123, loss = 0.32776970\n",
            "Iteration 124, loss = 0.32625400\n",
            "Iteration 125, loss = 0.32662936\n",
            "Iteration 126, loss = 0.32385840\n",
            "Iteration 127, loss = 0.32218247\n",
            "Iteration 128, loss = 0.32144838\n",
            "Iteration 129, loss = 0.32006224\n",
            "Iteration 130, loss = 0.31965989\n",
            "Iteration 131, loss = 0.31864746\n",
            "Iteration 132, loss = 0.31787085\n",
            "Iteration 133, loss = 0.31644167\n",
            "Iteration 134, loss = 0.31373436\n",
            "Iteration 135, loss = 0.31623028\n",
            "Iteration 136, loss = 0.31341285\n",
            "Iteration 137, loss = 0.31382612\n",
            "Iteration 138, loss = 0.31202239\n",
            "Iteration 139, loss = 0.31170285\n",
            "Iteration 140, loss = 0.31177059\n",
            "Iteration 141, loss = 0.31161021\n",
            "Iteration 142, loss = 0.30743988\n",
            "Iteration 143, loss = 0.30679056\n",
            "Iteration 144, loss = 0.30775073\n",
            "Iteration 145, loss = 0.30597966\n",
            "Iteration 146, loss = 0.30532947\n",
            "Iteration 147, loss = 0.30362766\n",
            "Iteration 148, loss = 0.30447501\n",
            "Iteration 149, loss = 0.30349203\n",
            "Iteration 150, loss = 0.30155256\n",
            "Iteration 151, loss = 0.30095333\n",
            "Iteration 152, loss = 0.30235231\n",
            "Iteration 153, loss = 0.30108278\n",
            "Iteration 154, loss = 0.30028898\n",
            "Iteration 155, loss = 0.29974626\n",
            "Iteration 156, loss = 0.30158923\n",
            "Iteration 157, loss = 0.29886374\n",
            "Iteration 158, loss = 0.30386910\n",
            "Iteration 159, loss = 0.29886517\n",
            "Iteration 160, loss = 0.29542220\n",
            "Iteration 161, loss = 0.29838729\n",
            "Iteration 162, loss = 0.29570676\n",
            "Iteration 163, loss = 0.29936035\n",
            "Iteration 164, loss = 0.29785100\n",
            "Iteration 165, loss = 0.29529099\n",
            "Iteration 166, loss = 0.29447815\n",
            "Iteration 167, loss = 0.29715437\n",
            "Iteration 168, loss = 0.29250285\n",
            "Iteration 169, loss = 0.29687107\n",
            "Iteration 170, loss = 0.29373149\n",
            "Iteration 171, loss = 0.28996633\n",
            "Iteration 172, loss = 0.29670292\n",
            "Iteration 173, loss = 0.29157037\n",
            "Iteration 174, loss = 0.29157623\n",
            "Iteration 175, loss = 0.29332473\n",
            "Iteration 176, loss = 0.29624363\n",
            "Iteration 177, loss = 0.28912971\n",
            "Iteration 178, loss = 0.29267249\n",
            "Iteration 179, loss = 0.29873986\n",
            "Iteration 180, loss = 0.28660905\n",
            "Iteration 181, loss = 0.29103611\n",
            "Iteration 182, loss = 0.29764109\n",
            "Iteration 183, loss = 0.29155735\n",
            "Iteration 184, loss = 0.29336712\n",
            "Iteration 185, loss = 0.28419442\n",
            "Iteration 186, loss = 0.28540751\n",
            "Iteration 187, loss = 0.28693393\n",
            "Iteration 188, loss = 0.29694732\n",
            "Iteration 189, loss = 0.29823337\n",
            "Iteration 190, loss = 0.29326056\n",
            "Iteration 191, loss = 0.29091765\n",
            "Iteration 192, loss = 0.30002229\n",
            "Iteration 193, loss = 0.29777704\n",
            "Iteration 194, loss = 0.29339638\n",
            "Iteration 195, loss = 0.29034328\n",
            "Iteration 196, loss = 0.30075364\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 197, loss = 0.27239343\n",
            "Iteration 198, loss = 0.27140856\n",
            "Iteration 199, loss = 0.27137423\n",
            "Iteration 200, loss = 0.27137673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.56873004\n",
            "Iteration 2, loss = 0.70048618\n",
            "Iteration 3, loss = 0.68505285\n",
            "Iteration 4, loss = 0.66781499\n",
            "Iteration 5, loss = 0.65762079\n",
            "Iteration 6, loss = 0.64494028\n",
            "Iteration 7, loss = 0.63803282\n",
            "Iteration 8, loss = 0.62638755\n",
            "Iteration 9, loss = 0.62255081\n",
            "Iteration 10, loss = 0.61445623\n",
            "Iteration 11, loss = 0.60930867\n",
            "Iteration 12, loss = 0.60367380\n",
            "Iteration 13, loss = 0.59919722\n",
            "Iteration 14, loss = 0.59604508\n",
            "Iteration 15, loss = 0.59200248\n",
            "Iteration 16, loss = 0.59236653\n",
            "Iteration 17, loss = 0.58786185\n",
            "Iteration 18, loss = 0.58734633\n",
            "Iteration 19, loss = 0.58152262\n",
            "Iteration 20, loss = 0.58441014\n",
            "Iteration 21, loss = 0.57841638\n",
            "Iteration 22, loss = 0.57939366\n",
            "Iteration 23, loss = 0.57674435\n",
            "Iteration 24, loss = 0.57623127\n",
            "Iteration 25, loss = 0.57350884\n",
            "Iteration 26, loss = 0.57527839\n",
            "Iteration 27, loss = 0.57458379\n",
            "Iteration 28, loss = 0.57151859\n",
            "Iteration 29, loss = 0.57255727\n",
            "Iteration 30, loss = 0.57091003\n",
            "Iteration 31, loss = 0.56951776\n",
            "Iteration 32, loss = 0.57311354\n",
            "Iteration 33, loss = 0.57058972\n",
            "Iteration 34, loss = 0.57179756\n",
            "Iteration 35, loss = 0.56700966\n",
            "Iteration 36, loss = 0.57216392\n",
            "Iteration 37, loss = 0.56813107\n",
            "Iteration 38, loss = 0.56803211\n",
            "Iteration 39, loss = 0.57171165\n",
            "Iteration 40, loss = 0.56695112\n",
            "Iteration 41, loss = 0.57166208\n",
            "Iteration 42, loss = 0.57258183\n",
            "Iteration 43, loss = 0.56914494\n",
            "Iteration 44, loss = 0.56748786\n",
            "Iteration 45, loss = 0.57223743\n",
            "Iteration 46, loss = 0.56854894\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.52957773\n",
            "Iteration 48, loss = 0.51728401\n",
            "Iteration 49, loss = 0.51087002\n",
            "Iteration 50, loss = 0.50424838\n",
            "Iteration 51, loss = 0.49711513\n",
            "Iteration 52, loss = 0.49282289\n",
            "Iteration 53, loss = 0.48738992\n",
            "Iteration 54, loss = 0.49119745\n",
            "Iteration 55, loss = 0.49093459\n",
            "Iteration 56, loss = 0.50288392\n",
            "Iteration 57, loss = 0.49931594\n",
            "Iteration 58, loss = 0.51644973\n",
            "Iteration 59, loss = 0.52299569\n",
            "Iteration 60, loss = 0.51001474\n",
            "Iteration 61, loss = 0.52150594\n",
            "Iteration 62, loss = 0.51045196\n",
            "Iteration 63, loss = 0.51820880\n",
            "Iteration 64, loss = 0.51162495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 65, loss = 0.44858851\n",
            "Iteration 66, loss = 0.44452502\n",
            "Iteration 67, loss = 0.44170327\n",
            "Iteration 68, loss = 0.43998800\n",
            "Iteration 69, loss = 0.43757292\n",
            "Iteration 70, loss = 0.43544542\n",
            "Iteration 71, loss = 0.43311338\n",
            "Iteration 72, loss = 0.43092023\n",
            "Iteration 73, loss = 0.42865186\n",
            "Iteration 74, loss = 0.42700501\n",
            "Iteration 75, loss = 0.42473407\n",
            "Iteration 76, loss = 0.42246842\n",
            "Iteration 77, loss = 0.42002283\n",
            "Iteration 78, loss = 0.41852673\n",
            "Iteration 79, loss = 0.41555017\n",
            "Iteration 80, loss = 0.41425503\n",
            "Iteration 81, loss = 0.41186486\n",
            "Iteration 82, loss = 0.41004928\n",
            "Iteration 83, loss = 0.40316685\n",
            "Iteration 84, loss = 0.39800099\n",
            "Iteration 85, loss = 0.39549685\n",
            "Iteration 86, loss = 0.39303454\n",
            "Iteration 87, loss = 0.39057964\n",
            "Iteration 88, loss = 0.38836429\n",
            "Iteration 89, loss = 0.38649784\n",
            "Iteration 90, loss = 0.38406958\n",
            "Iteration 91, loss = 0.38207362\n",
            "Iteration 92, loss = 0.38035215\n",
            "Iteration 93, loss = 0.37717078\n",
            "Iteration 94, loss = 0.37537468\n",
            "Iteration 95, loss = 0.37414822\n",
            "Iteration 96, loss = 0.37190158\n",
            "Iteration 97, loss = 0.36940186\n",
            "Iteration 98, loss = 0.36891379\n",
            "Iteration 99, loss = 0.36528070\n",
            "Iteration 100, loss = 0.36342447\n",
            "Iteration 101, loss = 0.36254284\n",
            "Iteration 102, loss = 0.36040328\n",
            "Iteration 103, loss = 0.35892808\n",
            "Iteration 104, loss = 0.35665930\n",
            "Iteration 105, loss = 0.35556020\n",
            "Iteration 106, loss = 0.35320425\n",
            "Iteration 107, loss = 0.35163043\n",
            "Iteration 108, loss = 0.35048656\n",
            "Iteration 109, loss = 0.34922841\n",
            "Iteration 110, loss = 0.34755603\n",
            "Iteration 111, loss = 0.34539926\n",
            "Iteration 112, loss = 0.34421975\n",
            "Iteration 113, loss = 0.34305525\n",
            "Iteration 114, loss = 0.34114574\n",
            "Iteration 115, loss = 0.33956286\n",
            "Iteration 116, loss = 0.33806265\n",
            "Iteration 117, loss = 0.33703551\n",
            "Iteration 118, loss = 0.33590810\n",
            "Iteration 119, loss = 0.33369075\n",
            "Iteration 120, loss = 0.33323805\n",
            "Iteration 121, loss = 0.33080673\n",
            "Iteration 122, loss = 0.32971275\n",
            "Iteration 123, loss = 0.32940245\n",
            "Iteration 124, loss = 0.32895791\n",
            "Iteration 125, loss = 0.32642022\n",
            "Iteration 126, loss = 0.32599871\n",
            "Iteration 127, loss = 0.32330977\n",
            "Iteration 128, loss = 0.32278339\n",
            "Iteration 129, loss = 0.32147515\n",
            "Iteration 130, loss = 0.32103897\n",
            "Iteration 131, loss = 0.31981419\n",
            "Iteration 132, loss = 0.31845855\n",
            "Iteration 133, loss = 0.32065640\n",
            "Iteration 134, loss = 0.31632174\n",
            "Iteration 135, loss = 0.31693523\n",
            "Iteration 136, loss = 0.31473512\n",
            "Iteration 137, loss = 0.31618730\n",
            "Iteration 138, loss = 0.31365758\n",
            "Iteration 139, loss = 0.31249408\n",
            "Iteration 140, loss = 0.31123278\n",
            "Iteration 141, loss = 0.31080801\n",
            "Iteration 142, loss = 0.30902845\n",
            "Iteration 143, loss = 0.31013476\n",
            "Iteration 144, loss = 0.30856032\n",
            "Iteration 145, loss = 0.30976254\n",
            "Iteration 146, loss = 0.30769153\n",
            "Iteration 147, loss = 0.30603114\n",
            "Iteration 148, loss = 0.30476118\n",
            "Iteration 149, loss = 0.30528836\n",
            "Iteration 150, loss = 0.30701953\n",
            "Iteration 151, loss = 0.30200512\n",
            "Iteration 152, loss = 0.30287955\n",
            "Iteration 153, loss = 0.30088523\n",
            "Iteration 154, loss = 0.30363603\n",
            "Iteration 155, loss = 0.30265805\n",
            "Iteration 156, loss = 0.30318106\n",
            "Iteration 157, loss = 0.30043529\n",
            "Iteration 158, loss = 0.29939636\n",
            "Iteration 159, loss = 0.30750388\n",
            "Iteration 160, loss = 0.29740479\n",
            "Iteration 161, loss = 0.30166808\n",
            "Iteration 162, loss = 0.29699721\n",
            "Iteration 163, loss = 0.29511358\n",
            "Iteration 164, loss = 0.29720927\n",
            "Iteration 165, loss = 0.30116483\n",
            "Iteration 166, loss = 0.29741157\n",
            "Iteration 167, loss = 0.29591857\n",
            "Iteration 168, loss = 0.29578388\n",
            "Iteration 169, loss = 0.29817352\n",
            "Iteration 170, loss = 0.29528773\n",
            "Iteration 171, loss = 0.29304280\n",
            "Iteration 172, loss = 0.29633793\n",
            "Iteration 173, loss = 0.30300546\n",
            "Iteration 174, loss = 0.29698124\n",
            "Iteration 175, loss = 0.29915567\n",
            "Iteration 176, loss = 0.29102753\n",
            "Iteration 177, loss = 0.29035696\n",
            "Iteration 178, loss = 0.29776510\n",
            "Iteration 179, loss = 0.29714017\n",
            "Iteration 180, loss = 0.29443430\n",
            "Iteration 181, loss = 0.29265829\n",
            "Iteration 182, loss = 0.29264261\n",
            "Iteration 183, loss = 0.29834570\n",
            "Iteration 184, loss = 0.29984858\n",
            "Iteration 185, loss = 0.28576484\n",
            "Iteration 186, loss = 0.28687875\n",
            "Iteration 187, loss = 0.30730757\n",
            "Iteration 188, loss = 0.28680852\n",
            "Iteration 189, loss = 0.29433632\n",
            "Iteration 190, loss = 0.30687319\n",
            "Iteration 191, loss = 0.29245819\n",
            "Iteration 192, loss = 0.28622355\n",
            "Iteration 193, loss = 0.30365135\n",
            "Iteration 194, loss = 0.29258474\n",
            "Iteration 195, loss = 0.30167307\n",
            "Iteration 196, loss = 0.28960924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 197, loss = 0.27286475\n",
            "Iteration 198, loss = 0.27239343\n",
            "Iteration 199, loss = 0.27238733\n",
            "Iteration 200, loss = 0.27242609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.73395382\n",
            "Iteration 3, loss = 0.71727890\n",
            "Iteration 4, loss = 0.69945994\n",
            "Iteration 5, loss = 0.68069225\n",
            "Iteration 6, loss = 0.66716164\n",
            "Iteration 7, loss = 0.65391803\n",
            "Iteration 8, loss = 0.64350892\n",
            "Iteration 9, loss = 0.63851948\n",
            "Iteration 10, loss = 0.62693272\n",
            "Iteration 11, loss = 0.61960425\n",
            "Iteration 12, loss = 0.61516640\n",
            "Iteration 13, loss = 0.61173716\n",
            "Iteration 14, loss = 0.60617725\n",
            "Iteration 15, loss = 0.60105729\n",
            "Iteration 16, loss = 0.59922612\n",
            "Iteration 17, loss = 0.59641720\n",
            "Iteration 18, loss = 0.59182540\n",
            "Iteration 19, loss = 0.58933708\n",
            "Iteration 20, loss = 0.58709759\n",
            "Iteration 21, loss = 0.58719313\n",
            "Iteration 22, loss = 0.58072084\n",
            "Iteration 23, loss = 0.58414414\n",
            "Iteration 24, loss = 0.58042494\n",
            "Iteration 25, loss = 0.57931285\n",
            "Iteration 26, loss = 0.57811078\n",
            "Iteration 27, loss = 0.57863079\n",
            "Iteration 28, loss = 0.57578597\n",
            "Iteration 29, loss = 0.57620432\n",
            "Iteration 30, loss = 0.58006407\n",
            "Iteration 31, loss = 0.57498383\n",
            "Iteration 32, loss = 0.57373074\n",
            "Iteration 33, loss = 0.57446426\n",
            "Iteration 34, loss = 0.57146872\n",
            "Iteration 35, loss = 0.57299866\n",
            "Iteration 36, loss = 0.57109455\n",
            "Iteration 37, loss = 0.57691502\n",
            "Iteration 38, loss = 0.57039296\n",
            "Iteration 39, loss = 0.57475876\n",
            "Iteration 40, loss = 0.57446358\n",
            "Iteration 41, loss = 0.57091158\n",
            "Iteration 42, loss = 0.57105086\n",
            "Iteration 43, loss = 0.57528358\n",
            "Iteration 44, loss = 0.56957902\n",
            "Iteration 45, loss = 0.56905001\n",
            "Iteration 46, loss = 0.57188509\n",
            "Iteration 47, loss = 0.57174760\n",
            "Iteration 48, loss = 0.57012495\n",
            "Iteration 49, loss = 0.57218580\n",
            "Iteration 50, loss = 0.57082362\n",
            "Iteration 51, loss = 0.57196938\n",
            "Iteration 52, loss = 0.56920315\n",
            "Iteration 53, loss = 0.57409831\n",
            "Iteration 54, loss = 0.57370499\n",
            "Iteration 55, loss = 0.56925559\n",
            "Iteration 56, loss = 0.57363289\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.54093976\n",
            "Iteration 58, loss = 0.52714524\n",
            "Iteration 59, loss = 0.52125473\n",
            "Iteration 60, loss = 0.51573586\n",
            "Iteration 61, loss = 0.51066466\n",
            "Iteration 62, loss = 0.50507229\n",
            "Iteration 63, loss = 0.50470793\n",
            "Iteration 64, loss = 0.50117766\n",
            "Iteration 65, loss = 0.50636684\n",
            "Iteration 66, loss = 0.51491141\n",
            "Iteration 67, loss = 0.52858196\n",
            "Iteration 68, loss = 0.51704881\n",
            "Iteration 69, loss = 0.51993918\n",
            "Iteration 70, loss = 0.52845330\n",
            "Iteration 71, loss = 0.52478713\n",
            "Iteration 72, loss = 0.51950956\n",
            "Iteration 73, loss = 0.52422398\n",
            "Iteration 74, loss = 0.53323552\n",
            "Iteration 75, loss = 0.52768648\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.46471835\n",
            "Iteration 77, loss = 0.45933645\n",
            "Iteration 78, loss = 0.45711644\n",
            "Iteration 79, loss = 0.45482442\n",
            "Iteration 80, loss = 0.45247941\n",
            "Iteration 81, loss = 0.45050080\n",
            "Iteration 82, loss = 0.44781278\n",
            "Iteration 83, loss = 0.44515489\n",
            "Iteration 84, loss = 0.44274463\n",
            "Iteration 85, loss = 0.44072265\n",
            "Iteration 86, loss = 0.43873974\n",
            "Iteration 87, loss = 0.43657476\n",
            "Iteration 88, loss = 0.43444682\n",
            "Iteration 89, loss = 0.43205248\n",
            "Iteration 90, loss = 0.43017339\n",
            "Iteration 91, loss = 0.42818091\n",
            "Iteration 92, loss = 0.42566402\n",
            "Iteration 93, loss = 0.42409274\n",
            "Iteration 94, loss = 0.42155760\n",
            "Iteration 95, loss = 0.41936416\n",
            "Iteration 96, loss = 0.41796049\n",
            "Iteration 97, loss = 0.41561078\n",
            "Iteration 98, loss = 0.41410592\n",
            "Iteration 99, loss = 0.41203714\n",
            "Iteration 100, loss = 0.41003863\n",
            "Iteration 101, loss = 0.40885259\n",
            "Iteration 102, loss = 0.40664831\n",
            "Iteration 103, loss = 0.40457541\n",
            "Iteration 104, loss = 0.40285544\n",
            "Iteration 105, loss = 0.40163058\n",
            "Iteration 106, loss = 0.40021739\n",
            "Iteration 107, loss = 0.39797097\n",
            "Iteration 108, loss = 0.39610993\n",
            "Iteration 109, loss = 0.39461863\n",
            "Iteration 110, loss = 0.39279135\n",
            "Iteration 111, loss = 0.39115536\n",
            "Iteration 112, loss = 0.39076702\n",
            "Iteration 113, loss = 0.38944040\n",
            "Iteration 114, loss = 0.38755680\n",
            "Iteration 115, loss = 0.38517865\n",
            "Iteration 116, loss = 0.38504919\n",
            "Iteration 117, loss = 0.38389990\n",
            "Iteration 118, loss = 0.38140365\n",
            "Iteration 119, loss = 0.38132184\n",
            "Iteration 120, loss = 0.37990382\n",
            "Iteration 121, loss = 0.37816575\n",
            "Iteration 122, loss = 0.37740231\n",
            "Iteration 123, loss = 0.37605223\n",
            "Iteration 124, loss = 0.37502859\n",
            "Iteration 125, loss = 0.37412861\n",
            "Iteration 126, loss = 0.37379252\n",
            "Iteration 127, loss = 0.37306816\n",
            "Iteration 128, loss = 0.37271452\n",
            "Iteration 129, loss = 0.36950408\n",
            "Iteration 130, loss = 0.36870344\n",
            "Iteration 131, loss = 0.36809663\n",
            "Iteration 132, loss = 0.36859877\n",
            "Iteration 133, loss = 0.36809264\n",
            "Iteration 134, loss = 0.36838660\n",
            "Iteration 135, loss = 0.36341171\n",
            "Iteration 136, loss = 0.36440933\n",
            "Iteration 137, loss = 0.36532432\n",
            "Iteration 138, loss = 0.36405077\n",
            "Iteration 139, loss = 0.36328428\n",
            "Iteration 140, loss = 0.36273511\n",
            "Iteration 141, loss = 0.36088553\n",
            "Iteration 142, loss = 0.35859996\n",
            "Iteration 143, loss = 0.35991070\n",
            "Iteration 144, loss = 0.36261333\n",
            "Iteration 145, loss = 0.36238286\n",
            "Iteration 146, loss = 0.36176043\n",
            "Iteration 147, loss = 0.36209408\n",
            "Iteration 148, loss = 0.36342710\n",
            "Iteration 149, loss = 0.36133322\n",
            "Iteration 150, loss = 0.35688600\n",
            "Iteration 151, loss = 0.35856115\n",
            "Iteration 152, loss = 0.36027501\n",
            "Iteration 153, loss = 0.35601190\n",
            "Iteration 154, loss = 0.35462478\n",
            "Iteration 155, loss = 0.35745034\n",
            "Iteration 156, loss = 0.35328993\n",
            "Iteration 157, loss = 0.35886194\n",
            "Iteration 158, loss = 0.36078185\n",
            "Iteration 159, loss = 0.35435666\n",
            "Iteration 160, loss = 0.35839500\n",
            "Iteration 161, loss = 0.36153410\n",
            "Iteration 162, loss = 0.35281680\n",
            "Iteration 163, loss = 0.35506211\n",
            "Iteration 164, loss = 0.35383606\n",
            "Iteration 165, loss = 0.35758478\n",
            "Iteration 166, loss = 0.35159774\n",
            "Iteration 167, loss = 0.35492104\n",
            "Iteration 168, loss = 0.35699874\n",
            "Iteration 169, loss = 0.35917785\n",
            "Iteration 170, loss = 0.36209920\n",
            "Iteration 171, loss = 0.35765560\n",
            "Iteration 172, loss = 0.35785427\n",
            "Iteration 173, loss = 0.35895571\n",
            "Iteration 174, loss = 0.35641324\n",
            "Iteration 175, loss = 0.36052855\n",
            "Iteration 176, loss = 0.35755151\n",
            "Iteration 177, loss = 0.35796332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 178, loss = 0.33000901\n",
            "Iteration 179, loss = 0.32714279\n",
            "Iteration 180, loss = 0.32705878\n",
            "Iteration 181, loss = 0.32681294\n",
            "Iteration 182, loss = 0.32667226\n",
            "Iteration 183, loss = 0.32643948\n",
            "Iteration 184, loss = 0.32654488\n",
            "Iteration 185, loss = 0.32618995\n",
            "Iteration 186, loss = 0.32615994\n",
            "Iteration 187, loss = 0.32603612\n",
            "Iteration 188, loss = 0.32595354\n",
            "Iteration 189, loss = 0.32563255\n",
            "Iteration 190, loss = 0.32555114\n",
            "Iteration 191, loss = 0.32536684\n",
            "Iteration 192, loss = 0.32545211\n",
            "Iteration 193, loss = 0.32517775\n",
            "Iteration 194, loss = 0.32500151\n",
            "Iteration 195, loss = 0.32493050\n",
            "Iteration 196, loss = 0.32500758\n",
            "Iteration 197, loss = 0.32467627\n",
            "Iteration 198, loss = 0.32477141\n",
            "Iteration 199, loss = 0.32459642\n",
            "Iteration 200, loss = 0.32448266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70231792\n",
            "Iteration 3, loss = 0.68205026\n",
            "Iteration 4, loss = 0.66643359\n",
            "Iteration 5, loss = 0.65232293\n",
            "Iteration 6, loss = 0.64495848\n",
            "Iteration 7, loss = 0.63871182\n",
            "Iteration 8, loss = 0.62894843\n",
            "Iteration 9, loss = 0.62114637\n",
            "Iteration 10, loss = 0.61242045\n",
            "Iteration 11, loss = 0.60801931\n",
            "Iteration 12, loss = 0.60845833\n",
            "Iteration 13, loss = 0.61340958\n",
            "Iteration 14, loss = 0.62507235\n",
            "Iteration 15, loss = 0.61663682\n",
            "Iteration 16, loss = 0.60693049\n",
            "Iteration 17, loss = 0.60276273\n",
            "Iteration 18, loss = 0.59881690\n",
            "Iteration 19, loss = 0.59219557\n",
            "Iteration 20, loss = 0.59023640\n",
            "Iteration 21, loss = 0.58738264\n",
            "Iteration 22, loss = 0.58409844\n",
            "Iteration 23, loss = 0.58517555\n",
            "Iteration 24, loss = 0.58579051\n",
            "Iteration 25, loss = 0.58012466\n",
            "Iteration 26, loss = 0.58159764\n",
            "Iteration 27, loss = 0.57664820\n",
            "Iteration 28, loss = 0.57891403\n",
            "Iteration 29, loss = 0.57434863\n",
            "Iteration 30, loss = 0.57709590\n",
            "Iteration 31, loss = 0.57328973\n",
            "Iteration 32, loss = 0.57421264\n",
            "Iteration 33, loss = 0.57197644\n",
            "Iteration 34, loss = 0.57498697\n",
            "Iteration 35, loss = 0.57324197\n",
            "Iteration 36, loss = 0.57278912\n",
            "Iteration 37, loss = 0.57009005\n",
            "Iteration 38, loss = 0.57033278\n",
            "Iteration 39, loss = 0.57032315\n",
            "Iteration 40, loss = 0.57034850\n",
            "Iteration 41, loss = 0.56650819\n",
            "Iteration 42, loss = 0.57526972\n",
            "Iteration 43, loss = 0.57201114\n",
            "Iteration 44, loss = 0.57086569\n",
            "Iteration 45, loss = 0.57163718\n",
            "Iteration 46, loss = 0.57152964\n",
            "Iteration 47, loss = 0.57339942\n",
            "Iteration 48, loss = 0.57128642\n",
            "Iteration 49, loss = 0.58933234\n",
            "Iteration 50, loss = 0.62371377\n",
            "Iteration 51, loss = 0.60842190\n",
            "Iteration 52, loss = 0.59980509\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.58382194\n",
            "Iteration 54, loss = 0.57976541\n",
            "Iteration 55, loss = 0.57794175\n",
            "Iteration 56, loss = 0.57566475\n",
            "Iteration 57, loss = 0.57451925\n",
            "Iteration 58, loss = 0.57246478\n",
            "Iteration 59, loss = 0.57105555\n",
            "Iteration 60, loss = 0.56950054\n",
            "Iteration 61, loss = 0.56797269\n",
            "Iteration 62, loss = 0.56629206\n",
            "Iteration 63, loss = 0.56496747\n",
            "Iteration 64, loss = 0.56326643\n",
            "Iteration 65, loss = 0.56144839\n",
            "Iteration 66, loss = 0.55948712\n",
            "Iteration 67, loss = 0.55806231\n",
            "Iteration 68, loss = 0.55568923\n",
            "Iteration 69, loss = 0.55437935\n",
            "Iteration 70, loss = 0.55153554\n",
            "Iteration 71, loss = 0.55031444\n",
            "Iteration 72, loss = 0.54857691\n",
            "Iteration 73, loss = 0.54942266\n",
            "Iteration 74, loss = 0.54841394\n",
            "Iteration 75, loss = 0.54575517\n",
            "Iteration 76, loss = 0.54442647\n",
            "Iteration 77, loss = 0.54582037\n",
            "Iteration 78, loss = 0.54562553\n",
            "Iteration 79, loss = 0.54461586\n",
            "Iteration 80, loss = 0.54011762\n",
            "Iteration 81, loss = 0.54597842\n",
            "Iteration 82, loss = 0.54847384\n",
            "Iteration 83, loss = 0.54355428\n",
            "Iteration 84, loss = 0.54612663\n",
            "Iteration 85, loss = 0.54269375\n",
            "Iteration 86, loss = 0.54090425\n",
            "Iteration 87, loss = 0.54104835\n",
            "Iteration 88, loss = 0.54199423\n",
            "Iteration 89, loss = 0.54423430\n",
            "Iteration 90, loss = 0.54021412\n",
            "Iteration 91, loss = 0.53947360\n",
            "Iteration 92, loss = 0.54312053\n",
            "Iteration 93, loss = 0.53827330\n",
            "Iteration 94, loss = 0.53467675\n",
            "Iteration 95, loss = 0.53919892\n",
            "Iteration 96, loss = 0.53963699\n",
            "Iteration 97, loss = 0.53703092\n",
            "Iteration 98, loss = 0.53755919\n",
            "Iteration 99, loss = 0.53989463\n",
            "Iteration 100, loss = 0.53565797\n",
            "Iteration 101, loss = 0.53331524\n",
            "Iteration 102, loss = 0.53162618\n",
            "Iteration 103, loss = 0.53229339\n",
            "Iteration 104, loss = 0.53626668\n",
            "Iteration 105, loss = 0.66503595\n",
            "Iteration 106, loss = 0.63694376\n",
            "Iteration 107, loss = 0.63341704\n",
            "Iteration 108, loss = 0.62957354\n",
            "Iteration 109, loss = 0.62506061\n",
            "Iteration 110, loss = 0.61964892\n",
            "Iteration 111, loss = 0.61362091\n",
            "Iteration 112, loss = 0.60762997\n",
            "Iteration 113, loss = 0.60219788\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 114, loss = 0.59764961\n",
            "Iteration 115, loss = 0.59697943\n",
            "Iteration 116, loss = 0.59575490\n",
            "Iteration 117, loss = 0.59504846\n",
            "Iteration 118, loss = 0.59427806\n",
            "Iteration 119, loss = 0.59311469\n",
            "Iteration 120, loss = 0.59262309\n",
            "Iteration 121, loss = 0.59182782\n",
            "Iteration 122, loss = 0.59129754\n",
            "Iteration 123, loss = 0.59056425\n",
            "Iteration 124, loss = 0.58993806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 125, loss = 0.58902907\n",
            "Iteration 126, loss = 0.58886747\n",
            "Iteration 127, loss = 0.58859373\n",
            "Iteration 128, loss = 0.58844123\n",
            "Iteration 129, loss = 0.58840804\n",
            "Iteration 130, loss = 0.58817749\n",
            "Iteration 131, loss = 0.58808826\n",
            "Iteration 132, loss = 0.58781489\n",
            "Iteration 133, loss = 0.58782929\n",
            "Iteration 134, loss = 0.58770869\n",
            "Iteration 135, loss = 0.58766160\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 136, loss = 0.58730690\n",
            "Iteration 137, loss = 0.58729750\n",
            "Iteration 138, loss = 0.58728571\n",
            "Iteration 139, loss = 0.58725062\n",
            "Iteration 140, loss = 0.58724099\n",
            "Iteration 141, loss = 0.58718342\n",
            "Iteration 142, loss = 0.58720244\n",
            "Iteration 143, loss = 0.58717279\n",
            "Iteration 144, loss = 0.58714492\n",
            "Iteration 145, loss = 0.58706920\n",
            "Iteration 146, loss = 0.58707661\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 147, loss = 0.58703496\n",
            "Iteration 148, loss = 0.58701201\n",
            "Iteration 149, loss = 0.58700707\n",
            "Iteration 150, loss = 0.58699656\n",
            "Iteration 151, loss = 0.58699801\n",
            "Iteration 152, loss = 0.58699076\n",
            "Iteration 153, loss = 0.58698036\n",
            "Iteration 154, loss = 0.58698214\n",
            "Iteration 155, loss = 0.58697358\n",
            "Iteration 156, loss = 0.58697061\n",
            "Iteration 157, loss = 0.58697312\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70808678\n",
            "Iteration 3, loss = 0.68743669\n",
            "Iteration 4, loss = 0.67119962\n",
            "Iteration 5, loss = 0.65791557\n",
            "Iteration 6, loss = 0.64747112\n",
            "Iteration 7, loss = 0.63665867\n",
            "Iteration 8, loss = 0.63083022\n",
            "Iteration 9, loss = 0.62155046\n",
            "Iteration 10, loss = 0.61461799\n",
            "Iteration 11, loss = 0.60924681\n",
            "Iteration 12, loss = 0.60691096\n",
            "Iteration 13, loss = 0.60233840\n",
            "Iteration 14, loss = 0.59826950\n",
            "Iteration 15, loss = 0.59443016\n",
            "Iteration 16, loss = 0.59222014\n",
            "Iteration 17, loss = 0.59046388\n",
            "Iteration 18, loss = 0.58735201\n",
            "Iteration 19, loss = 0.58401205\n",
            "Iteration 20, loss = 0.58913976\n",
            "Iteration 21, loss = 0.58116017\n",
            "Iteration 22, loss = 0.58235872\n",
            "Iteration 23, loss = 0.58075564\n",
            "Iteration 24, loss = 0.58472360\n",
            "Iteration 25, loss = 0.57721210\n",
            "Iteration 26, loss = 0.57702401\n",
            "Iteration 27, loss = 0.57513937\n",
            "Iteration 28, loss = 0.57612397\n",
            "Iteration 29, loss = 0.57359611\n",
            "Iteration 30, loss = 0.57436139\n",
            "Iteration 31, loss = 0.57435621\n",
            "Iteration 32, loss = 0.57488505\n",
            "Iteration 33, loss = 0.57268957\n",
            "Iteration 34, loss = 0.57455280\n",
            "Iteration 35, loss = 0.57026170\n",
            "Iteration 36, loss = 0.64210897\n",
            "Iteration 37, loss = 0.61414175\n",
            "Iteration 38, loss = 0.60214964\n",
            "Iteration 39, loss = 0.59555395\n",
            "Iteration 40, loss = 0.59106702\n",
            "Iteration 41, loss = 0.58825715\n",
            "Iteration 42, loss = 0.58439398\n",
            "Iteration 43, loss = 0.58118093\n",
            "Iteration 44, loss = 0.58020667\n",
            "Iteration 45, loss = 0.57836263\n",
            "Iteration 46, loss = 0.57602296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.55925507\n",
            "Iteration 48, loss = 0.55644330\n",
            "Iteration 49, loss = 0.55455761\n",
            "Iteration 50, loss = 0.55307060\n",
            "Iteration 51, loss = 0.55122980\n",
            "Iteration 52, loss = 0.54857951\n",
            "Iteration 53, loss = 0.54627392\n",
            "Iteration 54, loss = 0.54474488\n",
            "Iteration 55, loss = 0.54104200\n",
            "Iteration 56, loss = 0.53925758\n",
            "Iteration 57, loss = 0.53695789\n",
            "Iteration 58, loss = 0.53488463\n",
            "Iteration 59, loss = 0.53160014\n",
            "Iteration 60, loss = 0.53200066\n",
            "Iteration 61, loss = 0.53118095\n",
            "Iteration 62, loss = 0.53169436\n",
            "Iteration 63, loss = 0.53055904\n",
            "Iteration 64, loss = 0.53799450\n",
            "Iteration 65, loss = 0.54090087\n",
            "Iteration 66, loss = 0.53478648\n",
            "Iteration 67, loss = 0.53003414\n",
            "Iteration 68, loss = 0.53594417\n",
            "Iteration 69, loss = 0.53614100\n",
            "Iteration 70, loss = 0.53514794\n",
            "Iteration 71, loss = 0.54102969\n",
            "Iteration 72, loss = 0.54212732\n",
            "Iteration 73, loss = 0.53936061\n",
            "Iteration 74, loss = 0.53608616\n",
            "Iteration 75, loss = 0.53247361\n",
            "Iteration 76, loss = 0.52811921\n",
            "Iteration 77, loss = 0.53407646\n",
            "Iteration 78, loss = 0.54394538\n",
            "Iteration 79, loss = 0.53624595\n",
            "Iteration 80, loss = 0.53397375\n",
            "Iteration 81, loss = 0.53168714\n",
            "Iteration 82, loss = 0.53351221\n",
            "Iteration 83, loss = 0.53534459\n",
            "Iteration 84, loss = 0.53131718\n",
            "Iteration 85, loss = 0.53136918\n",
            "Iteration 86, loss = 0.53379898\n",
            "Iteration 87, loss = 0.53387759\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 88, loss = 0.47618482\n",
            "Iteration 89, loss = 0.47306477\n",
            "Iteration 90, loss = 0.47178774\n",
            "Iteration 91, loss = 0.46944079\n",
            "Iteration 92, loss = 0.46777773\n",
            "Iteration 93, loss = 0.46522318\n",
            "Iteration 94, loss = 0.46338065\n",
            "Iteration 95, loss = 0.46186935\n",
            "Iteration 96, loss = 0.45975043\n",
            "Iteration 97, loss = 0.45782557\n",
            "Iteration 98, loss = 0.45570934\n",
            "Iteration 99, loss = 0.45389346\n",
            "Iteration 100, loss = 0.45169200\n",
            "Iteration 101, loss = 0.44975141\n",
            "Iteration 102, loss = 0.44781117\n",
            "Iteration 103, loss = 0.44604909\n",
            "Iteration 104, loss = 0.44401746\n",
            "Iteration 105, loss = 0.44218847\n",
            "Iteration 106, loss = 0.44018717\n",
            "Iteration 107, loss = 0.43825927\n",
            "Iteration 108, loss = 0.43673206\n",
            "Iteration 109, loss = 0.43512069\n",
            "Iteration 110, loss = 0.43369021\n",
            "Iteration 111, loss = 0.43138108\n",
            "Iteration 112, loss = 0.42986523\n",
            "Iteration 113, loss = 0.42806846\n",
            "Iteration 114, loss = 0.42634262\n",
            "Iteration 115, loss = 0.42468344\n",
            "Iteration 116, loss = 0.42297249\n",
            "Iteration 117, loss = 0.42149500\n",
            "Iteration 118, loss = 0.42035277\n",
            "Iteration 119, loss = 0.41826551\n",
            "Iteration 120, loss = 0.41637238\n",
            "Iteration 121, loss = 0.41450717\n",
            "Iteration 122, loss = 0.41287433\n",
            "Iteration 123, loss = 0.41178803\n",
            "Iteration 124, loss = 0.41017711\n",
            "Iteration 125, loss = 0.40968491\n",
            "Iteration 126, loss = 0.40877562\n",
            "Iteration 127, loss = 0.40593195\n",
            "Iteration 128, loss = 0.40530652\n",
            "Iteration 129, loss = 0.40364374\n",
            "Iteration 130, loss = 0.40244284\n",
            "Iteration 131, loss = 0.40077456\n",
            "Iteration 132, loss = 0.39972412\n",
            "Iteration 133, loss = 0.39789248\n",
            "Iteration 134, loss = 0.39769362\n",
            "Iteration 135, loss = 0.39489825\n",
            "Iteration 136, loss = 0.39420109\n",
            "Iteration 137, loss = 0.39332652\n",
            "Iteration 138, loss = 0.39256939\n",
            "Iteration 139, loss = 0.39177619\n",
            "Iteration 140, loss = 0.39010690\n",
            "Iteration 141, loss = 0.39093418\n",
            "Iteration 142, loss = 0.38878268\n",
            "Iteration 143, loss = 0.38849546\n",
            "Iteration 144, loss = 0.38530578\n",
            "Iteration 145, loss = 0.38555755\n",
            "Iteration 146, loss = 0.38620488\n",
            "Iteration 147, loss = 0.38332760\n",
            "Iteration 148, loss = 0.38643934\n",
            "Iteration 149, loss = 0.38095310\n",
            "Iteration 150, loss = 0.38205538\n",
            "Iteration 151, loss = 0.38212666\n",
            "Iteration 152, loss = 0.38093833\n",
            "Iteration 153, loss = 0.37956415\n",
            "Iteration 154, loss = 0.38109677\n",
            "Iteration 155, loss = 0.38002296\n",
            "Iteration 156, loss = 0.37763965\n",
            "Iteration 157, loss = 0.37883289\n",
            "Iteration 158, loss = 0.38182231\n",
            "Iteration 159, loss = 0.37803687\n",
            "Iteration 160, loss = 0.37752298\n",
            "Iteration 161, loss = 0.37839282\n",
            "Iteration 162, loss = 0.37818652\n",
            "Iteration 163, loss = 0.37512073\n",
            "Iteration 164, loss = 0.37240970\n",
            "Iteration 165, loss = 0.37381633\n",
            "Iteration 166, loss = 0.37237546\n",
            "Iteration 167, loss = 0.37518982\n",
            "Iteration 168, loss = 0.37217720\n",
            "Iteration 169, loss = 0.37273283\n",
            "Iteration 170, loss = 0.37925899\n",
            "Iteration 171, loss = 0.38225666\n",
            "Iteration 172, loss = 0.37213845\n",
            "Iteration 173, loss = 0.36839909\n",
            "Iteration 174, loss = 0.37977096\n",
            "Iteration 175, loss = 0.37174438\n",
            "Iteration 176, loss = 0.37437421\n",
            "Iteration 177, loss = 0.37623820\n",
            "Iteration 178, loss = 0.37978805\n",
            "Iteration 179, loss = 0.37384513\n",
            "Iteration 180, loss = 0.37811054\n",
            "Iteration 181, loss = 0.37248230\n",
            "Iteration 182, loss = 0.37283552\n",
            "Iteration 183, loss = 0.37104846\n",
            "Iteration 184, loss = 0.37423303\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 185, loss = 0.34538127\n",
            "Iteration 186, loss = 0.34450117\n",
            "Iteration 187, loss = 0.34436064\n",
            "Iteration 188, loss = 0.34416939\n",
            "Iteration 189, loss = 0.34390823\n",
            "Iteration 190, loss = 0.34383072\n",
            "Iteration 191, loss = 0.34345031\n",
            "Iteration 192, loss = 0.34331051\n",
            "Iteration 193, loss = 0.34331197\n",
            "Iteration 194, loss = 0.34301138\n",
            "Iteration 195, loss = 0.34289940\n",
            "Iteration 196, loss = 0.34281684\n",
            "Iteration 197, loss = 0.34274420\n",
            "Iteration 198, loss = 0.34250955\n",
            "Iteration 199, loss = 0.34247635\n",
            "Iteration 200, loss = 0.34232566\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.73184640\n",
            "Iteration 3, loss = 0.71751907\n",
            "Iteration 4, loss = 0.70385423\n",
            "Iteration 5, loss = 0.69193343\n",
            "Iteration 6, loss = 0.67982280\n",
            "Iteration 7, loss = 0.66263123\n",
            "Iteration 8, loss = 0.64649066\n",
            "Iteration 9, loss = 0.63954765\n",
            "Iteration 10, loss = 0.62639205\n",
            "Iteration 11, loss = 0.62627032\n",
            "Iteration 12, loss = 0.61752003\n",
            "Iteration 13, loss = 0.60990950\n",
            "Iteration 14, loss = 0.60599557\n",
            "Iteration 15, loss = 0.60096603\n",
            "Iteration 16, loss = 0.59671784\n",
            "Iteration 17, loss = 0.59266713\n",
            "Iteration 18, loss = 0.59157028\n",
            "Iteration 19, loss = 0.58745153\n",
            "Iteration 20, loss = 0.62442595\n",
            "Iteration 21, loss = 0.60861144\n",
            "Iteration 22, loss = 0.60333153\n",
            "Iteration 23, loss = 0.59520123\n",
            "Iteration 24, loss = 0.59058500\n",
            "Iteration 25, loss = 0.59145963\n",
            "Iteration 26, loss = 0.58685081\n",
            "Iteration 27, loss = 0.58281411\n",
            "Iteration 28, loss = 0.58337463\n",
            "Iteration 29, loss = 0.57762725\n",
            "Iteration 30, loss = 0.57896204\n",
            "Iteration 31, loss = 0.57911839\n",
            "Iteration 32, loss = 0.57638994\n",
            "Iteration 33, loss = 0.57604238\n",
            "Iteration 34, loss = 0.57237433\n",
            "Iteration 35, loss = 0.57220013\n",
            "Iteration 36, loss = 0.57548126\n",
            "Iteration 37, loss = 0.57226388\n",
            "Iteration 38, loss = 0.57504518\n",
            "Iteration 39, loss = 0.57273914\n",
            "Iteration 40, loss = 0.62888816\n",
            "Iteration 41, loss = 0.60476510\n",
            "Iteration 42, loss = 0.59809967\n",
            "Iteration 43, loss = 0.59152569\n",
            "Iteration 44, loss = 0.58923847\n",
            "Iteration 45, loss = 0.58257327\n",
            "Iteration 46, loss = 0.58170717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 47, loss = 0.57137723\n",
            "Iteration 48, loss = 0.56552146\n",
            "Iteration 49, loss = 0.56459474\n",
            "Iteration 50, loss = 0.56326179\n",
            "Iteration 51, loss = 0.56163963\n",
            "Iteration 52, loss = 0.56011265\n",
            "Iteration 53, loss = 0.55832928\n",
            "Iteration 54, loss = 0.55707623\n",
            "Iteration 55, loss = 0.55571577\n",
            "Iteration 56, loss = 0.55368567\n",
            "Iteration 57, loss = 0.55226460\n",
            "Iteration 58, loss = 0.55028196\n",
            "Iteration 59, loss = 0.54789292\n",
            "Iteration 60, loss = 0.54587617\n",
            "Iteration 61, loss = 0.54429553\n",
            "Iteration 62, loss = 0.54259865\n",
            "Iteration 63, loss = 0.53903504\n",
            "Iteration 64, loss = 0.53840642\n",
            "Iteration 65, loss = 0.53537312\n",
            "Iteration 66, loss = 0.53839763\n",
            "Iteration 67, loss = 0.53798529\n",
            "Iteration 68, loss = 0.53785738\n",
            "Iteration 69, loss = 0.53506148\n",
            "Iteration 70, loss = 0.53723620\n",
            "Iteration 71, loss = 0.54013115\n",
            "Iteration 72, loss = 0.54032569\n",
            "Iteration 73, loss = 0.54499378\n",
            "Iteration 74, loss = 0.53944073\n",
            "Iteration 75, loss = 0.53724829\n",
            "Iteration 76, loss = 0.53803559\n",
            "Iteration 77, loss = 0.54073789\n",
            "Iteration 78, loss = 0.54058038\n",
            "Iteration 79, loss = 0.53745921\n",
            "Iteration 80, loss = 0.53480811\n",
            "Iteration 81, loss = 0.53124578\n",
            "Iteration 82, loss = 0.53744733\n",
            "Iteration 83, loss = 0.53180998\n",
            "Iteration 84, loss = 0.53495376\n",
            "Iteration 85, loss = 0.53593437\n",
            "Iteration 86, loss = 0.53366928\n",
            "Iteration 87, loss = 0.53450605\n",
            "Iteration 88, loss = 0.53535393\n",
            "Iteration 89, loss = 0.53095225\n",
            "Iteration 90, loss = 0.52957936\n",
            "Iteration 91, loss = 0.54042313\n",
            "Iteration 92, loss = 0.53672835\n",
            "Iteration 93, loss = 0.52677047\n",
            "Iteration 94, loss = 0.53131665\n",
            "Iteration 95, loss = 0.53498540\n",
            "Iteration 96, loss = 0.52838144\n",
            "Iteration 97, loss = 0.52986394\n",
            "Iteration 98, loss = 0.53722973\n",
            "Iteration 99, loss = 0.53076945\n",
            "Iteration 100, loss = 0.52689711\n",
            "Iteration 101, loss = 0.52391186\n",
            "Iteration 102, loss = 0.52870254\n",
            "Iteration 103, loss = 0.52831968\n",
            "Iteration 104, loss = 0.52819470\n",
            "Iteration 105, loss = 0.53076470\n",
            "Iteration 106, loss = 0.53438778\n",
            "Iteration 107, loss = 0.53058017\n",
            "Iteration 108, loss = 0.52425325\n",
            "Iteration 109, loss = 0.52609765\n",
            "Iteration 110, loss = 0.51901677\n",
            "Iteration 111, loss = 0.52583556\n",
            "Iteration 112, loss = 0.52353831\n",
            "Iteration 113, loss = 0.52568413\n",
            "Iteration 114, loss = 0.53271904\n",
            "Iteration 115, loss = 0.52305597\n",
            "Iteration 116, loss = 0.52485459\n",
            "Iteration 117, loss = 0.53266693\n",
            "Iteration 118, loss = 0.52162059\n",
            "Iteration 119, loss = 0.52863748\n",
            "Iteration 120, loss = 0.51434100\n",
            "Iteration 121, loss = 0.52874424\n",
            "Iteration 122, loss = 0.52904172\n",
            "Iteration 123, loss = 0.51871797\n",
            "Iteration 124, loss = 0.53244604\n",
            "Iteration 125, loss = 0.52495520\n",
            "Iteration 126, loss = 0.52712507\n",
            "Iteration 127, loss = 0.51993970\n",
            "Iteration 128, loss = 0.52893372\n",
            "Iteration 129, loss = 0.52259486\n",
            "Iteration 130, loss = 0.52822119\n",
            "Iteration 131, loss = 0.52226121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 132, loss = 0.45159539\n",
            "Iteration 133, loss = 0.44785872\n",
            "Iteration 134, loss = 0.44482055\n",
            "Iteration 135, loss = 0.44181805\n",
            "Iteration 136, loss = 0.43941382\n",
            "Iteration 137, loss = 0.43637728\n",
            "Iteration 138, loss = 0.43329977\n",
            "Iteration 139, loss = 0.43059932\n",
            "Iteration 140, loss = 0.42829490\n",
            "Iteration 141, loss = 0.42550444\n",
            "Iteration 142, loss = 0.42291383\n",
            "Iteration 143, loss = 0.41998805\n",
            "Iteration 144, loss = 0.41779661\n",
            "Iteration 145, loss = 0.41529456\n",
            "Iteration 146, loss = 0.41309564\n",
            "Iteration 147, loss = 0.41065156\n",
            "Iteration 148, loss = 0.40834608\n",
            "Iteration 149, loss = 0.40561161\n",
            "Iteration 150, loss = 0.40362880\n",
            "Iteration 151, loss = 0.40143554\n",
            "Iteration 152, loss = 0.39965455\n",
            "Iteration 153, loss = 0.39736955\n",
            "Iteration 154, loss = 0.39537008\n",
            "Iteration 155, loss = 0.39308155\n",
            "Iteration 156, loss = 0.39113642\n",
            "Iteration 157, loss = 0.38949761\n",
            "Iteration 158, loss = 0.38765372\n",
            "Iteration 159, loss = 0.38588021\n",
            "Iteration 160, loss = 0.38379071\n",
            "Iteration 161, loss = 0.38195081\n",
            "Iteration 162, loss = 0.38056976\n",
            "Iteration 163, loss = 0.37889803\n",
            "Iteration 164, loss = 0.37784393\n",
            "Iteration 165, loss = 0.37574540\n",
            "Iteration 166, loss = 0.37459674\n",
            "Iteration 167, loss = 0.37284583\n",
            "Iteration 168, loss = 0.37061662\n",
            "Iteration 169, loss = 0.37009029\n",
            "Iteration 170, loss = 0.36800120\n",
            "Iteration 171, loss = 0.36691434\n",
            "Iteration 172, loss = 0.36598488\n",
            "Iteration 173, loss = 0.36398059\n",
            "Iteration 174, loss = 0.36338999\n",
            "Iteration 175, loss = 0.36178915\n",
            "Iteration 176, loss = 0.36150328\n",
            "Iteration 177, loss = 0.36096786\n",
            "Iteration 178, loss = 0.36080812\n",
            "Iteration 179, loss = 0.35902152\n",
            "Iteration 180, loss = 0.35960590\n",
            "Iteration 181, loss = 0.35595477\n",
            "Iteration 182, loss = 0.35418166\n",
            "Iteration 183, loss = 0.35414608\n",
            "Iteration 184, loss = 0.35338545\n",
            "Iteration 185, loss = 0.35414067\n",
            "Iteration 186, loss = 0.35286047\n",
            "Iteration 187, loss = 0.35070857\n",
            "Iteration 188, loss = 0.35136869\n",
            "Iteration 189, loss = 0.35155990\n",
            "Iteration 190, loss = 0.35025116\n",
            "Iteration 191, loss = 0.34714498\n",
            "Iteration 192, loss = 0.34567927\n",
            "Iteration 193, loss = 0.34667810\n",
            "Iteration 194, loss = 0.34550145\n",
            "Iteration 195, loss = 0.34458414\n",
            "Iteration 196, loss = 0.34661033\n",
            "Iteration 197, loss = 0.34417411\n",
            "Iteration 198, loss = 0.34309231\n",
            "Iteration 199, loss = 0.34093527\n",
            "Iteration 200, loss = 0.34344618\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.68938778\n",
            "Iteration 3, loss = 0.67919494\n",
            "Iteration 4, loss = 0.66147955\n",
            "Iteration 5, loss = 0.65140653\n",
            "Iteration 6, loss = 0.64195094\n",
            "Iteration 7, loss = 0.63625908\n",
            "Iteration 8, loss = 0.62559414\n",
            "Iteration 9, loss = 0.61735129\n",
            "Iteration 10, loss = 0.61306423\n",
            "Iteration 11, loss = 0.60649998\n",
            "Iteration 12, loss = 0.60373896\n",
            "Iteration 13, loss = 0.59899246\n",
            "Iteration 14, loss = 0.59988125\n",
            "Iteration 15, loss = 0.59161700\n",
            "Iteration 16, loss = 0.59211597\n",
            "Iteration 17, loss = 0.58569126\n",
            "Iteration 18, loss = 0.58640811\n",
            "Iteration 19, loss = 0.58272184\n",
            "Iteration 20, loss = 0.58291241\n",
            "Iteration 21, loss = 0.58307618\n",
            "Iteration 22, loss = 0.57901910\n",
            "Iteration 23, loss = 0.58053833\n",
            "Iteration 24, loss = 0.57770147\n",
            "Iteration 25, loss = 0.57400425\n",
            "Iteration 26, loss = 0.57780484\n",
            "Iteration 27, loss = 0.57986256\n",
            "Iteration 28, loss = 0.58004458\n",
            "Iteration 29, loss = 0.57247324\n",
            "Iteration 30, loss = 0.57545657\n",
            "Iteration 31, loss = 0.57471799\n",
            "Iteration 32, loss = 0.57294533\n",
            "Iteration 33, loss = 0.57344466\n",
            "Iteration 34, loss = 0.57267608\n",
            "Iteration 35, loss = 0.57608149\n",
            "Iteration 36, loss = 0.57149465\n",
            "Iteration 37, loss = 0.57386845\n",
            "Iteration 38, loss = 0.57573776\n",
            "Iteration 39, loss = 0.57257226\n",
            "Iteration 40, loss = 0.57029481\n",
            "Iteration 41, loss = 0.57183264\n",
            "Iteration 42, loss = 0.57115916\n",
            "Iteration 43, loss = 0.57085574\n",
            "Iteration 44, loss = 0.57213532\n",
            "Iteration 45, loss = 0.57191608\n",
            "Iteration 46, loss = 0.57247257\n",
            "Iteration 47, loss = 0.57022327\n",
            "Iteration 48, loss = 0.57146272\n",
            "Iteration 49, loss = 0.57085765\n",
            "Iteration 50, loss = 0.57206050\n",
            "Iteration 51, loss = 0.57317720\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.53431406\n",
            "Iteration 53, loss = 0.52518633\n",
            "Iteration 54, loss = 0.51950543\n",
            "Iteration 55, loss = 0.51503441\n",
            "Iteration 56, loss = 0.50919228\n",
            "Iteration 57, loss = 0.50533585\n",
            "Iteration 58, loss = 0.50059424\n",
            "Iteration 59, loss = 0.50426583\n",
            "Iteration 60, loss = 0.51329462\n",
            "Iteration 61, loss = 0.51019691\n",
            "Iteration 62, loss = 0.52579496\n",
            "Iteration 63, loss = 0.52034200\n",
            "Iteration 64, loss = 0.52715000\n",
            "Iteration 65, loss = 0.52677095\n",
            "Iteration 66, loss = 0.52506874\n",
            "Iteration 67, loss = 0.52498198\n",
            "Iteration 68, loss = 0.51991663\n",
            "Iteration 69, loss = 0.51390314\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 70, loss = 0.45759590\n",
            "Iteration 71, loss = 0.45264076\n",
            "Iteration 72, loss = 0.45065771\n",
            "Iteration 73, loss = 0.44737946\n",
            "Iteration 74, loss = 0.44455243\n",
            "Iteration 75, loss = 0.44241660\n",
            "Iteration 76, loss = 0.43957712\n",
            "Iteration 77, loss = 0.43691468\n",
            "Iteration 78, loss = 0.43412948\n",
            "Iteration 79, loss = 0.43189152\n",
            "Iteration 80, loss = 0.42973262\n",
            "Iteration 81, loss = 0.42656211\n",
            "Iteration 82, loss = 0.42453076\n",
            "Iteration 83, loss = 0.42159313\n",
            "Iteration 84, loss = 0.41946829\n",
            "Iteration 85, loss = 0.41719641\n",
            "Iteration 86, loss = 0.41502510\n",
            "Iteration 87, loss = 0.41230330\n",
            "Iteration 88, loss = 0.41018182\n",
            "Iteration 89, loss = 0.40761668\n",
            "Iteration 90, loss = 0.40531778\n",
            "Iteration 91, loss = 0.40272011\n",
            "Iteration 92, loss = 0.40071531\n",
            "Iteration 93, loss = 0.39861515\n",
            "Iteration 94, loss = 0.39631404\n",
            "Iteration 95, loss = 0.39411040\n",
            "Iteration 96, loss = 0.39245647\n",
            "Iteration 97, loss = 0.38974325\n",
            "Iteration 98, loss = 0.38737747\n",
            "Iteration 99, loss = 0.38579482\n",
            "Iteration 100, loss = 0.38370826\n",
            "Iteration 101, loss = 0.38209370\n",
            "Iteration 102, loss = 0.37967675\n",
            "Iteration 103, loss = 0.37812901\n",
            "Iteration 104, loss = 0.37623542\n",
            "Iteration 105, loss = 0.37413569\n",
            "Iteration 106, loss = 0.37167365\n",
            "Iteration 107, loss = 0.37035126\n",
            "Iteration 108, loss = 0.36896209\n",
            "Iteration 109, loss = 0.36689846\n",
            "Iteration 110, loss = 0.36554254\n",
            "Iteration 111, loss = 0.36324659\n",
            "Iteration 112, loss = 0.36140290\n",
            "Iteration 113, loss = 0.35972712\n",
            "Iteration 114, loss = 0.35850694\n",
            "Iteration 115, loss = 0.35615163\n",
            "Iteration 116, loss = 0.35506745\n",
            "Iteration 117, loss = 0.35324000\n",
            "Iteration 118, loss = 0.35181447\n",
            "Iteration 119, loss = 0.35034567\n",
            "Iteration 120, loss = 0.34858024\n",
            "Iteration 121, loss = 0.34756207\n",
            "Iteration 122, loss = 0.34613850\n",
            "Iteration 123, loss = 0.34408928\n",
            "Iteration 124, loss = 0.34299862\n",
            "Iteration 125, loss = 0.34179306\n",
            "Iteration 126, loss = 0.34045800\n",
            "Iteration 127, loss = 0.33886582\n",
            "Iteration 128, loss = 0.33833342\n",
            "Iteration 129, loss = 0.33650893\n",
            "Iteration 130, loss = 0.33425693\n",
            "Iteration 131, loss = 0.33339748\n",
            "Iteration 132, loss = 0.33380941\n",
            "Iteration 133, loss = 0.33163603\n",
            "Iteration 134, loss = 0.33098998\n",
            "Iteration 135, loss = 0.32808076\n",
            "Iteration 136, loss = 0.32856199\n",
            "Iteration 137, loss = 0.32602807\n",
            "Iteration 138, loss = 0.32638651\n",
            "Iteration 139, loss = 0.32428891\n",
            "Iteration 140, loss = 0.32415539\n",
            "Iteration 141, loss = 0.32265738\n",
            "Iteration 142, loss = 0.32412261\n",
            "Iteration 143, loss = 0.32071897\n",
            "Iteration 144, loss = 0.32127076\n",
            "Iteration 145, loss = 0.31891123\n",
            "Iteration 146, loss = 0.31942992\n",
            "Iteration 147, loss = 0.31842529\n",
            "Iteration 148, loss = 0.31579215\n",
            "Iteration 149, loss = 0.31506166\n",
            "Iteration 150, loss = 0.31353981\n",
            "Iteration 151, loss = 0.31481010\n",
            "Iteration 152, loss = 0.31101147\n",
            "Iteration 153, loss = 0.31140575\n",
            "Iteration 154, loss = 0.31131471\n",
            "Iteration 155, loss = 0.30994818\n",
            "Iteration 156, loss = 0.31124891\n",
            "Iteration 157, loss = 0.31366544\n",
            "Iteration 158, loss = 0.30778385\n",
            "Iteration 159, loss = 0.30467835\n",
            "Iteration 160, loss = 0.31077871\n",
            "Iteration 161, loss = 0.30955396\n",
            "Iteration 162, loss = 0.30702389\n",
            "Iteration 163, loss = 0.30883033\n",
            "Iteration 164, loss = 0.30784642\n",
            "Iteration 165, loss = 0.30365587\n",
            "Iteration 166, loss = 0.30472625\n",
            "Iteration 167, loss = 0.30955242\n",
            "Iteration 168, loss = 0.30588441\n",
            "Iteration 169, loss = 0.30152696\n",
            "Iteration 170, loss = 0.30091461\n",
            "Iteration 171, loss = 0.30413523\n",
            "Iteration 172, loss = 0.30545197\n",
            "Iteration 173, loss = 0.30236760\n",
            "Iteration 174, loss = 0.30434780\n",
            "Iteration 175, loss = 0.31099468\n",
            "Iteration 176, loss = 0.30569002\n",
            "Iteration 177, loss = 0.31003818\n",
            "Iteration 178, loss = 0.29947453\n",
            "Iteration 179, loss = 0.30488537\n",
            "Iteration 180, loss = 0.30213477\n",
            "Iteration 181, loss = 0.29608407\n",
            "Iteration 182, loss = 0.30708057\n",
            "Iteration 183, loss = 0.30278465\n",
            "Iteration 184, loss = 0.30443633\n",
            "Iteration 185, loss = 0.29876108\n",
            "Iteration 186, loss = 0.29807659\n",
            "Iteration 187, loss = 0.30840255\n",
            "Iteration 188, loss = 0.30212733\n",
            "Iteration 189, loss = 0.31792878\n",
            "Iteration 190, loss = 0.32069586\n",
            "Iteration 191, loss = 0.30011853\n",
            "Iteration 192, loss = 0.29846491\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 193, loss = 0.28020326\n",
            "Iteration 194, loss = 0.27986566\n",
            "Iteration 195, loss = 0.27975411\n",
            "Iteration 196, loss = 0.27968601\n",
            "Iteration 197, loss = 0.27988457\n",
            "Iteration 198, loss = 0.27939471\n",
            "Iteration 199, loss = 0.27944768\n",
            "Iteration 200, loss = 0.27914813\n",
            "----------------------------------\n",
            "[[33313   414]\n",
            " [ 1132 15787]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     33727\n",
            "           1       0.97      0.93      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.97     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.756% (0.111%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.855% (8.664%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62799677\n",
            "Iteration 3, loss = 0.60759358\n",
            "Iteration 4, loss = 0.59527957\n",
            "Iteration 5, loss = 0.58962312\n",
            "Iteration 6, loss = 0.58649646\n",
            "Iteration 7, loss = 0.58208447\n",
            "Iteration 8, loss = 0.58016925\n",
            "Iteration 9, loss = 0.57764541\n",
            "Iteration 10, loss = 0.58148543\n",
            "Iteration 11, loss = 0.58247814\n",
            "Iteration 12, loss = 0.58179195\n",
            "Iteration 13, loss = 0.57934551\n",
            "Iteration 14, loss = 0.57712324\n",
            "Iteration 15, loss = 0.57886013\n",
            "Iteration 16, loss = 0.58136889\n",
            "Iteration 17, loss = 0.57794463\n",
            "Iteration 18, loss = 0.57656451\n",
            "Iteration 19, loss = 0.58160728\n",
            "Iteration 20, loss = 0.57911127\n",
            "Iteration 21, loss = 0.57891088\n",
            "Iteration 22, loss = 0.58178474\n",
            "Iteration 23, loss = 0.57778293\n",
            "Iteration 24, loss = 0.57847372\n",
            "Iteration 25, loss = 0.57646710\n",
            "Iteration 26, loss = 0.59745581\n",
            "Iteration 27, loss = 0.59057086\n",
            "Iteration 28, loss = 0.58239388\n",
            "Iteration 29, loss = 0.57672102\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.54276698\n",
            "Iteration 31, loss = 0.52299599\n",
            "Iteration 32, loss = 0.52853501\n",
            "Iteration 33, loss = 0.53609332\n",
            "Iteration 34, loss = 0.53228572\n",
            "Iteration 35, loss = 0.53323130\n",
            "Iteration 36, loss = 0.53219611\n",
            "Iteration 37, loss = 0.52986887\n",
            "Iteration 38, loss = 0.52955988\n",
            "Iteration 39, loss = 0.67125977\n",
            "Iteration 40, loss = 0.74521135\n",
            "Iteration 41, loss = 0.69895976\n",
            "Iteration 42, loss = 0.67285230\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 43, loss = 0.66255898\n",
            "Iteration 44, loss = 0.65997092\n",
            "Iteration 45, loss = 0.65766389\n",
            "Iteration 46, loss = 0.65559232\n",
            "Iteration 47, loss = 0.65370632\n",
            "Iteration 48, loss = 0.65203356\n",
            "Iteration 49, loss = 0.65051480\n",
            "Iteration 50, loss = 0.64909371\n",
            "Iteration 51, loss = 0.64780115\n",
            "Iteration 52, loss = 0.64662254\n",
            "Iteration 53, loss = 0.64553386\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 54, loss = 0.64484225\n",
            "Iteration 55, loss = 0.64462672\n",
            "Iteration 56, loss = 0.64443107\n",
            "Iteration 57, loss = 0.64423602\n",
            "Iteration 58, loss = 0.64403671\n",
            "Iteration 59, loss = 0.64384824\n",
            "Iteration 60, loss = 0.64365647\n",
            "Iteration 61, loss = 0.64347289\n",
            "Iteration 62, loss = 0.64328392\n",
            "Iteration 63, loss = 0.64310505\n",
            "Iteration 64, loss = 0.64291857\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 65, loss = 0.64279728\n",
            "Iteration 66, loss = 0.64275962\n",
            "Iteration 67, loss = 0.64272325\n",
            "Iteration 68, loss = 0.64268954\n",
            "Iteration 69, loss = 0.64265191\n",
            "Iteration 70, loss = 0.64261737\n",
            "Iteration 71, loss = 0.64258039\n",
            "Iteration 72, loss = 0.64254481\n",
            "Iteration 73, loss = 0.64250921\n",
            "Iteration 74, loss = 0.64247368\n",
            "Iteration 75, loss = 0.64243742\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 76, loss = 0.64241367\n",
            "Iteration 77, loss = 0.64240614\n",
            "Iteration 78, loss = 0.64239942\n",
            "Iteration 79, loss = 0.64239193\n",
            "Iteration 80, loss = 0.64238486\n",
            "Iteration 81, loss = 0.64237810\n",
            "Iteration 82, loss = 0.64237076\n",
            "Iteration 83, loss = 0.64236393\n",
            "Iteration 84, loss = 0.64235646\n",
            "Iteration 85, loss = 0.64234935\n",
            "Iteration 86, loss = 0.64234272\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.19672303\n",
            "Iteration 3, loss = 0.12778530\n",
            "Iteration 4, loss = 0.08300544\n",
            "Iteration 5, loss = 0.05391781\n",
            "Iteration 6, loss = 0.03502337\n",
            "Iteration 7, loss = 0.02275012\n",
            "Iteration 8, loss = 0.01477778\n",
            "Iteration 9, loss = 0.00959920\n",
            "Iteration 10, loss = 0.00623535\n",
            "Iteration 11, loss = 0.00405031\n",
            "Iteration 12, loss = 0.00263868\n",
            "Iteration 13, loss = 0.00186126\n",
            "Iteration 14, loss = 0.00150286\n",
            "Iteration 15, loss = 0.00128573\n",
            "Iteration 16, loss = 0.00114879\n",
            "Iteration 17, loss = 0.00106161\n",
            "Iteration 18, loss = 0.00100572\n",
            "Iteration 19, loss = 0.00096971\n",
            "Iteration 20, loss = 0.00094644\n",
            "Iteration 21, loss = 0.00093135\n",
            "Iteration 22, loss = 0.00092155\n",
            "Iteration 23, loss = 0.00091515\n",
            "Iteration 24, loss = 0.00091095\n",
            "Iteration 25, loss = 0.00090819\n",
            "Iteration 26, loss = 0.00090638\n",
            "Iteration 27, loss = 0.00090517\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 28, loss = 0.00090463\n",
            "Iteration 29, loss = 0.00090448\n",
            "Iteration 30, loss = 0.00090434\n",
            "Iteration 31, loss = 0.00090421\n",
            "Iteration 32, loss = 0.00090409\n",
            "Iteration 33, loss = 0.00090398\n",
            "Iteration 34, loss = 0.00090388\n",
            "Iteration 35, loss = 0.00090379\n",
            "Iteration 36, loss = 0.00090370\n",
            "Iteration 37, loss = 0.00090361\n",
            "Iteration 38, loss = 0.00090354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.00090349\n",
            "Iteration 40, loss = 0.00090348\n",
            "Iteration 41, loss = 0.00090346\n",
            "Iteration 42, loss = 0.00090345\n",
            "Iteration 43, loss = 0.00090343\n",
            "Iteration 44, loss = 0.00090342\n",
            "Iteration 45, loss = 0.00090341\n",
            "Iteration 46, loss = 0.00090339\n",
            "Iteration 47, loss = 0.00090338\n",
            "Iteration 48, loss = 0.00090337\n",
            "Iteration 49, loss = 0.00090335\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 50, loss = 0.00090335\n",
            "Iteration 51, loss = 0.00090334\n",
            "Iteration 52, loss = 0.00090334\n",
            "Iteration 53, loss = 0.00090334\n",
            "Iteration 54, loss = 0.00090334\n",
            "Iteration 55, loss = 0.00090333\n",
            "Iteration 56, loss = 0.00090333\n",
            "Iteration 57, loss = 0.00090333\n",
            "Iteration 58, loss = 0.00090333\n",
            "Iteration 59, loss = 0.00090332\n",
            "Iteration 60, loss = 0.00090332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 61, loss = 0.00090332\n",
            "Iteration 62, loss = 0.00090332\n",
            "Iteration 63, loss = 0.00090332\n",
            "Iteration 64, loss = 0.00090332\n",
            "Iteration 65, loss = 0.00090332\n",
            "Iteration 66, loss = 0.00090332\n",
            "Iteration 67, loss = 0.00090332\n",
            "Iteration 68, loss = 0.00090332\n",
            "Iteration 69, loss = 0.00090332\n",
            "Iteration 70, loss = 0.00090332\n",
            "Iteration 71, loss = 0.00090332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 72, loss = 0.00090332\n",
            "Iteration 73, loss = 0.00090331\n",
            "Iteration 74, loss = 0.00090331\n",
            "Iteration 75, loss = 0.00090331\n",
            "Iteration 76, loss = 0.00090331\n",
            "Iteration 77, loss = 0.00090331\n",
            "Iteration 78, loss = 0.00090331\n",
            "Iteration 79, loss = 0.00090331\n",
            "Iteration 80, loss = 0.00090331\n",
            "Iteration 81, loss = 0.00090331\n",
            "Iteration 82, loss = 0.00090331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11402169\n",
            "Iteration 2, loss = 0.07406935\n",
            "Iteration 3, loss = 0.04811571\n",
            "Iteration 4, loss = 0.03128686\n",
            "Iteration 5, loss = 0.02039079\n",
            "Iteration 6, loss = 0.01334409\n",
            "Iteration 7, loss = 0.00880554\n",
            "Iteration 8, loss = 0.00590309\n",
            "Iteration 9, loss = 0.00406335\n",
            "Iteration 10, loss = 0.00290621\n",
            "Iteration 11, loss = 0.00217714\n",
            "Iteration 12, loss = 0.00171290\n",
            "Iteration 13, loss = 0.00141548\n",
            "Iteration 14, loss = 0.00122405\n",
            "Iteration 15, loss = 0.00110048\n",
            "Iteration 16, loss = 0.00102055\n",
            "Iteration 17, loss = 0.00096875\n",
            "Iteration 18, loss = 0.00093514\n",
            "Iteration 19, loss = 0.00091327\n",
            "Iteration 20, loss = 0.00089905\n",
            "Iteration 21, loss = 0.00088978\n",
            "Iteration 22, loss = 0.00088373\n",
            "Iteration 23, loss = 0.00087977\n",
            "Iteration 24, loss = 0.00087718\n",
            "Iteration 25, loss = 0.00087546\n",
            "Iteration 26, loss = 0.00087432\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00087380\n",
            "Iteration 28, loss = 0.00087366\n",
            "Iteration 29, loss = 0.00087353\n",
            "Iteration 30, loss = 0.00087341\n",
            "Iteration 31, loss = 0.00087329\n",
            "Iteration 32, loss = 0.00087319\n",
            "Iteration 33, loss = 0.00087309\n",
            "Iteration 34, loss = 0.00087300\n",
            "Iteration 35, loss = 0.00087291\n",
            "Iteration 36, loss = 0.00087283\n",
            "Iteration 37, loss = 0.00087275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00087271\n",
            "Iteration 39, loss = 0.00087269\n",
            "Iteration 40, loss = 0.00087268\n",
            "Iteration 41, loss = 0.00087267\n",
            "Iteration 42, loss = 0.00087265\n",
            "Iteration 43, loss = 0.00087264\n",
            "Iteration 44, loss = 0.00087263\n",
            "Iteration 45, loss = 0.00087261\n",
            "Iteration 46, loss = 0.00087260\n",
            "Iteration 47, loss = 0.00087259\n",
            "Iteration 48, loss = 0.00087257\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00087257\n",
            "Iteration 50, loss = 0.00087256\n",
            "Iteration 51, loss = 0.00087256\n",
            "Iteration 52, loss = 0.00087256\n",
            "Iteration 53, loss = 0.00087256\n",
            "Iteration 54, loss = 0.00087255\n",
            "Iteration 55, loss = 0.00087255\n",
            "Iteration 56, loss = 0.00087255\n",
            "Iteration 57, loss = 0.00087255\n",
            "Iteration 58, loss = 0.00087254\n",
            "Iteration 59, loss = 0.00087254\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00087254\n",
            "Iteration 61, loss = 0.00087254\n",
            "Iteration 62, loss = 0.00087254\n",
            "Iteration 63, loss = 0.00087254\n",
            "Iteration 64, loss = 0.00087254\n",
            "Iteration 65, loss = 0.00087254\n",
            "Iteration 66, loss = 0.00087254\n",
            "Iteration 67, loss = 0.00087254\n",
            "Iteration 68, loss = 0.00087254\n",
            "Iteration 69, loss = 0.00087254\n",
            "Iteration 70, loss = 0.00087254\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00087254\n",
            "Iteration 72, loss = 0.00087254\n",
            "Iteration 73, loss = 0.00087254\n",
            "Iteration 74, loss = 0.00087254\n",
            "Iteration 75, loss = 0.00087254\n",
            "Iteration 76, loss = 0.00087254\n",
            "Iteration 77, loss = 0.00087254\n",
            "Iteration 78, loss = 0.00087254\n",
            "Iteration 79, loss = 0.00087253\n",
            "Iteration 80, loss = 0.00087253\n",
            "Iteration 81, loss = 0.00087253\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.28276899\n",
            "Iteration 2, loss = 0.18154870\n",
            "Iteration 3, loss = 0.11792851\n",
            "Iteration 4, loss = 0.07660278\n",
            "Iteration 5, loss = 0.04975883\n",
            "Iteration 6, loss = 0.03232183\n",
            "Iteration 7, loss = 0.02099528\n",
            "Iteration 8, loss = 0.01363789\n",
            "Iteration 9, loss = 0.00885876\n",
            "Iteration 10, loss = 0.00575438\n",
            "Iteration 11, loss = 0.00373790\n",
            "Iteration 12, loss = 0.00243784\n",
            "Iteration 13, loss = 0.00173685\n",
            "Iteration 14, loss = 0.00141371\n",
            "Iteration 15, loss = 0.00121872\n",
            "Iteration 16, loss = 0.00109669\n",
            "Iteration 17, loss = 0.00101942\n",
            "Iteration 18, loss = 0.00097008\n",
            "Iteration 19, loss = 0.00093840\n",
            "Iteration 20, loss = 0.00091796\n",
            "Iteration 21, loss = 0.00090471\n",
            "Iteration 22, loss = 0.00089610\n",
            "Iteration 23, loss = 0.00089048\n",
            "Iteration 24, loss = 0.00088681\n",
            "Iteration 25, loss = 0.00088440\n",
            "Iteration 26, loss = 0.00088280\n",
            "Iteration 27, loss = 0.00088173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 28, loss = 0.00088125\n",
            "Iteration 29, loss = 0.00088111\n",
            "Iteration 30, loss = 0.00088099\n",
            "Iteration 31, loss = 0.00088087\n",
            "Iteration 32, loss = 0.00088076\n",
            "Iteration 33, loss = 0.00088066\n",
            "Iteration 34, loss = 0.00088057\n",
            "Iteration 35, loss = 0.00088048\n",
            "Iteration 36, loss = 0.00088040\n",
            "Iteration 37, loss = 0.00088032\n",
            "Iteration 38, loss = 0.00088025\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.00088020\n",
            "Iteration 40, loss = 0.00088019\n",
            "Iteration 41, loss = 0.00088018\n",
            "Iteration 42, loss = 0.00088016\n",
            "Iteration 43, loss = 0.00088015\n",
            "Iteration 44, loss = 0.00088014\n",
            "Iteration 45, loss = 0.00088012\n",
            "Iteration 46, loss = 0.00088011\n",
            "Iteration 47, loss = 0.00088010\n",
            "Iteration 48, loss = 0.00088009\n",
            "Iteration 49, loss = 0.00088008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 50, loss = 0.00088007\n",
            "Iteration 51, loss = 0.00088007\n",
            "Iteration 52, loss = 0.00088006\n",
            "Iteration 53, loss = 0.00088006\n",
            "Iteration 54, loss = 0.00088006\n",
            "Iteration 55, loss = 0.00088006\n",
            "Iteration 56, loss = 0.00088005\n",
            "Iteration 57, loss = 0.00088005\n",
            "Iteration 58, loss = 0.00088005\n",
            "Iteration 59, loss = 0.00088005\n",
            "Iteration 60, loss = 0.00088004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 61, loss = 0.00088004\n",
            "Iteration 62, loss = 0.00088004\n",
            "Iteration 63, loss = 0.00088004\n",
            "Iteration 64, loss = 0.00088004\n",
            "Iteration 65, loss = 0.00088004\n",
            "Iteration 66, loss = 0.00088004\n",
            "Iteration 67, loss = 0.00088004\n",
            "Iteration 68, loss = 0.00088004\n",
            "Iteration 69, loss = 0.00088004\n",
            "Iteration 70, loss = 0.00088004\n",
            "Iteration 71, loss = 0.00088004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 72, loss = 0.00088004\n",
            "Iteration 73, loss = 0.00088004\n",
            "Iteration 74, loss = 0.00088004\n",
            "Iteration 75, loss = 0.00088004\n",
            "Iteration 76, loss = 0.00088004\n",
            "Iteration 77, loss = 0.00088004\n",
            "Iteration 78, loss = 0.00088004\n",
            "Iteration 79, loss = 0.00088004\n",
            "Iteration 80, loss = 0.00088004\n",
            "Iteration 81, loss = 0.00088004\n",
            "Iteration 82, loss = 0.00088004\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12297481\n",
            "Iteration 2, loss = 0.07988536\n",
            "Iteration 3, loss = 0.05189120\n",
            "Iteration 4, loss = 0.03372099\n",
            "Iteration 5, loss = 0.02198190\n",
            "Iteration 6, loss = 0.01440088\n",
            "Iteration 7, loss = 0.00951773\n",
            "Iteration 8, loss = 0.00638852\n",
            "Iteration 9, loss = 0.00439463\n",
            "Iteration 10, loss = 0.00312922\n",
            "Iteration 11, loss = 0.00232510\n",
            "Iteration 12, loss = 0.00181181\n",
            "Iteration 13, loss = 0.00148234\n",
            "Iteration 14, loss = 0.00127002\n",
            "Iteration 15, loss = 0.00113281\n",
            "Iteration 16, loss = 0.00104396\n",
            "Iteration 17, loss = 0.00098634\n",
            "Iteration 18, loss = 0.00094893\n",
            "Iteration 19, loss = 0.00092462\n",
            "Iteration 20, loss = 0.00090881\n",
            "Iteration 21, loss = 0.00089852\n",
            "Iteration 22, loss = 0.00089180\n",
            "Iteration 23, loss = 0.00088741\n",
            "Iteration 24, loss = 0.00088453\n",
            "Iteration 25, loss = 0.00088263\n",
            "Iteration 26, loss = 0.00088136\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00088079\n",
            "Iteration 28, loss = 0.00088064\n",
            "Iteration 29, loss = 0.00088049\n",
            "Iteration 30, loss = 0.00088036\n",
            "Iteration 31, loss = 0.00088023\n",
            "Iteration 32, loss = 0.00088012\n",
            "Iteration 33, loss = 0.00088001\n",
            "Iteration 34, loss = 0.00087991\n",
            "Iteration 35, loss = 0.00087982\n",
            "Iteration 36, loss = 0.00087973\n",
            "Iteration 37, loss = 0.00087965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00087960\n",
            "Iteration 39, loss = 0.00087959\n",
            "Iteration 40, loss = 0.00087957\n",
            "Iteration 41, loss = 0.00087956\n",
            "Iteration 42, loss = 0.00087954\n",
            "Iteration 43, loss = 0.00087953\n",
            "Iteration 44, loss = 0.00087952\n",
            "Iteration 45, loss = 0.00087950\n",
            "Iteration 46, loss = 0.00087949\n",
            "Iteration 47, loss = 0.00087947\n",
            "Iteration 48, loss = 0.00087946\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00087945\n",
            "Iteration 50, loss = 0.00087945\n",
            "Iteration 51, loss = 0.00087945\n",
            "Iteration 52, loss = 0.00087945\n",
            "Iteration 53, loss = 0.00087944\n",
            "Iteration 54, loss = 0.00087944\n",
            "Iteration 55, loss = 0.00087944\n",
            "Iteration 56, loss = 0.00087943\n",
            "Iteration 57, loss = 0.00087943\n",
            "Iteration 58, loss = 0.00087943\n",
            "Iteration 59, loss = 0.00087943\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00087943\n",
            "Iteration 61, loss = 0.00087942\n",
            "Iteration 62, loss = 0.00087942\n",
            "Iteration 63, loss = 0.00087942\n",
            "Iteration 64, loss = 0.00087942\n",
            "Iteration 65, loss = 0.00087942\n",
            "Iteration 66, loss = 0.00087942\n",
            "Iteration 67, loss = 0.00087942\n",
            "Iteration 68, loss = 0.00087942\n",
            "Iteration 69, loss = 0.00087942\n",
            "Iteration 70, loss = 0.00087942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00087942\n",
            "Iteration 72, loss = 0.00087942\n",
            "Iteration 73, loss = 0.00087942\n",
            "Iteration 74, loss = 0.00087942\n",
            "Iteration 75, loss = 0.00087942\n",
            "Iteration 76, loss = 0.00087942\n",
            "Iteration 77, loss = 0.00087942\n",
            "Iteration 78, loss = 0.00087942\n",
            "Iteration 79, loss = 0.00087942\n",
            "Iteration 80, loss = 0.00087942\n",
            "Iteration 81, loss = 0.00087942\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11722966\n",
            "Iteration 2, loss = 0.07615379\n",
            "Iteration 3, loss = 0.04948358\n",
            "Iteration 4, loss = 0.03218990\n",
            "Iteration 5, loss = 0.02098136\n",
            "Iteration 6, loss = 0.01373295\n",
            "Iteration 7, loss = 0.00906530\n",
            "Iteration 8, loss = 0.00608011\n",
            "Iteration 9, loss = 0.00418662\n",
            "Iteration 10, loss = 0.00299282\n",
            "Iteration 11, loss = 0.00223997\n",
            "Iteration 12, loss = 0.00176172\n",
            "Iteration 13, loss = 0.00145515\n",
            "Iteration 14, loss = 0.00125796\n",
            "Iteration 15, loss = 0.00113082\n",
            "Iteration 16, loss = 0.00104862\n",
            "Iteration 17, loss = 0.00099537\n",
            "Iteration 18, loss = 0.00096082\n",
            "Iteration 19, loss = 0.00093838\n",
            "Iteration 20, loss = 0.00092379\n",
            "Iteration 21, loss = 0.00091428\n",
            "Iteration 22, loss = 0.00090808\n",
            "Iteration 23, loss = 0.00090402\n",
            "Iteration 24, loss = 0.00090134\n",
            "Iteration 25, loss = 0.00089957\n",
            "Iteration 26, loss = 0.00089838\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00089785\n",
            "Iteration 28, loss = 0.00089770\n",
            "Iteration 29, loss = 0.00089757\n",
            "Iteration 30, loss = 0.00089744\n",
            "Iteration 31, loss = 0.00089732\n",
            "Iteration 32, loss = 0.00089722\n",
            "Iteration 33, loss = 0.00089711\n",
            "Iteration 34, loss = 0.00089702\n",
            "Iteration 35, loss = 0.00089693\n",
            "Iteration 36, loss = 0.00089685\n",
            "Iteration 37, loss = 0.00089677\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00089672\n",
            "Iteration 39, loss = 0.00089671\n",
            "Iteration 40, loss = 0.00089669\n",
            "Iteration 41, loss = 0.00089668\n",
            "Iteration 42, loss = 0.00089667\n",
            "Iteration 43, loss = 0.00089665\n",
            "Iteration 44, loss = 0.00089664\n",
            "Iteration 45, loss = 0.00089662\n",
            "Iteration 46, loss = 0.00089661\n",
            "Iteration 47, loss = 0.00089660\n",
            "Iteration 48, loss = 0.00089659\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00089658\n",
            "Iteration 50, loss = 0.00089657\n",
            "Iteration 51, loss = 0.00089657\n",
            "Iteration 52, loss = 0.00089657\n",
            "Iteration 53, loss = 0.00089657\n",
            "Iteration 54, loss = 0.00089656\n",
            "Iteration 55, loss = 0.00089656\n",
            "Iteration 56, loss = 0.00089656\n",
            "Iteration 57, loss = 0.00089656\n",
            "Iteration 58, loss = 0.00089655\n",
            "Iteration 59, loss = 0.00089655\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00089655\n",
            "Iteration 61, loss = 0.00089655\n",
            "Iteration 62, loss = 0.00089655\n",
            "Iteration 63, loss = 0.00089655\n",
            "Iteration 64, loss = 0.00089655\n",
            "Iteration 65, loss = 0.00089655\n",
            "Iteration 66, loss = 0.00089655\n",
            "Iteration 67, loss = 0.00089655\n",
            "Iteration 68, loss = 0.00089655\n",
            "Iteration 69, loss = 0.00089655\n",
            "Iteration 70, loss = 0.00089655\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00089655\n",
            "Iteration 72, loss = 0.00089655\n",
            "Iteration 73, loss = 0.00089655\n",
            "Iteration 74, loss = 0.00089655\n",
            "Iteration 75, loss = 0.00089654\n",
            "Iteration 76, loss = 0.00089654\n",
            "Iteration 77, loss = 0.00089654\n",
            "Iteration 78, loss = 0.00089654\n",
            "Iteration 79, loss = 0.00089654\n",
            "Iteration 80, loss = 0.00089654\n",
            "Iteration 81, loss = 0.00089654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11621563\n",
            "Iteration 2, loss = 0.07550051\n",
            "Iteration 3, loss = 0.04907255\n",
            "Iteration 4, loss = 0.03193074\n",
            "Iteration 5, loss = 0.02082310\n",
            "Iteration 6, loss = 0.01364185\n",
            "Iteration 7, loss = 0.00902022\n",
            "Iteration 8, loss = 0.00606763\n",
            "Iteration 9, loss = 0.00419302\n",
            "Iteration 10, loss = 0.00300184\n",
            "Iteration 11, loss = 0.00224268\n",
            "Iteration 12, loss = 0.00175681\n",
            "Iteration 13, loss = 0.00144420\n",
            "Iteration 14, loss = 0.00124241\n",
            "Iteration 15, loss = 0.00111185\n",
            "Iteration 16, loss = 0.00102725\n",
            "Iteration 17, loss = 0.00097237\n",
            "Iteration 18, loss = 0.00093674\n",
            "Iteration 19, loss = 0.00091358\n",
            "Iteration 20, loss = 0.00089851\n",
            "Iteration 21, loss = 0.00088870\n",
            "Iteration 22, loss = 0.00088229\n",
            "Iteration 23, loss = 0.00087809\n",
            "Iteration 24, loss = 0.00087533\n",
            "Iteration 25, loss = 0.00087351\n",
            "Iteration 26, loss = 0.00087230\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00087176\n",
            "Iteration 28, loss = 0.00087161\n",
            "Iteration 29, loss = 0.00087147\n",
            "Iteration 30, loss = 0.00087134\n",
            "Iteration 31, loss = 0.00087122\n",
            "Iteration 32, loss = 0.00087111\n",
            "Iteration 33, loss = 0.00087100\n",
            "Iteration 34, loss = 0.00087091\n",
            "Iteration 35, loss = 0.00087082\n",
            "Iteration 36, loss = 0.00087073\n",
            "Iteration 37, loss = 0.00087065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00087061\n",
            "Iteration 39, loss = 0.00087059\n",
            "Iteration 40, loss = 0.00087058\n",
            "Iteration 41, loss = 0.00087056\n",
            "Iteration 42, loss = 0.00087055\n",
            "Iteration 43, loss = 0.00087054\n",
            "Iteration 44, loss = 0.00087052\n",
            "Iteration 45, loss = 0.00087051\n",
            "Iteration 46, loss = 0.00087049\n",
            "Iteration 47, loss = 0.00087048\n",
            "Iteration 48, loss = 0.00087047\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00087046\n",
            "Iteration 50, loss = 0.00087046\n",
            "Iteration 51, loss = 0.00087045\n",
            "Iteration 52, loss = 0.00087045\n",
            "Iteration 53, loss = 0.00087045\n",
            "Iteration 54, loss = 0.00087045\n",
            "Iteration 55, loss = 0.00087044\n",
            "Iteration 56, loss = 0.00087044\n",
            "Iteration 57, loss = 0.00087044\n",
            "Iteration 58, loss = 0.00087044\n",
            "Iteration 59, loss = 0.00087043\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00087043\n",
            "Iteration 61, loss = 0.00087043\n",
            "Iteration 62, loss = 0.00087043\n",
            "Iteration 63, loss = 0.00087043\n",
            "Iteration 64, loss = 0.00087043\n",
            "Iteration 65, loss = 0.00087043\n",
            "Iteration 66, loss = 0.00087043\n",
            "Iteration 67, loss = 0.00087043\n",
            "Iteration 68, loss = 0.00087043\n",
            "Iteration 69, loss = 0.00087043\n",
            "Iteration 70, loss = 0.00087043\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00087043\n",
            "Iteration 72, loss = 0.00087043\n",
            "Iteration 73, loss = 0.00087043\n",
            "Iteration 74, loss = 0.00087043\n",
            "Iteration 75, loss = 0.00087043\n",
            "Iteration 76, loss = 0.00087043\n",
            "Iteration 77, loss = 0.00087043\n",
            "Iteration 78, loss = 0.00087043\n",
            "Iteration 79, loss = 0.00087043\n",
            "Iteration 80, loss = 0.00087043\n",
            "Iteration 81, loss = 0.00087043\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.12004927\n",
            "Iteration 2, loss = 0.07800710\n",
            "Iteration 3, loss = 0.05071076\n",
            "Iteration 4, loss = 0.03299812\n",
            "Iteration 5, loss = 0.02151724\n",
            "Iteration 6, loss = 0.01409237\n",
            "Iteration 7, loss = 0.00931072\n",
            "Iteration 8, loss = 0.00625174\n",
            "Iteration 9, loss = 0.00430857\n",
            "Iteration 10, loss = 0.00307851\n",
            "Iteration 11, loss = 0.00229844\n",
            "Iteration 12, loss = 0.00180140\n",
            "Iteration 13, loss = 0.00148301\n",
            "Iteration 14, loss = 0.00127799\n",
            "Iteration 15, loss = 0.00114555\n",
            "Iteration 16, loss = 0.00105981\n",
            "Iteration 17, loss = 0.00100422\n",
            "Iteration 18, loss = 0.00096812\n",
            "Iteration 19, loss = 0.00094464\n",
            "Iteration 20, loss = 0.00092934\n",
            "Iteration 21, loss = 0.00091939\n",
            "Iteration 22, loss = 0.00091291\n",
            "Iteration 23, loss = 0.00090868\n",
            "Iteration 24, loss = 0.00090590\n",
            "Iteration 25, loss = 0.00090407\n",
            "Iteration 26, loss = 0.00090284\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00090229\n",
            "Iteration 28, loss = 0.00090213\n",
            "Iteration 29, loss = 0.00090199\n",
            "Iteration 30, loss = 0.00090186\n",
            "Iteration 31, loss = 0.00090174\n",
            "Iteration 32, loss = 0.00090162\n",
            "Iteration 33, loss = 0.00090152\n",
            "Iteration 34, loss = 0.00090142\n",
            "Iteration 35, loss = 0.00090133\n",
            "Iteration 36, loss = 0.00090124\n",
            "Iteration 37, loss = 0.00090116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00090111\n",
            "Iteration 39, loss = 0.00090109\n",
            "Iteration 40, loss = 0.00090108\n",
            "Iteration 41, loss = 0.00090106\n",
            "Iteration 42, loss = 0.00090105\n",
            "Iteration 43, loss = 0.00090104\n",
            "Iteration 44, loss = 0.00090102\n",
            "Iteration 45, loss = 0.00090101\n",
            "Iteration 46, loss = 0.00090099\n",
            "Iteration 47, loss = 0.00090098\n",
            "Iteration 48, loss = 0.00090097\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00090096\n",
            "Iteration 50, loss = 0.00090095\n",
            "Iteration 51, loss = 0.00090095\n",
            "Iteration 52, loss = 0.00090095\n",
            "Iteration 53, loss = 0.00090095\n",
            "Iteration 54, loss = 0.00090094\n",
            "Iteration 55, loss = 0.00090094\n",
            "Iteration 56, loss = 0.00090094\n",
            "Iteration 57, loss = 0.00090094\n",
            "Iteration 58, loss = 0.00090093\n",
            "Iteration 59, loss = 0.00090093\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00090093\n",
            "Iteration 61, loss = 0.00090093\n",
            "Iteration 62, loss = 0.00090093\n",
            "Iteration 63, loss = 0.00090093\n",
            "Iteration 64, loss = 0.00090093\n",
            "Iteration 65, loss = 0.00090093\n",
            "Iteration 66, loss = 0.00090093\n",
            "Iteration 67, loss = 0.00090093\n",
            "Iteration 68, loss = 0.00090093\n",
            "Iteration 69, loss = 0.00090092\n",
            "Iteration 70, loss = 0.00090092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00090092\n",
            "Iteration 72, loss = 0.00090092\n",
            "Iteration 73, loss = 0.00090092\n",
            "Iteration 74, loss = 0.00090092\n",
            "Iteration 75, loss = 0.00090092\n",
            "Iteration 76, loss = 0.00090092\n",
            "Iteration 77, loss = 0.00090092\n",
            "Iteration 78, loss = 0.00090092\n",
            "Iteration 79, loss = 0.00090092\n",
            "Iteration 80, loss = 0.00090092\n",
            "Iteration 81, loss = 0.00090092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11846288\n",
            "Iteration 2, loss = 0.07695437\n",
            "Iteration 3, loss = 0.04998758\n",
            "Iteration 4, loss = 0.03249273\n",
            "Iteration 5, loss = 0.02118640\n",
            "Iteration 6, loss = 0.01387887\n",
            "Iteration 7, loss = 0.00917163\n",
            "Iteration 8, loss = 0.00615710\n",
            "Iteration 9, loss = 0.00423893\n",
            "Iteration 10, loss = 0.00302284\n",
            "Iteration 11, loss = 0.00225071\n",
            "Iteration 12, loss = 0.00175814\n",
            "Iteration 13, loss = 0.00144202\n",
            "Iteration 14, loss = 0.00123820\n",
            "Iteration 15, loss = 0.00110641\n",
            "Iteration 16, loss = 0.00102104\n",
            "Iteration 17, loss = 0.00096566\n",
            "Iteration 18, loss = 0.00092970\n",
            "Iteration 19, loss = 0.00090632\n",
            "Iteration 20, loss = 0.00089110\n",
            "Iteration 21, loss = 0.00088119\n",
            "Iteration 22, loss = 0.00087473\n",
            "Iteration 23, loss = 0.00087050\n",
            "Iteration 24, loss = 0.00086772\n",
            "Iteration 25, loss = 0.00086588\n",
            "Iteration 26, loss = 0.00086465\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00086409\n",
            "Iteration 28, loss = 0.00086394\n",
            "Iteration 29, loss = 0.00086380\n",
            "Iteration 30, loss = 0.00086367\n",
            "Iteration 31, loss = 0.00086355\n",
            "Iteration 32, loss = 0.00086344\n",
            "Iteration 33, loss = 0.00086333\n",
            "Iteration 34, loss = 0.00086323\n",
            "Iteration 35, loss = 0.00086314\n",
            "Iteration 36, loss = 0.00086306\n",
            "Iteration 37, loss = 0.00086298\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00086293\n",
            "Iteration 39, loss = 0.00086291\n",
            "Iteration 40, loss = 0.00086290\n",
            "Iteration 41, loss = 0.00086288\n",
            "Iteration 42, loss = 0.00086287\n",
            "Iteration 43, loss = 0.00086286\n",
            "Iteration 44, loss = 0.00086284\n",
            "Iteration 45, loss = 0.00086283\n",
            "Iteration 46, loss = 0.00086281\n",
            "Iteration 47, loss = 0.00086280\n",
            "Iteration 48, loss = 0.00086279\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00086278\n",
            "Iteration 50, loss = 0.00086278\n",
            "Iteration 51, loss = 0.00086277\n",
            "Iteration 52, loss = 0.00086277\n",
            "Iteration 53, loss = 0.00086277\n",
            "Iteration 54, loss = 0.00086277\n",
            "Iteration 55, loss = 0.00086276\n",
            "Iteration 56, loss = 0.00086276\n",
            "Iteration 57, loss = 0.00086276\n",
            "Iteration 58, loss = 0.00086276\n",
            "Iteration 59, loss = 0.00086275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00086275\n",
            "Iteration 61, loss = 0.00086275\n",
            "Iteration 62, loss = 0.00086275\n",
            "Iteration 63, loss = 0.00086275\n",
            "Iteration 64, loss = 0.00086275\n",
            "Iteration 65, loss = 0.00086275\n",
            "Iteration 66, loss = 0.00086275\n",
            "Iteration 67, loss = 0.00086275\n",
            "Iteration 68, loss = 0.00086275\n",
            "Iteration 69, loss = 0.00086275\n",
            "Iteration 70, loss = 0.00086275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00086275\n",
            "Iteration 72, loss = 0.00086275\n",
            "Iteration 73, loss = 0.00086275\n",
            "Iteration 74, loss = 0.00086275\n",
            "Iteration 75, loss = 0.00086275\n",
            "Iteration 76, loss = 0.00086275\n",
            "Iteration 77, loss = 0.00086275\n",
            "Iteration 78, loss = 0.00086275\n",
            "Iteration 79, loss = 0.00086275\n",
            "Iteration 80, loss = 0.00086275\n",
            "Iteration 81, loss = 0.00086275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11608126\n",
            "Iteration 2, loss = 0.07540726\n",
            "Iteration 3, loss = 0.04898227\n",
            "Iteration 4, loss = 0.03182023\n",
            "Iteration 5, loss = 0.02071582\n",
            "Iteration 6, loss = 0.01355662\n",
            "Iteration 7, loss = 0.00894741\n",
            "Iteration 8, loss = 0.00599942\n",
            "Iteration 9, loss = 0.00412993\n",
            "Iteration 10, loss = 0.00295134\n",
            "Iteration 11, loss = 0.00220496\n",
            "Iteration 12, loss = 0.00172926\n",
            "Iteration 13, loss = 0.00142435\n",
            "Iteration 14, loss = 0.00122801\n",
            "Iteration 15, loss = 0.00110120\n",
            "Iteration 16, loss = 0.00101910\n",
            "Iteration 17, loss = 0.00096588\n",
            "Iteration 18, loss = 0.00093134\n",
            "Iteration 19, loss = 0.00090889\n",
            "Iteration 20, loss = 0.00089426\n",
            "Iteration 21, loss = 0.00088474\n",
            "Iteration 22, loss = 0.00087854\n",
            "Iteration 23, loss = 0.00087449\n",
            "Iteration 24, loss = 0.00087182\n",
            "Iteration 25, loss = 0.00087006\n",
            "Iteration 26, loss = 0.00086889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00086836\n",
            "Iteration 28, loss = 0.00086821\n",
            "Iteration 29, loss = 0.00086808\n",
            "Iteration 30, loss = 0.00086795\n",
            "Iteration 31, loss = 0.00086784\n",
            "Iteration 32, loss = 0.00086773\n",
            "Iteration 33, loss = 0.00086763\n",
            "Iteration 34, loss = 0.00086753\n",
            "Iteration 35, loss = 0.00086744\n",
            "Iteration 36, loss = 0.00086736\n",
            "Iteration 37, loss = 0.00086728\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00086724\n",
            "Iteration 39, loss = 0.00086723\n",
            "Iteration 40, loss = 0.00086721\n",
            "Iteration 41, loss = 0.00086720\n",
            "Iteration 42, loss = 0.00086718\n",
            "Iteration 43, loss = 0.00086717\n",
            "Iteration 44, loss = 0.00086716\n",
            "Iteration 45, loss = 0.00086714\n",
            "Iteration 46, loss = 0.00086713\n",
            "Iteration 47, loss = 0.00086712\n",
            "Iteration 48, loss = 0.00086711\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00086710\n",
            "Iteration 50, loss = 0.00086710\n",
            "Iteration 51, loss = 0.00086709\n",
            "Iteration 52, loss = 0.00086709\n",
            "Iteration 53, loss = 0.00086709\n",
            "Iteration 54, loss = 0.00086709\n",
            "Iteration 55, loss = 0.00086708\n",
            "Iteration 56, loss = 0.00086708\n",
            "Iteration 57, loss = 0.00086708\n",
            "Iteration 58, loss = 0.00086708\n",
            "Iteration 59, loss = 0.00086707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00086707\n",
            "Iteration 61, loss = 0.00086707\n",
            "Iteration 62, loss = 0.00086707\n",
            "Iteration 63, loss = 0.00086707\n",
            "Iteration 64, loss = 0.00086707\n",
            "Iteration 65, loss = 0.00086707\n",
            "Iteration 66, loss = 0.00086707\n",
            "Iteration 67, loss = 0.00086707\n",
            "Iteration 68, loss = 0.00086707\n",
            "Iteration 69, loss = 0.00086707\n",
            "Iteration 70, loss = 0.00086707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00086707\n",
            "Iteration 72, loss = 0.00086707\n",
            "Iteration 73, loss = 0.00086707\n",
            "Iteration 74, loss = 0.00086707\n",
            "Iteration 75, loss = 0.00086707\n",
            "Iteration 76, loss = 0.00086707\n",
            "Iteration 77, loss = 0.00086707\n",
            "Iteration 78, loss = 0.00086707\n",
            "Iteration 79, loss = 0.00086707\n",
            "Iteration 80, loss = 0.00086707\n",
            "Iteration 81, loss = 0.00086707\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.11233077\n",
            "Iteration 2, loss = 0.07297091\n",
            "Iteration 3, loss = 0.04740050\n",
            "Iteration 4, loss = 0.03081791\n",
            "Iteration 5, loss = 0.02009448\n",
            "Iteration 6, loss = 0.01316272\n",
            "Iteration 7, loss = 0.00870149\n",
            "Iteration 8, loss = 0.00585317\n",
            "Iteration 9, loss = 0.00404917\n",
            "Iteration 10, loss = 0.00290947\n",
            "Iteration 11, loss = 0.00218770\n",
            "Iteration 12, loss = 0.00172823\n",
            "Iteration 13, loss = 0.00143391\n",
            "Iteration 14, loss = 0.00124430\n",
            "Iteration 15, loss = 0.00112177\n",
            "Iteration 16, loss = 0.00104245\n",
            "Iteration 17, loss = 0.00099101\n",
            "Iteration 18, loss = 0.00095762\n",
            "Iteration 19, loss = 0.00093593\n",
            "Iteration 20, loss = 0.00092181\n",
            "Iteration 21, loss = 0.00091262\n",
            "Iteration 22, loss = 0.00090661\n",
            "Iteration 23, loss = 0.00090266\n",
            "Iteration 24, loss = 0.00090007\n",
            "Iteration 25, loss = 0.00089835\n",
            "Iteration 26, loss = 0.00089721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 27, loss = 0.00089669\n",
            "Iteration 28, loss = 0.00089655\n",
            "Iteration 29, loss = 0.00089641\n",
            "Iteration 30, loss = 0.00089629\n",
            "Iteration 31, loss = 0.00089618\n",
            "Iteration 32, loss = 0.00089607\n",
            "Iteration 33, loss = 0.00089597\n",
            "Iteration 34, loss = 0.00089588\n",
            "Iteration 35, loss = 0.00089579\n",
            "Iteration 36, loss = 0.00089571\n",
            "Iteration 37, loss = 0.00089563\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.00089558\n",
            "Iteration 39, loss = 0.00089557\n",
            "Iteration 40, loss = 0.00089556\n",
            "Iteration 41, loss = 0.00089554\n",
            "Iteration 42, loss = 0.00089553\n",
            "Iteration 43, loss = 0.00089552\n",
            "Iteration 44, loss = 0.00089550\n",
            "Iteration 45, loss = 0.00089549\n",
            "Iteration 46, loss = 0.00089548\n",
            "Iteration 47, loss = 0.00089546\n",
            "Iteration 48, loss = 0.00089545\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 49, loss = 0.00089544\n",
            "Iteration 50, loss = 0.00089544\n",
            "Iteration 51, loss = 0.00089544\n",
            "Iteration 52, loss = 0.00089544\n",
            "Iteration 53, loss = 0.00089543\n",
            "Iteration 54, loss = 0.00089543\n",
            "Iteration 55, loss = 0.00089543\n",
            "Iteration 56, loss = 0.00089543\n",
            "Iteration 57, loss = 0.00089542\n",
            "Iteration 58, loss = 0.00089542\n",
            "Iteration 59, loss = 0.00089542\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 60, loss = 0.00089542\n",
            "Iteration 61, loss = 0.00089542\n",
            "Iteration 62, loss = 0.00089542\n",
            "Iteration 63, loss = 0.00089541\n",
            "Iteration 64, loss = 0.00089541\n",
            "Iteration 65, loss = 0.00089541\n",
            "Iteration 66, loss = 0.00089541\n",
            "Iteration 67, loss = 0.00089541\n",
            "Iteration 68, loss = 0.00089541\n",
            "Iteration 69, loss = 0.00089541\n",
            "Iteration 70, loss = 0.00089541\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 71, loss = 0.00089541\n",
            "Iteration 72, loss = 0.00089541\n",
            "Iteration 73, loss = 0.00089541\n",
            "Iteration 74, loss = 0.00089541\n",
            "Iteration 75, loss = 0.00089541\n",
            "Iteration 76, loss = 0.00089541\n",
            "Iteration 77, loss = 0.00089541\n",
            "Iteration 78, loss = 0.00089541\n",
            "Iteration 79, loss = 0.00089541\n",
            "Iteration 80, loss = 0.00089541\n",
            "Iteration 81, loss = 0.00089541\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  7.5min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.44459420\n",
            "Iteration 2, loss = 0.30754707\n",
            "Iteration 3, loss = 0.27557142\n",
            "Iteration 4, loss = 0.24692027\n",
            "Iteration 5, loss = 0.22124798\n",
            "Iteration 6, loss = 0.19824483\n",
            "Iteration 7, loss = 0.17763333\n",
            "Iteration 8, loss = 0.15916480\n",
            "Iteration 9, loss = 0.14261644\n",
            "Iteration 10, loss = 0.12778861\n",
            "Iteration 11, loss = 0.11450244\n",
            "Iteration 12, loss = 0.10259762\n",
            "Iteration 13, loss = 0.09193055\n",
            "Iteration 14, loss = 0.08237254\n",
            "Iteration 15, loss = 0.07380827\n",
            "Iteration 16, loss = 0.06613443\n",
            "Iteration 17, loss = 0.05925843\n",
            "Iteration 18, loss = 0.05309734\n",
            "Iteration 19, loss = 0.04757681\n",
            "Iteration 20, loss = 0.04263025\n",
            "Iteration 21, loss = 0.03819798\n",
            "Iteration 22, loss = 0.03422654\n",
            "Iteration 23, loss = 0.03066801\n",
            "Iteration 24, loss = 0.02747946\n",
            "Iteration 25, loss = 0.02462242\n",
            "Iteration 26, loss = 0.02206243\n",
            "Iteration 27, loss = 0.01976860\n",
            "Iteration 28, loss = 0.01771326\n",
            "Iteration 29, loss = 0.01587161\n",
            "Iteration 30, loss = 0.01422144\n",
            "Iteration 31, loss = 0.01274284\n",
            "Iteration 32, loss = 0.01141797\n",
            "Iteration 33, loss = 0.01023084\n",
            "Iteration 34, loss = 0.00916714\n",
            "Iteration 35, loss = 0.00821403\n",
            "Iteration 36, loss = 0.00736002\n",
            "Iteration 37, loss = 0.00659480\n",
            "Iteration 38, loss = 0.00590914\n",
            "Iteration 39, loss = 0.00529477\n",
            "Iteration 40, loss = 0.00474427\n",
            "Iteration 41, loss = 0.00425101\n",
            "Iteration 42, loss = 0.00380904\n",
            "Iteration 43, loss = 0.00341306\n",
            "Iteration 44, loss = 0.00305846\n",
            "Iteration 45, loss = 0.00274174\n",
            "Iteration 46, loss = 0.00246172\n",
            "Iteration 47, loss = 0.00222192\n",
            "Iteration 48, loss = 0.00202961\n",
            "Iteration 49, loss = 0.00188395\n",
            "Iteration 50, loss = 0.00176985\n",
            "Iteration 51, loss = 0.00167338\n",
            "Iteration 52, loss = 0.00158878\n",
            "Iteration 53, loss = 0.00151391\n",
            "Iteration 54, loss = 0.00144753\n",
            "Iteration 55, loss = 0.00138861\n",
            "Iteration 56, loss = 0.00133628\n",
            "Iteration 57, loss = 0.00128976\n",
            "Iteration 58, loss = 0.00124838\n",
            "Iteration 59, loss = 0.00121155\n",
            "Iteration 60, loss = 0.00117875\n",
            "Iteration 61, loss = 0.00114953\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 62, loss = 0.00113251\n",
            "Iteration 63, loss = 0.00112740\n",
            "Iteration 64, loss = 0.00112246\n",
            "Iteration 65, loss = 0.00111762\n",
            "Iteration 66, loss = 0.00111290\n",
            "Iteration 67, loss = 0.00110828\n",
            "Iteration 68, loss = 0.00110377\n",
            "Iteration 69, loss = 0.00109936\n",
            "Iteration 70, loss = 0.00109504\n",
            "Iteration 71, loss = 0.00109083\n",
            "Iteration 72, loss = 0.00108670\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.00108413\n",
            "Iteration 74, loss = 0.00108332\n",
            "Iteration 75, loss = 0.00108252\n",
            "Iteration 76, loss = 0.00108173\n",
            "Iteration 77, loss = 0.00108094\n",
            "Iteration 78, loss = 0.00108015\n",
            "Iteration 79, loss = 0.00107937\n",
            "Iteration 80, loss = 0.00107859\n",
            "Iteration 81, loss = 0.00107781\n",
            "Iteration 82, loss = 0.00107704\n",
            "Iteration 83, loss = 0.00107627\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 84, loss = 0.00107578\n",
            "Iteration 85, loss = 0.00107563\n",
            "Iteration 86, loss = 0.00107547\n",
            "Iteration 87, loss = 0.00107532\n",
            "Iteration 88, loss = 0.00107517\n",
            "Iteration 89, loss = 0.00107502\n",
            "Iteration 90, loss = 0.00107486\n",
            "Iteration 91, loss = 0.00107471\n",
            "Iteration 92, loss = 0.00107456\n",
            "Iteration 93, loss = 0.00107441\n",
            "Iteration 94, loss = 0.00107426\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 95, loss = 0.00107416\n",
            "Iteration 96, loss = 0.00107413\n",
            "Iteration 97, loss = 0.00107410\n",
            "Iteration 98, loss = 0.00107407\n",
            "Iteration 99, loss = 0.00107404\n",
            "Iteration 100, loss = 0.00107401\n",
            "Iteration 101, loss = 0.00107398\n",
            "Iteration 102, loss = 0.00107395\n",
            "Iteration 103, loss = 0.00107392\n",
            "Iteration 104, loss = 0.00107389\n",
            "Iteration 105, loss = 0.00107386\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 106, loss = 0.00107384\n",
            "Iteration 107, loss = 0.00107383\n",
            "Iteration 108, loss = 0.00107383\n",
            "Iteration 109, loss = 0.00107382\n",
            "Iteration 110, loss = 0.00107381\n",
            "Iteration 111, loss = 0.00107381\n",
            "Iteration 112, loss = 0.00107380\n",
            "Iteration 113, loss = 0.00107380\n",
            "Iteration 114, loss = 0.00107379\n",
            "Iteration 115, loss = 0.00107378\n",
            "Iteration 116, loss = 0.00107378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.23379682\n",
            "Iteration 2, loss = 0.20388436\n",
            "Iteration 3, loss = 0.18268651\n",
            "Iteration 4, loss = 0.16369260\n",
            "Iteration 5, loss = 0.14667349\n",
            "Iteration 6, loss = 0.13142385\n",
            "Iteration 7, loss = 0.11775972\n",
            "Iteration 8, loss = 0.10551624\n",
            "Iteration 9, loss = 0.09454573\n",
            "Iteration 10, loss = 0.08471581\n",
            "Iteration 11, loss = 0.07590791\n",
            "Iteration 12, loss = 0.06801577\n",
            "Iteration 13, loss = 0.06094417\n",
            "Iteration 14, loss = 0.05460781\n",
            "Iteration 15, loss = 0.04893024\n",
            "Iteration 16, loss = 0.04384296\n",
            "Iteration 17, loss = 0.03928461\n",
            "Iteration 18, loss = 0.03520019\n",
            "Iteration 19, loss = 0.03154043\n",
            "Iteration 20, loss = 0.02826117\n",
            "Iteration 21, loss = 0.02532286\n",
            "Iteration 22, loss = 0.02269004\n",
            "Iteration 23, loss = 0.02033096\n",
            "Iteration 24, loss = 0.01821715\n",
            "Iteration 25, loss = 0.01632311\n",
            "Iteration 26, loss = 0.01462600\n",
            "Iteration 27, loss = 0.01310534\n",
            "Iteration 28, loss = 0.01174277\n",
            "Iteration 29, loss = 0.01052188\n",
            "Iteration 30, loss = 0.00942792\n",
            "Iteration 31, loss = 0.00844770\n",
            "Iteration 32, loss = 0.00756939\n",
            "Iteration 33, loss = 0.00678240\n",
            "Iteration 34, loss = 0.00607724\n",
            "Iteration 35, loss = 0.00544539\n",
            "Iteration 36, loss = 0.00487923\n",
            "Iteration 37, loss = 0.00437194\n",
            "Iteration 38, loss = 0.00391741\n",
            "Iteration 39, loss = 0.00351023\n",
            "Iteration 40, loss = 0.00314586\n",
            "Iteration 41, loss = 0.00282129\n",
            "Iteration 42, loss = 0.00253680\n",
            "Iteration 43, loss = 0.00229770\n",
            "Iteration 44, loss = 0.00210936\n",
            "Iteration 45, loss = 0.00196440\n",
            "Iteration 46, loss = 0.00184594\n",
            "Iteration 47, loss = 0.00174287\n",
            "Iteration 48, loss = 0.00165116\n",
            "Iteration 49, loss = 0.00156936\n",
            "Iteration 50, loss = 0.00149644\n",
            "Iteration 51, loss = 0.00143138\n",
            "Iteration 52, loss = 0.00137334\n",
            "Iteration 53, loss = 0.00132153\n",
            "Iteration 54, loss = 0.00127527\n",
            "Iteration 55, loss = 0.00123394\n",
            "Iteration 56, loss = 0.00119702\n",
            "Iteration 57, loss = 0.00116403\n",
            "Iteration 58, loss = 0.00113455\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 59, loss = 0.00111735\n",
            "Iteration 60, loss = 0.00111219\n",
            "Iteration 61, loss = 0.00110718\n",
            "Iteration 62, loss = 0.00110228\n",
            "Iteration 63, loss = 0.00109750\n",
            "Iteration 64, loss = 0.00109281\n",
            "Iteration 65, loss = 0.00108824\n",
            "Iteration 66, loss = 0.00108376\n",
            "Iteration 67, loss = 0.00107938\n",
            "Iteration 68, loss = 0.00107510\n",
            "Iteration 69, loss = 0.00107091\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 70, loss = 0.00106829\n",
            "Iteration 71, loss = 0.00106747\n",
            "Iteration 72, loss = 0.00106666\n",
            "Iteration 73, loss = 0.00106585\n",
            "Iteration 74, loss = 0.00106505\n",
            "Iteration 75, loss = 0.00106425\n",
            "Iteration 76, loss = 0.00106345\n",
            "Iteration 77, loss = 0.00106266\n",
            "Iteration 78, loss = 0.00106187\n",
            "Iteration 79, loss = 0.00106108\n",
            "Iteration 80, loss = 0.00106030\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 81, loss = 0.00105980\n",
            "Iteration 82, loss = 0.00105965\n",
            "Iteration 83, loss = 0.00105949\n",
            "Iteration 84, loss = 0.00105934\n",
            "Iteration 85, loss = 0.00105918\n",
            "Iteration 86, loss = 0.00105903\n",
            "Iteration 87, loss = 0.00105887\n",
            "Iteration 88, loss = 0.00105872\n",
            "Iteration 89, loss = 0.00105856\n",
            "Iteration 90, loss = 0.00105841\n",
            "Iteration 91, loss = 0.00105825\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 92, loss = 0.00105815\n",
            "Iteration 93, loss = 0.00105812\n",
            "Iteration 94, loss = 0.00105809\n",
            "Iteration 95, loss = 0.00105806\n",
            "Iteration 96, loss = 0.00105803\n",
            "Iteration 97, loss = 0.00105800\n",
            "Iteration 98, loss = 0.00105797\n",
            "Iteration 99, loss = 0.00105794\n",
            "Iteration 100, loss = 0.00105791\n",
            "Iteration 101, loss = 0.00105788\n",
            "Iteration 102, loss = 0.00105785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 103, loss = 0.00105783\n",
            "Iteration 104, loss = 0.00105782\n",
            "Iteration 105, loss = 0.00105781\n",
            "Iteration 106, loss = 0.00105781\n",
            "Iteration 107, loss = 0.00105780\n",
            "Iteration 108, loss = 0.00105780\n",
            "Iteration 109, loss = 0.00105779\n",
            "Iteration 110, loss = 0.00105778\n",
            "Iteration 111, loss = 0.00105778\n",
            "Iteration 112, loss = 0.00105777\n",
            "Iteration 113, loss = 0.00105777\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.14265422\n",
            "Iteration 2, loss = 0.12785659\n",
            "Iteration 3, loss = 0.11456545\n",
            "Iteration 4, loss = 0.10265772\n",
            "Iteration 5, loss = 0.09198929\n",
            "Iteration 6, loss = 0.08243094\n",
            "Iteration 7, loss = 0.07386717\n",
            "Iteration 8, loss = 0.06619461\n",
            "Iteration 9, loss = 0.05932073\n",
            "Iteration 10, loss = 0.05316262\n",
            "Iteration 11, loss = 0.04764601\n",
            "Iteration 12, loss = 0.04270437\n",
            "Iteration 13, loss = 0.03827808\n",
            "Iteration 14, loss = 0.03431374\n",
            "Iteration 15, loss = 0.03076348\n",
            "Iteration 16, loss = 0.02758441\n",
            "Iteration 17, loss = 0.02473814\n",
            "Iteration 18, loss = 0.02219022\n",
            "Iteration 19, loss = 0.01990990\n",
            "Iteration 20, loss = 0.01786957\n",
            "Iteration 21, loss = 0.01604447\n",
            "Iteration 22, loss = 0.01441236\n",
            "Iteration 23, loss = 0.01295330\n",
            "Iteration 24, loss = 0.01164939\n",
            "Iteration 25, loss = 0.01048464\n",
            "Iteration 26, loss = 0.00944475\n",
            "Iteration 27, loss = 0.00851689\n",
            "Iteration 28, loss = 0.00768936\n",
            "Iteration 29, loss = 0.00695158\n",
            "Iteration 30, loss = 0.00629409\n",
            "Iteration 31, loss = 0.00570838\n",
            "Iteration 32, loss = 0.00518682\n",
            "Iteration 33, loss = 0.00472263\n",
            "Iteration 34, loss = 0.00430978\n",
            "Iteration 35, loss = 0.00394272\n",
            "Iteration 36, loss = 0.00361618\n",
            "Iteration 37, loss = 0.00332552\n",
            "Iteration 38, loss = 0.00306683\n",
            "Iteration 39, loss = 0.00283658\n",
            "Iteration 40, loss = 0.00263142\n",
            "Iteration 41, loss = 0.00244845\n",
            "Iteration 42, loss = 0.00228518\n",
            "Iteration 43, loss = 0.00213942\n",
            "Iteration 44, loss = 0.00200925\n",
            "Iteration 45, loss = 0.00189296\n",
            "Iteration 46, loss = 0.00178904\n",
            "Iteration 47, loss = 0.00169615\n",
            "Iteration 48, loss = 0.00161309\n",
            "Iteration 49, loss = 0.00153882\n",
            "Iteration 50, loss = 0.00147239\n",
            "Iteration 51, loss = 0.00141296\n",
            "Iteration 52, loss = 0.00135978\n",
            "Iteration 53, loss = 0.00131219\n",
            "Iteration 54, loss = 0.00126960\n",
            "Iteration 55, loss = 0.00123147\n",
            "Iteration 56, loss = 0.00119733\n",
            "Iteration 57, loss = 0.00116677\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 58, loss = 0.00114892\n",
            "Iteration 59, loss = 0.00114355\n",
            "Iteration 60, loss = 0.00113835\n",
            "Iteration 61, loss = 0.00113326\n",
            "Iteration 62, loss = 0.00112828\n",
            "Iteration 63, loss = 0.00112342\n",
            "Iteration 64, loss = 0.00111865\n",
            "Iteration 65, loss = 0.00111399\n",
            "Iteration 66, loss = 0.00110944\n",
            "Iteration 67, loss = 0.00110498\n",
            "Iteration 68, loss = 0.00110062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 69, loss = 0.00109789\n",
            "Iteration 70, loss = 0.00109703\n",
            "Iteration 71, loss = 0.00109619\n",
            "Iteration 72, loss = 0.00109535\n",
            "Iteration 73, loss = 0.00109451\n",
            "Iteration 74, loss = 0.00109368\n",
            "Iteration 75, loss = 0.00109285\n",
            "Iteration 76, loss = 0.00109202\n",
            "Iteration 77, loss = 0.00109120\n",
            "Iteration 78, loss = 0.00109038\n",
            "Iteration 79, loss = 0.00108956\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 80, loss = 0.00108905\n",
            "Iteration 81, loss = 0.00108888\n",
            "Iteration 82, loss = 0.00108872\n",
            "Iteration 83, loss = 0.00108856\n",
            "Iteration 84, loss = 0.00108840\n",
            "Iteration 85, loss = 0.00108824\n",
            "Iteration 86, loss = 0.00108807\n",
            "Iteration 87, loss = 0.00108791\n",
            "Iteration 88, loss = 0.00108775\n",
            "Iteration 89, loss = 0.00108759\n",
            "Iteration 90, loss = 0.00108743\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 91, loss = 0.00108733\n",
            "Iteration 92, loss = 0.00108729\n",
            "Iteration 93, loss = 0.00108726\n",
            "Iteration 94, loss = 0.00108723\n",
            "Iteration 95, loss = 0.00108720\n",
            "Iteration 96, loss = 0.00108717\n",
            "Iteration 97, loss = 0.00108713\n",
            "Iteration 98, loss = 0.00108710\n",
            "Iteration 99, loss = 0.00108707\n",
            "Iteration 100, loss = 0.00108704\n",
            "Iteration 101, loss = 0.00108701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 102, loss = 0.00108699\n",
            "Iteration 103, loss = 0.00108698\n",
            "Iteration 104, loss = 0.00108697\n",
            "Iteration 105, loss = 0.00108697\n",
            "Iteration 106, loss = 0.00108696\n",
            "Iteration 107, loss = 0.00108695\n",
            "Iteration 108, loss = 0.00108695\n",
            "Iteration 109, loss = 0.00108694\n",
            "Iteration 110, loss = 0.00108693\n",
            "Iteration 111, loss = 0.00108693\n",
            "Iteration 112, loss = 0.00108692\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13359277\n",
            "Iteration 2, loss = 0.11973426\n",
            "Iteration 3, loss = 0.10728552\n",
            "Iteration 4, loss = 0.09613117\n",
            "Iteration 5, loss = 0.08613687\n",
            "Iteration 6, loss = 0.07718264\n",
            "Iteration 7, loss = 0.06916127\n",
            "Iteration 8, loss = 0.06197617\n",
            "Iteration 9, loss = 0.05553996\n",
            "Iteration 10, loss = 0.04977443\n",
            "Iteration 11, loss = 0.04460982\n",
            "Iteration 12, loss = 0.03998375\n",
            "Iteration 13, loss = 0.03584037\n",
            "Iteration 14, loss = 0.03212961\n",
            "Iteration 15, loss = 0.02880664\n",
            "Iteration 16, loss = 0.02583127\n",
            "Iteration 17, loss = 0.02316750\n",
            "Iteration 18, loss = 0.02078306\n",
            "Iteration 19, loss = 0.01864902\n",
            "Iteration 20, loss = 0.01673948\n",
            "Iteration 21, loss = 0.01503125\n",
            "Iteration 22, loss = 0.01350353\n",
            "Iteration 23, loss = 0.01213771\n",
            "Iteration 24, loss = 0.01091706\n",
            "Iteration 25, loss = 0.00982660\n",
            "Iteration 26, loss = 0.00885290\n",
            "Iteration 27, loss = 0.00798390\n",
            "Iteration 28, loss = 0.00720874\n",
            "Iteration 29, loss = 0.00651763\n",
            "Iteration 30, loss = 0.00590173\n",
            "Iteration 31, loss = 0.00535322\n",
            "Iteration 32, loss = 0.00486511\n",
            "Iteration 33, loss = 0.00443107\n",
            "Iteration 34, loss = 0.00404511\n",
            "Iteration 35, loss = 0.00370185\n",
            "Iteration 36, loss = 0.00339657\n",
            "Iteration 37, loss = 0.00312502\n",
            "Iteration 38, loss = 0.00288344\n",
            "Iteration 39, loss = 0.00266852\n",
            "Iteration 40, loss = 0.00247740\n",
            "Iteration 41, loss = 0.00230728\n",
            "Iteration 42, loss = 0.00215568\n",
            "Iteration 43, loss = 0.00202052\n",
            "Iteration 44, loss = 0.00189996\n",
            "Iteration 45, loss = 0.00179242\n",
            "Iteration 46, loss = 0.00169647\n",
            "Iteration 47, loss = 0.00161079\n",
            "Iteration 48, loss = 0.00153426\n",
            "Iteration 49, loss = 0.00146587\n",
            "Iteration 50, loss = 0.00140473\n",
            "Iteration 51, loss = 0.00135006\n",
            "Iteration 52, loss = 0.00130117\n",
            "Iteration 53, loss = 0.00125743\n",
            "Iteration 54, loss = 0.00121829\n",
            "Iteration 55, loss = 0.00118327\n",
            "Iteration 56, loss = 0.00115193\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.00113363\n",
            "Iteration 58, loss = 0.00112812\n",
            "Iteration 59, loss = 0.00112279\n",
            "Iteration 60, loss = 0.00111758\n",
            "Iteration 61, loss = 0.00111247\n",
            "Iteration 62, loss = 0.00110748\n",
            "Iteration 63, loss = 0.00110260\n",
            "Iteration 64, loss = 0.00109783\n",
            "Iteration 65, loss = 0.00109316\n",
            "Iteration 66, loss = 0.00108859\n",
            "Iteration 67, loss = 0.00108412\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.00108133\n",
            "Iteration 69, loss = 0.00108045\n",
            "Iteration 70, loss = 0.00107958\n",
            "Iteration 71, loss = 0.00107872\n",
            "Iteration 72, loss = 0.00107786\n",
            "Iteration 73, loss = 0.00107701\n",
            "Iteration 74, loss = 0.00107616\n",
            "Iteration 75, loss = 0.00107531\n",
            "Iteration 76, loss = 0.00107447\n",
            "Iteration 77, loss = 0.00107363\n",
            "Iteration 78, loss = 0.00107280\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 79, loss = 0.00107227\n",
            "Iteration 80, loss = 0.00107210\n",
            "Iteration 81, loss = 0.00107193\n",
            "Iteration 82, loss = 0.00107177\n",
            "Iteration 83, loss = 0.00107160\n",
            "Iteration 84, loss = 0.00107144\n",
            "Iteration 85, loss = 0.00107127\n",
            "Iteration 86, loss = 0.00107110\n",
            "Iteration 87, loss = 0.00107094\n",
            "Iteration 88, loss = 0.00107077\n",
            "Iteration 89, loss = 0.00107061\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 90, loss = 0.00107050\n",
            "Iteration 91, loss = 0.00107047\n",
            "Iteration 92, loss = 0.00107044\n",
            "Iteration 93, loss = 0.00107041\n",
            "Iteration 94, loss = 0.00107037\n",
            "Iteration 95, loss = 0.00107034\n",
            "Iteration 96, loss = 0.00107031\n",
            "Iteration 97, loss = 0.00107027\n",
            "Iteration 98, loss = 0.00107024\n",
            "Iteration 99, loss = 0.00107021\n",
            "Iteration 100, loss = 0.00107018\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 101, loss = 0.00107015\n",
            "Iteration 102, loss = 0.00107015\n",
            "Iteration 103, loss = 0.00107014\n",
            "Iteration 104, loss = 0.00107013\n",
            "Iteration 105, loss = 0.00107013\n",
            "Iteration 106, loss = 0.00107012\n",
            "Iteration 107, loss = 0.00107011\n",
            "Iteration 108, loss = 0.00107011\n",
            "Iteration 109, loss = 0.00107010\n",
            "Iteration 110, loss = 0.00107009\n",
            "Iteration 111, loss = 0.00107009\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.23577589\n",
            "Iteration 3, loss = 0.21126228\n",
            "Iteration 4, loss = 0.18929735\n",
            "Iteration 5, loss = 0.16961612\n",
            "Iteration 6, loss = 0.15198113\n",
            "Iteration 7, loss = 0.13617966\n",
            "Iteration 8, loss = 0.12202107\n",
            "Iteration 9, loss = 0.10933454\n",
            "Iteration 10, loss = 0.09796704\n",
            "Iteration 11, loss = 0.08778141\n",
            "Iteration 12, loss = 0.07865478\n",
            "Iteration 13, loss = 0.07047704\n",
            "Iteration 14, loss = 0.06314955\n",
            "Iteration 15, loss = 0.05658389\n",
            "Iteration 16, loss = 0.05070087\n",
            "Iteration 17, loss = 0.04542950\n",
            "Iteration 18, loss = 0.04070620\n",
            "Iteration 19, loss = 0.03647398\n",
            "Iteration 20, loss = 0.03268178\n",
            "Iteration 21, loss = 0.02928385\n",
            "Iteration 22, loss = 0.02623921\n",
            "Iteration 23, loss = 0.02351112\n",
            "Iteration 24, loss = 0.02106667\n",
            "Iteration 25, loss = 0.01887637\n",
            "Iteration 26, loss = 0.01691380\n",
            "Iteration 27, loss = 0.01515527\n",
            "Iteration 28, loss = 0.01357958\n",
            "Iteration 29, loss = 0.01216771\n",
            "Iteration 30, loss = 0.01090263\n",
            "Iteration 31, loss = 0.00976909\n",
            "Iteration 32, loss = 0.00875339\n",
            "Iteration 33, loss = 0.00784330\n",
            "Iteration 34, loss = 0.00702784\n",
            "Iteration 35, loss = 0.00629715\n",
            "Iteration 36, loss = 0.00564244\n",
            "Iteration 37, loss = 0.00505580\n",
            "Iteration 38, loss = 0.00453021\n",
            "Iteration 39, loss = 0.00405953\n",
            "Iteration 40, loss = 0.00363899\n",
            "Iteration 41, loss = 0.00326650\n",
            "Iteration 42, loss = 0.00294476\n",
            "Iteration 43, loss = 0.00267869\n",
            "Iteration 44, loss = 0.00246345\n",
            "Iteration 45, loss = 0.00228343\n",
            "Iteration 46, loss = 0.00212686\n",
            "Iteration 47, loss = 0.00198879\n",
            "Iteration 48, loss = 0.00186676\n",
            "Iteration 49, loss = 0.00175884\n",
            "Iteration 50, loss = 0.00166338\n",
            "Iteration 51, loss = 0.00157890\n",
            "Iteration 52, loss = 0.00150409\n",
            "Iteration 53, loss = 0.00143780\n",
            "Iteration 54, loss = 0.00137902\n",
            "Iteration 55, loss = 0.00132685\n",
            "Iteration 56, loss = 0.00128051\n",
            "Iteration 57, loss = 0.00123931\n",
            "Iteration 58, loss = 0.00120267\n",
            "Iteration 59, loss = 0.00117006\n",
            "Iteration 60, loss = 0.00114102\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 61, loss = 0.00112412\n",
            "Iteration 62, loss = 0.00111905\n",
            "Iteration 63, loss = 0.00111414\n",
            "Iteration 64, loss = 0.00110935\n",
            "Iteration 65, loss = 0.00110466\n",
            "Iteration 66, loss = 0.00110008\n",
            "Iteration 67, loss = 0.00109560\n",
            "Iteration 68, loss = 0.00109122\n",
            "Iteration 69, loss = 0.00108695\n",
            "Iteration 70, loss = 0.00108277\n",
            "Iteration 71, loss = 0.00107868\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.00107613\n",
            "Iteration 73, loss = 0.00107533\n",
            "Iteration 74, loss = 0.00107454\n",
            "Iteration 75, loss = 0.00107375\n",
            "Iteration 76, loss = 0.00107297\n",
            "Iteration 77, loss = 0.00107219\n",
            "Iteration 78, loss = 0.00107141\n",
            "Iteration 79, loss = 0.00107064\n",
            "Iteration 80, loss = 0.00106987\n",
            "Iteration 81, loss = 0.00106910\n",
            "Iteration 82, loss = 0.00106834\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 83, loss = 0.00106786\n",
            "Iteration 84, loss = 0.00106770\n",
            "Iteration 85, loss = 0.00106755\n",
            "Iteration 86, loss = 0.00106740\n",
            "Iteration 87, loss = 0.00106725\n",
            "Iteration 88, loss = 0.00106710\n",
            "Iteration 89, loss = 0.00106695\n",
            "Iteration 90, loss = 0.00106680\n",
            "Iteration 91, loss = 0.00106665\n",
            "Iteration 92, loss = 0.00106650\n",
            "Iteration 93, loss = 0.00106635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 94, loss = 0.00106625\n",
            "Iteration 95, loss = 0.00106622\n",
            "Iteration 96, loss = 0.00106619\n",
            "Iteration 97, loss = 0.00106616\n",
            "Iteration 98, loss = 0.00106613\n",
            "Iteration 99, loss = 0.00106610\n",
            "Iteration 100, loss = 0.00106607\n",
            "Iteration 101, loss = 0.00106604\n",
            "Iteration 102, loss = 0.00106601\n",
            "Iteration 103, loss = 0.00106598\n",
            "Iteration 104, loss = 0.00106595\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 105, loss = 0.00106593\n",
            "Iteration 106, loss = 0.00106593\n",
            "Iteration 107, loss = 0.00106592\n",
            "Iteration 108, loss = 0.00106591\n",
            "Iteration 109, loss = 0.00106591\n",
            "Iteration 110, loss = 0.00106590\n",
            "Iteration 111, loss = 0.00106590\n",
            "Iteration 112, loss = 0.00106589\n",
            "Iteration 113, loss = 0.00106588\n",
            "Iteration 114, loss = 0.00106588\n",
            "Iteration 115, loss = 0.00106587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.44985605\n",
            "Iteration 2, loss = 0.32299097\n",
            "Iteration 3, loss = 0.28940961\n",
            "Iteration 4, loss = 0.25931970\n",
            "Iteration 5, loss = 0.23235824\n",
            "Iteration 6, loss = 0.20819997\n",
            "Iteration 7, loss = 0.18655343\n",
            "Iteration 8, loss = 0.16715747\n",
            "Iteration 9, loss = 0.14977812\n",
            "Iteration 10, loss = 0.13420569\n",
            "Iteration 11, loss = 0.12025233\n",
            "Iteration 12, loss = 0.10774970\n",
            "Iteration 13, loss = 0.09654697\n",
            "Iteration 14, loss = 0.08650899\n",
            "Iteration 15, loss = 0.07751465\n",
            "Iteration 16, loss = 0.06945546\n",
            "Iteration 17, loss = 0.06223417\n",
            "Iteration 18, loss = 0.05576369\n",
            "Iteration 19, loss = 0.04996594\n",
            "Iteration 20, loss = 0.04477098\n",
            "Iteration 21, loss = 0.04011615\n",
            "Iteration 22, loss = 0.03594527\n",
            "Iteration 23, loss = 0.03220804\n",
            "Iteration 24, loss = 0.02885938\n",
            "Iteration 25, loss = 0.02585887\n",
            "Iteration 26, loss = 0.02317032\n",
            "Iteration 27, loss = 0.02076130\n",
            "Iteration 28, loss = 0.01860275\n",
            "Iteration 29, loss = 0.01666863\n",
            "Iteration 30, loss = 0.01493559\n",
            "Iteration 31, loss = 0.01338274\n",
            "Iteration 32, loss = 0.01199133\n",
            "Iteration 33, loss = 0.01074460\n",
            "Iteration 34, loss = 0.00962748\n",
            "Iteration 35, loss = 0.00862651\n",
            "Iteration 36, loss = 0.00772961\n",
            "Iteration 37, loss = 0.00692597\n",
            "Iteration 38, loss = 0.00620587\n",
            "Iteration 39, loss = 0.00556065\n",
            "Iteration 40, loss = 0.00498251\n",
            "Iteration 41, loss = 0.00446448\n",
            "Iteration 42, loss = 0.00400031\n",
            "Iteration 43, loss = 0.00358443\n",
            "Iteration 44, loss = 0.00321192\n",
            "Iteration 45, loss = 0.00287880\n",
            "Iteration 46, loss = 0.00258277\n",
            "Iteration 47, loss = 0.00232512\n",
            "Iteration 48, loss = 0.00211143\n",
            "Iteration 49, loss = 0.00194469\n",
            "Iteration 50, loss = 0.00181508\n",
            "Iteration 51, loss = 0.00170785\n",
            "Iteration 52, loss = 0.00161469\n",
            "Iteration 53, loss = 0.00153244\n",
            "Iteration 54, loss = 0.00145955\n",
            "Iteration 55, loss = 0.00139489\n",
            "Iteration 56, loss = 0.00133756\n",
            "Iteration 57, loss = 0.00128671\n",
            "Iteration 58, loss = 0.00124157\n",
            "Iteration 59, loss = 0.00120148\n",
            "Iteration 60, loss = 0.00116584\n",
            "Iteration 61, loss = 0.00113413\n",
            "Iteration 62, loss = 0.00110590\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 63, loss = 0.00108948\n",
            "Iteration 64, loss = 0.00108456\n",
            "Iteration 65, loss = 0.00107979\n",
            "Iteration 66, loss = 0.00107513\n",
            "Iteration 67, loss = 0.00107057\n",
            "Iteration 68, loss = 0.00106612\n",
            "Iteration 69, loss = 0.00106177\n",
            "Iteration 70, loss = 0.00105752\n",
            "Iteration 71, loss = 0.00105337\n",
            "Iteration 72, loss = 0.00104931\n",
            "Iteration 73, loss = 0.00104534\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.00104286\n",
            "Iteration 75, loss = 0.00104208\n",
            "Iteration 76, loss = 0.00104131\n",
            "Iteration 77, loss = 0.00104055\n",
            "Iteration 78, loss = 0.00103978\n",
            "Iteration 79, loss = 0.00103903\n",
            "Iteration 80, loss = 0.00103827\n",
            "Iteration 81, loss = 0.00103752\n",
            "Iteration 82, loss = 0.00103677\n",
            "Iteration 83, loss = 0.00103603\n",
            "Iteration 84, loss = 0.00103529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 85, loss = 0.00103482\n",
            "Iteration 86, loss = 0.00103467\n",
            "Iteration 87, loss = 0.00103453\n",
            "Iteration 88, loss = 0.00103438\n",
            "Iteration 89, loss = 0.00103423\n",
            "Iteration 90, loss = 0.00103408\n",
            "Iteration 91, loss = 0.00103394\n",
            "Iteration 92, loss = 0.00103379\n",
            "Iteration 93, loss = 0.00103365\n",
            "Iteration 94, loss = 0.00103350\n",
            "Iteration 95, loss = 0.00103335\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 96, loss = 0.00103326\n",
            "Iteration 97, loss = 0.00103323\n",
            "Iteration 98, loss = 0.00103320\n",
            "Iteration 99, loss = 0.00103317\n",
            "Iteration 100, loss = 0.00103314\n",
            "Iteration 101, loss = 0.00103311\n",
            "Iteration 102, loss = 0.00103309\n",
            "Iteration 103, loss = 0.00103306\n",
            "Iteration 104, loss = 0.00103303\n",
            "Iteration 105, loss = 0.00103300\n",
            "Iteration 106, loss = 0.00103297\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 107, loss = 0.00103295\n",
            "Iteration 108, loss = 0.00103294\n",
            "Iteration 109, loss = 0.00103294\n",
            "Iteration 110, loss = 0.00103293\n",
            "Iteration 111, loss = 0.00103293\n",
            "Iteration 112, loss = 0.00103292\n",
            "Iteration 113, loss = 0.00103291\n",
            "Iteration 114, loss = 0.00103291\n",
            "Iteration 115, loss = 0.00103290\n",
            "Iteration 116, loss = 0.00103290\n",
            "Iteration 117, loss = 0.00103289\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.31213142\n",
            "Iteration 3, loss = 0.27967913\n",
            "Iteration 4, loss = 0.25060090\n",
            "Iteration 5, loss = 0.22454593\n",
            "Iteration 6, loss = 0.20119990\n",
            "Iteration 7, loss = 0.18028116\n",
            "Iteration 8, loss = 0.16153733\n",
            "Iteration 9, loss = 0.14474230\n",
            "Iteration 10, loss = 0.12969345\n",
            "Iteration 11, loss = 0.11620923\n",
            "Iteration 12, loss = 0.10412696\n",
            "Iteration 13, loss = 0.09330088\n",
            "Iteration 14, loss = 0.08360039\n",
            "Iteration 15, loss = 0.07490847\n",
            "Iteration 16, loss = 0.06712024\n",
            "Iteration 17, loss = 0.06014175\n",
            "Iteration 18, loss = 0.05388881\n",
            "Iteration 19, loss = 0.04828599\n",
            "Iteration 20, loss = 0.04326570\n",
            "Iteration 21, loss = 0.03876737\n",
            "Iteration 22, loss = 0.03473673\n",
            "Iteration 23, loss = 0.03112515\n",
            "Iteration 24, loss = 0.02788907\n",
            "Iteration 25, loss = 0.02498944\n",
            "Iteration 26, loss = 0.02239129\n",
            "Iteration 27, loss = 0.02006327\n",
            "Iteration 28, loss = 0.01797729\n",
            "Iteration 29, loss = 0.01610820\n",
            "Iteration 30, loss = 0.01443343\n",
            "Iteration 31, loss = 0.01293278\n",
            "Iteration 32, loss = 0.01158816\n",
            "Iteration 33, loss = 0.01038334\n",
            "Iteration 34, loss = 0.00930379\n",
            "Iteration 35, loss = 0.00833647\n",
            "Iteration 36, loss = 0.00746973\n",
            "Iteration 37, loss = 0.00669310\n",
            "Iteration 38, loss = 0.00599722\n",
            "Iteration 39, loss = 0.00537369\n",
            "Iteration 40, loss = 0.00481499\n",
            "Iteration 41, loss = 0.00431437\n",
            "Iteration 42, loss = 0.00386581\n",
            "Iteration 43, loss = 0.00346393\n",
            "Iteration 44, loss = 0.00310402\n",
            "Iteration 45, loss = 0.00278243\n",
            "Iteration 46, loss = 0.00249760\n",
            "Iteration 47, loss = 0.00225217\n",
            "Iteration 48, loss = 0.00205256\n",
            "Iteration 49, loss = 0.00189916\n",
            "Iteration 50, loss = 0.00177884\n",
            "Iteration 51, loss = 0.00167767\n",
            "Iteration 52, loss = 0.00158904\n",
            "Iteration 53, loss = 0.00151030\n",
            "Iteration 54, loss = 0.00144052\n",
            "Iteration 55, loss = 0.00137882\n",
            "Iteration 56, loss = 0.00132424\n",
            "Iteration 57, loss = 0.00127591\n",
            "Iteration 58, loss = 0.00123307\n",
            "Iteration 59, loss = 0.00119507\n",
            "Iteration 60, loss = 0.00116133\n",
            "Iteration 61, loss = 0.00113136\n",
            "Iteration 62, loss = 0.00110470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 63, loss = 0.00108921\n",
            "Iteration 64, loss = 0.00108456\n",
            "Iteration 65, loss = 0.00108006\n",
            "Iteration 66, loss = 0.00107567\n",
            "Iteration 67, loss = 0.00107138\n",
            "Iteration 68, loss = 0.00106718\n",
            "Iteration 69, loss = 0.00106308\n",
            "Iteration 70, loss = 0.00105908\n",
            "Iteration 71, loss = 0.00105516\n",
            "Iteration 72, loss = 0.00105134\n",
            "Iteration 73, loss = 0.00104760\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.00104527\n",
            "Iteration 75, loss = 0.00104453\n",
            "Iteration 76, loss = 0.00104381\n",
            "Iteration 77, loss = 0.00104309\n",
            "Iteration 78, loss = 0.00104237\n",
            "Iteration 79, loss = 0.00104166\n",
            "Iteration 80, loss = 0.00104095\n",
            "Iteration 81, loss = 0.00104024\n",
            "Iteration 82, loss = 0.00103954\n",
            "Iteration 83, loss = 0.00103884\n",
            "Iteration 84, loss = 0.00103814\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 85, loss = 0.00103770\n",
            "Iteration 86, loss = 0.00103756\n",
            "Iteration 87, loss = 0.00103742\n",
            "Iteration 88, loss = 0.00103728\n",
            "Iteration 89, loss = 0.00103715\n",
            "Iteration 90, loss = 0.00103701\n",
            "Iteration 91, loss = 0.00103687\n",
            "Iteration 92, loss = 0.00103673\n",
            "Iteration 93, loss = 0.00103660\n",
            "Iteration 94, loss = 0.00103646\n",
            "Iteration 95, loss = 0.00103632\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 96, loss = 0.00103623\n",
            "Iteration 97, loss = 0.00103620\n",
            "Iteration 98, loss = 0.00103618\n",
            "Iteration 99, loss = 0.00103615\n",
            "Iteration 100, loss = 0.00103612\n",
            "Iteration 101, loss = 0.00103610\n",
            "Iteration 102, loss = 0.00103607\n",
            "Iteration 103, loss = 0.00103604\n",
            "Iteration 104, loss = 0.00103601\n",
            "Iteration 105, loss = 0.00103599\n",
            "Iteration 106, loss = 0.00103596\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 107, loss = 0.00103594\n",
            "Iteration 108, loss = 0.00103594\n",
            "Iteration 109, loss = 0.00103593\n",
            "Iteration 110, loss = 0.00103592\n",
            "Iteration 111, loss = 0.00103592\n",
            "Iteration 112, loss = 0.00103591\n",
            "Iteration 113, loss = 0.00103591\n",
            "Iteration 114, loss = 0.00103590\n",
            "Iteration 115, loss = 0.00103590\n",
            "Iteration 116, loss = 0.00103589\n",
            "Iteration 117, loss = 0.00103589\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13820657\n",
            "Iteration 2, loss = 0.12323676\n",
            "Iteration 3, loss = 0.11042384\n",
            "Iteration 4, loss = 0.09894308\n",
            "Iteration 5, loss = 0.08865597\n",
            "Iteration 6, loss = 0.07943841\n",
            "Iteration 7, loss = 0.07117920\n",
            "Iteration 8, loss = 0.06377871\n",
            "Iteration 9, loss = 0.05714764\n",
            "Iteration 10, loss = 0.05120600\n",
            "Iteration 11, loss = 0.04588211\n",
            "Iteration 12, loss = 0.04111175\n",
            "Iteration 13, loss = 0.03683736\n",
            "Iteration 14, loss = 0.03300739\n",
            "Iteration 15, loss = 0.02957561\n",
            "Iteration 16, loss = 0.02650064\n",
            "Iteration 17, loss = 0.02374544\n",
            "Iteration 18, loss = 0.02127697\n",
            "Iteration 19, loss = 0.01906634\n",
            "Iteration 20, loss = 0.01708955\n",
            "Iteration 21, loss = 0.01532781\n",
            "Iteration 22, loss = 0.01376297\n",
            "Iteration 23, loss = 0.01237177\n",
            "Iteration 24, loss = 0.01113122\n",
            "Iteration 25, loss = 0.01002364\n",
            "Iteration 26, loss = 0.00903481\n",
            "Iteration 27, loss = 0.00815234\n",
            "Iteration 28, loss = 0.00736517\n",
            "Iteration 29, loss = 0.00666335\n",
            "Iteration 30, loss = 0.00603779\n",
            "Iteration 31, loss = 0.00548039\n",
            "Iteration 32, loss = 0.00498381\n",
            "Iteration 33, loss = 0.00454143\n",
            "Iteration 34, loss = 0.00414731\n",
            "Iteration 35, loss = 0.00379613\n",
            "Iteration 36, loss = 0.00348316\n",
            "Iteration 37, loss = 0.00320419\n",
            "Iteration 38, loss = 0.00295546\n",
            "Iteration 39, loss = 0.00273362\n",
            "Iteration 40, loss = 0.00253574\n",
            "Iteration 41, loss = 0.00235921\n",
            "Iteration 42, loss = 0.00220168\n",
            "Iteration 43, loss = 0.00206105\n",
            "Iteration 44, loss = 0.00193549\n",
            "Iteration 45, loss = 0.00182333\n",
            "Iteration 46, loss = 0.00172310\n",
            "Iteration 47, loss = 0.00163351\n",
            "Iteration 48, loss = 0.00155340\n",
            "Iteration 49, loss = 0.00148176\n",
            "Iteration 50, loss = 0.00141767\n",
            "Iteration 51, loss = 0.00136033\n",
            "Iteration 52, loss = 0.00130902\n",
            "Iteration 53, loss = 0.00126310\n",
            "Iteration 54, loss = 0.00122200\n",
            "Iteration 55, loss = 0.00118520\n",
            "Iteration 56, loss = 0.00115226\n",
            "Iteration 57, loss = 0.00112276\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 58, loss = 0.00110553\n",
            "Iteration 59, loss = 0.00110035\n",
            "Iteration 60, loss = 0.00109533\n",
            "Iteration 61, loss = 0.00109042\n",
            "Iteration 62, loss = 0.00108561\n",
            "Iteration 63, loss = 0.00108091\n",
            "Iteration 64, loss = 0.00107631\n",
            "Iteration 65, loss = 0.00107182\n",
            "Iteration 66, loss = 0.00106742\n",
            "Iteration 67, loss = 0.00106311\n",
            "Iteration 68, loss = 0.00105890\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 69, loss = 0.00105627\n",
            "Iteration 70, loss = 0.00105545\n",
            "Iteration 71, loss = 0.00105463\n",
            "Iteration 72, loss = 0.00105382\n",
            "Iteration 73, loss = 0.00105301\n",
            "Iteration 74, loss = 0.00105220\n",
            "Iteration 75, loss = 0.00105140\n",
            "Iteration 76, loss = 0.00105061\n",
            "Iteration 77, loss = 0.00104981\n",
            "Iteration 78, loss = 0.00104902\n",
            "Iteration 79, loss = 0.00104823\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 80, loss = 0.00104773\n",
            "Iteration 81, loss = 0.00104758\n",
            "Iteration 82, loss = 0.00104742\n",
            "Iteration 83, loss = 0.00104726\n",
            "Iteration 84, loss = 0.00104711\n",
            "Iteration 85, loss = 0.00104695\n",
            "Iteration 86, loss = 0.00104680\n",
            "Iteration 87, loss = 0.00104664\n",
            "Iteration 88, loss = 0.00104648\n",
            "Iteration 89, loss = 0.00104633\n",
            "Iteration 90, loss = 0.00104617\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 91, loss = 0.00104607\n",
            "Iteration 92, loss = 0.00104604\n",
            "Iteration 93, loss = 0.00104601\n",
            "Iteration 94, loss = 0.00104598\n",
            "Iteration 95, loss = 0.00104595\n",
            "Iteration 96, loss = 0.00104592\n",
            "Iteration 97, loss = 0.00104589\n",
            "Iteration 98, loss = 0.00104586\n",
            "Iteration 99, loss = 0.00104583\n",
            "Iteration 100, loss = 0.00104580\n",
            "Iteration 101, loss = 0.00104576\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 102, loss = 0.00104574\n",
            "Iteration 103, loss = 0.00104574\n",
            "Iteration 104, loss = 0.00104573\n",
            "Iteration 105, loss = 0.00104573\n",
            "Iteration 106, loss = 0.00104572\n",
            "Iteration 107, loss = 0.00104571\n",
            "Iteration 108, loss = 0.00104571\n",
            "Iteration 109, loss = 0.00104570\n",
            "Iteration 110, loss = 0.00104569\n",
            "Iteration 111, loss = 0.00104569\n",
            "Iteration 112, loss = 0.00104568\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.30345159\n",
            "Iteration 3, loss = 0.27190174\n",
            "Iteration 4, loss = 0.24363213\n",
            "Iteration 5, loss = 0.21830171\n",
            "Iteration 6, loss = 0.19560489\n",
            "Iteration 7, loss = 0.17526786\n",
            "Iteration 8, loss = 0.15704526\n",
            "Iteration 9, loss = 0.14071727\n",
            "Iteration 10, loss = 0.12608691\n",
            "Iteration 11, loss = 0.11297765\n",
            "Iteration 12, loss = 0.10123137\n",
            "Iteration 13, loss = 0.09070635\n",
            "Iteration 14, loss = 0.08127562\n",
            "Iteration 15, loss = 0.07282539\n",
            "Iteration 16, loss = 0.06525374\n",
            "Iteration 17, loss = 0.05846931\n",
            "Iteration 18, loss = 0.05239026\n",
            "Iteration 19, loss = 0.04694325\n",
            "Iteration 20, loss = 0.04206256\n",
            "Iteration 21, loss = 0.03768932\n",
            "Iteration 22, loss = 0.03377076\n",
            "Iteration 23, loss = 0.03025962\n",
            "Iteration 24, loss = 0.02711352\n",
            "Iteration 25, loss = 0.02429453\n",
            "Iteration 26, loss = 0.02176863\n",
            "Iteration 27, loss = 0.01950535\n",
            "Iteration 28, loss = 0.01747738\n",
            "Iteration 29, loss = 0.01566025\n",
            "Iteration 30, loss = 0.01403206\n",
            "Iteration 31, loss = 0.01257315\n",
            "Iteration 32, loss = 0.01126592\n",
            "Iteration 33, loss = 0.01009460\n",
            "Iteration 34, loss = 0.00904506\n",
            "Iteration 35, loss = 0.00810465\n",
            "Iteration 36, loss = 0.00726201\n",
            "Iteration 37, loss = 0.00650698\n",
            "Iteration 38, loss = 0.00583045\n",
            "Iteration 39, loss = 0.00522426\n",
            "Iteration 40, loss = 0.00468109\n",
            "Iteration 41, loss = 0.00419440\n",
            "Iteration 42, loss = 0.00375832\n",
            "Iteration 43, loss = 0.00336765\n",
            "Iteration 44, loss = 0.00301794\n",
            "Iteration 45, loss = 0.00270600\n",
            "Iteration 46, loss = 0.00243132\n",
            "Iteration 47, loss = 0.00219782\n",
            "Iteration 48, loss = 0.00201113\n",
            "Iteration 49, loss = 0.00186730\n",
            "Iteration 50, loss = 0.00175175\n",
            "Iteration 51, loss = 0.00165301\n",
            "Iteration 52, loss = 0.00156630\n",
            "Iteration 53, loss = 0.00148963\n",
            "Iteration 54, loss = 0.00142172\n",
            "Iteration 55, loss = 0.00136150\n",
            "Iteration 56, loss = 0.00130806\n",
            "Iteration 57, loss = 0.00126061\n",
            "Iteration 58, loss = 0.00121843\n",
            "Iteration 59, loss = 0.00118092\n",
            "Iteration 60, loss = 0.00114752\n",
            "Iteration 61, loss = 0.00111774\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 62, loss = 0.00110037\n",
            "Iteration 63, loss = 0.00109513\n",
            "Iteration 64, loss = 0.00109006\n",
            "Iteration 65, loss = 0.00108509\n",
            "Iteration 66, loss = 0.00108022\n",
            "Iteration 67, loss = 0.00107546\n",
            "Iteration 68, loss = 0.00107081\n",
            "Iteration 69, loss = 0.00106625\n",
            "Iteration 70, loss = 0.00106180\n",
            "Iteration 71, loss = 0.00105745\n",
            "Iteration 72, loss = 0.00105321\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.00105056\n",
            "Iteration 74, loss = 0.00104972\n",
            "Iteration 75, loss = 0.00104890\n",
            "Iteration 76, loss = 0.00104809\n",
            "Iteration 77, loss = 0.00104727\n",
            "Iteration 78, loss = 0.00104646\n",
            "Iteration 79, loss = 0.00104566\n",
            "Iteration 80, loss = 0.00104486\n",
            "Iteration 81, loss = 0.00104406\n",
            "Iteration 82, loss = 0.00104326\n",
            "Iteration 83, loss = 0.00104247\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 84, loss = 0.00104197\n",
            "Iteration 85, loss = 0.00104181\n",
            "Iteration 86, loss = 0.00104166\n",
            "Iteration 87, loss = 0.00104150\n",
            "Iteration 88, loss = 0.00104134\n",
            "Iteration 89, loss = 0.00104119\n",
            "Iteration 90, loss = 0.00104103\n",
            "Iteration 91, loss = 0.00104087\n",
            "Iteration 92, loss = 0.00104072\n",
            "Iteration 93, loss = 0.00104056\n",
            "Iteration 94, loss = 0.00104041\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 95, loss = 0.00104031\n",
            "Iteration 96, loss = 0.00104027\n",
            "Iteration 97, loss = 0.00104024\n",
            "Iteration 98, loss = 0.00104021\n",
            "Iteration 99, loss = 0.00104018\n",
            "Iteration 100, loss = 0.00104015\n",
            "Iteration 101, loss = 0.00104012\n",
            "Iteration 102, loss = 0.00104009\n",
            "Iteration 103, loss = 0.00104006\n",
            "Iteration 104, loss = 0.00104003\n",
            "Iteration 105, loss = 0.00103999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 106, loss = 0.00103997\n",
            "Iteration 107, loss = 0.00103997\n",
            "Iteration 108, loss = 0.00103996\n",
            "Iteration 109, loss = 0.00103996\n",
            "Iteration 110, loss = 0.00103995\n",
            "Iteration 111, loss = 0.00103994\n",
            "Iteration 112, loss = 0.00103994\n",
            "Iteration 113, loss = 0.00103993\n",
            "Iteration 114, loss = 0.00103993\n",
            "Iteration 115, loss = 0.00103992\n",
            "Iteration 116, loss = 0.00103991\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.13648097\n",
            "Iteration 2, loss = 0.12232285\n",
            "Iteration 3, loss = 0.10960495\n",
            "Iteration 4, loss = 0.09820932\n",
            "Iteration 5, loss = 0.08799850\n",
            "Iteration 6, loss = 0.07884930\n",
            "Iteration 7, loss = 0.07065134\n",
            "Iteration 8, loss = 0.06330573\n",
            "Iteration 9, loss = 0.05672383\n",
            "Iteration 10, loss = 0.05082626\n",
            "Iteration 11, loss = 0.04554186\n",
            "Iteration 12, loss = 0.04080688\n",
            "Iteration 13, loss = 0.03656427\n",
            "Iteration 14, loss = 0.03276308\n",
            "Iteration 15, loss = 0.02935835\n",
            "Iteration 16, loss = 0.02631135\n",
            "Iteration 17, loss = 0.02358804\n",
            "Iteration 18, loss = 0.02115477\n",
            "Iteration 19, loss = 0.01897878\n",
            "Iteration 20, loss = 0.01703192\n",
            "Iteration 21, loss = 0.01529019\n",
            "Iteration 22, loss = 0.01373237\n",
            "Iteration 23, loss = 0.01233953\n",
            "Iteration 24, loss = 0.01109470\n",
            "Iteration 25, loss = 0.00998262\n",
            "Iteration 26, loss = 0.00898960\n",
            "Iteration 27, loss = 0.00810341\n",
            "Iteration 28, loss = 0.00731308\n",
            "Iteration 29, loss = 0.00660866\n",
            "Iteration 30, loss = 0.00598120\n",
            "Iteration 31, loss = 0.00542262\n",
            "Iteration 32, loss = 0.00492558\n",
            "Iteration 33, loss = 0.00448348\n",
            "Iteration 34, loss = 0.00409037\n",
            "Iteration 35, loss = 0.00374125\n",
            "Iteration 36, loss = 0.00343118\n",
            "Iteration 37, loss = 0.00315557\n",
            "Iteration 38, loss = 0.00291052\n",
            "Iteration 39, loss = 0.00269257\n",
            "Iteration 40, loss = 0.00249865\n",
            "Iteration 41, loss = 0.00232606\n",
            "Iteration 42, loss = 0.00217238\n",
            "Iteration 43, loss = 0.00203551\n",
            "Iteration 44, loss = 0.00191355\n",
            "Iteration 45, loss = 0.00180484\n",
            "Iteration 46, loss = 0.00170788\n",
            "Iteration 47, loss = 0.00162137\n",
            "Iteration 48, loss = 0.00154412\n",
            "Iteration 49, loss = 0.00147512\n",
            "Iteration 50, loss = 0.00141347\n",
            "Iteration 51, loss = 0.00135837\n",
            "Iteration 52, loss = 0.00130910\n",
            "Iteration 53, loss = 0.00126504\n",
            "Iteration 54, loss = 0.00122563\n",
            "Iteration 55, loss = 0.00119038\n",
            "Iteration 56, loss = 0.00115883\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 57, loss = 0.00114041\n",
            "Iteration 58, loss = 0.00113488\n",
            "Iteration 59, loss = 0.00112951\n",
            "Iteration 60, loss = 0.00112427\n",
            "Iteration 61, loss = 0.00111913\n",
            "Iteration 62, loss = 0.00111411\n",
            "Iteration 63, loss = 0.00110920\n",
            "Iteration 64, loss = 0.00110440\n",
            "Iteration 65, loss = 0.00109970\n",
            "Iteration 66, loss = 0.00109511\n",
            "Iteration 67, loss = 0.00109061\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.00108780\n",
            "Iteration 69, loss = 0.00108692\n",
            "Iteration 70, loss = 0.00108605\n",
            "Iteration 71, loss = 0.00108518\n",
            "Iteration 72, loss = 0.00108432\n",
            "Iteration 73, loss = 0.00108346\n",
            "Iteration 74, loss = 0.00108261\n",
            "Iteration 75, loss = 0.00108175\n",
            "Iteration 76, loss = 0.00108091\n",
            "Iteration 77, loss = 0.00108006\n",
            "Iteration 78, loss = 0.00107922\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 79, loss = 0.00107869\n",
            "Iteration 80, loss = 0.00107852\n",
            "Iteration 81, loss = 0.00107835\n",
            "Iteration 82, loss = 0.00107819\n",
            "Iteration 83, loss = 0.00107802\n",
            "Iteration 84, loss = 0.00107785\n",
            "Iteration 85, loss = 0.00107769\n",
            "Iteration 86, loss = 0.00107752\n",
            "Iteration 87, loss = 0.00107736\n",
            "Iteration 88, loss = 0.00107719\n",
            "Iteration 89, loss = 0.00107702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 90, loss = 0.00107692\n",
            "Iteration 91, loss = 0.00107689\n",
            "Iteration 92, loss = 0.00107685\n",
            "Iteration 93, loss = 0.00107682\n",
            "Iteration 94, loss = 0.00107679\n",
            "Iteration 95, loss = 0.00107675\n",
            "Iteration 96, loss = 0.00107672\n",
            "Iteration 97, loss = 0.00107669\n",
            "Iteration 98, loss = 0.00107665\n",
            "Iteration 99, loss = 0.00107662\n",
            "Iteration 100, loss = 0.00107659\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 101, loss = 0.00107657\n",
            "Iteration 102, loss = 0.00107656\n",
            "Iteration 103, loss = 0.00107655\n",
            "Iteration 104, loss = 0.00107655\n",
            "Iteration 105, loss = 0.00107654\n",
            "Iteration 106, loss = 0.00107653\n",
            "Iteration 107, loss = 0.00107653\n",
            "Iteration 108, loss = 0.00107652\n",
            "Iteration 109, loss = 0.00107651\n",
            "Iteration 110, loss = 0.00107651\n",
            "Iteration 111, loss = 0.00107650\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.7min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[33727     0]\n",
            " [16919     0]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80     33727\n",
            "           1       0.00      0.00      0.00     16919\n",
            "\n",
            "    accuracy                           0.67     50646\n",
            "   macro avg       0.33      0.50      0.40     50646\n",
            "weighted avg       0.44      0.67      0.53     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Prediction Accuracy (Shuffle Split) : 100.000% (0.000%)\n",
            "Iteration 1, loss = 0.84150944\n",
            "Iteration 2, loss = 0.64721403\n",
            "Iteration 3, loss = 0.61875589\n",
            "Iteration 4, loss = 0.60364069\n",
            "Iteration 5, loss = 0.59537763\n",
            "Iteration 6, loss = 0.59533047\n",
            "Iteration 7, loss = 0.59113003\n",
            "Iteration 8, loss = 0.59082951\n",
            "Iteration 9, loss = 0.58910007\n",
            "Iteration 10, loss = 0.58738562\n",
            "Iteration 11, loss = 0.58649435\n",
            "Iteration 12, loss = 0.58611092\n",
            "Iteration 13, loss = 0.58767165\n",
            "Iteration 14, loss = 0.58645150\n",
            "Iteration 15, loss = 0.58637206\n",
            "Iteration 16, loss = 0.58661012\n",
            "Iteration 17, loss = 0.58695421\n",
            "Iteration 18, loss = 0.58658188\n",
            "Iteration 19, loss = 0.58761146\n",
            "Iteration 20, loss = 0.58634430\n",
            "Iteration 21, loss = 0.58659103\n",
            "Iteration 22, loss = 0.58676295\n",
            "Iteration 23, loss = 0.58584489\n",
            "Iteration 24, loss = 0.58656649\n",
            "Iteration 25, loss = 0.58585972\n",
            "Iteration 26, loss = 0.58575176\n",
            "Iteration 27, loss = 0.58616605\n",
            "Iteration 28, loss = 0.58531518\n",
            "Iteration 29, loss = 0.59882658\n",
            "Iteration 30, loss = 0.62093594\n",
            "Iteration 31, loss = 0.60638088\n",
            "Iteration 32, loss = 0.59682904\n",
            "Iteration 33, loss = 0.59084331\n",
            "Iteration 34, loss = 0.60421938\n",
            "Iteration 35, loss = 0.60120301\n",
            "Iteration 36, loss = 0.59262694\n",
            "Iteration 37, loss = 0.58713272\n",
            "Iteration 38, loss = 0.60064817\n",
            "Iteration 39, loss = 0.59935971\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 40, loss = 0.58207872\n",
            "Iteration 41, loss = 0.57800704\n",
            "Iteration 42, loss = 0.57321719\n",
            "Iteration 43, loss = 0.56680688\n",
            "Iteration 44, loss = 0.56101484\n",
            "Iteration 45, loss = 0.55782087\n",
            "Iteration 46, loss = 0.55709207\n",
            "Iteration 47, loss = 0.55758623\n",
            "Iteration 48, loss = 0.55701566\n",
            "Iteration 49, loss = 0.55416170\n",
            "Iteration 50, loss = 0.55441520\n",
            "Iteration 51, loss = 0.55352295\n",
            "Iteration 52, loss = 0.60420830\n",
            "Iteration 53, loss = 0.63134686\n",
            "Iteration 54, loss = 0.61300408\n",
            "Iteration 55, loss = 0.60029884\n",
            "Iteration 56, loss = 0.59251662\n",
            "Iteration 57, loss = 0.58500413\n",
            "Iteration 58, loss = 0.58007147\n",
            "Iteration 59, loss = 0.57830670\n",
            "Iteration 60, loss = 0.57182899\n",
            "Iteration 61, loss = 0.57074816\n",
            "Iteration 62, loss = 0.56868636\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 63, loss = 0.53964640\n",
            "Iteration 64, loss = 0.53473351\n",
            "Iteration 65, loss = 0.53004707\n",
            "Iteration 66, loss = 0.52484119\n",
            "Iteration 67, loss = 0.51937986\n",
            "Iteration 68, loss = 0.51407133\n",
            "Iteration 69, loss = 0.50847588\n",
            "Iteration 70, loss = 0.50314722\n",
            "Iteration 71, loss = 0.49767212\n",
            "Iteration 72, loss = 0.49224233\n",
            "Iteration 73, loss = 0.48723317\n",
            "Iteration 74, loss = 0.48208499\n",
            "Iteration 75, loss = 0.47764890\n",
            "Iteration 76, loss = 0.47298517\n",
            "Iteration 77, loss = 0.46842472\n",
            "Iteration 78, loss = 0.46530922\n",
            "Iteration 79, loss = 0.46111932\n",
            "Iteration 80, loss = 0.45761908\n",
            "Iteration 81, loss = 0.45430981\n",
            "Iteration 82, loss = 0.45235228\n",
            "Iteration 83, loss = 0.45271968\n",
            "Iteration 84, loss = 0.44981676\n",
            "Iteration 85, loss = 0.44869230\n",
            "Iteration 86, loss = 0.44843125\n",
            "Iteration 87, loss = 0.44655100\n",
            "Iteration 88, loss = 0.44439159\n",
            "Iteration 89, loss = 0.44497066\n",
            "Iteration 90, loss = 0.44229076\n",
            "Iteration 91, loss = 0.44153254\n",
            "Iteration 92, loss = 0.44101461\n",
            "Iteration 93, loss = 0.44057000\n",
            "Iteration 94, loss = 0.44039486\n",
            "Iteration 95, loss = 0.43920184\n",
            "Iteration 96, loss = 0.43439817\n",
            "Iteration 97, loss = 0.43321486\n",
            "Iteration 98, loss = 0.43658400\n",
            "Iteration 99, loss = 0.43280819\n",
            "Iteration 100, loss = 0.43213116\n",
            "Iteration 101, loss = 0.43187359\n",
            "Iteration 102, loss = 0.42852264\n",
            "Iteration 103, loss = 0.43279924\n",
            "Iteration 104, loss = 0.42271931\n",
            "Iteration 105, loss = 0.42791537\n",
            "Iteration 106, loss = 0.42621906\n",
            "Iteration 107, loss = 0.42105434\n",
            "Iteration 108, loss = 0.41957922\n",
            "Iteration 109, loss = 0.41786908\n",
            "Iteration 110, loss = 0.42131307\n",
            "Iteration 111, loss = 0.42039459\n",
            "Iteration 112, loss = 0.41204334\n",
            "Iteration 113, loss = 0.41545247\n",
            "Iteration 114, loss = 0.41463925\n",
            "Iteration 115, loss = 0.41902919\n",
            "Iteration 116, loss = 0.41055700\n",
            "Iteration 117, loss = 0.41046720\n",
            "Iteration 118, loss = 0.41420623\n",
            "Iteration 119, loss = 0.40947389\n",
            "Iteration 120, loss = 0.40735798\n",
            "Iteration 121, loss = 0.40568914\n",
            "Iteration 122, loss = 0.40831345\n",
            "Iteration 123, loss = 0.40667624\n",
            "Iteration 124, loss = 0.40315814\n",
            "Iteration 125, loss = 0.40177569\n",
            "Iteration 126, loss = 0.40367085\n",
            "Iteration 127, loss = 0.39778732\n",
            "Iteration 128, loss = 0.39496199\n",
            "Iteration 129, loss = 0.40500864\n",
            "Iteration 130, loss = 0.39956373\n",
            "Iteration 131, loss = 0.40076366\n",
            "Iteration 132, loss = 0.39755466\n",
            "Iteration 133, loss = 0.40295692\n",
            "Iteration 134, loss = 0.39667872\n",
            "Iteration 135, loss = 0.39657074\n",
            "Iteration 136, loss = 0.39207071\n",
            "Iteration 137, loss = 0.39594007\n",
            "Iteration 138, loss = 0.39662265\n",
            "Iteration 139, loss = 0.39402662\n",
            "Iteration 140, loss = 0.39204412\n",
            "Iteration 141, loss = 0.39244330\n",
            "Iteration 142, loss = 0.39513400\n",
            "Iteration 143, loss = 0.39287633\n",
            "Iteration 144, loss = 0.39584091\n",
            "Iteration 145, loss = 0.39414082\n",
            "Iteration 146, loss = 0.39327150\n",
            "Iteration 147, loss = 0.38840958\n",
            "Iteration 148, loss = 0.39248213\n",
            "Iteration 149, loss = 0.38837850\n",
            "Iteration 150, loss = 0.38920864\n",
            "Iteration 151, loss = 0.38899936\n",
            "Iteration 152, loss = 0.38967062\n",
            "Iteration 153, loss = 0.38895025\n",
            "Iteration 154, loss = 0.38775005\n",
            "Iteration 155, loss = 0.38781303\n",
            "Iteration 156, loss = 0.38032922\n",
            "Iteration 157, loss = 0.39087412\n",
            "Iteration 158, loss = 0.38684313\n",
            "Iteration 159, loss = 0.38022451\n",
            "Iteration 160, loss = 0.38735490\n",
            "Iteration 161, loss = 0.38405832\n",
            "Iteration 162, loss = 0.38176220\n",
            "Iteration 163, loss = 0.38774964\n",
            "Iteration 164, loss = 0.38343591\n",
            "Iteration 165, loss = 0.38284109\n",
            "Iteration 166, loss = 0.38517584\n",
            "Iteration 167, loss = 0.38198791\n",
            "Iteration 168, loss = 0.38476254\n",
            "Iteration 169, loss = 0.37604128\n",
            "Iteration 170, loss = 0.38499411\n",
            "Iteration 171, loss = 0.38422043\n",
            "Iteration 172, loss = 0.37995940\n",
            "Iteration 173, loss = 0.38077568\n",
            "Iteration 174, loss = 0.38513640\n",
            "Iteration 175, loss = 0.37515435\n",
            "Iteration 176, loss = 0.38589016\n",
            "Iteration 177, loss = 0.38356496\n",
            "Iteration 178, loss = 0.37818785\n",
            "Iteration 179, loss = 0.37564949\n",
            "Iteration 180, loss = 0.37304726\n",
            "Iteration 181, loss = 0.37573104\n",
            "Iteration 182, loss = 0.37931473\n",
            "Iteration 183, loss = 0.38063248\n",
            "Iteration 184, loss = 0.38198212\n",
            "Iteration 185, loss = 0.38112682\n",
            "Iteration 186, loss = 0.37375177\n",
            "Iteration 187, loss = 0.37635795\n",
            "Iteration 188, loss = 0.38172582\n",
            "Iteration 189, loss = 0.37800411\n",
            "Iteration 190, loss = 0.37875961\n",
            "Iteration 191, loss = 0.38658900\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 192, loss = 0.33940292\n",
            "Iteration 193, loss = 0.33922118\n",
            "Iteration 194, loss = 0.33904468\n",
            "Iteration 195, loss = 0.33891716\n",
            "Iteration 196, loss = 0.33885671\n",
            "Iteration 197, loss = 0.33875377\n",
            "Iteration 198, loss = 0.33874776\n",
            "Iteration 199, loss = 0.33868841\n",
            "Iteration 200, loss = 0.33858347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63626261\n",
            "Iteration 3, loss = 0.60570269\n",
            "Iteration 4, loss = 0.58971621\n",
            "Iteration 5, loss = 0.57827965\n",
            "Iteration 6, loss = 0.57255224\n",
            "Iteration 7, loss = 0.56804741\n",
            "Iteration 8, loss = 0.56656641\n",
            "Iteration 9, loss = 0.56447963\n",
            "Iteration 10, loss = 0.56532593\n",
            "Iteration 11, loss = 0.56421580\n",
            "Iteration 12, loss = 0.56430619\n",
            "Iteration 13, loss = 0.56344896\n",
            "Iteration 14, loss = 0.56332432\n",
            "Iteration 15, loss = 0.56554455\n",
            "Iteration 16, loss = 0.56390534\n",
            "Iteration 17, loss = 0.56408499\n",
            "Iteration 18, loss = 0.56377393\n",
            "Iteration 19, loss = 0.56511895\n",
            "Iteration 20, loss = 0.56409241\n",
            "Iteration 21, loss = 0.56629520\n",
            "Iteration 22, loss = 0.56499241\n",
            "Iteration 23, loss = 0.56413094\n",
            "Iteration 24, loss = 0.56444705\n",
            "Iteration 25, loss = 0.56350039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.50637264\n",
            "Iteration 27, loss = 0.47908463\n",
            "Iteration 28, loss = 0.49987571\n",
            "Iteration 29, loss = 0.50923587\n",
            "Iteration 30, loss = 0.52135671\n",
            "Iteration 31, loss = 0.51061128\n",
            "Iteration 32, loss = 0.51495575\n",
            "Iteration 33, loss = 0.50921761\n",
            "Iteration 34, loss = 0.51219175\n",
            "Iteration 35, loss = 0.51005096\n",
            "Iteration 36, loss = 0.50825240\n",
            "Iteration 37, loss = 0.51355800\n",
            "Iteration 38, loss = 0.51092429\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.41169592\n",
            "Iteration 40, loss = 0.39729065\n",
            "Iteration 41, loss = 0.38444393\n",
            "Iteration 42, loss = 0.37251288\n",
            "Iteration 43, loss = 0.36178661\n",
            "Iteration 44, loss = 0.35215384\n",
            "Iteration 45, loss = 0.34355429\n",
            "Iteration 46, loss = 0.33543986\n",
            "Iteration 47, loss = 0.32849433\n",
            "Iteration 48, loss = 0.32220775\n",
            "Iteration 49, loss = 0.31627944\n",
            "Iteration 50, loss = 0.31101987\n",
            "Iteration 51, loss = 0.30631808\n",
            "Iteration 52, loss = 0.30216560\n",
            "Iteration 53, loss = 0.29790043\n",
            "Iteration 54, loss = 0.29458079\n",
            "Iteration 55, loss = 0.29119526\n",
            "Iteration 56, loss = 0.28840313\n",
            "Iteration 57, loss = 0.28599787\n",
            "Iteration 58, loss = 0.28358444\n",
            "Iteration 59, loss = 0.28168392\n",
            "Iteration 60, loss = 0.27973234\n",
            "Iteration 61, loss = 0.27745998\n",
            "Iteration 62, loss = 0.27660272\n",
            "Iteration 63, loss = 0.27414888\n",
            "Iteration 64, loss = 0.27329310\n",
            "Iteration 65, loss = 0.27212140\n",
            "Iteration 66, loss = 0.27142815\n",
            "Iteration 67, loss = 0.27001148\n",
            "Iteration 68, loss = 0.26993224\n",
            "Iteration 69, loss = 0.26778709\n",
            "Iteration 70, loss = 0.26855734\n",
            "Iteration 71, loss = 0.26730633\n",
            "Iteration 72, loss = 0.26719011\n",
            "Iteration 73, loss = 0.26540002\n",
            "Iteration 74, loss = 0.26678560\n",
            "Iteration 75, loss = 0.26586053\n",
            "Iteration 76, loss = 0.26532102\n",
            "Iteration 77, loss = 0.26517107\n",
            "Iteration 78, loss = 0.26410009\n",
            "Iteration 79, loss = 0.26653045\n",
            "Iteration 80, loss = 0.26278783\n",
            "Iteration 81, loss = 0.26448263\n",
            "Iteration 82, loss = 0.26358216\n",
            "Iteration 83, loss = 0.26434808\n",
            "Iteration 84, loss = 0.26327619\n",
            "Iteration 85, loss = 0.26503124\n",
            "Iteration 86, loss = 0.26610870\n",
            "Iteration 87, loss = 0.26901741\n",
            "Iteration 88, loss = 0.26658841\n",
            "Iteration 89, loss = 0.26862098\n",
            "Iteration 90, loss = 0.26427434\n",
            "Iteration 91, loss = 0.27148386\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 92, loss = 0.25221938\n",
            "Iteration 93, loss = 0.25184285\n",
            "Iteration 94, loss = 0.25178445\n",
            "Iteration 95, loss = 0.25170193\n",
            "Iteration 96, loss = 0.25172700\n",
            "Iteration 97, loss = 0.25163219\n",
            "Iteration 98, loss = 0.25155135\n",
            "Iteration 99, loss = 0.25153810\n",
            "Iteration 100, loss = 0.25144599\n",
            "Iteration 101, loss = 0.25147544\n",
            "Iteration 102, loss = 0.25135579\n",
            "Iteration 103, loss = 0.25129662\n",
            "Iteration 104, loss = 0.25134584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 105, loss = 0.25097555\n",
            "Iteration 106, loss = 0.25101980\n",
            "Iteration 107, loss = 0.25097039\n",
            "Iteration 108, loss = 0.25096349\n",
            "Iteration 109, loss = 0.25095609\n",
            "Iteration 110, loss = 0.25095388\n",
            "Iteration 111, loss = 0.25094346\n",
            "Iteration 112, loss = 0.25094128\n",
            "Iteration 113, loss = 0.25094680\n",
            "Iteration 114, loss = 0.25091897\n",
            "Iteration 115, loss = 0.25092324\n",
            "Iteration 116, loss = 0.25089033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 117, loss = 0.25080095\n",
            "Iteration 118, loss = 0.25079703\n",
            "Iteration 119, loss = 0.25080056\n",
            "Iteration 120, loss = 0.25080141\n",
            "Iteration 121, loss = 0.25080339\n",
            "Iteration 122, loss = 0.25078771\n",
            "Iteration 123, loss = 0.25079073\n",
            "Iteration 124, loss = 0.25079287\n",
            "Iteration 125, loss = 0.25079219\n",
            "Iteration 126, loss = 0.25078711\n",
            "Iteration 127, loss = 0.25078189\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66660286\n",
            "Iteration 3, loss = 0.62325090\n",
            "Iteration 4, loss = 0.60163137\n",
            "Iteration 5, loss = 0.58885695\n",
            "Iteration 6, loss = 0.58222534\n",
            "Iteration 7, loss = 0.57751702\n",
            "Iteration 8, loss = 0.57414434\n",
            "Iteration 9, loss = 0.57231217\n",
            "Iteration 10, loss = 0.57180541\n",
            "Iteration 11, loss = 0.57100808\n",
            "Iteration 12, loss = 0.57226736\n",
            "Iteration 13, loss = 0.57257617\n",
            "Iteration 14, loss = 0.57128181\n",
            "Iteration 15, loss = 0.57079728\n",
            "Iteration 16, loss = 0.57259683\n",
            "Iteration 17, loss = 0.57260764\n",
            "Iteration 18, loss = 0.57194444\n",
            "Iteration 19, loss = 0.57271406\n",
            "Iteration 20, loss = 0.57284822\n",
            "Iteration 21, loss = 0.57388185\n",
            "Iteration 22, loss = 0.57279261\n",
            "Iteration 23, loss = 0.57271120\n",
            "Iteration 24, loss = 0.57223467\n",
            "Iteration 25, loss = 0.57001693\n",
            "Iteration 26, loss = 0.57062864\n",
            "Iteration 27, loss = 0.56841250\n",
            "Iteration 28, loss = 0.57039349\n",
            "Iteration 29, loss = 0.56917779\n",
            "Iteration 30, loss = 0.56896294\n",
            "Iteration 31, loss = 0.56844790\n",
            "Iteration 32, loss = 0.57195835\n",
            "Iteration 33, loss = 0.57132376\n",
            "Iteration 34, loss = 0.56836763\n",
            "Iteration 35, loss = 0.56974961\n",
            "Iteration 36, loss = 0.57098294\n",
            "Iteration 37, loss = 0.57042102\n",
            "Iteration 38, loss = 0.57053357\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 39, loss = 0.51384787\n",
            "Iteration 40, loss = 0.49460508\n",
            "Iteration 41, loss = 0.51480893\n",
            "Iteration 42, loss = 0.51682516\n",
            "Iteration 43, loss = 0.51980572\n",
            "Iteration 44, loss = 0.51900893\n",
            "Iteration 45, loss = 0.51846118\n",
            "Iteration 46, loss = 0.51841286\n",
            "Iteration 47, loss = 0.52046428\n",
            "Iteration 48, loss = 0.51687090\n",
            "Iteration 49, loss = 0.51664092\n",
            "Iteration 50, loss = 0.51732074\n",
            "Iteration 51, loss = 0.51941457\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.43133628\n",
            "Iteration 53, loss = 0.41858266\n",
            "Iteration 54, loss = 0.40686802\n",
            "Iteration 55, loss = 0.39627721\n",
            "Iteration 56, loss = 0.38642648\n",
            "Iteration 57, loss = 0.37766018\n",
            "Iteration 58, loss = 0.36983154\n",
            "Iteration 59, loss = 0.36278253\n",
            "Iteration 60, loss = 0.35634509\n",
            "Iteration 61, loss = 0.35043425\n",
            "Iteration 62, loss = 0.34591881\n",
            "Iteration 63, loss = 0.34162448\n",
            "Iteration 64, loss = 0.33810172\n",
            "Iteration 65, loss = 0.33452853\n",
            "Iteration 66, loss = 0.33042129\n",
            "Iteration 67, loss = 0.32820645\n",
            "Iteration 68, loss = 0.32649932\n",
            "Iteration 69, loss = 0.32433202\n",
            "Iteration 70, loss = 0.32284104\n",
            "Iteration 71, loss = 0.31973902\n",
            "Iteration 72, loss = 0.32013705\n",
            "Iteration 73, loss = 0.31698104\n",
            "Iteration 74, loss = 0.29868243\n",
            "Iteration 75, loss = 0.29547606\n",
            "Iteration 76, loss = 0.29249111\n",
            "Iteration 77, loss = 0.29014165\n",
            "Iteration 78, loss = 0.28817065\n",
            "Iteration 79, loss = 0.28689991\n",
            "Iteration 80, loss = 0.28439997\n",
            "Iteration 81, loss = 0.28304009\n",
            "Iteration 82, loss = 0.28283703\n",
            "Iteration 83, loss = 0.28032717\n",
            "Iteration 84, loss = 0.28170617\n",
            "Iteration 85, loss = 0.27945765\n",
            "Iteration 86, loss = 0.27940125\n",
            "Iteration 87, loss = 0.27748643\n",
            "Iteration 88, loss = 0.27840854\n",
            "Iteration 89, loss = 0.27973386\n",
            "Iteration 90, loss = 0.28070777\n",
            "Iteration 91, loss = 0.27743519\n",
            "Iteration 92, loss = 0.27848469\n",
            "Iteration 93, loss = 0.28143102\n",
            "Iteration 94, loss = 0.27487402\n",
            "Iteration 95, loss = 0.27823349\n",
            "Iteration 96, loss = 0.27779000\n",
            "Iteration 97, loss = 0.27463828\n",
            "Iteration 98, loss = 0.28115103\n",
            "Iteration 99, loss = 0.28191977\n",
            "Iteration 100, loss = 0.28231000\n",
            "Iteration 101, loss = 0.27384122\n",
            "Iteration 102, loss = 0.28431133\n",
            "Iteration 103, loss = 0.28884151\n",
            "Iteration 104, loss = 0.29117974\n",
            "Iteration 105, loss = 0.28241383\n",
            "Iteration 106, loss = 0.28315809\n",
            "Iteration 107, loss = 0.28034697\n",
            "Iteration 108, loss = 0.29413155\n",
            "Iteration 109, loss = 0.29819642\n",
            "Iteration 110, loss = 0.28313764\n",
            "Iteration 111, loss = 0.28282163\n",
            "Iteration 112, loss = 0.30248802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 113, loss = 0.25673231\n",
            "Iteration 114, loss = 0.25660131\n",
            "Iteration 115, loss = 0.25652067\n",
            "Iteration 116, loss = 0.25642614\n",
            "Iteration 117, loss = 0.25636662\n",
            "Iteration 118, loss = 0.25634075\n",
            "Iteration 119, loss = 0.25627865\n",
            "Iteration 120, loss = 0.25618522\n",
            "Iteration 121, loss = 0.25615426\n",
            "Iteration 122, loss = 0.25608373\n",
            "Iteration 123, loss = 0.25606381\n",
            "Iteration 124, loss = 0.25596988\n",
            "Iteration 125, loss = 0.25595827\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 126, loss = 0.25562734\n",
            "Iteration 127, loss = 0.25561858\n",
            "Iteration 128, loss = 0.25559279\n",
            "Iteration 129, loss = 0.25558226\n",
            "Iteration 130, loss = 0.25559305\n",
            "Iteration 131, loss = 0.25556703\n",
            "Iteration 132, loss = 0.25556051\n",
            "Iteration 133, loss = 0.25557141\n",
            "Iteration 134, loss = 0.25553407\n",
            "Iteration 135, loss = 0.25552105\n",
            "Iteration 136, loss = 0.25552181\n",
            "Iteration 137, loss = 0.25551241\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 138, loss = 0.25541787\n",
            "Iteration 139, loss = 0.25541509\n",
            "Iteration 140, loss = 0.25540312\n",
            "Iteration 141, loss = 0.25540975\n",
            "Iteration 142, loss = 0.25540754\n",
            "Iteration 143, loss = 0.25538883\n",
            "Iteration 144, loss = 0.25539764\n",
            "Iteration 145, loss = 0.25539214\n",
            "Iteration 146, loss = 0.25539989\n",
            "Iteration 147, loss = 0.25539374\n",
            "Iteration 148, loss = 0.25538474\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.89684059\n",
            "Iteration 2, loss = 0.63603383\n",
            "Iteration 3, loss = 0.60745976\n",
            "Iteration 4, loss = 0.58893278\n",
            "Iteration 5, loss = 0.57956201\n",
            "Iteration 6, loss = 0.57251978\n",
            "Iteration 7, loss = 0.57191801\n",
            "Iteration 8, loss = 0.56679692\n",
            "Iteration 9, loss = 0.56561057\n",
            "Iteration 10, loss = 0.56489074\n",
            "Iteration 11, loss = 0.56438126\n",
            "Iteration 12, loss = 0.56374277\n",
            "Iteration 13, loss = 0.56492901\n",
            "Iteration 14, loss = 0.56593766\n",
            "Iteration 15, loss = 0.56660917\n",
            "Iteration 16, loss = 0.56646244\n",
            "Iteration 17, loss = 0.56636673\n",
            "Iteration 18, loss = 0.56659500\n",
            "Iteration 19, loss = 0.56579058\n",
            "Iteration 20, loss = 0.56735152\n",
            "Iteration 21, loss = 0.56670705\n",
            "Iteration 22, loss = 0.56592898\n",
            "Iteration 23, loss = 0.56519224\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.50068504\n",
            "Iteration 25, loss = 0.47671830\n",
            "Iteration 26, loss = 0.50647303\n",
            "Iteration 27, loss = 0.50850645\n",
            "Iteration 28, loss = 0.50876991\n",
            "Iteration 29, loss = 0.51019520\n",
            "Iteration 30, loss = 0.50551415\n",
            "Iteration 31, loss = 0.50891475\n",
            "Iteration 32, loss = 0.50599379\n",
            "Iteration 33, loss = 0.51089309\n",
            "Iteration 34, loss = 0.50597141\n",
            "Iteration 35, loss = 0.50461324\n",
            "Iteration 36, loss = 0.50775548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.40657811\n",
            "Iteration 38, loss = 0.39244914\n",
            "Iteration 39, loss = 0.37953995\n",
            "Iteration 40, loss = 0.36802694\n",
            "Iteration 41, loss = 0.35749192\n",
            "Iteration 42, loss = 0.34802127\n",
            "Iteration 43, loss = 0.33955583\n",
            "Iteration 44, loss = 0.33172856\n",
            "Iteration 45, loss = 0.32478663\n",
            "Iteration 46, loss = 0.31893794\n",
            "Iteration 47, loss = 0.31302175\n",
            "Iteration 48, loss = 0.30802376\n",
            "Iteration 49, loss = 0.30337209\n",
            "Iteration 50, loss = 0.29936233\n",
            "Iteration 51, loss = 0.29602799\n",
            "Iteration 52, loss = 0.29249839\n",
            "Iteration 53, loss = 0.28930114\n",
            "Iteration 54, loss = 0.28668587\n",
            "Iteration 55, loss = 0.28437562\n",
            "Iteration 56, loss = 0.28183832\n",
            "Iteration 57, loss = 0.27951376\n",
            "Iteration 58, loss = 0.27814724\n",
            "Iteration 59, loss = 0.27608710\n",
            "Iteration 60, loss = 0.27481899\n",
            "Iteration 61, loss = 0.27314111\n",
            "Iteration 62, loss = 0.27196977\n",
            "Iteration 63, loss = 0.27022094\n",
            "Iteration 64, loss = 0.27042870\n",
            "Iteration 65, loss = 0.26861218\n",
            "Iteration 66, loss = 0.26808339\n",
            "Iteration 67, loss = 0.26774555\n",
            "Iteration 68, loss = 0.26568331\n",
            "Iteration 69, loss = 0.26569386\n",
            "Iteration 70, loss = 0.26587126\n",
            "Iteration 71, loss = 0.26549030\n",
            "Iteration 72, loss = 0.26363604\n",
            "Iteration 73, loss = 0.26421598\n",
            "Iteration 74, loss = 0.26259525\n",
            "Iteration 75, loss = 0.26485524\n",
            "Iteration 76, loss = 0.26247860\n",
            "Iteration 77, loss = 0.26238287\n",
            "Iteration 78, loss = 0.26320948\n",
            "Iteration 79, loss = 0.26155902\n",
            "Iteration 80, loss = 0.26243343\n",
            "Iteration 81, loss = 0.26382109\n",
            "Iteration 82, loss = 0.26151650\n",
            "Iteration 83, loss = 0.26079198\n",
            "Iteration 84, loss = 0.26251750\n",
            "Iteration 85, loss = 0.26207745\n",
            "Iteration 86, loss = 0.26265509\n",
            "Iteration 87, loss = 0.26654046\n",
            "Iteration 88, loss = 0.26255433\n",
            "Iteration 89, loss = 0.26127993\n",
            "Iteration 90, loss = 0.26473738\n",
            "Iteration 91, loss = 0.26769999\n",
            "Iteration 92, loss = 0.26390104\n",
            "Iteration 93, loss = 0.27296945\n",
            "Iteration 94, loss = 0.26488197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 95, loss = 0.25078874\n",
            "Iteration 96, loss = 0.25072394\n",
            "Iteration 97, loss = 0.25065868\n",
            "Iteration 98, loss = 0.25063564\n",
            "Iteration 99, loss = 0.25059537\n",
            "Iteration 100, loss = 0.25057828\n",
            "Iteration 101, loss = 0.25052618\n",
            "Iteration 102, loss = 0.25044274\n",
            "Iteration 103, loss = 0.25043998\n",
            "Iteration 104, loss = 0.25037637\n",
            "Iteration 105, loss = 0.25030382\n",
            "Iteration 106, loss = 0.25035083\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 107, loss = 0.25003552\n",
            "Iteration 108, loss = 0.25002253\n",
            "Iteration 109, loss = 0.25001645\n",
            "Iteration 110, loss = 0.25002738\n",
            "Iteration 111, loss = 0.25002156\n",
            "Iteration 112, loss = 0.24998700\n",
            "Iteration 113, loss = 0.24995559\n",
            "Iteration 114, loss = 0.24998502\n",
            "Iteration 115, loss = 0.24998196\n",
            "Iteration 116, loss = 0.24995760\n",
            "Iteration 117, loss = 0.24996038\n",
            "Iteration 118, loss = 0.24996289\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 119, loss = 0.24987223\n",
            "Iteration 120, loss = 0.24986303\n",
            "Iteration 121, loss = 0.24986574\n",
            "Iteration 122, loss = 0.24986347\n",
            "Iteration 123, loss = 0.24986978\n",
            "Iteration 124, loss = 0.24985645\n",
            "Iteration 125, loss = 0.24986098\n",
            "Iteration 126, loss = 0.24985498\n",
            "Iteration 127, loss = 0.24986738\n",
            "Iteration 128, loss = 0.24986420\n",
            "Iteration 129, loss = 0.24985275\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63444349\n",
            "Iteration 3, loss = 0.60353016\n",
            "Iteration 4, loss = 0.58652702\n",
            "Iteration 5, loss = 0.57742303\n",
            "Iteration 6, loss = 0.57123421\n",
            "Iteration 7, loss = 0.56782852\n",
            "Iteration 8, loss = 0.56600858\n",
            "Iteration 9, loss = 0.56486408\n",
            "Iteration 10, loss = 0.56437682\n",
            "Iteration 11, loss = 0.56383655\n",
            "Iteration 12, loss = 0.56363524\n",
            "Iteration 13, loss = 0.56376864\n",
            "Iteration 14, loss = 0.56421577\n",
            "Iteration 15, loss = 0.56390990\n",
            "Iteration 16, loss = 0.57138036\n",
            "Iteration 17, loss = 0.56865329\n",
            "Iteration 18, loss = 0.56723124\n",
            "Iteration 19, loss = 0.56630187\n",
            "Iteration 20, loss = 0.56799363\n",
            "Iteration 21, loss = 0.60233586\n",
            "Iteration 22, loss = 0.59848844\n",
            "Iteration 23, loss = 0.57841361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.55151551\n",
            "Iteration 25, loss = 0.54133390\n",
            "Iteration 26, loss = 0.52986296\n",
            "Iteration 27, loss = 0.51970356\n",
            "Iteration 28, loss = 0.52420511\n",
            "Iteration 29, loss = 0.52713366\n",
            "Iteration 30, loss = 0.52598204\n",
            "Iteration 31, loss = 0.52201792\n",
            "Iteration 32, loss = 0.52177420\n",
            "Iteration 33, loss = 0.52086759\n",
            "Iteration 34, loss = 0.51226539\n",
            "Iteration 35, loss = 0.51750911\n",
            "Iteration 36, loss = 0.51743387\n",
            "Iteration 37, loss = 0.51213451\n",
            "Iteration 38, loss = 0.51669520\n",
            "Iteration 39, loss = 0.51124082\n",
            "Iteration 40, loss = 0.50786627\n",
            "Iteration 41, loss = 0.51064828\n",
            "Iteration 42, loss = 0.50928051\n",
            "Iteration 43, loss = 0.51327922\n",
            "Iteration 44, loss = 0.50990719\n",
            "Iteration 45, loss = 0.51244134\n",
            "Iteration 46, loss = 0.51299397\n",
            "Iteration 47, loss = 0.50912910\n",
            "Iteration 48, loss = 0.51467087\n",
            "Iteration 49, loss = 0.51045207\n",
            "Iteration 50, loss = 0.51167401\n",
            "Iteration 51, loss = 0.51191985\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 52, loss = 0.40489444\n",
            "Iteration 53, loss = 0.38985261\n",
            "Iteration 54, loss = 0.37649078\n",
            "Iteration 55, loss = 0.36464256\n",
            "Iteration 56, loss = 0.35394069\n",
            "Iteration 57, loss = 0.34452511\n",
            "Iteration 58, loss = 0.33615037\n",
            "Iteration 59, loss = 0.32857550\n",
            "Iteration 60, loss = 0.32205549\n",
            "Iteration 61, loss = 0.31583843\n",
            "Iteration 62, loss = 0.31051673\n",
            "Iteration 63, loss = 0.30599720\n",
            "Iteration 64, loss = 0.30186600\n",
            "Iteration 65, loss = 0.29740768\n",
            "Iteration 66, loss = 0.29420492\n",
            "Iteration 67, loss = 0.29129809\n",
            "Iteration 68, loss = 0.28795687\n",
            "Iteration 69, loss = 0.28543363\n",
            "Iteration 70, loss = 0.28321840\n",
            "Iteration 71, loss = 0.28096041\n",
            "Iteration 72, loss = 0.27936113\n",
            "Iteration 73, loss = 0.27720070\n",
            "Iteration 74, loss = 0.27570094\n",
            "Iteration 75, loss = 0.27443008\n",
            "Iteration 76, loss = 0.27304873\n",
            "Iteration 77, loss = 0.27140438\n",
            "Iteration 78, loss = 0.27075340\n",
            "Iteration 79, loss = 0.27004051\n",
            "Iteration 80, loss = 0.26821121\n",
            "Iteration 81, loss = 0.26771769\n",
            "Iteration 82, loss = 0.26825775\n",
            "Iteration 83, loss = 0.26614165\n",
            "Iteration 84, loss = 0.26616674\n",
            "Iteration 85, loss = 0.26517419\n",
            "Iteration 86, loss = 0.26532276\n",
            "Iteration 87, loss = 0.26382688\n",
            "Iteration 88, loss = 0.26395441\n",
            "Iteration 89, loss = 0.26260207\n",
            "Iteration 90, loss = 0.26278597\n",
            "Iteration 91, loss = 0.26233744\n",
            "Iteration 92, loss = 0.26163825\n",
            "Iteration 93, loss = 0.26186878\n",
            "Iteration 94, loss = 0.26076263\n",
            "Iteration 95, loss = 0.26244007\n",
            "Iteration 96, loss = 0.26141345\n",
            "Iteration 97, loss = 0.26033204\n",
            "Iteration 98, loss = 0.26273986\n",
            "Iteration 99, loss = 0.26079983\n",
            "Iteration 100, loss = 0.26154697\n",
            "Iteration 101, loss = 0.26144636\n",
            "Iteration 102, loss = 0.26454311\n",
            "Iteration 103, loss = 0.26110043\n",
            "Iteration 104, loss = 0.26252856\n",
            "Iteration 105, loss = 0.26043967\n",
            "Iteration 106, loss = 0.26041933\n",
            "Iteration 107, loss = 0.26104859\n",
            "Iteration 108, loss = 0.26700681\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 109, loss = 0.25212837\n",
            "Iteration 110, loss = 0.25207395\n",
            "Iteration 111, loss = 0.25198030\n",
            "Iteration 112, loss = 0.25199381\n",
            "Iteration 113, loss = 0.25199371\n",
            "Iteration 114, loss = 0.25192310\n",
            "Iteration 115, loss = 0.25183668\n",
            "Iteration 116, loss = 0.25183293\n",
            "Iteration 117, loss = 0.25181878\n",
            "Iteration 118, loss = 0.25178729\n",
            "Iteration 119, loss = 0.25172218\n",
            "Iteration 120, loss = 0.25170553\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 121, loss = 0.25140696\n",
            "Iteration 122, loss = 0.25137332\n",
            "Iteration 123, loss = 0.25137108\n",
            "Iteration 124, loss = 0.25136831\n",
            "Iteration 125, loss = 0.25137228\n",
            "Iteration 126, loss = 0.25134245\n",
            "Iteration 127, loss = 0.25135898\n",
            "Iteration 128, loss = 0.25134058\n",
            "Iteration 129, loss = 0.25132726\n",
            "Iteration 130, loss = 0.25131495\n",
            "Iteration 131, loss = 0.25133118\n",
            "Iteration 132, loss = 0.25133141\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 133, loss = 0.25122484\n",
            "Iteration 134, loss = 0.25121795\n",
            "Iteration 135, loss = 0.25122373\n",
            "Iteration 136, loss = 0.25122192\n",
            "Iteration 137, loss = 0.25122138\n",
            "Iteration 138, loss = 0.25121058\n",
            "Iteration 139, loss = 0.25121821\n",
            "Iteration 140, loss = 0.25121385\n",
            "Iteration 141, loss = 0.25121591\n",
            "Iteration 142, loss = 0.25120296\n",
            "Iteration 143, loss = 0.25121564\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.65959572\n",
            "Iteration 3, loss = 0.62320343\n",
            "Iteration 4, loss = 0.60063299\n",
            "Iteration 5, loss = 0.58755668\n",
            "Iteration 6, loss = 0.57982068\n",
            "Iteration 7, loss = 0.57500639\n",
            "Iteration 8, loss = 0.61414325\n",
            "Iteration 9, loss = 0.59567306\n",
            "Iteration 10, loss = 0.58258978\n",
            "Iteration 11, loss = 0.57737104\n",
            "Iteration 12, loss = 0.57313266\n",
            "Iteration 13, loss = 0.57188558\n",
            "Iteration 14, loss = 0.57078329\n",
            "Iteration 15, loss = 0.57143652\n",
            "Iteration 16, loss = 0.57108950\n",
            "Iteration 17, loss = 0.56960522\n",
            "Iteration 18, loss = 0.56749460\n",
            "Iteration 19, loss = 0.56921033\n",
            "Iteration 20, loss = 0.56806734\n",
            "Iteration 21, loss = 0.56817821\n",
            "Iteration 22, loss = 0.56782120\n",
            "Iteration 23, loss = 0.56806167\n",
            "Iteration 24, loss = 0.56975572\n",
            "Iteration 25, loss = 0.56851194\n",
            "Iteration 26, loss = 0.56966928\n",
            "Iteration 27, loss = 0.56853808\n",
            "Iteration 28, loss = 0.56929990\n",
            "Iteration 29, loss = 0.57040157\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 30, loss = 0.51697623\n",
            "Iteration 31, loss = 0.49608335\n",
            "Iteration 32, loss = 0.51383582\n",
            "Iteration 33, loss = 0.51959404\n",
            "Iteration 34, loss = 0.51830665\n",
            "Iteration 35, loss = 0.52029464\n",
            "Iteration 36, loss = 0.51955837\n",
            "Iteration 37, loss = 0.51994764\n",
            "Iteration 38, loss = 0.51821551\n",
            "Iteration 39, loss = 0.51729746\n",
            "Iteration 40, loss = 0.51736307\n",
            "Iteration 41, loss = 0.52166613\n",
            "Iteration 42, loss = 0.51869221\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 43, loss = 0.43336856\n",
            "Iteration 44, loss = 0.42073457\n",
            "Iteration 45, loss = 0.40922736\n",
            "Iteration 46, loss = 0.39885707\n",
            "Iteration 47, loss = 0.38921450\n",
            "Iteration 48, loss = 0.38065589\n",
            "Iteration 49, loss = 0.37279975\n",
            "Iteration 50, loss = 0.36566683\n",
            "Iteration 51, loss = 0.35932194\n",
            "Iteration 52, loss = 0.35406370\n",
            "Iteration 53, loss = 0.34933053\n",
            "Iteration 54, loss = 0.34493784\n",
            "Iteration 55, loss = 0.34077895\n",
            "Iteration 56, loss = 0.33785847\n",
            "Iteration 57, loss = 0.33425918\n",
            "Iteration 58, loss = 0.33228428\n",
            "Iteration 59, loss = 0.33024128\n",
            "Iteration 60, loss = 0.32820467\n",
            "Iteration 61, loss = 0.32573777\n",
            "Iteration 62, loss = 0.32414802\n",
            "Iteration 63, loss = 0.32405207\n",
            "Iteration 64, loss = 0.32384727\n",
            "Iteration 65, loss = 0.32057803\n",
            "Iteration 66, loss = 0.32200910\n",
            "Iteration 67, loss = 0.31996838\n",
            "Iteration 68, loss = 0.32059831\n",
            "Iteration 69, loss = 0.31702233\n",
            "Iteration 70, loss = 0.32153060\n",
            "Iteration 71, loss = 0.31835293\n",
            "Iteration 72, loss = 0.32145651\n",
            "Iteration 73, loss = 0.31906046\n",
            "Iteration 74, loss = 0.32046885\n",
            "Iteration 75, loss = 0.32309983\n",
            "Iteration 76, loss = 0.32168812\n",
            "Iteration 77, loss = 0.31808567\n",
            "Iteration 78, loss = 0.31584356\n",
            "Iteration 79, loss = 0.32615436\n",
            "Iteration 80, loss = 0.32358039\n",
            "Iteration 81, loss = 0.32126912\n",
            "Iteration 82, loss = 0.31850601\n",
            "Iteration 83, loss = 0.31815098\n",
            "Iteration 84, loss = 0.31933838\n",
            "Iteration 85, loss = 0.32686172\n",
            "Iteration 86, loss = 0.32455764\n",
            "Iteration 87, loss = 0.32013792\n",
            "Iteration 88, loss = 0.32066895\n",
            "Iteration 89, loss = 0.32065647\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 90, loss = 0.28787490\n",
            "Iteration 91, loss = 0.28717386\n",
            "Iteration 92, loss = 0.28694120\n",
            "Iteration 93, loss = 0.28677590\n",
            "Iteration 94, loss = 0.28662003\n",
            "Iteration 95, loss = 0.28651301\n",
            "Iteration 96, loss = 0.28633586\n",
            "Iteration 97, loss = 0.28621925\n",
            "Iteration 98, loss = 0.28606711\n",
            "Iteration 99, loss = 0.28592941\n",
            "Iteration 100, loss = 0.28583946\n",
            "Iteration 101, loss = 0.28570322\n",
            "Iteration 102, loss = 0.28553606\n",
            "Iteration 103, loss = 0.28548579\n",
            "Iteration 104, loss = 0.28530439\n",
            "Iteration 105, loss = 0.28517503\n",
            "Iteration 106, loss = 0.28503772\n",
            "Iteration 107, loss = 0.28494988\n",
            "Iteration 108, loss = 0.28479057\n",
            "Iteration 109, loss = 0.28464121\n",
            "Iteration 110, loss = 0.28451924\n",
            "Iteration 111, loss = 0.28441197\n",
            "Iteration 112, loss = 0.28435399\n",
            "Iteration 113, loss = 0.28421333\n",
            "Iteration 114, loss = 0.28401242\n",
            "Iteration 115, loss = 0.28396287\n",
            "Iteration 116, loss = 0.28389913\n",
            "Iteration 117, loss = 0.28378441\n",
            "Iteration 118, loss = 0.28358238\n",
            "Iteration 119, loss = 0.28350176\n",
            "Iteration 120, loss = 0.28339765\n",
            "Iteration 121, loss = 0.28328367\n",
            "Iteration 122, loss = 0.28317002\n",
            "Iteration 123, loss = 0.28306152\n",
            "Iteration 124, loss = 0.28296793\n",
            "Iteration 125, loss = 0.28283022\n",
            "Iteration 126, loss = 0.28271853\n",
            "Iteration 127, loss = 0.28265201\n",
            "Iteration 128, loss = 0.28254368\n",
            "Iteration 129, loss = 0.28235521\n",
            "Iteration 130, loss = 0.28230591\n",
            "Iteration 131, loss = 0.28211809\n",
            "Iteration 132, loss = 0.28211744\n",
            "Iteration 133, loss = 0.28198441\n",
            "Iteration 134, loss = 0.28190124\n",
            "Iteration 135, loss = 0.28173620\n",
            "Iteration 136, loss = 0.28173184\n",
            "Iteration 137, loss = 0.28152749\n",
            "Iteration 138, loss = 0.28148884\n",
            "Iteration 139, loss = 0.28138688\n",
            "Iteration 140, loss = 0.28130545\n",
            "Iteration 141, loss = 0.28116053\n",
            "Iteration 142, loss = 0.28105571\n",
            "Iteration 143, loss = 0.28095694\n",
            "Iteration 144, loss = 0.28086148\n",
            "Iteration 145, loss = 0.28075865\n",
            "Iteration 146, loss = 0.28069719\n",
            "Iteration 147, loss = 0.28059794\n",
            "Iteration 148, loss = 0.28055403\n",
            "Iteration 149, loss = 0.28042986\n",
            "Iteration 150, loss = 0.28028154\n",
            "Iteration 151, loss = 0.28020434\n",
            "Iteration 152, loss = 0.28008900\n",
            "Iteration 153, loss = 0.28008851\n",
            "Iteration 154, loss = 0.27999534\n",
            "Iteration 155, loss = 0.27987194\n",
            "Iteration 156, loss = 0.27977769\n",
            "Iteration 157, loss = 0.27975769\n",
            "Iteration 158, loss = 0.27964225\n",
            "Iteration 159, loss = 0.27948979\n",
            "Iteration 160, loss = 0.27934880\n",
            "Iteration 161, loss = 0.27936141\n",
            "Iteration 162, loss = 0.27929031\n",
            "Iteration 163, loss = 0.27914806\n",
            "Iteration 164, loss = 0.27911564\n",
            "Iteration 165, loss = 0.27902583\n",
            "Iteration 166, loss = 0.27900889\n",
            "Iteration 167, loss = 0.27882703\n",
            "Iteration 168, loss = 0.27873706\n",
            "Iteration 169, loss = 0.27862778\n",
            "Iteration 170, loss = 0.27857236\n",
            "Iteration 171, loss = 0.27847540\n",
            "Iteration 172, loss = 0.27840114\n",
            "Iteration 173, loss = 0.27829318\n",
            "Iteration 174, loss = 0.27819985\n",
            "Iteration 175, loss = 0.27821611\n",
            "Iteration 176, loss = 0.27810340\n",
            "Iteration 177, loss = 0.27796106\n",
            "Iteration 178, loss = 0.27800475\n",
            "Iteration 179, loss = 0.27788944\n",
            "Iteration 180, loss = 0.27780545\n",
            "Iteration 181, loss = 0.27776076\n",
            "Iteration 182, loss = 0.27763650\n",
            "Iteration 183, loss = 0.27760088\n",
            "Iteration 184, loss = 0.27747540\n",
            "Iteration 185, loss = 0.27741892\n",
            "Iteration 186, loss = 0.27740325\n",
            "Iteration 187, loss = 0.27725559\n",
            "Iteration 188, loss = 0.27719770\n",
            "Iteration 189, loss = 0.27711509\n",
            "Iteration 190, loss = 0.27704469\n",
            "Iteration 191, loss = 0.27695422\n",
            "Iteration 192, loss = 0.27693128\n",
            "Iteration 193, loss = 0.27678304\n",
            "Iteration 194, loss = 0.27667925\n",
            "Iteration 195, loss = 0.27665037\n",
            "Iteration 196, loss = 0.27661090\n",
            "Iteration 197, loss = 0.27655185\n",
            "Iteration 198, loss = 0.27649652\n",
            "Iteration 199, loss = 0.27644781\n",
            "Iteration 200, loss = 0.27633095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63789279\n",
            "Iteration 3, loss = 0.61408433\n",
            "Iteration 4, loss = 0.59641397\n",
            "Iteration 5, loss = 0.58477042\n",
            "Iteration 6, loss = 0.57753149\n",
            "Iteration 7, loss = 0.57245104\n",
            "Iteration 8, loss = 0.57070811\n",
            "Iteration 9, loss = 0.56880500\n",
            "Iteration 10, loss = 0.56818233\n",
            "Iteration 11, loss = 0.56919398\n",
            "Iteration 12, loss = 0.56828231\n",
            "Iteration 13, loss = 0.57650374\n",
            "Iteration 14, loss = 0.60851346\n",
            "Iteration 15, loss = 0.58422712\n",
            "Iteration 16, loss = 0.57406632\n",
            "Iteration 17, loss = 0.57207136\n",
            "Iteration 18, loss = 0.56814877\n",
            "Iteration 19, loss = 0.56775107\n",
            "Iteration 20, loss = 0.56621272\n",
            "Iteration 21, loss = 0.56621758\n",
            "Iteration 22, loss = 0.56767563\n",
            "Iteration 23, loss = 0.56687526\n",
            "Iteration 24, loss = 0.56922403\n",
            "Iteration 25, loss = 0.58299714\n",
            "Iteration 26, loss = 0.60592829\n",
            "Iteration 27, loss = 0.59132387\n",
            "Iteration 28, loss = 0.58097630\n",
            "Iteration 29, loss = 0.57324976\n",
            "Iteration 30, loss = 0.57015038\n",
            "Iteration 31, loss = 0.56813474\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.53636697\n",
            "Iteration 33, loss = 0.52264511\n",
            "Iteration 34, loss = 0.51447073\n",
            "Iteration 35, loss = 0.51991784\n",
            "Iteration 36, loss = 0.52784102\n",
            "Iteration 37, loss = 0.52768993\n",
            "Iteration 38, loss = 0.52758278\n",
            "Iteration 39, loss = 0.52282369\n",
            "Iteration 40, loss = 0.52269716\n",
            "Iteration 41, loss = 0.52367788\n",
            "Iteration 42, loss = 0.52054966\n",
            "Iteration 43, loss = 0.52255667\n",
            "Iteration 44, loss = 0.52125292\n",
            "Iteration 45, loss = 0.51887807\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.44523399\n",
            "Iteration 47, loss = 0.43454708\n",
            "Iteration 48, loss = 0.42436299\n",
            "Iteration 49, loss = 0.41503897\n",
            "Iteration 50, loss = 0.40621572\n",
            "Iteration 51, loss = 0.39793401\n",
            "Iteration 52, loss = 0.39066888\n",
            "Iteration 53, loss = 0.38365823\n",
            "Iteration 54, loss = 0.37720691\n",
            "Iteration 55, loss = 0.37115581\n",
            "Iteration 56, loss = 0.36642780\n",
            "Iteration 57, loss = 0.36175202\n",
            "Iteration 58, loss = 0.35805748\n",
            "Iteration 59, loss = 0.35543838\n",
            "Iteration 60, loss = 0.35062759\n",
            "Iteration 61, loss = 0.34807072\n",
            "Iteration 62, loss = 0.34654228\n",
            "Iteration 63, loss = 0.34395215\n",
            "Iteration 64, loss = 0.34202200\n",
            "Iteration 65, loss = 0.34157985\n",
            "Iteration 66, loss = 0.33993322\n",
            "Iteration 67, loss = 0.32413250\n",
            "Iteration 68, loss = 0.30929287\n",
            "Iteration 69, loss = 0.30514000\n",
            "Iteration 70, loss = 0.30420483\n",
            "Iteration 71, loss = 0.30180139\n",
            "Iteration 72, loss = 0.29704543\n",
            "Iteration 73, loss = 0.29661882\n",
            "Iteration 74, loss = 0.29398210\n",
            "Iteration 75, loss = 0.29308873\n",
            "Iteration 76, loss = 0.28965619\n",
            "Iteration 77, loss = 0.29308694\n",
            "Iteration 78, loss = 0.28928663\n",
            "Iteration 79, loss = 0.28942791\n",
            "Iteration 80, loss = 0.29346199\n",
            "Iteration 81, loss = 0.28775998\n",
            "Iteration 82, loss = 0.28578401\n",
            "Iteration 83, loss = 0.29002318\n",
            "Iteration 84, loss = 0.28893238\n",
            "Iteration 85, loss = 0.29199391\n",
            "Iteration 86, loss = 0.28297224\n",
            "Iteration 87, loss = 0.29299141\n",
            "Iteration 88, loss = 0.29016535\n",
            "Iteration 89, loss = 0.29266027\n",
            "Iteration 90, loss = 0.28827528\n",
            "Iteration 91, loss = 0.29175392\n",
            "Iteration 92, loss = 0.29391454\n",
            "Iteration 93, loss = 0.29395212\n",
            "Iteration 94, loss = 0.28895893\n",
            "Iteration 95, loss = 0.29967578\n",
            "Iteration 96, loss = 0.28695276\n",
            "Iteration 97, loss = 0.29122728\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.26041763\n",
            "Iteration 99, loss = 0.25926580\n",
            "Iteration 100, loss = 0.25906518\n",
            "Iteration 101, loss = 0.25899631\n",
            "Iteration 102, loss = 0.25890048\n",
            "Iteration 103, loss = 0.25874902\n",
            "Iteration 104, loss = 0.25872734\n",
            "Iteration 105, loss = 0.25854828\n",
            "Iteration 106, loss = 0.25842453\n",
            "Iteration 107, loss = 0.25840731\n",
            "Iteration 108, loss = 0.25830523\n",
            "Iteration 109, loss = 0.25822828\n",
            "Iteration 110, loss = 0.25809940\n",
            "Iteration 111, loss = 0.25798080\n",
            "Iteration 112, loss = 0.25789210\n",
            "Iteration 113, loss = 0.25776180\n",
            "Iteration 114, loss = 0.25774069\n",
            "Iteration 115, loss = 0.25766253\n",
            "Iteration 116, loss = 0.25753221\n",
            "Iteration 117, loss = 0.25744471\n",
            "Iteration 118, loss = 0.25732829\n",
            "Iteration 119, loss = 0.25725800\n",
            "Iteration 120, loss = 0.25727460\n",
            "Iteration 121, loss = 0.25712619\n",
            "Iteration 122, loss = 0.25704573\n",
            "Iteration 123, loss = 0.25702400\n",
            "Iteration 124, loss = 0.25686720\n",
            "Iteration 125, loss = 0.25681555\n",
            "Iteration 126, loss = 0.25670581\n",
            "Iteration 127, loss = 0.25660908\n",
            "Iteration 128, loss = 0.25657174\n",
            "Iteration 129, loss = 0.25651193\n",
            "Iteration 130, loss = 0.25644202\n",
            "Iteration 131, loss = 0.25637977\n",
            "Iteration 132, loss = 0.25633463\n",
            "Iteration 133, loss = 0.25616522\n",
            "Iteration 134, loss = 0.25618344\n",
            "Iteration 135, loss = 0.25608577\n",
            "Iteration 136, loss = 0.25602449\n",
            "Iteration 137, loss = 0.25594853\n",
            "Iteration 138, loss = 0.25586882\n",
            "Iteration 139, loss = 0.25580180\n",
            "Iteration 140, loss = 0.25574839\n",
            "Iteration 141, loss = 0.25569804\n",
            "Iteration 142, loss = 0.25560962\n",
            "Iteration 143, loss = 0.25553127\n",
            "Iteration 144, loss = 0.25544032\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 145, loss = 0.25518466\n",
            "Iteration 146, loss = 0.25514008\n",
            "Iteration 147, loss = 0.25513808\n",
            "Iteration 148, loss = 0.25510079\n",
            "Iteration 149, loss = 0.25508065\n",
            "Iteration 150, loss = 0.25508572\n",
            "Iteration 151, loss = 0.25509813\n",
            "Iteration 152, loss = 0.25506780\n",
            "Iteration 153, loss = 0.25505919\n",
            "Iteration 154, loss = 0.25506203\n",
            "Iteration 155, loss = 0.25501861\n",
            "Iteration 156, loss = 0.25498668\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 157, loss = 0.25490945\n",
            "Iteration 158, loss = 0.25490604\n",
            "Iteration 159, loss = 0.25491289\n",
            "Iteration 160, loss = 0.25490043\n",
            "Iteration 161, loss = 0.25490240\n",
            "Iteration 162, loss = 0.25489682\n",
            "Iteration 163, loss = 0.25490243\n",
            "Iteration 164, loss = 0.25490190\n",
            "Iteration 165, loss = 0.25489148\n",
            "Iteration 166, loss = 0.25488781\n",
            "Iteration 167, loss = 0.25489088\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63301312\n",
            "Iteration 3, loss = 0.60509304\n",
            "Iteration 4, loss = 0.59534636\n",
            "Iteration 5, loss = 0.58303385\n",
            "Iteration 6, loss = 0.57566770\n",
            "Iteration 7, loss = 0.56998014\n",
            "Iteration 8, loss = 0.56732244\n",
            "Iteration 9, loss = 0.56481843\n",
            "Iteration 10, loss = 0.56550200\n",
            "Iteration 11, loss = 0.56377401\n",
            "Iteration 12, loss = 0.56438779\n",
            "Iteration 13, loss = 0.56336942\n",
            "Iteration 14, loss = 0.56423094\n",
            "Iteration 15, loss = 0.56354324\n",
            "Iteration 16, loss = 0.56327494\n",
            "Iteration 17, loss = 0.56388817\n",
            "Iteration 18, loss = 0.56369339\n",
            "Iteration 19, loss = 0.56450707\n",
            "Iteration 20, loss = 0.56317263\n",
            "Iteration 21, loss = 0.56329929\n",
            "Iteration 22, loss = 0.56328105\n",
            "Iteration 23, loss = 0.56387760\n",
            "Iteration 24, loss = 0.56477899\n",
            "Iteration 25, loss = 0.56251495\n",
            "Iteration 26, loss = 0.56310974\n",
            "Iteration 27, loss = 0.56252448\n",
            "Iteration 28, loss = 0.56222012\n",
            "Iteration 29, loss = 0.56286009\n",
            "Iteration 30, loss = 0.56304870\n",
            "Iteration 31, loss = 0.56191573\n",
            "Iteration 32, loss = 0.56244403\n",
            "Iteration 33, loss = 0.56128945\n",
            "Iteration 34, loss = 0.56461218\n",
            "Iteration 35, loss = 0.56117598\n",
            "Iteration 36, loss = 0.56170297\n",
            "Iteration 37, loss = 0.56117762\n",
            "Iteration 38, loss = 0.56191667\n",
            "Iteration 39, loss = 0.56048119\n",
            "Iteration 40, loss = 0.56041300\n",
            "Iteration 41, loss = 0.56086320\n",
            "Iteration 42, loss = 0.56014147\n",
            "Iteration 43, loss = 0.56078704\n",
            "Iteration 44, loss = 0.56039047\n",
            "Iteration 45, loss = 0.55933044\n",
            "Iteration 46, loss = 0.56105226\n",
            "Iteration 47, loss = 0.56012560\n",
            "Iteration 48, loss = 0.56112148\n",
            "Iteration 49, loss = 0.55948232\n",
            "Iteration 50, loss = 0.55858645\n",
            "Iteration 51, loss = 0.56080637\n",
            "Iteration 52, loss = 0.56559251\n",
            "Iteration 53, loss = 0.57070172\n",
            "Iteration 54, loss = 0.56502031\n",
            "Iteration 55, loss = 0.57850211\n",
            "Iteration 56, loss = 0.56616108\n",
            "Iteration 57, loss = 0.56198985\n",
            "Iteration 58, loss = 0.55848572\n",
            "Iteration 59, loss = 0.55932061\n",
            "Iteration 60, loss = 0.55788432\n",
            "Iteration 61, loss = 0.55663606\n",
            "Iteration 62, loss = 0.57483549\n",
            "Iteration 63, loss = 0.59470510\n",
            "Iteration 64, loss = 0.57038415\n",
            "Iteration 65, loss = 0.56099876\n",
            "Iteration 66, loss = 0.55771997\n",
            "Iteration 67, loss = 0.55505892\n",
            "Iteration 68, loss = 0.55558940\n",
            "Iteration 69, loss = 0.55523187\n",
            "Iteration 70, loss = 0.55497132\n",
            "Iteration 71, loss = 0.55372637\n",
            "Iteration 72, loss = 0.55386000\n",
            "Iteration 73, loss = 0.55313558\n",
            "Iteration 74, loss = 0.55484115\n",
            "Iteration 75, loss = 0.55482320\n",
            "Iteration 76, loss = 0.55350190\n",
            "Iteration 77, loss = 0.55592634\n",
            "Iteration 78, loss = 0.55359956\n",
            "Iteration 79, loss = 0.55372823\n",
            "Iteration 80, loss = 0.55462654\n",
            "Iteration 81, loss = 0.55631987\n",
            "Iteration 82, loss = 0.56168696\n",
            "Iteration 83, loss = 0.55714192\n",
            "Iteration 84, loss = 0.56842328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 85, loss = 0.52818560\n",
            "Iteration 86, loss = 0.50962760\n",
            "Iteration 87, loss = 0.50766586\n",
            "Iteration 88, loss = 0.51757043\n",
            "Iteration 89, loss = 0.51646229\n",
            "Iteration 90, loss = 0.51200468\n",
            "Iteration 91, loss = 0.51464477\n",
            "Iteration 92, loss = 0.51113230\n",
            "Iteration 93, loss = 0.50740763\n",
            "Iteration 94, loss = 0.50316114\n",
            "Iteration 95, loss = 0.50582001\n",
            "Iteration 96, loss = 0.50630451\n",
            "Iteration 97, loss = 0.50561214\n",
            "Iteration 98, loss = 0.52535197\n",
            "Iteration 99, loss = 0.52959642\n",
            "Iteration 100, loss = 0.52190094\n",
            "Iteration 101, loss = 0.52192629\n",
            "Iteration 102, loss = 0.51767504\n",
            "Iteration 103, loss = 0.50947490\n",
            "Iteration 104, loss = 0.52105976\n",
            "Iteration 105, loss = 0.51637398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 106, loss = 0.42641989\n",
            "Iteration 107, loss = 0.41344624\n",
            "Iteration 108, loss = 0.40131702\n",
            "Iteration 109, loss = 0.39017610\n",
            "Iteration 110, loss = 0.37942997\n",
            "Iteration 111, loss = 0.36990503\n",
            "Iteration 112, loss = 0.36036617\n",
            "Iteration 113, loss = 0.35232327\n",
            "Iteration 114, loss = 0.34469934\n",
            "Iteration 115, loss = 0.33761268\n",
            "Iteration 116, loss = 0.33129620\n",
            "Iteration 117, loss = 0.32599875\n",
            "Iteration 118, loss = 0.32017799\n",
            "Iteration 119, loss = 0.31535813\n",
            "Iteration 120, loss = 0.31082735\n",
            "Iteration 121, loss = 0.30690716\n",
            "Iteration 122, loss = 0.30316996\n",
            "Iteration 123, loss = 0.29937454\n",
            "Iteration 124, loss = 0.29637423\n",
            "Iteration 125, loss = 0.29320748\n",
            "Iteration 126, loss = 0.29054860\n",
            "Iteration 127, loss = 0.28907608\n",
            "Iteration 128, loss = 0.28599348\n",
            "Iteration 129, loss = 0.28402360\n",
            "Iteration 130, loss = 0.28218728\n",
            "Iteration 131, loss = 0.28052918\n",
            "Iteration 132, loss = 0.27936581\n",
            "Iteration 133, loss = 0.27919223\n",
            "Iteration 134, loss = 0.27588170\n",
            "Iteration 135, loss = 0.27507743\n",
            "Iteration 136, loss = 0.27325539\n",
            "Iteration 137, loss = 0.27213845\n",
            "Iteration 138, loss = 0.27171599\n",
            "Iteration 139, loss = 0.27254945\n",
            "Iteration 140, loss = 0.27040313\n",
            "Iteration 141, loss = 0.27182405\n",
            "Iteration 142, loss = 0.26996457\n",
            "Iteration 143, loss = 0.26730561\n",
            "Iteration 144, loss = 0.26902402\n",
            "Iteration 145, loss = 0.26777897\n",
            "Iteration 146, loss = 0.26733496\n",
            "Iteration 147, loss = 0.26886617\n",
            "Iteration 148, loss = 0.26868626\n",
            "Iteration 149, loss = 0.27373172\n",
            "Iteration 150, loss = 0.26723134\n",
            "Iteration 151, loss = 0.26444126\n",
            "Iteration 152, loss = 0.26756529\n",
            "Iteration 153, loss = 0.26523817\n",
            "Iteration 154, loss = 0.26972106\n",
            "Iteration 155, loss = 0.27230205\n",
            "Iteration 156, loss = 0.27156710\n",
            "Iteration 157, loss = 0.26979556\n",
            "Iteration 158, loss = 0.26597533\n",
            "Iteration 159, loss = 0.26954877\n",
            "Iteration 160, loss = 0.26350267\n",
            "Iteration 161, loss = 0.26555252\n",
            "Iteration 162, loss = 0.26518894\n",
            "Iteration 163, loss = 0.26836602\n",
            "Iteration 164, loss = 0.26575346\n",
            "Iteration 165, loss = 0.27156253\n",
            "Iteration 166, loss = 0.26655804\n",
            "Iteration 167, loss = 0.26811850\n",
            "Iteration 168, loss = 0.27247945\n",
            "Iteration 169, loss = 0.26566867\n",
            "Iteration 170, loss = 0.26815812\n",
            "Iteration 171, loss = 0.26528470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 172, loss = 0.24918713\n",
            "Iteration 173, loss = 0.24915050\n",
            "Iteration 174, loss = 0.24907499\n",
            "Iteration 175, loss = 0.24901811\n",
            "Iteration 176, loss = 0.24897219\n",
            "Iteration 177, loss = 0.24893519\n",
            "Iteration 178, loss = 0.24890337\n",
            "Iteration 179, loss = 0.24883240\n",
            "Iteration 180, loss = 0.24881065\n",
            "Iteration 181, loss = 0.24879038\n",
            "Iteration 182, loss = 0.24874786\n",
            "Iteration 183, loss = 0.24869816\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 184, loss = 0.24840450\n",
            "Iteration 185, loss = 0.24843057\n",
            "Iteration 186, loss = 0.24838345\n",
            "Iteration 187, loss = 0.24836843\n",
            "Iteration 188, loss = 0.24836931\n",
            "Iteration 189, loss = 0.24838777\n",
            "Iteration 190, loss = 0.24835840\n",
            "Iteration 191, loss = 0.24834411\n",
            "Iteration 192, loss = 0.24833801\n",
            "Iteration 193, loss = 0.24834168\n",
            "Iteration 194, loss = 0.24831950\n",
            "Iteration 195, loss = 0.24834624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 196, loss = 0.24823897\n",
            "Iteration 197, loss = 0.24823504\n",
            "Iteration 198, loss = 0.24823199\n",
            "Iteration 199, loss = 0.24822221\n",
            "Iteration 200, loss = 0.24822418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.97746466\n",
            "Iteration 2, loss = 0.64512444\n",
            "Iteration 3, loss = 0.61437255\n",
            "Iteration 4, loss = 0.59692320\n",
            "Iteration 5, loss = 0.58581172\n",
            "Iteration 6, loss = 0.57954611\n",
            "Iteration 7, loss = 0.57336022\n",
            "Iteration 8, loss = 0.57101247\n",
            "Iteration 9, loss = 0.57038350\n",
            "Iteration 10, loss = 0.56967795\n",
            "Iteration 11, loss = 0.56767612\n",
            "Iteration 12, loss = 0.56734336\n",
            "Iteration 13, loss = 0.59879859\n",
            "Iteration 14, loss = 0.59467234\n",
            "Iteration 15, loss = 0.57982995\n",
            "Iteration 16, loss = 0.57236338\n",
            "Iteration 17, loss = 0.56984843\n",
            "Iteration 18, loss = 0.56870175\n",
            "Iteration 19, loss = 0.56824005\n",
            "Iteration 20, loss = 0.56625598\n",
            "Iteration 21, loss = 0.56668268\n",
            "Iteration 22, loss = 0.56834708\n",
            "Iteration 23, loss = 0.56855714\n",
            "Iteration 24, loss = 0.56795168\n",
            "Iteration 25, loss = 0.56927444\n",
            "Iteration 26, loss = 0.56790895\n",
            "Iteration 27, loss = 0.57099120\n",
            "Iteration 28, loss = 0.57018370\n",
            "Iteration 29, loss = 0.57135947\n",
            "Iteration 30, loss = 0.56893322\n",
            "Iteration 31, loss = 0.57027266\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.51475126\n",
            "Iteration 33, loss = 0.48989415\n",
            "Iteration 34, loss = 0.51507360\n",
            "Iteration 35, loss = 0.51671825\n",
            "Iteration 36, loss = 0.51731141\n",
            "Iteration 37, loss = 0.51747237\n",
            "Iteration 38, loss = 0.51708652\n",
            "Iteration 39, loss = 0.51640410\n",
            "Iteration 40, loss = 0.51658271\n",
            "Iteration 41, loss = 0.51293513\n",
            "Iteration 42, loss = 0.51569926\n",
            "Iteration 43, loss = 0.51716049\n",
            "Iteration 44, loss = 0.51632323\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.41828091\n",
            "Iteration 46, loss = 0.40378352\n",
            "Iteration 47, loss = 0.39116785\n",
            "Iteration 48, loss = 0.37924334\n",
            "Iteration 49, loss = 0.36838023\n",
            "Iteration 50, loss = 0.35866141\n",
            "Iteration 51, loss = 0.34986295\n",
            "Iteration 52, loss = 0.34202351\n",
            "Iteration 53, loss = 0.33467641\n",
            "Iteration 54, loss = 0.32786053\n",
            "Iteration 55, loss = 0.32249590\n",
            "Iteration 56, loss = 0.31704169\n",
            "Iteration 57, loss = 0.31219154\n",
            "Iteration 58, loss = 0.30769768\n",
            "Iteration 59, loss = 0.30379214\n",
            "Iteration 60, loss = 0.30001024\n",
            "Iteration 61, loss = 0.29689483\n",
            "Iteration 62, loss = 0.29330985\n",
            "Iteration 63, loss = 0.29135572\n",
            "Iteration 64, loss = 0.28918669\n",
            "Iteration 65, loss = 0.28709550\n",
            "Iteration 66, loss = 0.28452124\n",
            "Iteration 67, loss = 0.28242792\n",
            "Iteration 68, loss = 0.28059822\n",
            "Iteration 69, loss = 0.27997455\n",
            "Iteration 70, loss = 0.27806046\n",
            "Iteration 71, loss = 0.27805368\n",
            "Iteration 72, loss = 0.27673533\n",
            "Iteration 73, loss = 0.27468222\n",
            "Iteration 74, loss = 0.27489953\n",
            "Iteration 75, loss = 0.27259334\n",
            "Iteration 76, loss = 0.27431455\n",
            "Iteration 77, loss = 0.27257832\n",
            "Iteration 78, loss = 0.27107590\n",
            "Iteration 79, loss = 0.27181972\n",
            "Iteration 80, loss = 0.27185275\n",
            "Iteration 81, loss = 0.26945606\n",
            "Iteration 82, loss = 0.27253005\n",
            "Iteration 83, loss = 0.27045676\n",
            "Iteration 84, loss = 0.27144353\n",
            "Iteration 85, loss = 0.27030991\n",
            "Iteration 86, loss = 0.26810947\n",
            "Iteration 87, loss = 0.26908887\n",
            "Iteration 88, loss = 0.27039817\n",
            "Iteration 89, loss = 0.27310175\n",
            "Iteration 90, loss = 0.27105675\n",
            "Iteration 91, loss = 0.27125390\n",
            "Iteration 92, loss = 0.27020983\n",
            "Iteration 93, loss = 0.27382036\n",
            "Iteration 94, loss = 0.27524219\n",
            "Iteration 95, loss = 0.27416785\n",
            "Iteration 96, loss = 0.27528876\n",
            "Iteration 97, loss = 0.27531259\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 98, loss = 0.25560598\n",
            "Iteration 99, loss = 0.25555459\n",
            "Iteration 100, loss = 0.25546506\n",
            "Iteration 101, loss = 0.25537273\n",
            "Iteration 102, loss = 0.25533888\n",
            "Iteration 103, loss = 0.25532220\n",
            "Iteration 104, loss = 0.25529688\n",
            "Iteration 105, loss = 0.25519550\n",
            "Iteration 106, loss = 0.25516143\n",
            "Iteration 107, loss = 0.25514757\n",
            "Iteration 108, loss = 0.25506721\n",
            "Iteration 109, loss = 0.25498983\n",
            "Iteration 110, loss = 0.25496838\n",
            "Iteration 111, loss = 0.25491310\n",
            "Iteration 112, loss = 0.25488415\n",
            "Iteration 113, loss = 0.25484836\n",
            "Iteration 114, loss = 0.25482740\n",
            "Iteration 115, loss = 0.25476836\n",
            "Iteration 116, loss = 0.25469098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 117, loss = 0.25441200\n",
            "Iteration 118, loss = 0.25438949\n",
            "Iteration 119, loss = 0.25436821\n",
            "Iteration 120, loss = 0.25432941\n",
            "Iteration 121, loss = 0.25434289\n",
            "Iteration 122, loss = 0.25433583\n",
            "Iteration 123, loss = 0.25432410\n",
            "Iteration 124, loss = 0.25432016\n",
            "Iteration 125, loss = 0.25433170\n",
            "Iteration 126, loss = 0.25430782\n",
            "Iteration 127, loss = 0.25429843\n",
            "Iteration 128, loss = 0.25426746\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 129, loss = 0.25418767\n",
            "Iteration 130, loss = 0.25417624\n",
            "Iteration 131, loss = 0.25416913\n",
            "Iteration 132, loss = 0.25417134\n",
            "Iteration 133, loss = 0.25417539\n",
            "Iteration 134, loss = 0.25416194\n",
            "Iteration 135, loss = 0.25416853\n",
            "Iteration 136, loss = 0.25415740\n",
            "Iteration 137, loss = 0.25415380\n",
            "Iteration 138, loss = 0.25416327\n",
            "Iteration 139, loss = 0.25415872\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.97587313\n",
            "Iteration 2, loss = 0.64104258\n",
            "Iteration 3, loss = 0.61314526\n",
            "Iteration 4, loss = 0.59571676\n",
            "Iteration 5, loss = 0.58445293\n",
            "Iteration 6, loss = 0.57757154\n",
            "Iteration 7, loss = 0.57320134\n",
            "Iteration 8, loss = 0.57011915\n",
            "Iteration 9, loss = 0.57187164\n",
            "Iteration 10, loss = 0.56996341\n",
            "Iteration 11, loss = 0.56802609\n",
            "Iteration 12, loss = 0.56840219\n",
            "Iteration 13, loss = 0.56819869\n",
            "Iteration 14, loss = 0.56778669\n",
            "Iteration 15, loss = 0.56906090\n",
            "Iteration 16, loss = 0.56770637\n",
            "Iteration 17, loss = 0.56991717\n",
            "Iteration 18, loss = 0.56915235\n",
            "Iteration 19, loss = 0.57043685\n",
            "Iteration 20, loss = 0.56867425\n",
            "Iteration 21, loss = 0.56969000\n",
            "Iteration 22, loss = 0.57105541\n",
            "Iteration 23, loss = 0.56911213\n",
            "Iteration 24, loss = 0.56921072\n",
            "Iteration 25, loss = 0.57267805\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.50833323\n",
            "Iteration 27, loss = 0.48736746\n",
            "Iteration 28, loss = 0.51600159\n",
            "Iteration 29, loss = 0.51350680\n",
            "Iteration 30, loss = 0.51673079\n",
            "Iteration 31, loss = 0.51617069\n",
            "Iteration 32, loss = 0.51473837\n",
            "Iteration 33, loss = 0.51063932\n",
            "Iteration 34, loss = 0.51748962\n",
            "Iteration 35, loss = 0.51630151\n",
            "Iteration 36, loss = 0.51345176\n",
            "Iteration 37, loss = 0.51682175\n",
            "Iteration 38, loss = 0.51064938\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.41265215\n",
            "Iteration 40, loss = 0.39833838\n",
            "Iteration 41, loss = 0.38515561\n",
            "Iteration 42, loss = 0.37352079\n",
            "Iteration 43, loss = 0.36273773\n",
            "Iteration 44, loss = 0.35307465\n",
            "Iteration 45, loss = 0.34425050\n",
            "Iteration 46, loss = 0.33633479\n",
            "Iteration 47, loss = 0.32929768\n",
            "Iteration 48, loss = 0.32291553\n",
            "Iteration 49, loss = 0.31673394\n",
            "Iteration 50, loss = 0.31176643\n",
            "Iteration 51, loss = 0.30710696\n",
            "Iteration 52, loss = 0.30236466\n",
            "Iteration 53, loss = 0.29866185\n",
            "Iteration 54, loss = 0.29479927\n",
            "Iteration 55, loss = 0.29234234\n",
            "Iteration 56, loss = 0.28864228\n",
            "Iteration 57, loss = 0.28659300\n",
            "Iteration 58, loss = 0.28387134\n",
            "Iteration 59, loss = 0.28200663\n",
            "Iteration 60, loss = 0.27926377\n",
            "Iteration 61, loss = 0.27813714\n",
            "Iteration 62, loss = 0.27769658\n",
            "Iteration 63, loss = 0.27517060\n",
            "Iteration 64, loss = 0.27316499\n",
            "Iteration 65, loss = 0.27188442\n",
            "Iteration 66, loss = 0.27132817\n",
            "Iteration 67, loss = 0.27070956\n",
            "Iteration 68, loss = 0.27010870\n",
            "Iteration 69, loss = 0.26847656\n",
            "Iteration 70, loss = 0.26726952\n",
            "Iteration 71, loss = 0.26748312\n",
            "Iteration 72, loss = 0.26573822\n",
            "Iteration 73, loss = 0.26555021\n",
            "Iteration 74, loss = 0.26630257\n",
            "Iteration 75, loss = 0.26567356\n",
            "Iteration 76, loss = 0.26494278\n",
            "Iteration 77, loss = 0.26377169\n",
            "Iteration 78, loss = 0.26445850\n",
            "Iteration 79, loss = 0.26417133\n",
            "Iteration 80, loss = 0.26451209\n",
            "Iteration 81, loss = 0.26417070\n",
            "Iteration 82, loss = 0.26847990\n",
            "Iteration 83, loss = 0.26653821\n",
            "Iteration 84, loss = 0.26475435\n",
            "Iteration 85, loss = 0.26494218\n",
            "Iteration 86, loss = 0.27022110\n",
            "Iteration 87, loss = 0.26392156\n",
            "Iteration 88, loss = 0.26527489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 89, loss = 0.25263556\n",
            "Iteration 90, loss = 0.25251832\n",
            "Iteration 91, loss = 0.25251941\n",
            "Iteration 92, loss = 0.25247174\n",
            "Iteration 93, loss = 0.25244174\n",
            "Iteration 94, loss = 0.25232106\n",
            "Iteration 95, loss = 0.25235166\n",
            "Iteration 96, loss = 0.25224160\n",
            "Iteration 97, loss = 0.25220177\n",
            "Iteration 98, loss = 0.25209109\n",
            "Iteration 99, loss = 0.25210406\n",
            "Iteration 100, loss = 0.25199762\n",
            "Iteration 101, loss = 0.25199334\n",
            "Iteration 102, loss = 0.25190165\n",
            "Iteration 103, loss = 0.25186793\n",
            "Iteration 104, loss = 0.25181540\n",
            "Iteration 105, loss = 0.25179092\n",
            "Iteration 106, loss = 0.25174954\n",
            "Iteration 107, loss = 0.25172215\n",
            "Iteration 108, loss = 0.25166930\n",
            "Iteration 109, loss = 0.25156971\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 110, loss = 0.25131148\n",
            "Iteration 111, loss = 0.25128729\n",
            "Iteration 112, loss = 0.25124993\n",
            "Iteration 113, loss = 0.25125531\n",
            "Iteration 114, loss = 0.25126626\n",
            "Iteration 115, loss = 0.25124482\n",
            "Iteration 116, loss = 0.25124617\n",
            "Iteration 117, loss = 0.25121149\n",
            "Iteration 118, loss = 0.25124474\n",
            "Iteration 119, loss = 0.25119838\n",
            "Iteration 120, loss = 0.25121211\n",
            "Iteration 121, loss = 0.25119405\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 122, loss = 0.25109654\n",
            "Iteration 123, loss = 0.25108567\n",
            "Iteration 124, loss = 0.25108652\n",
            "Iteration 125, loss = 0.25108008\n",
            "Iteration 126, loss = 0.25107899\n",
            "Iteration 127, loss = 0.25106515\n",
            "Iteration 128, loss = 0.25107573\n",
            "Iteration 129, loss = 0.25107376\n",
            "Iteration 130, loss = 0.25105870\n",
            "Iteration 131, loss = 0.25106871\n",
            "Iteration 132, loss = 0.25106898\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63259377\n",
            "Iteration 3, loss = 0.60639542\n",
            "Iteration 4, loss = 0.59186865\n",
            "Iteration 5, loss = 0.58052343\n",
            "Iteration 6, loss = 0.57450335\n",
            "Iteration 7, loss = 0.57616323\n",
            "Iteration 8, loss = 0.57310162\n",
            "Iteration 9, loss = 0.57010684\n",
            "Iteration 10, loss = 0.57048819\n",
            "Iteration 11, loss = 0.56945665\n",
            "Iteration 12, loss = 0.56897381\n",
            "Iteration 13, loss = 0.56817971\n",
            "Iteration 14, loss = 0.58688047\n",
            "Iteration 15, loss = 0.60543541\n",
            "Iteration 16, loss = 0.58528640\n",
            "Iteration 17, loss = 0.57606799\n",
            "Iteration 18, loss = 0.57073801\n",
            "Iteration 19, loss = 0.56964971\n",
            "Iteration 20, loss = 0.56721169\n",
            "Iteration 21, loss = 0.56731771\n",
            "Iteration 22, loss = 0.56852948\n",
            "Iteration 23, loss = 0.56723607\n",
            "Iteration 24, loss = 0.56715337\n",
            "Iteration 25, loss = 0.56750605\n",
            "Iteration 26, loss = 0.57016946\n",
            "Iteration 27, loss = 0.56751238\n",
            "Iteration 28, loss = 0.56788240\n",
            "Iteration 29, loss = 0.56846341\n",
            "Iteration 30, loss = 0.56808599\n",
            "Iteration 31, loss = 0.57029278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 32, loss = 0.51754758\n",
            "Iteration 33, loss = 0.49693738\n",
            "Iteration 34, loss = 0.51180920\n",
            "Iteration 35, loss = 0.52096494\n",
            "Iteration 36, loss = 0.51629264\n",
            "Iteration 37, loss = 0.52028592\n",
            "Iteration 38, loss = 0.51934721\n",
            "Iteration 39, loss = 0.52125740\n",
            "Iteration 40, loss = 0.51745688\n",
            "Iteration 41, loss = 0.51729810\n",
            "Iteration 42, loss = 0.51700645\n",
            "Iteration 43, loss = 0.51947453\n",
            "Iteration 44, loss = 0.51540361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 45, loss = 0.43193175\n",
            "Iteration 46, loss = 0.41924514\n",
            "Iteration 47, loss = 0.40812006\n",
            "Iteration 48, loss = 0.39784639\n",
            "Iteration 49, loss = 0.38847700\n",
            "Iteration 50, loss = 0.37990567\n",
            "Iteration 51, loss = 0.37226915\n",
            "Iteration 52, loss = 0.36521806\n",
            "Iteration 53, loss = 0.35907685\n",
            "Iteration 54, loss = 0.35361831\n",
            "Iteration 55, loss = 0.34864248\n",
            "Iteration 56, loss = 0.34471058\n",
            "Iteration 57, loss = 0.34098572\n",
            "Iteration 58, loss = 0.33741008\n",
            "Iteration 59, loss = 0.33434657\n",
            "Iteration 60, loss = 0.33229292\n",
            "Iteration 61, loss = 0.32953053\n",
            "Iteration 62, loss = 0.32655305\n",
            "Iteration 63, loss = 0.32586670\n",
            "Iteration 64, loss = 0.32469009\n",
            "Iteration 65, loss = 0.32424052\n",
            "Iteration 66, loss = 0.32307897\n",
            "Iteration 67, loss = 0.31997796\n",
            "Iteration 68, loss = 0.32145611\n",
            "Iteration 69, loss = 0.31990800\n",
            "Iteration 70, loss = 0.32173265\n",
            "Iteration 71, loss = 0.32107335\n",
            "Iteration 72, loss = 0.32075980\n",
            "Iteration 73, loss = 0.32132435\n",
            "Iteration 74, loss = 0.32152615\n",
            "Iteration 75, loss = 0.32229201\n",
            "Iteration 76, loss = 0.32230078\n",
            "Iteration 77, loss = 0.31987972\n",
            "Iteration 78, loss = 0.31791731\n",
            "Iteration 79, loss = 0.32107249\n",
            "Iteration 80, loss = 0.32166626\n",
            "Iteration 81, loss = 0.31988258\n",
            "Iteration 82, loss = 0.32283686\n",
            "Iteration 83, loss = 0.32072907\n",
            "Iteration 84, loss = 0.31915257\n",
            "Iteration 85, loss = 0.32214904\n",
            "Iteration 86, loss = 0.32256825\n",
            "Iteration 87, loss = 0.32568077\n",
            "Iteration 88, loss = 0.32177674\n",
            "Iteration 89, loss = 0.32075839\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 90, loss = 0.28821125\n",
            "Iteration 91, loss = 0.28787230\n",
            "Iteration 92, loss = 0.28769950\n",
            "Iteration 93, loss = 0.28750605\n",
            "Iteration 94, loss = 0.28733967\n",
            "Iteration 95, loss = 0.28716017\n",
            "Iteration 96, loss = 0.28703383\n",
            "Iteration 97, loss = 0.28690371\n",
            "Iteration 98, loss = 0.28676708\n",
            "Iteration 99, loss = 0.28659800\n",
            "Iteration 100, loss = 0.28652803\n",
            "Iteration 101, loss = 0.28638486\n",
            "Iteration 102, loss = 0.28616018\n",
            "Iteration 103, loss = 0.28604206\n",
            "Iteration 104, loss = 0.28590206\n",
            "Iteration 105, loss = 0.28579251\n",
            "Iteration 106, loss = 0.28566713\n",
            "Iteration 107, loss = 0.28547532\n",
            "Iteration 108, loss = 0.28535289\n",
            "Iteration 109, loss = 0.28527371\n",
            "Iteration 110, loss = 0.28513289\n",
            "Iteration 111, loss = 0.28495976\n",
            "Iteration 112, loss = 0.28490647\n",
            "Iteration 113, loss = 0.28481080\n",
            "Iteration 114, loss = 0.28457869\n",
            "Iteration 115, loss = 0.28441384\n",
            "Iteration 116, loss = 0.28435852\n",
            "Iteration 117, loss = 0.28421325\n",
            "Iteration 118, loss = 0.28409098\n",
            "Iteration 119, loss = 0.28401043\n",
            "Iteration 120, loss = 0.28386053\n",
            "Iteration 121, loss = 0.28378057\n",
            "Iteration 122, loss = 0.28365814\n",
            "Iteration 123, loss = 0.28353405\n",
            "Iteration 124, loss = 0.28336931\n",
            "Iteration 125, loss = 0.28326278\n",
            "Iteration 126, loss = 0.28315867\n",
            "Iteration 127, loss = 0.28303524\n",
            "Iteration 128, loss = 0.28295231\n",
            "Iteration 129, loss = 0.28283395\n",
            "Iteration 130, loss = 0.28269905\n",
            "Iteration 131, loss = 0.28258312\n",
            "Iteration 132, loss = 0.28248195\n",
            "Iteration 133, loss = 0.28237395\n",
            "Iteration 134, loss = 0.28228810\n",
            "Iteration 135, loss = 0.28217243\n",
            "Iteration 136, loss = 0.28206313\n",
            "Iteration 137, loss = 0.28190991\n",
            "Iteration 138, loss = 0.28183306\n",
            "Iteration 139, loss = 0.28175554\n",
            "Iteration 140, loss = 0.28161687\n",
            "Iteration 141, loss = 0.28149681\n",
            "Iteration 142, loss = 0.28141049\n",
            "Iteration 143, loss = 0.28132879\n",
            "Iteration 144, loss = 0.28118734\n",
            "Iteration 145, loss = 0.28109731\n",
            "Iteration 146, loss = 0.28099971\n",
            "Iteration 147, loss = 0.28088290\n",
            "Iteration 148, loss = 0.28082959\n",
            "Iteration 149, loss = 0.28069077\n",
            "Iteration 150, loss = 0.28066056\n",
            "Iteration 151, loss = 0.28050208\n",
            "Iteration 152, loss = 0.28039842\n",
            "Iteration 153, loss = 0.28032990\n",
            "Iteration 154, loss = 0.28020918\n",
            "Iteration 155, loss = 0.28015922\n",
            "Iteration 156, loss = 0.28004565\n",
            "Iteration 157, loss = 0.27998615\n",
            "Iteration 158, loss = 0.27985102\n",
            "Iteration 159, loss = 0.27980378\n",
            "Iteration 160, loss = 0.27970322\n",
            "Iteration 161, loss = 0.27963798\n",
            "Iteration 162, loss = 0.27950176\n",
            "Iteration 163, loss = 0.27941804\n",
            "Iteration 164, loss = 0.27933549\n",
            "Iteration 165, loss = 0.27922317\n",
            "Iteration 166, loss = 0.27910409\n",
            "Iteration 167, loss = 0.27908517\n",
            "Iteration 168, loss = 0.27895980\n",
            "Iteration 169, loss = 0.27888859\n",
            "Iteration 170, loss = 0.27874067\n",
            "Iteration 171, loss = 0.27867960\n",
            "Iteration 172, loss = 0.27863098\n",
            "Iteration 173, loss = 0.27850935\n",
            "Iteration 174, loss = 0.27846318\n",
            "Iteration 175, loss = 0.27837081\n",
            "Iteration 176, loss = 0.27831243\n",
            "Iteration 177, loss = 0.27814894\n",
            "Iteration 178, loss = 0.27812303\n",
            "Iteration 179, loss = 0.27800494\n",
            "Iteration 180, loss = 0.27794730\n",
            "Iteration 181, loss = 0.27789587\n",
            "Iteration 182, loss = 0.27775677\n",
            "Iteration 183, loss = 0.27766619\n",
            "Iteration 184, loss = 0.27765166\n",
            "Iteration 185, loss = 0.27754680\n",
            "Iteration 186, loss = 0.27748412\n",
            "Iteration 187, loss = 0.27730976\n",
            "Iteration 188, loss = 0.27731310\n",
            "Iteration 189, loss = 0.27720885\n",
            "Iteration 190, loss = 0.27711789\n",
            "Iteration 191, loss = 0.27709100\n",
            "Iteration 192, loss = 0.27700490\n",
            "Iteration 193, loss = 0.27688854\n",
            "Iteration 194, loss = 0.27686361\n",
            "Iteration 195, loss = 0.27680160\n",
            "Iteration 196, loss = 0.27666893\n",
            "Iteration 197, loss = 0.27664398\n",
            "Iteration 198, loss = 0.27655041\n",
            "Iteration 199, loss = 0.27647499\n",
            "Iteration 200, loss = 0.27637603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 14.4min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70912737\n",
            "Iteration 3, loss = 0.68512155\n",
            "Iteration 4, loss = 0.67147912\n",
            "Iteration 5, loss = 0.65805540\n",
            "Iteration 6, loss = 0.64489168\n",
            "Iteration 7, loss = 0.63526790\n",
            "Iteration 8, loss = 0.62854761\n",
            "Iteration 9, loss = 0.62139996\n",
            "Iteration 10, loss = 0.61305387\n",
            "Iteration 11, loss = 0.61009616\n",
            "Iteration 12, loss = 0.60265274\n",
            "Iteration 13, loss = 0.59940101\n",
            "Iteration 14, loss = 0.59339663\n",
            "Iteration 15, loss = 0.59001562\n",
            "Iteration 16, loss = 0.58565671\n",
            "Iteration 17, loss = 0.58416858\n",
            "Iteration 18, loss = 0.58156132\n",
            "Iteration 19, loss = 0.57898478\n",
            "Iteration 20, loss = 0.57668489\n",
            "Iteration 21, loss = 0.57590590\n",
            "Iteration 22, loss = 0.57520693\n",
            "Iteration 23, loss = 0.56969338\n",
            "Iteration 24, loss = 0.57262373\n",
            "Iteration 25, loss = 0.57367973\n",
            "Iteration 26, loss = 0.56760370\n",
            "Iteration 27, loss = 0.56903142\n",
            "Iteration 28, loss = 0.56842459\n",
            "Iteration 29, loss = 0.56787396\n",
            "Iteration 30, loss = 0.56626516\n",
            "Iteration 31, loss = 0.56860648\n",
            "Iteration 32, loss = 0.56444306\n",
            "Iteration 33, loss = 0.56710834\n",
            "Iteration 34, loss = 0.56732081\n",
            "Iteration 35, loss = 0.56431956\n",
            "Iteration 36, loss = 0.56667550\n",
            "Iteration 37, loss = 0.56348074\n",
            "Iteration 38, loss = 0.56317820\n",
            "Iteration 39, loss = 0.56415034\n",
            "Iteration 40, loss = 0.56480539\n",
            "Iteration 41, loss = 0.56409059\n",
            "Iteration 42, loss = 0.56553455\n",
            "Iteration 43, loss = 0.56531068\n",
            "Iteration 44, loss = 0.56100103\n",
            "Iteration 45, loss = 0.56547426\n",
            "Iteration 46, loss = 0.56545092\n",
            "Iteration 47, loss = 0.56302418\n",
            "Iteration 48, loss = 0.56625847\n",
            "Iteration 49, loss = 0.56469982\n",
            "Iteration 50, loss = 0.56474421\n",
            "Iteration 51, loss = 0.56876885\n",
            "Iteration 52, loss = 0.56279942\n",
            "Iteration 53, loss = 0.56494525\n",
            "Iteration 54, loss = 0.56497028\n",
            "Iteration 55, loss = 0.56447174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 56, loss = 0.52241506\n",
            "Iteration 57, loss = 0.51148194\n",
            "Iteration 58, loss = 0.50379792\n",
            "Iteration 59, loss = 0.49804529\n",
            "Iteration 60, loss = 0.49296407\n",
            "Iteration 61, loss = 0.48675106\n",
            "Iteration 62, loss = 0.48456036\n",
            "Iteration 63, loss = 0.48514644\n",
            "Iteration 64, loss = 0.47908173\n",
            "Iteration 65, loss = 0.49652520\n",
            "Iteration 66, loss = 0.49602532\n",
            "Iteration 67, loss = 0.50474317\n",
            "Iteration 68, loss = 0.50970515\n",
            "Iteration 69, loss = 0.50490743\n",
            "Iteration 70, loss = 0.50527510\n",
            "Iteration 71, loss = 0.51888970\n",
            "Iteration 72, loss = 0.50580148\n",
            "Iteration 73, loss = 0.51270145\n",
            "Iteration 74, loss = 0.50603404\n",
            "Iteration 75, loss = 0.51473489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.44010171\n",
            "Iteration 77, loss = 0.43711112\n",
            "Iteration 78, loss = 0.43475952\n",
            "Iteration 79, loss = 0.43257168\n",
            "Iteration 80, loss = 0.43036504\n",
            "Iteration 81, loss = 0.42783042\n",
            "Iteration 82, loss = 0.42555909\n",
            "Iteration 83, loss = 0.42380155\n",
            "Iteration 84, loss = 0.42138527\n",
            "Iteration 85, loss = 0.41932870\n",
            "Iteration 86, loss = 0.41687054\n",
            "Iteration 87, loss = 0.41532814\n",
            "Iteration 88, loss = 0.41288552\n",
            "Iteration 89, loss = 0.41092155\n",
            "Iteration 90, loss = 0.40917298\n",
            "Iteration 91, loss = 0.40706521\n",
            "Iteration 92, loss = 0.40418675\n",
            "Iteration 93, loss = 0.40342770\n",
            "Iteration 94, loss = 0.40129429\n",
            "Iteration 95, loss = 0.39856648\n",
            "Iteration 96, loss = 0.39699211\n",
            "Iteration 97, loss = 0.39560857\n",
            "Iteration 98, loss = 0.39348189\n",
            "Iteration 99, loss = 0.39149854\n",
            "Iteration 100, loss = 0.38519279\n",
            "Iteration 101, loss = 0.37980331\n",
            "Iteration 102, loss = 0.37736246\n",
            "Iteration 103, loss = 0.37494671\n",
            "Iteration 104, loss = 0.37325944\n",
            "Iteration 105, loss = 0.37109596\n",
            "Iteration 106, loss = 0.36915505\n",
            "Iteration 107, loss = 0.36703276\n",
            "Iteration 108, loss = 0.36529422\n",
            "Iteration 109, loss = 0.36320927\n",
            "Iteration 110, loss = 0.36143713\n",
            "Iteration 111, loss = 0.35972961\n",
            "Iteration 112, loss = 0.35881038\n",
            "Iteration 113, loss = 0.35619241\n",
            "Iteration 114, loss = 0.35501766\n",
            "Iteration 115, loss = 0.35259977\n",
            "Iteration 116, loss = 0.35117020\n",
            "Iteration 117, loss = 0.34974409\n",
            "Iteration 118, loss = 0.34806246\n",
            "Iteration 119, loss = 0.34634023\n",
            "Iteration 120, loss = 0.34438614\n",
            "Iteration 121, loss = 0.34357687\n",
            "Iteration 122, loss = 0.34160378\n",
            "Iteration 123, loss = 0.33979838\n",
            "Iteration 124, loss = 0.33870077\n",
            "Iteration 125, loss = 0.33759332\n",
            "Iteration 126, loss = 0.33489658\n",
            "Iteration 127, loss = 0.33359450\n",
            "Iteration 128, loss = 0.33380633\n",
            "Iteration 129, loss = 0.33230906\n",
            "Iteration 130, loss = 0.33065044\n",
            "Iteration 131, loss = 0.32934436\n",
            "Iteration 132, loss = 0.32749142\n",
            "Iteration 133, loss = 0.32741227\n",
            "Iteration 134, loss = 0.32494097\n",
            "Iteration 135, loss = 0.32488325\n",
            "Iteration 136, loss = 0.32206767\n",
            "Iteration 137, loss = 0.32180585\n",
            "Iteration 138, loss = 0.32066960\n",
            "Iteration 139, loss = 0.32044841\n",
            "Iteration 140, loss = 0.31882100\n",
            "Iteration 141, loss = 0.31763346\n",
            "Iteration 142, loss = 0.31529288\n",
            "Iteration 143, loss = 0.31543171\n",
            "Iteration 144, loss = 0.31438036\n",
            "Iteration 145, loss = 0.31323923\n",
            "Iteration 146, loss = 0.31378916\n",
            "Iteration 147, loss = 0.31154221\n",
            "Iteration 148, loss = 0.31143420\n",
            "Iteration 149, loss = 0.31048897\n",
            "Iteration 150, loss = 0.31032875\n",
            "Iteration 151, loss = 0.30914174\n",
            "Iteration 152, loss = 0.30824260\n",
            "Iteration 153, loss = 0.30718042\n",
            "Iteration 154, loss = 0.30591680\n",
            "Iteration 155, loss = 0.30466670\n",
            "Iteration 156, loss = 0.30406177\n",
            "Iteration 157, loss = 0.30284218\n",
            "Iteration 158, loss = 0.30427658\n",
            "Iteration 159, loss = 0.30260914\n",
            "Iteration 160, loss = 0.30210822\n",
            "Iteration 161, loss = 0.30246098\n",
            "Iteration 162, loss = 0.29993654\n",
            "Iteration 163, loss = 0.29998283\n",
            "Iteration 164, loss = 0.30159614\n",
            "Iteration 165, loss = 0.29955263\n",
            "Iteration 166, loss = 0.29950410\n",
            "Iteration 167, loss = 0.29764683\n",
            "Iteration 168, loss = 0.29670282\n",
            "Iteration 169, loss = 0.29633113\n",
            "Iteration 170, loss = 0.30073016\n",
            "Iteration 171, loss = 0.29613698\n",
            "Iteration 172, loss = 0.29597243\n",
            "Iteration 173, loss = 0.29414864\n",
            "Iteration 174, loss = 0.29578725\n",
            "Iteration 175, loss = 0.29213663\n",
            "Iteration 176, loss = 0.29565149\n",
            "Iteration 177, loss = 0.29227501\n",
            "Iteration 178, loss = 0.29224044\n",
            "Iteration 179, loss = 0.29459249\n",
            "Iteration 180, loss = 0.28969961\n",
            "Iteration 181, loss = 0.29247025\n",
            "Iteration 182, loss = 0.29625045\n",
            "Iteration 183, loss = 0.29057938\n",
            "Iteration 184, loss = 0.29199437\n",
            "Iteration 185, loss = 0.29174705\n",
            "Iteration 186, loss = 0.29453276\n",
            "Iteration 187, loss = 0.29159946\n",
            "Iteration 188, loss = 0.29005592\n",
            "Iteration 189, loss = 0.28968278\n",
            "Iteration 190, loss = 0.29245763\n",
            "Iteration 191, loss = 0.28859418\n",
            "Iteration 192, loss = 0.29349035\n",
            "Iteration 193, loss = 0.29042471\n",
            "Iteration 194, loss = 0.28778339\n",
            "Iteration 195, loss = 0.28588067\n",
            "Iteration 196, loss = 0.28906045\n",
            "Iteration 197, loss = 0.29062278\n",
            "Iteration 198, loss = 0.29370033\n",
            "Iteration 199, loss = 0.28313281\n",
            "Iteration 200, loss = 0.28576282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69534639\n",
            "Iteration 3, loss = 0.67590640\n",
            "Iteration 4, loss = 0.66342927\n",
            "Iteration 5, loss = 0.64867208\n",
            "Iteration 6, loss = 0.63728690\n",
            "Iteration 7, loss = 0.62728487\n",
            "Iteration 8, loss = 0.61901979\n",
            "Iteration 9, loss = 0.61402416\n",
            "Iteration 10, loss = 0.60796160\n",
            "Iteration 11, loss = 0.59877920\n",
            "Iteration 12, loss = 0.60180006\n",
            "Iteration 13, loss = 0.59044115\n",
            "Iteration 14, loss = 0.59116340\n",
            "Iteration 15, loss = 0.58216127\n",
            "Iteration 16, loss = 0.59471252\n",
            "Iteration 17, loss = 0.59226115\n",
            "Iteration 18, loss = 0.58998066\n",
            "Iteration 19, loss = 0.59662061\n",
            "Iteration 20, loss = 0.59616378\n",
            "Iteration 21, loss = 0.58728211\n",
            "Iteration 22, loss = 0.58676204\n",
            "Iteration 23, loss = 0.58339704\n",
            "Iteration 24, loss = 0.58424180\n",
            "Iteration 25, loss = 0.58269140\n",
            "Iteration 26, loss = 0.58056900\n",
            "Iteration 27, loss = 0.57752244\n",
            "Iteration 28, loss = 0.57629780\n",
            "Iteration 29, loss = 0.57582892\n",
            "Iteration 30, loss = 0.57479549\n",
            "Iteration 31, loss = 0.57276602\n",
            "Iteration 32, loss = 0.56991877\n",
            "Iteration 33, loss = 0.57434546\n",
            "Iteration 34, loss = 0.56853713\n",
            "Iteration 35, loss = 0.56989344\n",
            "Iteration 36, loss = 0.57245716\n",
            "Iteration 37, loss = 0.56824807\n",
            "Iteration 38, loss = 0.57112026\n",
            "Iteration 39, loss = 0.56998527\n",
            "Iteration 40, loss = 0.56932202\n",
            "Iteration 41, loss = 0.57216956\n",
            "Iteration 42, loss = 0.56614707\n",
            "Iteration 43, loss = 0.56943749\n",
            "Iteration 44, loss = 0.56937608\n",
            "Iteration 45, loss = 0.57057329\n",
            "Iteration 46, loss = 0.56702599\n",
            "Iteration 47, loss = 0.56803789\n",
            "Iteration 48, loss = 0.56892042\n",
            "Iteration 49, loss = 0.56988538\n",
            "Iteration 50, loss = 0.56669437\n",
            "Iteration 51, loss = 0.56738524\n",
            "Iteration 52, loss = 0.56947608\n",
            "Iteration 53, loss = 0.56447184\n",
            "Iteration 54, loss = 0.56937317\n",
            "Iteration 55, loss = 0.56700755\n",
            "Iteration 56, loss = 0.56857985\n",
            "Iteration 57, loss = 0.57210024\n",
            "Iteration 58, loss = 0.56658454\n",
            "Iteration 59, loss = 0.57053701\n",
            "Iteration 60, loss = 0.56829941\n",
            "Iteration 61, loss = 0.56693057\n",
            "Iteration 62, loss = 0.57086363\n",
            "Iteration 63, loss = 0.56752498\n",
            "Iteration 64, loss = 0.57067741\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 65, loss = 0.52852983\n",
            "Iteration 66, loss = 0.52166536\n",
            "Iteration 67, loss = 0.51600057\n",
            "Iteration 68, loss = 0.50951794\n",
            "Iteration 69, loss = 0.50418308\n",
            "Iteration 70, loss = 0.50142487\n",
            "Iteration 71, loss = 0.49691468\n",
            "Iteration 72, loss = 0.49974119\n",
            "Iteration 73, loss = 0.50554126\n",
            "Iteration 74, loss = 0.50761745\n",
            "Iteration 75, loss = 0.51160946\n",
            "Iteration 76, loss = 0.52207061\n",
            "Iteration 77, loss = 0.52299462\n",
            "Iteration 78, loss = 0.52009447\n",
            "Iteration 79, loss = 0.52253296\n",
            "Iteration 80, loss = 0.52341315\n",
            "Iteration 81, loss = 0.51414641\n",
            "Iteration 82, loss = 0.52650631\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 83, loss = 0.45865623\n",
            "Iteration 84, loss = 0.45615823\n",
            "Iteration 85, loss = 0.45369504\n",
            "Iteration 86, loss = 0.45156135\n",
            "Iteration 87, loss = 0.44924825\n",
            "Iteration 88, loss = 0.44699244\n",
            "Iteration 89, loss = 0.44484514\n",
            "Iteration 90, loss = 0.44270333\n",
            "Iteration 91, loss = 0.44060401\n",
            "Iteration 92, loss = 0.43833444\n",
            "Iteration 93, loss = 0.43644883\n",
            "Iteration 94, loss = 0.43393895\n",
            "Iteration 95, loss = 0.43238873\n",
            "Iteration 96, loss = 0.42985158\n",
            "Iteration 97, loss = 0.42817664\n",
            "Iteration 98, loss = 0.42629280\n",
            "Iteration 99, loss = 0.42390641\n",
            "Iteration 100, loss = 0.42235359\n",
            "Iteration 101, loss = 0.42024007\n",
            "Iteration 102, loss = 0.41852448\n",
            "Iteration 103, loss = 0.41628719\n",
            "Iteration 104, loss = 0.41472401\n",
            "Iteration 105, loss = 0.41268703\n",
            "Iteration 106, loss = 0.41058914\n",
            "Iteration 107, loss = 0.40902781\n",
            "Iteration 108, loss = 0.40746091\n",
            "Iteration 109, loss = 0.40558895\n",
            "Iteration 110, loss = 0.40349370\n",
            "Iteration 111, loss = 0.40247847\n",
            "Iteration 112, loss = 0.40044595\n",
            "Iteration 113, loss = 0.39840718\n",
            "Iteration 114, loss = 0.39768628\n",
            "Iteration 115, loss = 0.39540550\n",
            "Iteration 116, loss = 0.39409837\n",
            "Iteration 117, loss = 0.39315645\n",
            "Iteration 118, loss = 0.39148744\n",
            "Iteration 119, loss = 0.39087447\n",
            "Iteration 120, loss = 0.38908496\n",
            "Iteration 121, loss = 0.38689489\n",
            "Iteration 122, loss = 0.38539848\n",
            "Iteration 123, loss = 0.38451538\n",
            "Iteration 124, loss = 0.38344932\n",
            "Iteration 125, loss = 0.38150659\n",
            "Iteration 126, loss = 0.38104994\n",
            "Iteration 127, loss = 0.37991735\n",
            "Iteration 128, loss = 0.37850431\n",
            "Iteration 129, loss = 0.37850564\n",
            "Iteration 130, loss = 0.37604071\n",
            "Iteration 131, loss = 0.37512217\n",
            "Iteration 132, loss = 0.37498731\n",
            "Iteration 133, loss = 0.37331972\n",
            "Iteration 134, loss = 0.37136163\n",
            "Iteration 135, loss = 0.37056309\n",
            "Iteration 136, loss = 0.36832376\n",
            "Iteration 137, loss = 0.37068702\n",
            "Iteration 138, loss = 0.36957652\n",
            "Iteration 139, loss = 0.36803040\n",
            "Iteration 140, loss = 0.36707607\n",
            "Iteration 141, loss = 0.36485272\n",
            "Iteration 142, loss = 0.36515908\n",
            "Iteration 143, loss = 0.36461384\n",
            "Iteration 144, loss = 0.36519401\n",
            "Iteration 145, loss = 0.36538372\n",
            "Iteration 146, loss = 0.36214420\n",
            "Iteration 147, loss = 0.36144933\n",
            "Iteration 148, loss = 0.36066458\n",
            "Iteration 149, loss = 0.35928037\n",
            "Iteration 150, loss = 0.36219490\n",
            "Iteration 151, loss = 0.36019238\n",
            "Iteration 152, loss = 0.35852934\n",
            "Iteration 153, loss = 0.36101723\n",
            "Iteration 154, loss = 0.35810247\n",
            "Iteration 155, loss = 0.35859430\n",
            "Iteration 156, loss = 0.35988742\n",
            "Iteration 157, loss = 0.35806992\n",
            "Iteration 158, loss = 0.35729348\n",
            "Iteration 159, loss = 0.35730340\n",
            "Iteration 160, loss = 0.35422669\n",
            "Iteration 161, loss = 0.35561734\n",
            "Iteration 162, loss = 0.35767022\n",
            "Iteration 163, loss = 0.35188271\n",
            "Iteration 164, loss = 0.35478826\n",
            "Iteration 165, loss = 0.35314048\n",
            "Iteration 166, loss = 0.35801705\n",
            "Iteration 167, loss = 0.35959524\n",
            "Iteration 168, loss = 0.35122802\n",
            "Iteration 169, loss = 0.35899633\n",
            "Iteration 170, loss = 0.35143529\n",
            "Iteration 171, loss = 0.35136203\n",
            "Iteration 172, loss = 0.35654720\n",
            "Iteration 173, loss = 0.35659448\n",
            "Iteration 174, loss = 0.35236613\n",
            "Iteration 175, loss = 0.35638533\n",
            "Iteration 176, loss = 0.35879218\n",
            "Iteration 177, loss = 0.35704565\n",
            "Iteration 178, loss = 0.35622669\n",
            "Iteration 179, loss = 0.35705457\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 180, loss = 0.33106065\n",
            "Iteration 181, loss = 0.33070179\n",
            "Iteration 182, loss = 0.33057045\n",
            "Iteration 183, loss = 0.33032757\n",
            "Iteration 184, loss = 0.33018461\n",
            "Iteration 185, loss = 0.33000833\n",
            "Iteration 186, loss = 0.33009156\n",
            "Iteration 187, loss = 0.32987464\n",
            "Iteration 188, loss = 0.32980230\n",
            "Iteration 189, loss = 0.32957926\n",
            "Iteration 190, loss = 0.32946768\n",
            "Iteration 191, loss = 0.32936301\n",
            "Iteration 192, loss = 0.32927035\n",
            "Iteration 193, loss = 0.32901254\n",
            "Iteration 194, loss = 0.32911249\n",
            "Iteration 195, loss = 0.32877031\n",
            "Iteration 196, loss = 0.32879268\n",
            "Iteration 197, loss = 0.32873222\n",
            "Iteration 198, loss = 0.32845140\n",
            "Iteration 199, loss = 0.32830721\n",
            "Iteration 200, loss = 0.32826570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70353302\n",
            "Iteration 3, loss = 0.68257706\n",
            "Iteration 4, loss = 0.66626585\n",
            "Iteration 5, loss = 0.65608291\n",
            "Iteration 6, loss = 0.64125148\n",
            "Iteration 7, loss = 0.63499107\n",
            "Iteration 8, loss = 0.62266418\n",
            "Iteration 9, loss = 0.61568710\n",
            "Iteration 10, loss = 0.60939822\n",
            "Iteration 11, loss = 0.60132227\n",
            "Iteration 12, loss = 0.59759139\n",
            "Iteration 13, loss = 0.59272289\n",
            "Iteration 14, loss = 0.58896363\n",
            "Iteration 15, loss = 0.58976399\n",
            "Iteration 16, loss = 0.58529882\n",
            "Iteration 17, loss = 0.58076276\n",
            "Iteration 18, loss = 0.57927398\n",
            "Iteration 19, loss = 0.57974298\n",
            "Iteration 20, loss = 0.57096163\n",
            "Iteration 21, loss = 0.57791528\n",
            "Iteration 22, loss = 0.57066669\n",
            "Iteration 23, loss = 0.56719649\n",
            "Iteration 24, loss = 0.57057298\n",
            "Iteration 25, loss = 0.57011058\n",
            "Iteration 26, loss = 0.57115150\n",
            "Iteration 27, loss = 0.56459642\n",
            "Iteration 28, loss = 0.57136034\n",
            "Iteration 29, loss = 0.57604088\n",
            "Iteration 30, loss = 0.57183294\n",
            "Iteration 31, loss = 0.56623911\n",
            "Iteration 32, loss = 0.56791966\n",
            "Iteration 33, loss = 0.56761250\n",
            "Iteration 34, loss = 0.56704599\n",
            "Iteration 35, loss = 0.56736220\n",
            "Iteration 36, loss = 0.56402599\n",
            "Iteration 37, loss = 0.56477757\n",
            "Iteration 38, loss = 0.56440459\n",
            "Iteration 39, loss = 0.56192165\n",
            "Iteration 40, loss = 0.56419057\n",
            "Iteration 41, loss = 0.56904051\n",
            "Iteration 42, loss = 0.56622646\n",
            "Iteration 43, loss = 0.56087914\n",
            "Iteration 44, loss = 0.56611376\n",
            "Iteration 45, loss = 0.56275958\n",
            "Iteration 46, loss = 0.56796995\n",
            "Iteration 47, loss = 0.56266457\n",
            "Iteration 48, loss = 0.56496838\n",
            "Iteration 49, loss = 0.56819181\n",
            "Iteration 50, loss = 0.56094656\n",
            "Iteration 51, loss = 0.56994134\n",
            "Iteration 52, loss = 0.56588539\n",
            "Iteration 53, loss = 0.56206854\n",
            "Iteration 54, loss = 0.56557701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 55, loss = 0.53458211\n",
            "Iteration 56, loss = 0.51516202\n",
            "Iteration 57, loss = 0.50627239\n",
            "Iteration 58, loss = 0.49950114\n",
            "Iteration 59, loss = 0.49176815\n",
            "Iteration 60, loss = 0.48481529\n",
            "Iteration 61, loss = 0.48044993\n",
            "Iteration 62, loss = 0.47576001\n",
            "Iteration 63, loss = 0.49196338\n",
            "Iteration 64, loss = 0.48907967\n",
            "Iteration 65, loss = 0.50989128\n",
            "Iteration 66, loss = 0.51423869\n",
            "Iteration 67, loss = 0.51195867\n",
            "Iteration 68, loss = 0.51123954\n",
            "Iteration 69, loss = 0.51395956\n",
            "Iteration 70, loss = 0.50466992\n",
            "Iteration 71, loss = 0.51141083\n",
            "Iteration 72, loss = 0.49998904\n",
            "Iteration 73, loss = 0.51649356\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 74, loss = 0.43687970\n",
            "Iteration 75, loss = 0.43378984\n",
            "Iteration 76, loss = 0.43108279\n",
            "Iteration 77, loss = 0.42841603\n",
            "Iteration 78, loss = 0.42530026\n",
            "Iteration 79, loss = 0.42251503\n",
            "Iteration 80, loss = 0.42046687\n",
            "Iteration 81, loss = 0.41729450\n",
            "Iteration 82, loss = 0.41464956\n",
            "Iteration 83, loss = 0.41232045\n",
            "Iteration 84, loss = 0.40965350\n",
            "Iteration 85, loss = 0.40730303\n",
            "Iteration 86, loss = 0.40468183\n",
            "Iteration 87, loss = 0.40224615\n",
            "Iteration 88, loss = 0.39934988\n",
            "Iteration 89, loss = 0.39710338\n",
            "Iteration 90, loss = 0.39488020\n",
            "Iteration 91, loss = 0.39211560\n",
            "Iteration 92, loss = 0.38987846\n",
            "Iteration 93, loss = 0.38765641\n",
            "Iteration 94, loss = 0.38525629\n",
            "Iteration 95, loss = 0.38311876\n",
            "Iteration 96, loss = 0.38078555\n",
            "Iteration 97, loss = 0.37875537\n",
            "Iteration 98, loss = 0.37657634\n",
            "Iteration 99, loss = 0.37442334\n",
            "Iteration 100, loss = 0.37237749\n",
            "Iteration 101, loss = 0.37043304\n",
            "Iteration 102, loss = 0.36851775\n",
            "Iteration 103, loss = 0.36685551\n",
            "Iteration 104, loss = 0.36436628\n",
            "Iteration 105, loss = 0.36236862\n",
            "Iteration 106, loss = 0.36062311\n",
            "Iteration 107, loss = 0.35866247\n",
            "Iteration 108, loss = 0.35691921\n",
            "Iteration 109, loss = 0.35469003\n",
            "Iteration 110, loss = 0.35315769\n",
            "Iteration 111, loss = 0.35118407\n",
            "Iteration 112, loss = 0.34939224\n",
            "Iteration 113, loss = 0.34817385\n",
            "Iteration 114, loss = 0.34712401\n",
            "Iteration 115, loss = 0.34477747\n",
            "Iteration 116, loss = 0.34248899\n",
            "Iteration 117, loss = 0.34125093\n",
            "Iteration 118, loss = 0.33989118\n",
            "Iteration 119, loss = 0.33861344\n",
            "Iteration 120, loss = 0.33678444\n",
            "Iteration 121, loss = 0.33669262\n",
            "Iteration 122, loss = 0.33436405\n",
            "Iteration 123, loss = 0.33377944\n",
            "Iteration 124, loss = 0.33092863\n",
            "Iteration 125, loss = 0.33016809\n",
            "Iteration 126, loss = 0.32797730\n",
            "Iteration 127, loss = 0.32706281\n",
            "Iteration 128, loss = 0.32550707\n",
            "Iteration 129, loss = 0.32458668\n",
            "Iteration 130, loss = 0.32311451\n",
            "Iteration 131, loss = 0.32293799\n",
            "Iteration 132, loss = 0.32104074\n",
            "Iteration 133, loss = 0.32055005\n",
            "Iteration 134, loss = 0.31824396\n",
            "Iteration 135, loss = 0.31734441\n",
            "Iteration 136, loss = 0.31634003\n",
            "Iteration 137, loss = 0.31560549\n",
            "Iteration 138, loss = 0.31428498\n",
            "Iteration 139, loss = 0.31385075\n",
            "Iteration 140, loss = 0.31336763\n",
            "Iteration 141, loss = 0.31355896\n",
            "Iteration 142, loss = 0.31044315\n",
            "Iteration 143, loss = 0.30942509\n",
            "Iteration 144, loss = 0.30881634\n",
            "Iteration 145, loss = 0.30759166\n",
            "Iteration 146, loss = 0.30739351\n",
            "Iteration 147, loss = 0.30534769\n",
            "Iteration 148, loss = 0.30586832\n",
            "Iteration 149, loss = 0.30623017\n",
            "Iteration 150, loss = 0.30329336\n",
            "Iteration 151, loss = 0.30393233\n",
            "Iteration 152, loss = 0.30268674\n",
            "Iteration 153, loss = 0.30115602\n",
            "Iteration 154, loss = 0.30053117\n",
            "Iteration 155, loss = 0.29989751\n",
            "Iteration 156, loss = 0.29920414\n",
            "Iteration 157, loss = 0.29703147\n",
            "Iteration 158, loss = 0.29909251\n",
            "Iteration 159, loss = 0.29764691\n",
            "Iteration 160, loss = 0.29735397\n",
            "Iteration 161, loss = 0.29554522\n",
            "Iteration 162, loss = 0.29289471\n",
            "Iteration 163, loss = 0.29583404\n",
            "Iteration 164, loss = 0.29293918\n",
            "Iteration 165, loss = 0.29314756\n",
            "Iteration 166, loss = 0.29510265\n",
            "Iteration 167, loss = 0.29318903\n",
            "Iteration 168, loss = 0.29166200\n",
            "Iteration 169, loss = 0.29348441\n",
            "Iteration 170, loss = 0.29266831\n",
            "Iteration 171, loss = 0.29128388\n",
            "Iteration 172, loss = 0.29367141\n",
            "Iteration 173, loss = 0.29160179\n",
            "Iteration 174, loss = 0.28902298\n",
            "Iteration 175, loss = 0.29033521\n",
            "Iteration 176, loss = 0.29032959\n",
            "Iteration 177, loss = 0.28955256\n",
            "Iteration 178, loss = 0.28756456\n",
            "Iteration 179, loss = 0.28813924\n",
            "Iteration 180, loss = 0.28840234\n",
            "Iteration 181, loss = 0.28864021\n",
            "Iteration 182, loss = 0.29083107\n",
            "Iteration 183, loss = 0.28724565\n",
            "Iteration 184, loss = 0.28542635\n",
            "Iteration 185, loss = 0.28633856\n",
            "Iteration 186, loss = 0.28943103\n",
            "Iteration 187, loss = 0.29110666\n",
            "Iteration 188, loss = 0.28670493\n",
            "Iteration 189, loss = 0.28658750\n",
            "Iteration 190, loss = 0.29090205\n",
            "Iteration 191, loss = 0.28353119\n",
            "Iteration 192, loss = 0.28081831\n",
            "Iteration 193, loss = 0.28203093\n",
            "Iteration 194, loss = 0.29104899\n",
            "Iteration 195, loss = 0.28166300\n",
            "Iteration 196, loss = 0.28547852\n",
            "Iteration 197, loss = 0.28669192\n",
            "Iteration 198, loss = 0.28698745\n",
            "Iteration 199, loss = 0.29549552\n",
            "Iteration 200, loss = 0.27794671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69376505\n",
            "Iteration 3, loss = 0.67892872\n",
            "Iteration 4, loss = 0.66295166\n",
            "Iteration 5, loss = 0.65138907\n",
            "Iteration 6, loss = 0.64039478\n",
            "Iteration 7, loss = 0.63235165\n",
            "Iteration 8, loss = 0.62502518\n",
            "Iteration 9, loss = 0.62209048\n",
            "Iteration 10, loss = 0.61424974\n",
            "Iteration 11, loss = 0.60707419\n",
            "Iteration 12, loss = 0.60192336\n",
            "Iteration 13, loss = 0.59796218\n",
            "Iteration 14, loss = 0.59564780\n",
            "Iteration 15, loss = 0.59003590\n",
            "Iteration 16, loss = 0.59033913\n",
            "Iteration 17, loss = 0.58978326\n",
            "Iteration 18, loss = 0.58524514\n",
            "Iteration 19, loss = 0.57955583\n",
            "Iteration 20, loss = 0.58157997\n",
            "Iteration 21, loss = 0.57853919\n",
            "Iteration 22, loss = 0.57794614\n",
            "Iteration 23, loss = 0.57685526\n",
            "Iteration 24, loss = 0.57342202\n",
            "Iteration 25, loss = 0.57250170\n",
            "Iteration 26, loss = 0.57489265\n",
            "Iteration 27, loss = 0.57278502\n",
            "Iteration 28, loss = 0.57385844\n",
            "Iteration 29, loss = 0.57095194\n",
            "Iteration 30, loss = 0.57138429\n",
            "Iteration 31, loss = 0.56954231\n",
            "Iteration 32, loss = 0.56682406\n",
            "Iteration 33, loss = 0.57088960\n",
            "Iteration 34, loss = 0.56892709\n",
            "Iteration 35, loss = 0.56928112\n",
            "Iteration 36, loss = 0.57113319\n",
            "Iteration 37, loss = 0.56884906\n",
            "Iteration 38, loss = 0.56815953\n",
            "Iteration 39, loss = 0.57287597\n",
            "Iteration 40, loss = 0.56785836\n",
            "Iteration 41, loss = 0.56645092\n",
            "Iteration 42, loss = 0.56500862\n",
            "Iteration 43, loss = 0.63176725\n",
            "Iteration 44, loss = 0.62029747\n",
            "Iteration 45, loss = 0.60441839\n",
            "Iteration 46, loss = 0.59728255\n",
            "Iteration 47, loss = 0.59094794\n",
            "Iteration 48, loss = 0.58473023\n",
            "Iteration 49, loss = 0.58262626\n",
            "Iteration 50, loss = 0.58230540\n",
            "Iteration 51, loss = 0.57717520\n",
            "Iteration 52, loss = 0.57356172\n",
            "Iteration 53, loss = 0.57341133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.55669134\n",
            "Iteration 55, loss = 0.55303694\n",
            "Iteration 56, loss = 0.55085667\n",
            "Iteration 57, loss = 0.54896236\n",
            "Iteration 58, loss = 0.54690020\n",
            "Iteration 59, loss = 0.54551441\n",
            "Iteration 60, loss = 0.54353831\n",
            "Iteration 61, loss = 0.54131643\n",
            "Iteration 62, loss = 0.53884165\n",
            "Iteration 63, loss = 0.53556728\n",
            "Iteration 64, loss = 0.53470933\n",
            "Iteration 65, loss = 0.53078075\n",
            "Iteration 66, loss = 0.52904204\n",
            "Iteration 67, loss = 0.52729162\n",
            "Iteration 68, loss = 0.52580180\n",
            "Iteration 69, loss = 0.52636222\n",
            "Iteration 70, loss = 0.52596870\n",
            "Iteration 71, loss = 0.52660571\n",
            "Iteration 72, loss = 0.52634538\n",
            "Iteration 73, loss = 0.53401429\n",
            "Iteration 74, loss = 0.52654603\n",
            "Iteration 75, loss = 0.53504247\n",
            "Iteration 76, loss = 0.53076889\n",
            "Iteration 77, loss = 0.53182932\n",
            "Iteration 78, loss = 0.53190474\n",
            "Iteration 79, loss = 0.53424840\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 80, loss = 0.51249336\n",
            "Iteration 81, loss = 0.48562298\n",
            "Iteration 82, loss = 0.48394812\n",
            "Iteration 83, loss = 0.48219859\n",
            "Iteration 84, loss = 0.48108117\n",
            "Iteration 85, loss = 0.47959497\n",
            "Iteration 86, loss = 0.47807062\n",
            "Iteration 87, loss = 0.47673426\n",
            "Iteration 88, loss = 0.47490858\n",
            "Iteration 89, loss = 0.47369461\n",
            "Iteration 90, loss = 0.47252778\n",
            "Iteration 91, loss = 0.47012708\n",
            "Iteration 92, loss = 0.46840498\n",
            "Iteration 93, loss = 0.46737139\n",
            "Iteration 94, loss = 0.46565747\n",
            "Iteration 95, loss = 0.46423585\n",
            "Iteration 96, loss = 0.46218007\n",
            "Iteration 97, loss = 0.46091082\n",
            "Iteration 98, loss = 0.45935488\n",
            "Iteration 99, loss = 0.45778367\n",
            "Iteration 100, loss = 0.45585232\n",
            "Iteration 101, loss = 0.45472140\n",
            "Iteration 102, loss = 0.45296019\n",
            "Iteration 103, loss = 0.45186436\n",
            "Iteration 104, loss = 0.45002405\n",
            "Iteration 105, loss = 0.44850527\n",
            "Iteration 106, loss = 0.44708493\n",
            "Iteration 107, loss = 0.44540331\n",
            "Iteration 108, loss = 0.44389909\n",
            "Iteration 109, loss = 0.44172920\n",
            "Iteration 110, loss = 0.44065901\n",
            "Iteration 111, loss = 0.43895033\n",
            "Iteration 112, loss = 0.43737301\n",
            "Iteration 113, loss = 0.43635928\n",
            "Iteration 114, loss = 0.43522752\n",
            "Iteration 115, loss = 0.43269647\n",
            "Iteration 116, loss = 0.43078989\n",
            "Iteration 117, loss = 0.42946299\n",
            "Iteration 118, loss = 0.42887817\n",
            "Iteration 119, loss = 0.42645422\n",
            "Iteration 120, loss = 0.42614421\n",
            "Iteration 121, loss = 0.42398574\n",
            "Iteration 122, loss = 0.42352356\n",
            "Iteration 123, loss = 0.42112722\n",
            "Iteration 124, loss = 0.42161304\n",
            "Iteration 125, loss = 0.41872877\n",
            "Iteration 126, loss = 0.41780665\n",
            "Iteration 127, loss = 0.41567402\n",
            "Iteration 128, loss = 0.41533185\n",
            "Iteration 129, loss = 0.41373291\n",
            "Iteration 130, loss = 0.41243395\n",
            "Iteration 131, loss = 0.41185664\n",
            "Iteration 132, loss = 0.40865463\n",
            "Iteration 133, loss = 0.40850180\n",
            "Iteration 134, loss = 0.40798843\n",
            "Iteration 135, loss = 0.40632851\n",
            "Iteration 136, loss = 0.40485562\n",
            "Iteration 137, loss = 0.40527852\n",
            "Iteration 138, loss = 0.40391144\n",
            "Iteration 139, loss = 0.40176093\n",
            "Iteration 140, loss = 0.40121678\n",
            "Iteration 141, loss = 0.40106056\n",
            "Iteration 142, loss = 0.39973061\n",
            "Iteration 143, loss = 0.39736865\n",
            "Iteration 144, loss = 0.39807428\n",
            "Iteration 145, loss = 0.39483252\n",
            "Iteration 146, loss = 0.39458413\n",
            "Iteration 147, loss = 0.39544349\n",
            "Iteration 148, loss = 0.39478376\n",
            "Iteration 149, loss = 0.39447280\n",
            "Iteration 150, loss = 0.39268191\n",
            "Iteration 151, loss = 0.39265085\n",
            "Iteration 152, loss = 0.39580446\n",
            "Iteration 153, loss = 0.39265246\n",
            "Iteration 154, loss = 0.39080812\n",
            "Iteration 155, loss = 0.38772026\n",
            "Iteration 156, loss = 0.38887285\n",
            "Iteration 157, loss = 0.38501811\n",
            "Iteration 158, loss = 0.38796260\n",
            "Iteration 159, loss = 0.38701266\n",
            "Iteration 160, loss = 0.38638606\n",
            "Iteration 161, loss = 0.38510134\n",
            "Iteration 162, loss = 0.38465810\n",
            "Iteration 163, loss = 0.39205720\n",
            "Iteration 164, loss = 0.38424932\n",
            "Iteration 165, loss = 0.38358046\n",
            "Iteration 166, loss = 0.38653333\n",
            "Iteration 167, loss = 0.38408905\n",
            "Iteration 168, loss = 0.38845473\n",
            "Iteration 169, loss = 0.38978787\n",
            "Iteration 170, loss = 0.38607697\n",
            "Iteration 171, loss = 0.38224523\n",
            "Iteration 172, loss = 0.38417940\n",
            "Iteration 173, loss = 0.38747903\n",
            "Iteration 174, loss = 0.38535698\n",
            "Iteration 175, loss = 0.38192255\n",
            "Iteration 176, loss = 0.39152426\n",
            "Iteration 177, loss = 0.38336808\n",
            "Iteration 178, loss = 0.38556697\n",
            "Iteration 179, loss = 0.38295368\n",
            "Iteration 180, loss = 0.37865136\n",
            "Iteration 181, loss = 0.38879621\n",
            "Iteration 182, loss = 0.38164739\n",
            "Iteration 183, loss = 0.38387899\n",
            "Iteration 184, loss = 0.38031064\n",
            "Iteration 185, loss = 0.38216936\n",
            "Iteration 186, loss = 0.38695462\n",
            "Iteration 187, loss = 0.38501768\n",
            "Iteration 188, loss = 0.38386967\n",
            "Iteration 189, loss = 0.37387924\n",
            "Iteration 190, loss = 0.38670384\n",
            "Iteration 191, loss = 0.38094928\n",
            "Iteration 192, loss = 0.38059363\n",
            "Iteration 193, loss = 0.38594844\n",
            "Iteration 194, loss = 0.37835108\n",
            "Iteration 195, loss = 0.38214790\n",
            "Iteration 196, loss = 0.38122636\n",
            "Iteration 197, loss = 0.37233983\n",
            "Iteration 198, loss = 0.38083382\n",
            "Iteration 199, loss = 0.38188065\n",
            "Iteration 200, loss = 0.37421507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69506319\n",
            "Iteration 3, loss = 0.68015199\n",
            "Iteration 4, loss = 0.66398936\n",
            "Iteration 5, loss = 0.65008445\n",
            "Iteration 6, loss = 0.63616290\n",
            "Iteration 7, loss = 0.62877581\n",
            "Iteration 8, loss = 0.62224799\n",
            "Iteration 9, loss = 0.61429499\n",
            "Iteration 10, loss = 0.60822082\n",
            "Iteration 11, loss = 0.60106441\n",
            "Iteration 12, loss = 0.60035004\n",
            "Iteration 13, loss = 0.59189739\n",
            "Iteration 14, loss = 0.59029287\n",
            "Iteration 15, loss = 0.58263675\n",
            "Iteration 16, loss = 0.58654730\n",
            "Iteration 17, loss = 0.58071767\n",
            "Iteration 18, loss = 0.58046528\n",
            "Iteration 19, loss = 0.57743524\n",
            "Iteration 20, loss = 0.57391559\n",
            "Iteration 21, loss = 0.57099222\n",
            "Iteration 22, loss = 0.57304573\n",
            "Iteration 23, loss = 0.57126364\n",
            "Iteration 24, loss = 0.56852045\n",
            "Iteration 25, loss = 0.57081394\n",
            "Iteration 26, loss = 0.56817565\n",
            "Iteration 27, loss = 0.56521035\n",
            "Iteration 28, loss = 0.56585633\n",
            "Iteration 29, loss = 0.56592478\n",
            "Iteration 30, loss = 0.56608266\n",
            "Iteration 31, loss = 0.56590030\n",
            "Iteration 32, loss = 0.56675464\n",
            "Iteration 33, loss = 0.56618353\n",
            "Iteration 34, loss = 0.56372644\n",
            "Iteration 35, loss = 0.56354847\n",
            "Iteration 36, loss = 0.56128553\n",
            "Iteration 37, loss = 0.56534758\n",
            "Iteration 38, loss = 0.56578976\n",
            "Iteration 39, loss = 0.56273821\n",
            "Iteration 40, loss = 0.56221639\n",
            "Iteration 41, loss = 0.55959790\n",
            "Iteration 42, loss = 0.56477221\n",
            "Iteration 43, loss = 0.56129165\n",
            "Iteration 44, loss = 0.56426750\n",
            "Iteration 45, loss = 0.56179666\n",
            "Iteration 46, loss = 0.56539295\n",
            "Iteration 47, loss = 0.56383077\n",
            "Iteration 48, loss = 0.56174339\n",
            "Iteration 49, loss = 0.56434784\n",
            "Iteration 50, loss = 0.56496346\n",
            "Iteration 51, loss = 0.56344364\n",
            "Iteration 52, loss = 0.56160472\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 53, loss = 0.52277925\n",
            "Iteration 54, loss = 0.51131248\n",
            "Iteration 55, loss = 0.50468063\n",
            "Iteration 56, loss = 0.49909215\n",
            "Iteration 57, loss = 0.49368596\n",
            "Iteration 58, loss = 0.48756083\n",
            "Iteration 59, loss = 0.48061198\n",
            "Iteration 60, loss = 0.47781663\n",
            "Iteration 61, loss = 0.48085844\n",
            "Iteration 62, loss = 0.48879784\n",
            "Iteration 63, loss = 0.50161695\n",
            "Iteration 64, loss = 0.50196094\n",
            "Iteration 65, loss = 0.50502929\n",
            "Iteration 66, loss = 0.50995122\n",
            "Iteration 67, loss = 0.50726073\n",
            "Iteration 68, loss = 0.50683751\n",
            "Iteration 69, loss = 0.51671586\n",
            "Iteration 70, loss = 0.50917369\n",
            "Iteration 71, loss = 0.51124518\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 72, loss = 0.44033367\n",
            "Iteration 73, loss = 0.43769319\n",
            "Iteration 74, loss = 0.43551471\n",
            "Iteration 75, loss = 0.43353582\n",
            "Iteration 76, loss = 0.43121276\n",
            "Iteration 77, loss = 0.42907060\n",
            "Iteration 78, loss = 0.42697002\n",
            "Iteration 79, loss = 0.42476113\n",
            "Iteration 80, loss = 0.42295036\n",
            "Iteration 81, loss = 0.42077273\n",
            "Iteration 82, loss = 0.41846499\n",
            "Iteration 83, loss = 0.41633304\n",
            "Iteration 84, loss = 0.41437555\n",
            "Iteration 85, loss = 0.41240735\n",
            "Iteration 86, loss = 0.41023783\n",
            "Iteration 87, loss = 0.40839629\n",
            "Iteration 88, loss = 0.40490706\n",
            "Iteration 89, loss = 0.39612194\n",
            "Iteration 90, loss = 0.39328137\n",
            "Iteration 91, loss = 0.39123026\n",
            "Iteration 92, loss = 0.38888658\n",
            "Iteration 93, loss = 0.38635176\n",
            "Iteration 94, loss = 0.38432207\n",
            "Iteration 95, loss = 0.38230102\n",
            "Iteration 96, loss = 0.37991747\n",
            "Iteration 97, loss = 0.37887585\n",
            "Iteration 98, loss = 0.37569421\n",
            "Iteration 99, loss = 0.37360303\n",
            "Iteration 100, loss = 0.37184523\n",
            "Iteration 101, loss = 0.36981110\n",
            "Iteration 102, loss = 0.36834556\n",
            "Iteration 103, loss = 0.36608307\n",
            "Iteration 104, loss = 0.36372457\n",
            "Iteration 105, loss = 0.36198263\n",
            "Iteration 106, loss = 0.35958873\n",
            "Iteration 107, loss = 0.35801013\n",
            "Iteration 108, loss = 0.35648514\n",
            "Iteration 109, loss = 0.35510778\n",
            "Iteration 110, loss = 0.35286868\n",
            "Iteration 111, loss = 0.35200152\n",
            "Iteration 112, loss = 0.34947186\n",
            "Iteration 113, loss = 0.34805164\n",
            "Iteration 114, loss = 0.34665950\n",
            "Iteration 115, loss = 0.34483899\n",
            "Iteration 116, loss = 0.34311528\n",
            "Iteration 117, loss = 0.34169428\n",
            "Iteration 118, loss = 0.34037981\n",
            "Iteration 119, loss = 0.33838283\n",
            "Iteration 120, loss = 0.33710708\n",
            "Iteration 121, loss = 0.33655264\n",
            "Iteration 122, loss = 0.33432915\n",
            "Iteration 123, loss = 0.33440133\n",
            "Iteration 124, loss = 0.33109610\n",
            "Iteration 125, loss = 0.33031279\n",
            "Iteration 126, loss = 0.32946940\n",
            "Iteration 127, loss = 0.32774965\n",
            "Iteration 128, loss = 0.32586448\n",
            "Iteration 129, loss = 0.32511084\n",
            "Iteration 130, loss = 0.32463036\n",
            "Iteration 131, loss = 0.32338712\n",
            "Iteration 132, loss = 0.32198228\n",
            "Iteration 133, loss = 0.32133955\n",
            "Iteration 134, loss = 0.31933294\n",
            "Iteration 135, loss = 0.31781533\n",
            "Iteration 136, loss = 0.31731546\n",
            "Iteration 137, loss = 0.31583128\n",
            "Iteration 138, loss = 0.31708215\n",
            "Iteration 139, loss = 0.31445756\n",
            "Iteration 140, loss = 0.31314478\n",
            "Iteration 141, loss = 0.31107108\n",
            "Iteration 142, loss = 0.31159378\n",
            "Iteration 143, loss = 0.31209860\n",
            "Iteration 144, loss = 0.30941583\n",
            "Iteration 145, loss = 0.30743731\n",
            "Iteration 146, loss = 0.30726938\n",
            "Iteration 147, loss = 0.30651599\n",
            "Iteration 148, loss = 0.30670531\n",
            "Iteration 149, loss = 0.30683387\n",
            "Iteration 150, loss = 0.30481535\n",
            "Iteration 151, loss = 0.30671467\n",
            "Iteration 152, loss = 0.30261807\n",
            "Iteration 153, loss = 0.30456352\n",
            "Iteration 154, loss = 0.30198397\n",
            "Iteration 155, loss = 0.30233508\n",
            "Iteration 156, loss = 0.30000672\n",
            "Iteration 157, loss = 0.30111376\n",
            "Iteration 158, loss = 0.29827755\n",
            "Iteration 159, loss = 0.29993066\n",
            "Iteration 160, loss = 0.29805282\n",
            "Iteration 161, loss = 0.29643060\n",
            "Iteration 162, loss = 0.30008116\n",
            "Iteration 163, loss = 0.29754082\n",
            "Iteration 164, loss = 0.29823879\n",
            "Iteration 165, loss = 0.29304457\n",
            "Iteration 166, loss = 0.29460270\n",
            "Iteration 167, loss = 0.29479196\n",
            "Iteration 168, loss = 0.29410959\n",
            "Iteration 169, loss = 0.29665421\n",
            "Iteration 170, loss = 0.29636292\n",
            "Iteration 171, loss = 0.29261267\n",
            "Iteration 172, loss = 0.29288520\n",
            "Iteration 173, loss = 0.29438079\n",
            "Iteration 174, loss = 0.28994800\n",
            "Iteration 175, loss = 0.29411878\n",
            "Iteration 176, loss = 0.28928693\n",
            "Iteration 177, loss = 0.29113597\n",
            "Iteration 178, loss = 0.28953717\n",
            "Iteration 179, loss = 0.29158327\n",
            "Iteration 180, loss = 0.29233533\n",
            "Iteration 181, loss = 0.29206749\n",
            "Iteration 182, loss = 0.29077752\n",
            "Iteration 183, loss = 0.28486545\n",
            "Iteration 184, loss = 0.28961358\n",
            "Iteration 185, loss = 0.28907880\n",
            "Iteration 186, loss = 0.28932059\n",
            "Iteration 187, loss = 0.28893395\n",
            "Iteration 188, loss = 0.29043992\n",
            "Iteration 189, loss = 0.28707211\n",
            "Iteration 190, loss = 0.29279921\n",
            "Iteration 191, loss = 0.28701252\n",
            "Iteration 192, loss = 0.28568651\n",
            "Iteration 193, loss = 0.29295514\n",
            "Iteration 194, loss = 0.28333147\n",
            "Iteration 195, loss = 0.29528998\n",
            "Iteration 196, loss = 0.28855784\n",
            "Iteration 197, loss = 0.29405590\n",
            "Iteration 198, loss = 0.28502660\n",
            "Iteration 199, loss = 0.29426637\n",
            "Iteration 200, loss = 0.28439288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68964793\n",
            "Iteration 3, loss = 0.67033974\n",
            "Iteration 4, loss = 0.65438465\n",
            "Iteration 5, loss = 0.64458362\n",
            "Iteration 6, loss = 0.63287201\n",
            "Iteration 7, loss = 0.62568396\n",
            "Iteration 8, loss = 0.61790354\n",
            "Iteration 9, loss = 0.61009485\n",
            "Iteration 10, loss = 0.60484346\n",
            "Iteration 11, loss = 0.60046883\n",
            "Iteration 12, loss = 0.59608273\n",
            "Iteration 13, loss = 0.59024392\n",
            "Iteration 14, loss = 0.58702078\n",
            "Iteration 15, loss = 0.58408672\n",
            "Iteration 16, loss = 0.57869881\n",
            "Iteration 17, loss = 0.57870020\n",
            "Iteration 18, loss = 0.57601844\n",
            "Iteration 19, loss = 0.57579233\n",
            "Iteration 20, loss = 0.57132590\n",
            "Iteration 21, loss = 0.57105163\n",
            "Iteration 22, loss = 0.57195608\n",
            "Iteration 23, loss = 0.57091746\n",
            "Iteration 24, loss = 0.56758998\n",
            "Iteration 25, loss = 0.56690077\n",
            "Iteration 26, loss = 0.56370240\n",
            "Iteration 27, loss = 0.56580989\n",
            "Iteration 28, loss = 0.56780207\n",
            "Iteration 29, loss = 0.56428028\n",
            "Iteration 30, loss = 0.56244908\n",
            "Iteration 31, loss = 0.56831742\n",
            "Iteration 32, loss = 0.56333737\n",
            "Iteration 33, loss = 0.56436315\n",
            "Iteration 34, loss = 0.56417219\n",
            "Iteration 35, loss = 0.56389821\n",
            "Iteration 36, loss = 0.56221344\n",
            "Iteration 37, loss = 0.56495847\n",
            "Iteration 38, loss = 0.56341037\n",
            "Iteration 39, loss = 0.56308072\n",
            "Iteration 40, loss = 0.56181569\n",
            "Iteration 41, loss = 0.56197470\n",
            "Iteration 42, loss = 0.56377041\n",
            "Iteration 43, loss = 0.56222904\n",
            "Iteration 44, loss = 0.56344835\n",
            "Iteration 45, loss = 0.56377135\n",
            "Iteration 46, loss = 0.56241636\n",
            "Iteration 47, loss = 0.56412733\n",
            "Iteration 48, loss = 0.56530914\n",
            "Iteration 49, loss = 0.56412524\n",
            "Iteration 50, loss = 0.56587419\n",
            "Iteration 51, loss = 0.56203848\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 52, loss = 0.52279574\n",
            "Iteration 53, loss = 0.51155550\n",
            "Iteration 54, loss = 0.50517265\n",
            "Iteration 55, loss = 0.49871232\n",
            "Iteration 56, loss = 0.49321800\n",
            "Iteration 57, loss = 0.48854141\n",
            "Iteration 58, loss = 0.48738746\n",
            "Iteration 59, loss = 0.47653045\n",
            "Iteration 60, loss = 0.48384026\n",
            "Iteration 61, loss = 0.48905403\n",
            "Iteration 62, loss = 0.50202487\n",
            "Iteration 63, loss = 0.49926225\n",
            "Iteration 64, loss = 0.50490719\n",
            "Iteration 65, loss = 0.50952857\n",
            "Iteration 66, loss = 0.51709154\n",
            "Iteration 67, loss = 0.49995120\n",
            "Iteration 68, loss = 0.51284904\n",
            "Iteration 69, loss = 0.51590306\n",
            "Iteration 70, loss = 0.50540862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 71, loss = 0.44012083\n",
            "Iteration 72, loss = 0.43749992\n",
            "Iteration 73, loss = 0.43577591\n",
            "Iteration 74, loss = 0.43296712\n",
            "Iteration 75, loss = 0.43135116\n",
            "Iteration 76, loss = 0.42935830\n",
            "Iteration 77, loss = 0.42713864\n",
            "Iteration 78, loss = 0.42491993\n",
            "Iteration 79, loss = 0.42291581\n",
            "Iteration 80, loss = 0.42099319\n",
            "Iteration 81, loss = 0.41873811\n",
            "Iteration 82, loss = 0.41679527\n",
            "Iteration 83, loss = 0.41457534\n",
            "Iteration 84, loss = 0.41249634\n",
            "Iteration 85, loss = 0.41082128\n",
            "Iteration 86, loss = 0.40871985\n",
            "Iteration 87, loss = 0.40716532\n",
            "Iteration 88, loss = 0.40474190\n",
            "Iteration 89, loss = 0.40280035\n",
            "Iteration 90, loss = 0.40091059\n",
            "Iteration 91, loss = 0.39907840\n",
            "Iteration 92, loss = 0.39579760\n",
            "Iteration 93, loss = 0.38729198\n",
            "Iteration 94, loss = 0.38468925\n",
            "Iteration 95, loss = 0.38189033\n",
            "Iteration 96, loss = 0.37994481\n",
            "Iteration 97, loss = 0.37789565\n",
            "Iteration 98, loss = 0.37604860\n",
            "Iteration 99, loss = 0.37391205\n",
            "Iteration 100, loss = 0.37246940\n",
            "Iteration 101, loss = 0.37005811\n",
            "Iteration 102, loss = 0.36815050\n",
            "Iteration 103, loss = 0.36663311\n",
            "Iteration 104, loss = 0.36401688\n",
            "Iteration 105, loss = 0.36234073\n",
            "Iteration 106, loss = 0.36041966\n",
            "Iteration 107, loss = 0.35903985\n",
            "Iteration 108, loss = 0.35735737\n",
            "Iteration 109, loss = 0.35582480\n",
            "Iteration 110, loss = 0.35386024\n",
            "Iteration 111, loss = 0.35170064\n",
            "Iteration 112, loss = 0.34971647\n",
            "Iteration 113, loss = 0.34775654\n",
            "Iteration 114, loss = 0.34716359\n",
            "Iteration 115, loss = 0.34640371\n",
            "Iteration 116, loss = 0.34459175\n",
            "Iteration 117, loss = 0.34223280\n",
            "Iteration 118, loss = 0.34063271\n",
            "Iteration 119, loss = 0.33958613\n",
            "Iteration 120, loss = 0.33756482\n",
            "Iteration 121, loss = 0.33593156\n",
            "Iteration 122, loss = 0.33607245\n",
            "Iteration 123, loss = 0.33409896\n",
            "Iteration 124, loss = 0.33319747\n",
            "Iteration 125, loss = 0.33088745\n",
            "Iteration 126, loss = 0.33071422\n",
            "Iteration 127, loss = 0.32956370\n",
            "Iteration 128, loss = 0.32747606\n",
            "Iteration 129, loss = 0.32565730\n",
            "Iteration 130, loss = 0.32630598\n",
            "Iteration 131, loss = 0.32337558\n",
            "Iteration 132, loss = 0.32265845\n",
            "Iteration 133, loss = 0.32173044\n",
            "Iteration 134, loss = 0.32156906\n",
            "Iteration 135, loss = 0.31999182\n",
            "Iteration 136, loss = 0.31901844\n",
            "Iteration 137, loss = 0.31705832\n",
            "Iteration 138, loss = 0.31790135\n",
            "Iteration 139, loss = 0.31499001\n",
            "Iteration 140, loss = 0.31396998\n",
            "Iteration 141, loss = 0.31325479\n",
            "Iteration 142, loss = 0.31300456\n",
            "Iteration 143, loss = 0.31234205\n",
            "Iteration 144, loss = 0.31041365\n",
            "Iteration 145, loss = 0.30992337\n",
            "Iteration 146, loss = 0.30889014\n",
            "Iteration 147, loss = 0.30776477\n",
            "Iteration 148, loss = 0.30794543\n",
            "Iteration 149, loss = 0.30716935\n",
            "Iteration 150, loss = 0.30524595\n",
            "Iteration 151, loss = 0.30793755\n",
            "Iteration 152, loss = 0.30439243\n",
            "Iteration 153, loss = 0.30484110\n",
            "Iteration 154, loss = 0.30304551\n",
            "Iteration 155, loss = 0.30355670\n",
            "Iteration 156, loss = 0.30209741\n",
            "Iteration 157, loss = 0.30213341\n",
            "Iteration 158, loss = 0.30078148\n",
            "Iteration 159, loss = 0.29819125\n",
            "Iteration 160, loss = 0.30232363\n",
            "Iteration 161, loss = 0.30033521\n",
            "Iteration 162, loss = 0.30066030\n",
            "Iteration 163, loss = 0.29867690\n",
            "Iteration 164, loss = 0.29513082\n",
            "Iteration 165, loss = 0.30088604\n",
            "Iteration 166, loss = 0.29777469\n",
            "Iteration 167, loss = 0.29486670\n",
            "Iteration 168, loss = 0.29947264\n",
            "Iteration 169, loss = 0.30156143\n",
            "Iteration 170, loss = 0.30401328\n",
            "Iteration 171, loss = 0.29442426\n",
            "Iteration 172, loss = 0.29493807\n",
            "Iteration 173, loss = 0.29473802\n",
            "Iteration 174, loss = 0.30018104\n",
            "Iteration 175, loss = 0.29526259\n",
            "Iteration 176, loss = 0.29114215\n",
            "Iteration 177, loss = 0.29236887\n",
            "Iteration 178, loss = 0.29559253\n",
            "Iteration 179, loss = 0.29341278\n",
            "Iteration 180, loss = 0.29064373\n",
            "Iteration 181, loss = 0.28988064\n",
            "Iteration 182, loss = 0.29125809\n",
            "Iteration 183, loss = 0.29090628\n",
            "Iteration 184, loss = 0.29338222\n",
            "Iteration 185, loss = 0.29062538\n",
            "Iteration 186, loss = 0.29017536\n",
            "Iteration 187, loss = 0.29181666\n",
            "Iteration 188, loss = 0.28964450\n",
            "Iteration 189, loss = 0.28504098\n",
            "Iteration 190, loss = 0.29283056\n",
            "Iteration 191, loss = 0.28416875\n",
            "Iteration 192, loss = 0.29365702\n",
            "Iteration 193, loss = 0.28816365\n",
            "Iteration 194, loss = 0.29503651\n",
            "Iteration 195, loss = 0.29141864\n",
            "Iteration 196, loss = 0.29423433\n",
            "Iteration 197, loss = 0.28611298\n",
            "Iteration 198, loss = 0.30185600\n",
            "Iteration 199, loss = 0.28787383\n",
            "Iteration 200, loss = 0.28410124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69819377\n",
            "Iteration 3, loss = 0.68051046\n",
            "Iteration 4, loss = 0.66520813\n",
            "Iteration 5, loss = 0.65185559\n",
            "Iteration 6, loss = 0.64166374\n",
            "Iteration 7, loss = 0.63075211\n",
            "Iteration 8, loss = 0.62402828\n",
            "Iteration 9, loss = 0.61886802\n",
            "Iteration 10, loss = 0.61084914\n",
            "Iteration 11, loss = 0.60350046\n",
            "Iteration 12, loss = 0.60054441\n",
            "Iteration 13, loss = 0.59836303\n",
            "Iteration 14, loss = 0.59516543\n",
            "Iteration 15, loss = 0.58764830\n",
            "Iteration 16, loss = 0.58848455\n",
            "Iteration 17, loss = 0.59177834\n",
            "Iteration 18, loss = 0.58250964\n",
            "Iteration 19, loss = 0.58446414\n",
            "Iteration 20, loss = 0.57543196\n",
            "Iteration 21, loss = 0.57940739\n",
            "Iteration 22, loss = 0.57749138\n",
            "Iteration 23, loss = 0.57830993\n",
            "Iteration 24, loss = 0.57425242\n",
            "Iteration 25, loss = 0.57236144\n",
            "Iteration 26, loss = 0.57554304\n",
            "Iteration 27, loss = 0.57205762\n",
            "Iteration 28, loss = 0.57113154\n",
            "Iteration 29, loss = 0.57096040\n",
            "Iteration 30, loss = 0.57155693\n",
            "Iteration 31, loss = 0.57006685\n",
            "Iteration 32, loss = 0.56998034\n",
            "Iteration 33, loss = 0.56986031\n",
            "Iteration 34, loss = 0.57233544\n",
            "Iteration 35, loss = 0.56719578\n",
            "Iteration 36, loss = 0.56640136\n",
            "Iteration 37, loss = 0.56994149\n",
            "Iteration 38, loss = 0.56657167\n",
            "Iteration 39, loss = 0.56569202\n",
            "Iteration 40, loss = 0.56633149\n",
            "Iteration 41, loss = 0.56541340\n",
            "Iteration 42, loss = 0.57347729\n",
            "Iteration 43, loss = 0.56803978\n",
            "Iteration 44, loss = 0.56585830\n",
            "Iteration 45, loss = 0.56777931\n",
            "Iteration 46, loss = 0.56868018\n",
            "Iteration 47, loss = 0.56933972\n",
            "Iteration 48, loss = 0.56602264\n",
            "Iteration 49, loss = 0.56787356\n",
            "Iteration 50, loss = 0.56468834\n",
            "Iteration 51, loss = 0.56733705\n",
            "Iteration 52, loss = 0.56791838\n",
            "Iteration 53, loss = 0.56765096\n",
            "Iteration 54, loss = 0.56902577\n",
            "Iteration 55, loss = 0.56656494\n",
            "Iteration 56, loss = 0.56381008\n",
            "Iteration 57, loss = 0.56713043\n",
            "Iteration 58, loss = 0.56563375\n",
            "Iteration 59, loss = 0.56831848\n",
            "Iteration 60, loss = 0.56697511\n",
            "Iteration 61, loss = 0.56683299\n",
            "Iteration 62, loss = 0.56809158\n",
            "Iteration 63, loss = 0.56878422\n",
            "Iteration 64, loss = 0.56821750\n",
            "Iteration 65, loss = 0.56778559\n",
            "Iteration 66, loss = 0.56765210\n",
            "Iteration 67, loss = 0.56629198\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 68, loss = 0.52923014\n",
            "Iteration 69, loss = 0.51868001\n",
            "Iteration 70, loss = 0.51314626\n",
            "Iteration 71, loss = 0.50755400\n",
            "Iteration 72, loss = 0.50259580\n",
            "Iteration 73, loss = 0.49757134\n",
            "Iteration 74, loss = 0.49710628\n",
            "Iteration 75, loss = 0.49361152\n",
            "Iteration 76, loss = 0.49886746\n",
            "Iteration 77, loss = 0.50526346\n",
            "Iteration 78, loss = 0.50682879\n",
            "Iteration 79, loss = 0.52176554\n",
            "Iteration 80, loss = 0.51646047\n",
            "Iteration 81, loss = 0.51635515\n",
            "Iteration 82, loss = 0.52103564\n",
            "Iteration 83, loss = 0.52514717\n",
            "Iteration 84, loss = 0.51860473\n",
            "Iteration 85, loss = 0.52502929\n",
            "Iteration 86, loss = 0.52161264\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.45615182\n",
            "Iteration 88, loss = 0.45247267\n",
            "Iteration 89, loss = 0.44974276\n",
            "Iteration 90, loss = 0.44770618\n",
            "Iteration 91, loss = 0.44532973\n",
            "Iteration 92, loss = 0.44299990\n",
            "Iteration 93, loss = 0.44048477\n",
            "Iteration 94, loss = 0.43833372\n",
            "Iteration 95, loss = 0.43624760\n",
            "Iteration 96, loss = 0.43393208\n",
            "Iteration 97, loss = 0.43139696\n",
            "Iteration 98, loss = 0.42946746\n",
            "Iteration 99, loss = 0.42696483\n",
            "Iteration 100, loss = 0.42477532\n",
            "Iteration 101, loss = 0.42294794\n",
            "Iteration 102, loss = 0.42067314\n",
            "Iteration 103, loss = 0.41879290\n",
            "Iteration 104, loss = 0.41681244\n",
            "Iteration 105, loss = 0.41463720\n",
            "Iteration 106, loss = 0.41273950\n",
            "Iteration 107, loss = 0.41078592\n",
            "Iteration 108, loss = 0.40873509\n",
            "Iteration 109, loss = 0.40693227\n",
            "Iteration 110, loss = 0.40534657\n",
            "Iteration 111, loss = 0.40335167\n",
            "Iteration 112, loss = 0.40162021\n",
            "Iteration 113, loss = 0.40018204\n",
            "Iteration 114, loss = 0.39732348\n",
            "Iteration 115, loss = 0.39637702\n",
            "Iteration 116, loss = 0.39471955\n",
            "Iteration 117, loss = 0.39288289\n",
            "Iteration 118, loss = 0.39161175\n",
            "Iteration 119, loss = 0.38944104\n",
            "Iteration 120, loss = 0.38777237\n",
            "Iteration 121, loss = 0.38650529\n",
            "Iteration 122, loss = 0.38577908\n",
            "Iteration 123, loss = 0.38335909\n",
            "Iteration 124, loss = 0.38309589\n",
            "Iteration 125, loss = 0.38111643\n",
            "Iteration 126, loss = 0.37919168\n",
            "Iteration 127, loss = 0.37754043\n",
            "Iteration 128, loss = 0.37696970\n",
            "Iteration 129, loss = 0.37599266\n",
            "Iteration 130, loss = 0.37419883\n",
            "Iteration 131, loss = 0.37280958\n",
            "Iteration 132, loss = 0.37206521\n",
            "Iteration 133, loss = 0.36974118\n",
            "Iteration 134, loss = 0.37049137\n",
            "Iteration 135, loss = 0.36788584\n",
            "Iteration 136, loss = 0.36783349\n",
            "Iteration 137, loss = 0.36665735\n",
            "Iteration 138, loss = 0.36528554\n",
            "Iteration 139, loss = 0.36597343\n",
            "Iteration 140, loss = 0.36446105\n",
            "Iteration 141, loss = 0.36318748\n",
            "Iteration 142, loss = 0.36355321\n",
            "Iteration 143, loss = 0.36213939\n",
            "Iteration 144, loss = 0.36159232\n",
            "Iteration 145, loss = 0.35833508\n",
            "Iteration 146, loss = 0.35743952\n",
            "Iteration 147, loss = 0.35871907\n",
            "Iteration 148, loss = 0.35684790\n",
            "Iteration 149, loss = 0.35643928\n",
            "Iteration 150, loss = 0.35650426\n",
            "Iteration 151, loss = 0.35523581\n",
            "Iteration 152, loss = 0.35492303\n",
            "Iteration 153, loss = 0.35372696\n",
            "Iteration 154, loss = 0.35605220\n",
            "Iteration 155, loss = 0.35416120\n",
            "Iteration 156, loss = 0.35152267\n",
            "Iteration 157, loss = 0.35156330\n",
            "Iteration 158, loss = 0.35448141\n",
            "Iteration 159, loss = 0.35385385\n",
            "Iteration 160, loss = 0.35351039\n",
            "Iteration 161, loss = 0.35305300\n",
            "Iteration 162, loss = 0.35065634\n",
            "Iteration 163, loss = 0.35039455\n",
            "Iteration 164, loss = 0.35069364\n",
            "Iteration 165, loss = 0.35093017\n",
            "Iteration 166, loss = 0.34967409\n",
            "Iteration 167, loss = 0.35359835\n",
            "Iteration 168, loss = 0.34794767\n",
            "Iteration 169, loss = 0.34430985\n",
            "Iteration 170, loss = 0.34831995\n",
            "Iteration 171, loss = 0.35133867\n",
            "Iteration 172, loss = 0.34898980\n",
            "Iteration 173, loss = 0.35199708\n",
            "Iteration 174, loss = 0.35372709\n",
            "Iteration 175, loss = 0.34905427\n",
            "Iteration 176, loss = 0.34919097\n",
            "Iteration 177, loss = 0.34911476\n",
            "Iteration 178, loss = 0.34811901\n",
            "Iteration 179, loss = 0.34644242\n",
            "Iteration 180, loss = 0.35159153\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 181, loss = 0.32818608\n",
            "Iteration 182, loss = 0.32659430\n",
            "Iteration 183, loss = 0.32654268\n",
            "Iteration 184, loss = 0.32628831\n",
            "Iteration 185, loss = 0.32623577\n",
            "Iteration 186, loss = 0.32606357\n",
            "Iteration 187, loss = 0.32614798\n",
            "Iteration 188, loss = 0.32584602\n",
            "Iteration 189, loss = 0.32567732\n",
            "Iteration 190, loss = 0.32568423\n",
            "Iteration 191, loss = 0.32541041\n",
            "Iteration 192, loss = 0.32535782\n",
            "Iteration 193, loss = 0.32526730\n",
            "Iteration 194, loss = 0.32510036\n",
            "Iteration 195, loss = 0.32497570\n",
            "Iteration 196, loss = 0.32481745\n",
            "Iteration 197, loss = 0.32454826\n",
            "Iteration 198, loss = 0.32461399\n",
            "Iteration 199, loss = 0.32444755\n",
            "Iteration 200, loss = 0.32431592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70233878\n",
            "Iteration 3, loss = 0.67294999\n",
            "Iteration 4, loss = 0.65688748\n",
            "Iteration 5, loss = 0.64668023\n",
            "Iteration 6, loss = 0.63621641\n",
            "Iteration 7, loss = 0.62799247\n",
            "Iteration 8, loss = 0.62033711\n",
            "Iteration 9, loss = 0.61250098\n",
            "Iteration 10, loss = 0.60596771\n",
            "Iteration 11, loss = 0.59975932\n",
            "Iteration 12, loss = 0.59486143\n",
            "Iteration 13, loss = 0.59211398\n",
            "Iteration 14, loss = 0.58792519\n",
            "Iteration 15, loss = 0.58345446\n",
            "Iteration 16, loss = 0.58134584\n",
            "Iteration 17, loss = 0.57901908\n",
            "Iteration 18, loss = 0.57533626\n",
            "Iteration 19, loss = 0.57690492\n",
            "Iteration 20, loss = 0.57147616\n",
            "Iteration 21, loss = 0.57273429\n",
            "Iteration 22, loss = 0.57087196\n",
            "Iteration 23, loss = 0.56673203\n",
            "Iteration 24, loss = 0.56806021\n",
            "Iteration 25, loss = 0.56695716\n",
            "Iteration 26, loss = 0.56290115\n",
            "Iteration 27, loss = 0.56766738\n",
            "Iteration 28, loss = 0.56581696\n",
            "Iteration 29, loss = 0.56360764\n",
            "Iteration 30, loss = 0.56039427\n",
            "Iteration 31, loss = 0.56597786\n",
            "Iteration 32, loss = 0.56292401\n",
            "Iteration 33, loss = 0.56506545\n",
            "Iteration 34, loss = 0.56387607\n",
            "Iteration 35, loss = 0.56463933\n",
            "Iteration 36, loss = 0.56078772\n",
            "Iteration 37, loss = 0.56133332\n",
            "Iteration 38, loss = 0.56244461\n",
            "Iteration 39, loss = 0.56267597\n",
            "Iteration 40, loss = 0.55923277\n",
            "Iteration 41, loss = 0.56424465\n",
            "Iteration 42, loss = 0.56295447\n",
            "Iteration 43, loss = 0.56178110\n",
            "Iteration 44, loss = 0.56016276\n",
            "Iteration 45, loss = 0.56326192\n",
            "Iteration 46, loss = 0.56294654\n",
            "Iteration 47, loss = 0.56230848\n",
            "Iteration 48, loss = 0.56372852\n",
            "Iteration 49, loss = 0.56281316\n",
            "Iteration 50, loss = 0.56140641\n",
            "Iteration 51, loss = 0.55743553\n",
            "Iteration 52, loss = 0.56374672\n",
            "Iteration 53, loss = 0.56217169\n",
            "Iteration 54, loss = 0.55837951\n",
            "Iteration 55, loss = 0.56197169\n",
            "Iteration 56, loss = 0.56162319\n",
            "Iteration 57, loss = 0.55970105\n",
            "Iteration 58, loss = 0.56226586\n",
            "Iteration 59, loss = 0.56511826\n",
            "Iteration 60, loss = 0.56180456\n",
            "Iteration 61, loss = 0.56835327\n",
            "Iteration 62, loss = 0.56740623\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 63, loss = 0.53039738\n",
            "Iteration 64, loss = 0.51125406\n",
            "Iteration 65, loss = 0.50310173\n",
            "Iteration 66, loss = 0.49595381\n",
            "Iteration 67, loss = 0.48924173\n",
            "Iteration 68, loss = 0.48144357\n",
            "Iteration 69, loss = 0.47794162\n",
            "Iteration 70, loss = 0.47872707\n",
            "Iteration 71, loss = 0.49642311\n",
            "Iteration 72, loss = 0.51180978\n",
            "Iteration 73, loss = 0.50780596\n",
            "Iteration 74, loss = 0.50261996\n",
            "Iteration 75, loss = 0.51029705\n",
            "Iteration 76, loss = 0.52029600\n",
            "Iteration 77, loss = 0.51514169\n",
            "Iteration 78, loss = 0.51044913\n",
            "Iteration 79, loss = 0.50496313\n",
            "Iteration 80, loss = 0.50636124\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 81, loss = 0.43634528\n",
            "Iteration 82, loss = 0.43333476\n",
            "Iteration 83, loss = 0.43050247\n",
            "Iteration 84, loss = 0.42727303\n",
            "Iteration 85, loss = 0.42477753\n",
            "Iteration 86, loss = 0.42205344\n",
            "Iteration 87, loss = 0.41919691\n",
            "Iteration 88, loss = 0.41669348\n",
            "Iteration 89, loss = 0.41377564\n",
            "Iteration 90, loss = 0.41114037\n",
            "Iteration 91, loss = 0.40896586\n",
            "Iteration 92, loss = 0.40624336\n",
            "Iteration 93, loss = 0.40402096\n",
            "Iteration 94, loss = 0.40127229\n",
            "Iteration 95, loss = 0.39924754\n",
            "Iteration 96, loss = 0.39632013\n",
            "Iteration 97, loss = 0.39387923\n",
            "Iteration 98, loss = 0.39173223\n",
            "Iteration 99, loss = 0.38918708\n",
            "Iteration 100, loss = 0.38688979\n",
            "Iteration 101, loss = 0.38473655\n",
            "Iteration 102, loss = 0.38248295\n",
            "Iteration 103, loss = 0.38060847\n",
            "Iteration 104, loss = 0.37841826\n",
            "Iteration 105, loss = 0.37606910\n",
            "Iteration 106, loss = 0.37356590\n",
            "Iteration 107, loss = 0.37189638\n",
            "Iteration 108, loss = 0.36952203\n",
            "Iteration 109, loss = 0.36719882\n",
            "Iteration 110, loss = 0.36561773\n",
            "Iteration 111, loss = 0.36329735\n",
            "Iteration 112, loss = 0.36176296\n",
            "Iteration 113, loss = 0.35988026\n",
            "Iteration 114, loss = 0.35811188\n",
            "Iteration 115, loss = 0.35661921\n",
            "Iteration 116, loss = 0.35421632\n",
            "Iteration 117, loss = 0.35288883\n",
            "Iteration 118, loss = 0.35097239\n",
            "Iteration 119, loss = 0.34847267\n",
            "Iteration 120, loss = 0.34731318\n",
            "Iteration 121, loss = 0.34622337\n",
            "Iteration 122, loss = 0.34384268\n",
            "Iteration 123, loss = 0.34233182\n",
            "Iteration 124, loss = 0.34056597\n",
            "Iteration 125, loss = 0.33961376\n",
            "Iteration 126, loss = 0.33887163\n",
            "Iteration 127, loss = 0.33655038\n",
            "Iteration 128, loss = 0.33588627\n",
            "Iteration 129, loss = 0.33383006\n",
            "Iteration 130, loss = 0.33187728\n",
            "Iteration 131, loss = 0.33152011\n",
            "Iteration 132, loss = 0.32980952\n",
            "Iteration 133, loss = 0.32894681\n",
            "Iteration 134, loss = 0.32636714\n",
            "Iteration 135, loss = 0.32508342\n",
            "Iteration 136, loss = 0.32396973\n",
            "Iteration 137, loss = 0.32335404\n",
            "Iteration 138, loss = 0.32205717\n",
            "Iteration 139, loss = 0.32168749\n",
            "Iteration 140, loss = 0.31936819\n",
            "Iteration 141, loss = 0.31861784\n",
            "Iteration 142, loss = 0.31912354\n",
            "Iteration 143, loss = 0.31671431\n",
            "Iteration 144, loss = 0.31643462\n",
            "Iteration 145, loss = 0.31547072\n",
            "Iteration 146, loss = 0.31521564\n",
            "Iteration 147, loss = 0.31264344\n",
            "Iteration 148, loss = 0.31119583\n",
            "Iteration 149, loss = 0.30981401\n",
            "Iteration 150, loss = 0.31041063\n",
            "Iteration 151, loss = 0.30925916\n",
            "Iteration 152, loss = 0.30764677\n",
            "Iteration 153, loss = 0.30751778\n",
            "Iteration 154, loss = 0.30665911\n",
            "Iteration 155, loss = 0.30534642\n",
            "Iteration 156, loss = 0.30660668\n",
            "Iteration 157, loss = 0.30426378\n",
            "Iteration 158, loss = 0.30546443\n",
            "Iteration 159, loss = 0.30290539\n",
            "Iteration 160, loss = 0.30116478\n",
            "Iteration 161, loss = 0.30181259\n",
            "Iteration 162, loss = 0.30110264\n",
            "Iteration 163, loss = 0.30071637\n",
            "Iteration 164, loss = 0.29941656\n",
            "Iteration 165, loss = 0.29863128\n",
            "Iteration 166, loss = 0.29909881\n",
            "Iteration 167, loss = 0.29720995\n",
            "Iteration 168, loss = 0.29624318\n",
            "Iteration 169, loss = 0.29682881\n",
            "Iteration 170, loss = 0.29748371\n",
            "Iteration 171, loss = 0.29591387\n",
            "Iteration 172, loss = 0.29547722\n",
            "Iteration 173, loss = 0.29420465\n",
            "Iteration 174, loss = 0.29577434\n",
            "Iteration 175, loss = 0.29424953\n",
            "Iteration 176, loss = 0.29297526\n",
            "Iteration 177, loss = 0.29226535\n",
            "Iteration 178, loss = 0.29207156\n",
            "Iteration 179, loss = 0.29101839\n",
            "Iteration 180, loss = 0.29127922\n",
            "Iteration 181, loss = 0.28923814\n",
            "Iteration 182, loss = 0.28640121\n",
            "Iteration 183, loss = 0.29411371\n",
            "Iteration 184, loss = 0.29164306\n",
            "Iteration 185, loss = 0.29193489\n",
            "Iteration 186, loss = 0.29202329\n",
            "Iteration 187, loss = 0.28918488\n",
            "Iteration 188, loss = 0.28855239\n",
            "Iteration 189, loss = 0.28902026\n",
            "Iteration 190, loss = 0.28660258\n",
            "Iteration 191, loss = 0.28501144\n",
            "Iteration 192, loss = 0.28626207\n",
            "Iteration 193, loss = 0.28588080\n",
            "Iteration 194, loss = 0.28439506\n",
            "Iteration 195, loss = 0.28378308\n",
            "Iteration 196, loss = 0.29045976\n",
            "Iteration 197, loss = 0.29293566\n",
            "Iteration 198, loss = 0.29077140\n",
            "Iteration 199, loss = 0.28610715\n",
            "Iteration 200, loss = 0.28278970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.68977139\n",
            "Iteration 3, loss = 0.67293991\n",
            "Iteration 4, loss = 0.65264048\n",
            "Iteration 5, loss = 0.64673587\n",
            "Iteration 6, loss = 0.63238270\n",
            "Iteration 7, loss = 0.62424141\n",
            "Iteration 8, loss = 0.61653293\n",
            "Iteration 9, loss = 0.60941522\n",
            "Iteration 10, loss = 0.60741453\n",
            "Iteration 11, loss = 0.59971820\n",
            "Iteration 12, loss = 0.60087945\n",
            "Iteration 13, loss = 0.59029072\n",
            "Iteration 14, loss = 0.59104410\n",
            "Iteration 15, loss = 0.58654902\n",
            "Iteration 16, loss = 0.58290225\n",
            "Iteration 17, loss = 0.58062182\n",
            "Iteration 18, loss = 0.57952276\n",
            "Iteration 19, loss = 0.57817020\n",
            "Iteration 20, loss = 0.57667704\n",
            "Iteration 21, loss = 0.57580186\n",
            "Iteration 22, loss = 0.57432009\n",
            "Iteration 23, loss = 0.57588664\n",
            "Iteration 24, loss = 0.57103538\n",
            "Iteration 25, loss = 0.57310241\n",
            "Iteration 26, loss = 0.57057501\n",
            "Iteration 27, loss = 0.56868519\n",
            "Iteration 28, loss = 0.57201535\n",
            "Iteration 29, loss = 0.56623234\n",
            "Iteration 30, loss = 0.57285940\n",
            "Iteration 31, loss = 0.56468442\n",
            "Iteration 32, loss = 0.56504059\n",
            "Iteration 33, loss = 0.56246225\n",
            "Iteration 34, loss = 0.56812226\n",
            "Iteration 35, loss = 0.56542505\n",
            "Iteration 36, loss = 0.58999805\n",
            "Iteration 37, loss = 0.62976882\n",
            "Iteration 38, loss = 0.61855640\n",
            "Iteration 39, loss = 0.60956338\n",
            "Iteration 40, loss = 0.60366611\n",
            "Iteration 41, loss = 0.62608695\n",
            "Iteration 42, loss = 0.61027434\n",
            "Iteration 43, loss = 0.60136647\n",
            "Iteration 44, loss = 0.61620564\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 45, loss = 0.61642299\n",
            "Iteration 46, loss = 0.61437333\n",
            "Iteration 47, loss = 0.61183799\n",
            "Iteration 48, loss = 0.60889640\n",
            "Iteration 49, loss = 0.60529524\n",
            "Iteration 50, loss = 0.60071969\n",
            "Iteration 51, loss = 0.59655156\n",
            "Iteration 52, loss = 0.59237563\n",
            "Iteration 53, loss = 0.58883203\n",
            "Iteration 54, loss = 0.58520335\n",
            "Iteration 55, loss = 0.58294969\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 56, loss = 0.58030609\n",
            "Iteration 57, loss = 0.57986066\n",
            "Iteration 58, loss = 0.57940018\n",
            "Iteration 59, loss = 0.57882231\n",
            "Iteration 60, loss = 0.57847832\n",
            "Iteration 61, loss = 0.57848849\n",
            "Iteration 62, loss = 0.57800664\n",
            "Iteration 63, loss = 0.57733968\n",
            "Iteration 64, loss = 0.57717148\n",
            "Iteration 65, loss = 0.57677970\n",
            "Iteration 66, loss = 0.57663792\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 67, loss = 0.57582980\n",
            "Iteration 68, loss = 0.57571453\n",
            "Iteration 69, loss = 0.57557587\n",
            "Iteration 70, loss = 0.57548074\n",
            "Iteration 71, loss = 0.57548798\n",
            "Iteration 72, loss = 0.57527230\n",
            "Iteration 73, loss = 0.57535116\n",
            "Iteration 74, loss = 0.57521141\n",
            "Iteration 75, loss = 0.57511171\n",
            "Iteration 76, loss = 0.57502423\n",
            "Iteration 77, loss = 0.57505062\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 78, loss = 0.57482452\n",
            "Iteration 79, loss = 0.57479405\n",
            "Iteration 80, loss = 0.57480201\n",
            "Iteration 81, loss = 0.57481118\n",
            "Iteration 82, loss = 0.57474069\n",
            "Iteration 83, loss = 0.57474753\n",
            "Iteration 84, loss = 0.57470817\n",
            "Iteration 85, loss = 0.57470852\n",
            "Iteration 86, loss = 0.57471927\n",
            "Iteration 87, loss = 0.57468029\n",
            "Iteration 88, loss = 0.57466824\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 89, loss = 0.57463200\n",
            "Iteration 90, loss = 0.57462321\n",
            "Iteration 91, loss = 0.57461713\n",
            "Iteration 92, loss = 0.57461432\n",
            "Iteration 93, loss = 0.57461474\n",
            "Iteration 94, loss = 0.57460990\n",
            "Iteration 95, loss = 0.57460551\n",
            "Iteration 96, loss = 0.57460348\n",
            "Iteration 97, loss = 0.57460258\n",
            "Iteration 98, loss = 0.57459514\n",
            "Iteration 99, loss = 0.57459571\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.71814699\n",
            "Iteration 3, loss = 0.70188268\n",
            "Iteration 4, loss = 0.68593741\n",
            "Iteration 5, loss = 0.66569887\n",
            "Iteration 6, loss = 0.65204244\n",
            "Iteration 7, loss = 0.64233970\n",
            "Iteration 8, loss = 0.63068585\n",
            "Iteration 9, loss = 0.62167416\n",
            "Iteration 10, loss = 0.61766821\n",
            "Iteration 11, loss = 0.60845202\n",
            "Iteration 12, loss = 0.60331358\n",
            "Iteration 13, loss = 0.60000373\n",
            "Iteration 14, loss = 0.59655757\n",
            "Iteration 15, loss = 0.58942857\n",
            "Iteration 16, loss = 0.58822855\n",
            "Iteration 17, loss = 0.58714998\n",
            "Iteration 18, loss = 0.58153846\n",
            "Iteration 19, loss = 0.58160215\n",
            "Iteration 20, loss = 0.57830834\n",
            "Iteration 21, loss = 0.57536705\n",
            "Iteration 22, loss = 0.57708835\n",
            "Iteration 23, loss = 0.57443966\n",
            "Iteration 24, loss = 0.57619364\n",
            "Iteration 25, loss = 0.56976540\n",
            "Iteration 26, loss = 0.57079928\n",
            "Iteration 27, loss = 0.56918665\n",
            "Iteration 28, loss = 0.57243818\n",
            "Iteration 29, loss = 0.56879001\n",
            "Iteration 30, loss = 0.56872097\n",
            "Iteration 31, loss = 0.56788419\n",
            "Iteration 32, loss = 0.56878424\n",
            "Iteration 33, loss = 0.56833606\n",
            "Iteration 34, loss = 0.56925687\n",
            "Iteration 35, loss = 0.56702606\n",
            "Iteration 36, loss = 0.56705555\n",
            "Iteration 37, loss = 0.56463491\n",
            "Iteration 38, loss = 0.59411767\n",
            "Iteration 39, loss = 0.63322959\n",
            "Iteration 40, loss = 0.62234380\n",
            "Iteration 41, loss = 0.61064089\n",
            "Iteration 42, loss = 0.60426382\n",
            "Iteration 43, loss = 0.59815938\n",
            "Iteration 44, loss = 0.59435494\n",
            "Iteration 45, loss = 0.58920716\n",
            "Iteration 46, loss = 0.58840761\n",
            "Iteration 47, loss = 0.58317998\n",
            "Iteration 48, loss = 0.60612578\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.61814938\n",
            "Iteration 50, loss = 0.61560049\n",
            "Iteration 51, loss = 0.61388082\n",
            "Iteration 52, loss = 0.61189231\n",
            "Iteration 53, loss = 0.60940098\n",
            "Iteration 54, loss = 0.60608333\n",
            "Iteration 55, loss = 0.60282984\n",
            "Iteration 56, loss = 0.59799385\n",
            "Iteration 57, loss = 0.59343185\n",
            "Iteration 58, loss = 0.58897613\n",
            "Iteration 59, loss = 0.58462428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 60, loss = 0.58151575\n",
            "Iteration 61, loss = 0.58055331\n",
            "Iteration 62, loss = 0.58003808\n",
            "Iteration 63, loss = 0.57943211\n",
            "Iteration 64, loss = 0.57854066\n",
            "Iteration 65, loss = 0.57802715\n",
            "Iteration 66, loss = 0.57757552\n",
            "Iteration 67, loss = 0.57704938\n",
            "Iteration 68, loss = 0.57664797\n",
            "Iteration 69, loss = 0.57620819\n",
            "Iteration 70, loss = 0.57577754\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.57500274\n",
            "Iteration 72, loss = 0.57477600\n",
            "Iteration 73, loss = 0.57475216\n",
            "Iteration 74, loss = 0.57459090\n",
            "Iteration 75, loss = 0.57458774\n",
            "Iteration 76, loss = 0.57448105\n",
            "Iteration 77, loss = 0.57432835\n",
            "Iteration 78, loss = 0.57429969\n",
            "Iteration 79, loss = 0.57417800\n",
            "Iteration 80, loss = 0.57413383\n",
            "Iteration 81, loss = 0.57416245\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 82, loss = 0.57392613\n",
            "Iteration 83, loss = 0.57385810\n",
            "Iteration 84, loss = 0.57382982\n",
            "Iteration 85, loss = 0.57381813\n",
            "Iteration 86, loss = 0.57383003\n",
            "Iteration 87, loss = 0.57377607\n",
            "Iteration 88, loss = 0.57379729\n",
            "Iteration 89, loss = 0.57376066\n",
            "Iteration 90, loss = 0.57373560\n",
            "Iteration 91, loss = 0.57373511\n",
            "Iteration 92, loss = 0.57370996\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 93, loss = 0.57367237\n",
            "Iteration 94, loss = 0.57366605\n",
            "Iteration 95, loss = 0.57366003\n",
            "Iteration 96, loss = 0.57365412\n",
            "Iteration 97, loss = 0.57366448\n",
            "Iteration 98, loss = 0.57365845\n",
            "Iteration 99, loss = 0.57364659\n",
            "Iteration 100, loss = 0.57364433\n",
            "Iteration 101, loss = 0.57364262\n",
            "Iteration 102, loss = 0.57364333\n",
            "Iteration 103, loss = 0.57363286\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "----------------------------------\n",
            "[[33275   452]\n",
            " [ 1265 15654]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97     33727\n",
            "           1       0.97      0.93      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.96     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.628% (0.128%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.934% (11.908%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64473661\n",
            "Iteration 3, loss = 0.61913716\n",
            "Iteration 4, loss = 0.60485657\n",
            "Iteration 5, loss = 0.59746383\n",
            "Iteration 6, loss = 0.61120943\n",
            "Iteration 7, loss = 0.60505511\n",
            "Iteration 8, loss = 0.59311253\n",
            "Iteration 9, loss = 0.58823527\n",
            "Iteration 10, loss = 0.58665229\n",
            "Iteration 11, loss = 0.58612816\n",
            "Iteration 12, loss = 0.58599082\n",
            "Iteration 13, loss = 0.61207553\n",
            "Iteration 14, loss = 0.59856108\n",
            "Iteration 15, loss = 0.60579508\n",
            "Iteration 16, loss = 0.60052779\n",
            "Iteration 17, loss = 0.58999297\n",
            "Iteration 18, loss = 0.58625962\n",
            "Iteration 19, loss = 0.58609830\n",
            "Iteration 20, loss = 0.59895524\n",
            "Iteration 21, loss = 0.60261425\n",
            "Iteration 22, loss = 0.59118018\n",
            "Iteration 23, loss = 0.58697315\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.56841621\n",
            "Iteration 25, loss = 0.56030905\n",
            "Iteration 26, loss = 0.55134394\n",
            "Iteration 27, loss = 0.54928698\n",
            "Iteration 28, loss = 0.55375553\n",
            "Iteration 29, loss = 0.55630773\n",
            "Iteration 30, loss = 0.55978537\n",
            "Iteration 31, loss = 0.63917806\n",
            "Iteration 32, loss = 0.62510469\n",
            "Iteration 33, loss = 0.60722030\n",
            "Iteration 34, loss = 0.59553239\n",
            "Iteration 35, loss = 0.58817447\n",
            "Iteration 36, loss = 0.58166893\n",
            "Iteration 37, loss = 0.57453845\n",
            "Iteration 38, loss = 0.56864660\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.55147896\n",
            "Iteration 40, loss = 0.54756011\n",
            "Iteration 41, loss = 0.54352490\n",
            "Iteration 42, loss = 0.53916117\n",
            "Iteration 43, loss = 0.53450317\n",
            "Iteration 44, loss = 0.52982051\n",
            "Iteration 45, loss = 0.52505726\n",
            "Iteration 46, loss = 0.51971808\n",
            "Iteration 47, loss = 0.51473853\n",
            "Iteration 48, loss = 0.50932092\n",
            "Iteration 49, loss = 0.50397307\n",
            "Iteration 50, loss = 0.49842523\n",
            "Iteration 51, loss = 0.49345282\n",
            "Iteration 52, loss = 0.48771749\n",
            "Iteration 53, loss = 0.48267567\n",
            "Iteration 54, loss = 0.47802080\n",
            "Iteration 55, loss = 0.47315471\n",
            "Iteration 56, loss = 0.47010115\n",
            "Iteration 57, loss = 0.46607458\n",
            "Iteration 58, loss = 0.46323269\n",
            "Iteration 59, loss = 0.45998941\n",
            "Iteration 60, loss = 0.45622352\n",
            "Iteration 61, loss = 0.45532912\n",
            "Iteration 62, loss = 0.45475238\n",
            "Iteration 63, loss = 0.45271570\n",
            "Iteration 64, loss = 0.45440911\n",
            "Iteration 65, loss = 0.45042356\n",
            "Iteration 66, loss = 0.44901580\n",
            "Iteration 67, loss = 0.45143557\n",
            "Iteration 68, loss = 0.44511258\n",
            "Iteration 69, loss = 0.44486045\n",
            "Iteration 70, loss = 0.44448180\n",
            "Iteration 71, loss = 0.44182780\n",
            "Iteration 72, loss = 0.44512595\n",
            "Iteration 73, loss = 0.43814575\n",
            "Iteration 74, loss = 0.43834456\n",
            "Iteration 75, loss = 0.43723127\n",
            "Iteration 76, loss = 0.43594630\n",
            "Iteration 77, loss = 0.43378509\n",
            "Iteration 78, loss = 0.43752426\n",
            "Iteration 79, loss = 0.43046390\n",
            "Iteration 80, loss = 0.42948530\n",
            "Iteration 81, loss = 0.42754900\n",
            "Iteration 82, loss = 0.42691216\n",
            "Iteration 83, loss = 0.42855937\n",
            "Iteration 84, loss = 0.42434242\n",
            "Iteration 85, loss = 0.42276734\n",
            "Iteration 86, loss = 0.42472105\n",
            "Iteration 87, loss = 0.42045697\n",
            "Iteration 88, loss = 0.42091119\n",
            "Iteration 89, loss = 0.41499880\n",
            "Iteration 90, loss = 0.42015286\n",
            "Iteration 91, loss = 0.41730745\n",
            "Iteration 92, loss = 0.41704106\n",
            "Iteration 93, loss = 0.41523599\n",
            "Iteration 94, loss = 0.41067937\n",
            "Iteration 95, loss = 0.41079052\n",
            "Iteration 96, loss = 0.41303416\n",
            "Iteration 97, loss = 0.40695233\n",
            "Iteration 98, loss = 0.40696109\n",
            "Iteration 99, loss = 0.40370888\n",
            "Iteration 100, loss = 0.40381682\n",
            "Iteration 101, loss = 0.40358535\n",
            "Iteration 102, loss = 0.40617787\n",
            "Iteration 103, loss = 0.40476394\n",
            "Iteration 104, loss = 0.40394427\n",
            "Iteration 105, loss = 0.39998920\n",
            "Iteration 106, loss = 0.40102121\n",
            "Iteration 107, loss = 0.39998468\n",
            "Iteration 108, loss = 0.40288646\n",
            "Iteration 109, loss = 0.40060020\n",
            "Iteration 110, loss = 0.39938127\n",
            "Iteration 111, loss = 0.39914845\n",
            "Iteration 112, loss = 0.39674920\n",
            "Iteration 113, loss = 0.39834381\n",
            "Iteration 114, loss = 0.39196772\n",
            "Iteration 115, loss = 0.39402577\n",
            "Iteration 116, loss = 0.39406850\n",
            "Iteration 117, loss = 0.39165528\n",
            "Iteration 118, loss = 0.38908410\n",
            "Iteration 119, loss = 0.39258475\n",
            "Iteration 120, loss = 0.39083212\n",
            "Iteration 121, loss = 0.38719435\n",
            "Iteration 122, loss = 0.38731342\n",
            "Iteration 123, loss = 0.39080593\n",
            "Iteration 124, loss = 0.39475461\n",
            "Iteration 125, loss = 0.39111320\n",
            "Iteration 126, loss = 0.39135086\n",
            "Iteration 127, loss = 0.38573429\n",
            "Iteration 128, loss = 0.38386273\n",
            "Iteration 129, loss = 0.38892374\n",
            "Iteration 130, loss = 0.38105255\n",
            "Iteration 131, loss = 0.38204535\n",
            "Iteration 132, loss = 0.38198666\n",
            "Iteration 133, loss = 0.37893924\n",
            "Iteration 134, loss = 0.38827731\n",
            "Iteration 135, loss = 0.38442926\n",
            "Iteration 136, loss = 0.38371494\n",
            "Iteration 137, loss = 0.38642952\n",
            "Iteration 138, loss = 0.38478320\n",
            "Iteration 139, loss = 0.38010292\n",
            "Iteration 140, loss = 0.38207929\n",
            "Iteration 141, loss = 0.38352422\n",
            "Iteration 142, loss = 0.37936608\n",
            "Iteration 143, loss = 0.37941202\n",
            "Iteration 144, loss = 0.38445113\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 145, loss = 0.34101053\n",
            "Iteration 146, loss = 0.34065360\n",
            "Iteration 147, loss = 0.34041547\n",
            "Iteration 148, loss = 0.34026901\n",
            "Iteration 149, loss = 0.34018853\n",
            "Iteration 150, loss = 0.34007724\n",
            "Iteration 151, loss = 0.33993898\n",
            "Iteration 152, loss = 0.33984422\n",
            "Iteration 153, loss = 0.33979827\n",
            "Iteration 154, loss = 0.33972577\n",
            "Iteration 155, loss = 0.33958025\n",
            "Iteration 156, loss = 0.33950571\n",
            "Iteration 157, loss = 0.33941638\n",
            "Iteration 158, loss = 0.33930783\n",
            "Iteration 159, loss = 0.33922700\n",
            "Iteration 160, loss = 0.33906574\n",
            "Iteration 161, loss = 0.33894773\n",
            "Iteration 162, loss = 0.33888010\n",
            "Iteration 163, loss = 0.33880510\n",
            "Iteration 164, loss = 0.33864570\n",
            "Iteration 165, loss = 0.33865228\n",
            "Iteration 166, loss = 0.33852918\n",
            "Iteration 167, loss = 0.33840628\n",
            "Iteration 168, loss = 0.33824070\n",
            "Iteration 169, loss = 0.33823506\n",
            "Iteration 170, loss = 0.33817585\n",
            "Iteration 171, loss = 0.33804415\n",
            "Iteration 172, loss = 0.33798704\n",
            "Iteration 173, loss = 0.33795407\n",
            "Iteration 174, loss = 0.33773364\n",
            "Iteration 175, loss = 0.33775700\n",
            "Iteration 176, loss = 0.33761335\n",
            "Iteration 177, loss = 0.33757455\n",
            "Iteration 178, loss = 0.33745463\n",
            "Iteration 179, loss = 0.33743121\n",
            "Iteration 180, loss = 0.33740707\n",
            "Iteration 181, loss = 0.33721705\n",
            "Iteration 182, loss = 0.33716597\n",
            "Iteration 183, loss = 0.33709774\n",
            "Iteration 184, loss = 0.33697033\n",
            "Iteration 185, loss = 0.33693514\n",
            "Iteration 186, loss = 0.33686822\n",
            "Iteration 187, loss = 0.33680285\n",
            "Iteration 188, loss = 0.33660211\n",
            "Iteration 189, loss = 0.33656869\n",
            "Iteration 190, loss = 0.33655360\n",
            "Iteration 191, loss = 0.33647195\n",
            "Iteration 192, loss = 0.33636912\n",
            "Iteration 193, loss = 0.33633210\n",
            "Iteration 194, loss = 0.33627496\n",
            "Iteration 195, loss = 0.33612179\n",
            "Iteration 196, loss = 0.33608794\n",
            "Iteration 197, loss = 0.33599928\n",
            "Iteration 198, loss = 0.33593940\n",
            "Iteration 199, loss = 0.33589286\n",
            "Iteration 200, loss = 0.33584617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62904872\n",
            "Iteration 3, loss = 0.60374533\n",
            "Iteration 4, loss = 0.59844700\n",
            "Iteration 5, loss = 0.58367673\n",
            "Iteration 6, loss = 0.57298759\n",
            "Iteration 7, loss = 0.56912691\n",
            "Iteration 8, loss = 0.60075994\n",
            "Iteration 9, loss = 0.58637740\n",
            "Iteration 10, loss = 0.57547214\n",
            "Iteration 11, loss = 0.57000586\n",
            "Iteration 12, loss = 0.56783307\n",
            "Iteration 13, loss = 0.56668914\n",
            "Iteration 14, loss = 0.56565418\n",
            "Iteration 15, loss = 0.56628077\n",
            "Iteration 16, loss = 0.56693858\n",
            "Iteration 17, loss = 0.56681845\n",
            "Iteration 18, loss = 0.56695695\n",
            "Iteration 19, loss = 0.56602833\n",
            "Iteration 20, loss = 0.56733140\n",
            "Iteration 21, loss = 0.56695683\n",
            "Iteration 22, loss = 0.56636935\n",
            "Iteration 23, loss = 0.56792754\n",
            "Iteration 24, loss = 0.56697866\n",
            "Iteration 25, loss = 0.56815673\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.63708880\n",
            "Iteration 27, loss = 0.61914226\n",
            "Iteration 28, loss = 0.59773762\n",
            "Iteration 29, loss = 0.58231401\n",
            "Iteration 30, loss = 0.57410293\n",
            "Iteration 31, loss = 0.56630492\n",
            "Iteration 32, loss = 0.55747621\n",
            "Iteration 33, loss = 0.55025174\n",
            "Iteration 34, loss = 0.54665323\n",
            "Iteration 35, loss = 0.54593840\n",
            "Iteration 36, loss = 0.54392353\n",
            "Iteration 37, loss = 0.54054488\n",
            "Iteration 38, loss = 0.54069667\n",
            "Iteration 39, loss = 0.53811382\n",
            "Iteration 40, loss = 0.53716456\n",
            "Iteration 41, loss = 0.53525288\n",
            "Iteration 42, loss = 0.53174278\n",
            "Iteration 43, loss = 0.53168032\n",
            "Iteration 44, loss = 0.52924948\n",
            "Iteration 45, loss = 0.52545367\n",
            "Iteration 46, loss = 0.52843769\n",
            "Iteration 47, loss = 0.52398952\n",
            "Iteration 48, loss = 0.52326165\n",
            "Iteration 49, loss = 0.52445100\n",
            "Iteration 50, loss = 0.52174580\n",
            "Iteration 51, loss = 0.52268647\n",
            "Iteration 52, loss = 0.51896200\n",
            "Iteration 53, loss = 0.52089240\n",
            "Iteration 54, loss = 0.51955388\n",
            "Iteration 55, loss = 0.52179743\n",
            "Iteration 56, loss = 0.51835161\n",
            "Iteration 57, loss = 0.52278903\n",
            "Iteration 58, loss = 0.51836325\n",
            "Iteration 59, loss = 0.52076071\n",
            "Iteration 60, loss = 0.52174423\n",
            "Iteration 61, loss = 0.51840175\n",
            "Iteration 62, loss = 0.52042790\n",
            "Iteration 63, loss = 0.52081893\n",
            "Iteration 64, loss = 0.51884960\n",
            "Iteration 65, loss = 0.52068482\n",
            "Iteration 66, loss = 0.52029174\n",
            "Iteration 67, loss = 0.52079039\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.42314094\n",
            "Iteration 69, loss = 0.40875023\n",
            "Iteration 70, loss = 0.39617209\n",
            "Iteration 71, loss = 0.38498824\n",
            "Iteration 72, loss = 0.37470530\n",
            "Iteration 73, loss = 0.36613028\n",
            "Iteration 74, loss = 0.35802839\n",
            "Iteration 75, loss = 0.35116976\n",
            "Iteration 76, loss = 0.34516850\n",
            "Iteration 77, loss = 0.33992975\n",
            "Iteration 78, loss = 0.33508412\n",
            "Iteration 79, loss = 0.33136067\n",
            "Iteration 80, loss = 0.32797307\n",
            "Iteration 81, loss = 0.32478379\n",
            "Iteration 82, loss = 0.32168109\n",
            "Iteration 83, loss = 0.31934826\n",
            "Iteration 84, loss = 0.31666813\n",
            "Iteration 85, loss = 0.31458588\n",
            "Iteration 86, loss = 0.31354422\n",
            "Iteration 87, loss = 0.31170100\n",
            "Iteration 88, loss = 0.31071623\n",
            "Iteration 89, loss = 0.30916957\n",
            "Iteration 90, loss = 0.30839957\n",
            "Iteration 91, loss = 0.30729928\n",
            "Iteration 92, loss = 0.30649683\n",
            "Iteration 93, loss = 0.30507042\n",
            "Iteration 94, loss = 0.30605409\n",
            "Iteration 95, loss = 0.30467152\n",
            "Iteration 96, loss = 0.30456930\n",
            "Iteration 97, loss = 0.30369433\n",
            "Iteration 98, loss = 0.30191889\n",
            "Iteration 99, loss = 0.30283107\n",
            "Iteration 100, loss = 0.30279623\n",
            "Iteration 101, loss = 0.30418874\n",
            "Iteration 102, loss = 0.30610610\n",
            "Iteration 103, loss = 0.30363894\n",
            "Iteration 104, loss = 0.30544473\n",
            "Iteration 105, loss = 0.30343618\n",
            "Iteration 106, loss = 0.30292469\n",
            "Iteration 107, loss = 0.30679840\n",
            "Iteration 108, loss = 0.30439750\n",
            "Iteration 109, loss = 0.30279494\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 110, loss = 0.28459582\n",
            "Iteration 111, loss = 0.28430769\n",
            "Iteration 112, loss = 0.28411727\n",
            "Iteration 113, loss = 0.28402772\n",
            "Iteration 114, loss = 0.28387645\n",
            "Iteration 115, loss = 0.28375585\n",
            "Iteration 116, loss = 0.28358837\n",
            "Iteration 117, loss = 0.28353838\n",
            "Iteration 118, loss = 0.28339064\n",
            "Iteration 119, loss = 0.28328888\n",
            "Iteration 120, loss = 0.28319605\n",
            "Iteration 121, loss = 0.28306338\n",
            "Iteration 122, loss = 0.28292730\n",
            "Iteration 123, loss = 0.28278398\n",
            "Iteration 124, loss = 0.28264477\n",
            "Iteration 125, loss = 0.28258513\n",
            "Iteration 126, loss = 0.28243307\n",
            "Iteration 127, loss = 0.28237086\n",
            "Iteration 128, loss = 0.28227265\n",
            "Iteration 129, loss = 0.28220356\n",
            "Iteration 130, loss = 0.28201694\n",
            "Iteration 131, loss = 0.28186280\n",
            "Iteration 132, loss = 0.28186307\n",
            "Iteration 133, loss = 0.28173532\n",
            "Iteration 134, loss = 0.28161791\n",
            "Iteration 135, loss = 0.28148088\n",
            "Iteration 136, loss = 0.28140587\n",
            "Iteration 137, loss = 0.28127255\n",
            "Iteration 138, loss = 0.28118623\n",
            "Iteration 139, loss = 0.28106420\n",
            "Iteration 140, loss = 0.28099215\n",
            "Iteration 141, loss = 0.28088038\n",
            "Iteration 142, loss = 0.28086081\n",
            "Iteration 143, loss = 0.28062788\n",
            "Iteration 144, loss = 0.28059599\n",
            "Iteration 145, loss = 0.28047649\n",
            "Iteration 146, loss = 0.28040075\n",
            "Iteration 147, loss = 0.28030802\n",
            "Iteration 148, loss = 0.28022814\n",
            "Iteration 149, loss = 0.28012069\n",
            "Iteration 150, loss = 0.28003862\n",
            "Iteration 151, loss = 0.27990132\n",
            "Iteration 152, loss = 0.27978161\n",
            "Iteration 153, loss = 0.27974762\n",
            "Iteration 154, loss = 0.27965763\n",
            "Iteration 155, loss = 0.27956487\n",
            "Iteration 156, loss = 0.27950841\n",
            "Iteration 157, loss = 0.27934384\n",
            "Iteration 158, loss = 0.27931356\n",
            "Iteration 159, loss = 0.27919118\n",
            "Iteration 160, loss = 0.27911906\n",
            "Iteration 161, loss = 0.27906781\n",
            "Iteration 162, loss = 0.27888206\n",
            "Iteration 163, loss = 0.27891326\n",
            "Iteration 164, loss = 0.27873212\n",
            "Iteration 165, loss = 0.27868611\n",
            "Iteration 166, loss = 0.27861445\n",
            "Iteration 167, loss = 0.27856279\n",
            "Iteration 168, loss = 0.27847440\n",
            "Iteration 169, loss = 0.27831719\n",
            "Iteration 170, loss = 0.27828568\n",
            "Iteration 171, loss = 0.27824013\n",
            "Iteration 172, loss = 0.27806324\n",
            "Iteration 173, loss = 0.27804699\n",
            "Iteration 174, loss = 0.27791610\n",
            "Iteration 175, loss = 0.27789296\n",
            "Iteration 176, loss = 0.27778860\n",
            "Iteration 177, loss = 0.27774649\n",
            "Iteration 178, loss = 0.27764129\n",
            "Iteration 179, loss = 0.27755723\n",
            "Iteration 180, loss = 0.27751286\n",
            "Iteration 181, loss = 0.27739275\n",
            "Iteration 182, loss = 0.27734139\n",
            "Iteration 183, loss = 0.27726139\n",
            "Iteration 184, loss = 0.27722609\n",
            "Iteration 185, loss = 0.27712787\n",
            "Iteration 186, loss = 0.27707222\n",
            "Iteration 187, loss = 0.27695135\n",
            "Iteration 188, loss = 0.27687834\n",
            "Iteration 189, loss = 0.27681797\n",
            "Iteration 190, loss = 0.27676749\n",
            "Iteration 191, loss = 0.27666034\n",
            "Iteration 192, loss = 0.27657359\n",
            "Iteration 193, loss = 0.27660468\n",
            "Iteration 194, loss = 0.27645576\n",
            "Iteration 195, loss = 0.27637431\n",
            "Iteration 196, loss = 0.27632118\n",
            "Iteration 197, loss = 0.27621523\n",
            "Iteration 198, loss = 0.27611344\n",
            "Iteration 199, loss = 0.27610335\n",
            "Iteration 200, loss = 0.27603873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63899017\n",
            "Iteration 3, loss = 0.61025814\n",
            "Iteration 4, loss = 0.59346821\n",
            "Iteration 5, loss = 0.58329553\n",
            "Iteration 6, loss = 0.57571461\n",
            "Iteration 7, loss = 0.57267005\n",
            "Iteration 8, loss = 0.57159724\n",
            "Iteration 9, loss = 0.56820884\n",
            "Iteration 10, loss = 0.56830774\n",
            "Iteration 11, loss = 0.56806212\n",
            "Iteration 12, loss = 0.56821520\n",
            "Iteration 13, loss = 0.56720992\n",
            "Iteration 14, loss = 0.56804448\n",
            "Iteration 15, loss = 0.56716018\n",
            "Iteration 16, loss = 0.56835613\n",
            "Iteration 17, loss = 0.56756759\n",
            "Iteration 18, loss = 0.56887121\n",
            "Iteration 19, loss = 0.56720331\n",
            "Iteration 20, loss = 0.56744272\n",
            "Iteration 21, loss = 0.56861002\n",
            "Iteration 22, loss = 0.56843137\n",
            "Iteration 23, loss = 0.56744354\n",
            "Iteration 24, loss = 0.56743043\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.51515086\n",
            "Iteration 26, loss = 0.49396374\n",
            "Iteration 27, loss = 0.51342966\n",
            "Iteration 28, loss = 0.51881767\n",
            "Iteration 29, loss = 0.51662594\n",
            "Iteration 30, loss = 0.51840914\n",
            "Iteration 31, loss = 0.51745236\n",
            "Iteration 32, loss = 0.52114576\n",
            "Iteration 33, loss = 0.51390911\n",
            "Iteration 34, loss = 0.51590133\n",
            "Iteration 35, loss = 0.51652500\n",
            "Iteration 36, loss = 0.51665936\n",
            "Iteration 37, loss = 0.51766747\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.43222131\n",
            "Iteration 39, loss = 0.41892670\n",
            "Iteration 40, loss = 0.40757991\n",
            "Iteration 41, loss = 0.39735569\n",
            "Iteration 42, loss = 0.38774396\n",
            "Iteration 43, loss = 0.37917833\n",
            "Iteration 44, loss = 0.37138599\n",
            "Iteration 45, loss = 0.36416409\n",
            "Iteration 46, loss = 0.35776988\n",
            "Iteration 47, loss = 0.35231553\n",
            "Iteration 48, loss = 0.34784721\n",
            "Iteration 49, loss = 0.34335781\n",
            "Iteration 50, loss = 0.33963762\n",
            "Iteration 51, loss = 0.33541990\n",
            "Iteration 52, loss = 0.33364407\n",
            "Iteration 53, loss = 0.33056678\n",
            "Iteration 54, loss = 0.32786830\n",
            "Iteration 55, loss = 0.32606003\n",
            "Iteration 56, loss = 0.32442408\n",
            "Iteration 57, loss = 0.32434245\n",
            "Iteration 58, loss = 0.32278132\n",
            "Iteration 59, loss = 0.32206392\n",
            "Iteration 60, loss = 0.32349096\n",
            "Iteration 61, loss = 0.32063039\n",
            "Iteration 62, loss = 0.31843321\n",
            "Iteration 63, loss = 0.31888327\n",
            "Iteration 64, loss = 0.32008522\n",
            "Iteration 65, loss = 0.31919111\n",
            "Iteration 66, loss = 0.31852479\n",
            "Iteration 67, loss = 0.32015886\n",
            "Iteration 68, loss = 0.32130019\n",
            "Iteration 69, loss = 0.32136904\n",
            "Iteration 70, loss = 0.31745250\n",
            "Iteration 71, loss = 0.32186701\n",
            "Iteration 72, loss = 0.32051140\n",
            "Iteration 73, loss = 0.31977818\n",
            "Iteration 74, loss = 0.31657359\n",
            "Iteration 75, loss = 0.32150816\n",
            "Iteration 76, loss = 0.32509161\n",
            "Iteration 77, loss = 0.32008357\n",
            "Iteration 78, loss = 0.32309725\n",
            "Iteration 79, loss = 0.32034596\n",
            "Iteration 80, loss = 0.32273958\n",
            "Iteration 81, loss = 0.32310135\n",
            "Iteration 82, loss = 0.31995964\n",
            "Iteration 83, loss = 0.32094836\n",
            "Iteration 84, loss = 0.31777824\n",
            "Iteration 85, loss = 0.31943957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 86, loss = 0.28550159\n",
            "Iteration 87, loss = 0.28482952\n",
            "Iteration 88, loss = 0.28467599\n",
            "Iteration 89, loss = 0.28449330\n",
            "Iteration 90, loss = 0.28432578\n",
            "Iteration 91, loss = 0.28424122\n",
            "Iteration 92, loss = 0.28402951\n",
            "Iteration 93, loss = 0.28386547\n",
            "Iteration 94, loss = 0.28372019\n",
            "Iteration 95, loss = 0.28366834\n",
            "Iteration 96, loss = 0.28357742\n",
            "Iteration 97, loss = 0.28344417\n",
            "Iteration 98, loss = 0.28329154\n",
            "Iteration 99, loss = 0.28313922\n",
            "Iteration 100, loss = 0.28293753\n",
            "Iteration 101, loss = 0.28283718\n",
            "Iteration 102, loss = 0.28276454\n",
            "Iteration 103, loss = 0.28260695\n",
            "Iteration 104, loss = 0.28248807\n",
            "Iteration 105, loss = 0.28235587\n",
            "Iteration 106, loss = 0.28224198\n",
            "Iteration 107, loss = 0.28208941\n",
            "Iteration 108, loss = 0.28204032\n",
            "Iteration 109, loss = 0.28189618\n",
            "Iteration 110, loss = 0.28175537\n",
            "Iteration 111, loss = 0.28165646\n",
            "Iteration 112, loss = 0.28150490\n",
            "Iteration 113, loss = 0.28136499\n",
            "Iteration 114, loss = 0.28131485\n",
            "Iteration 115, loss = 0.28121738\n",
            "Iteration 116, loss = 0.28113108\n",
            "Iteration 117, loss = 0.28103775\n",
            "Iteration 118, loss = 0.28087899\n",
            "Iteration 119, loss = 0.28073319\n",
            "Iteration 120, loss = 0.28070524\n",
            "Iteration 121, loss = 0.28053431\n",
            "Iteration 122, loss = 0.28042740\n",
            "Iteration 123, loss = 0.28032751\n",
            "Iteration 124, loss = 0.28023460\n",
            "Iteration 125, loss = 0.28014537\n",
            "Iteration 126, loss = 0.27999516\n",
            "Iteration 127, loss = 0.27989555\n",
            "Iteration 128, loss = 0.27978254\n",
            "Iteration 129, loss = 0.27972090\n",
            "Iteration 130, loss = 0.27958559\n",
            "Iteration 131, loss = 0.27941533\n",
            "Iteration 132, loss = 0.27943607\n",
            "Iteration 133, loss = 0.27930545\n",
            "Iteration 134, loss = 0.27917336\n",
            "Iteration 135, loss = 0.27918712\n",
            "Iteration 136, loss = 0.27899588\n",
            "Iteration 137, loss = 0.27892931\n",
            "Iteration 138, loss = 0.27880771\n",
            "Iteration 139, loss = 0.27875246\n",
            "Iteration 140, loss = 0.27866934\n",
            "Iteration 141, loss = 0.27854430\n",
            "Iteration 142, loss = 0.27845125\n",
            "Iteration 143, loss = 0.27833582\n",
            "Iteration 144, loss = 0.27825935\n",
            "Iteration 145, loss = 0.27813307\n",
            "Iteration 146, loss = 0.27803717\n",
            "Iteration 147, loss = 0.27791788\n",
            "Iteration 148, loss = 0.27783496\n",
            "Iteration 149, loss = 0.27772003\n",
            "Iteration 150, loss = 0.27760860\n",
            "Iteration 151, loss = 0.27753397\n",
            "Iteration 152, loss = 0.27747258\n",
            "Iteration 153, loss = 0.27739421\n",
            "Iteration 154, loss = 0.27737044\n",
            "Iteration 155, loss = 0.27721611\n",
            "Iteration 156, loss = 0.27715005\n",
            "Iteration 157, loss = 0.27701415\n",
            "Iteration 158, loss = 0.27693939\n",
            "Iteration 159, loss = 0.27685961\n",
            "Iteration 160, loss = 0.27680502\n",
            "Iteration 161, loss = 0.27668226\n",
            "Iteration 162, loss = 0.27658920\n",
            "Iteration 163, loss = 0.27656738\n",
            "Iteration 164, loss = 0.27648271\n",
            "Iteration 165, loss = 0.27639996\n",
            "Iteration 166, loss = 0.27628146\n",
            "Iteration 167, loss = 0.27620129\n",
            "Iteration 168, loss = 0.27607627\n",
            "Iteration 169, loss = 0.27607242\n",
            "Iteration 170, loss = 0.27594695\n",
            "Iteration 171, loss = 0.27588995\n",
            "Iteration 172, loss = 0.27579460\n",
            "Iteration 173, loss = 0.27573577\n",
            "Iteration 174, loss = 0.27567384\n",
            "Iteration 175, loss = 0.27561534\n",
            "Iteration 176, loss = 0.27552893\n",
            "Iteration 177, loss = 0.27543910\n",
            "Iteration 178, loss = 0.27534450\n",
            "Iteration 179, loss = 0.27535834\n",
            "Iteration 180, loss = 0.27521552\n",
            "Iteration 181, loss = 0.27510761\n",
            "Iteration 182, loss = 0.27509688\n",
            "Iteration 183, loss = 0.27499274\n",
            "Iteration 184, loss = 0.27490507\n",
            "Iteration 185, loss = 0.27480574\n",
            "Iteration 186, loss = 0.27474384\n",
            "Iteration 187, loss = 0.27469668\n",
            "Iteration 188, loss = 0.27462097\n",
            "Iteration 189, loss = 0.27453314\n",
            "Iteration 190, loss = 0.27446338\n",
            "Iteration 191, loss = 0.27439273\n",
            "Iteration 192, loss = 0.27434292\n",
            "Iteration 193, loss = 0.27425575\n",
            "Iteration 194, loss = 0.27415290\n",
            "Iteration 195, loss = 0.27410127\n",
            "Iteration 196, loss = 0.27405625\n",
            "Iteration 197, loss = 0.27398497\n",
            "Iteration 198, loss = 0.27389149\n",
            "Iteration 199, loss = 0.27393273\n",
            "Iteration 200, loss = 0.27377546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.62978488\n",
            "Iteration 3, loss = 0.60402686\n",
            "Iteration 4, loss = 0.58745180\n",
            "Iteration 5, loss = 0.57768287\n",
            "Iteration 6, loss = 0.57362830\n",
            "Iteration 7, loss = 0.56954666\n",
            "Iteration 8, loss = 0.56567282\n",
            "Iteration 9, loss = 0.56474972\n",
            "Iteration 10, loss = 0.56343336\n",
            "Iteration 11, loss = 0.56517783\n",
            "Iteration 12, loss = 0.56182289\n",
            "Iteration 13, loss = 0.56402276\n",
            "Iteration 14, loss = 0.56282951\n",
            "Iteration 15, loss = 0.56263475\n",
            "Iteration 16, loss = 0.56461375\n",
            "Iteration 17, loss = 0.56214791\n",
            "Iteration 18, loss = 0.56321922\n",
            "Iteration 19, loss = 0.56211846\n",
            "Iteration 20, loss = 0.57403012\n",
            "Iteration 21, loss = 0.57866250\n",
            "Iteration 22, loss = 0.57068686\n",
            "Iteration 23, loss = 0.56599529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 24, loss = 0.51622494\n",
            "Iteration 25, loss = 0.49097327\n",
            "Iteration 26, loss = 0.50161712\n",
            "Iteration 27, loss = 0.51083546\n",
            "Iteration 28, loss = 0.51191483\n",
            "Iteration 29, loss = 0.51055224\n",
            "Iteration 30, loss = 0.51107146\n",
            "Iteration 31, loss = 0.50993815\n",
            "Iteration 32, loss = 0.54879453\n",
            "Iteration 33, loss = 0.65134974\n",
            "Iteration 34, loss = 0.63295774\n",
            "Iteration 35, loss = 0.62207292\n",
            "Iteration 36, loss = 0.60624008\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 37, loss = 0.59158421\n",
            "Iteration 38, loss = 0.58825094\n",
            "Iteration 39, loss = 0.58482107\n",
            "Iteration 40, loss = 0.58214803\n",
            "Iteration 41, loss = 0.57927370\n",
            "Iteration 42, loss = 0.57707613\n",
            "Iteration 43, loss = 0.57459723\n",
            "Iteration 44, loss = 0.57200314\n",
            "Iteration 45, loss = 0.56975388\n",
            "Iteration 46, loss = 0.56739748\n",
            "Iteration 47, loss = 0.56491844\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 48, loss = 0.56248938\n",
            "Iteration 49, loss = 0.56198256\n",
            "Iteration 50, loss = 0.56136282\n",
            "Iteration 51, loss = 0.56083215\n",
            "Iteration 52, loss = 0.56035811\n",
            "Iteration 53, loss = 0.55989410\n",
            "Iteration 54, loss = 0.55921877\n",
            "Iteration 55, loss = 0.55867186\n",
            "Iteration 56, loss = 0.55801021\n",
            "Iteration 57, loss = 0.55749042\n",
            "Iteration 58, loss = 0.55694304\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 59, loss = 0.55631627\n",
            "Iteration 60, loss = 0.55615058\n",
            "Iteration 61, loss = 0.55604847\n",
            "Iteration 62, loss = 0.55596584\n",
            "Iteration 63, loss = 0.55582497\n",
            "Iteration 64, loss = 0.55564682\n",
            "Iteration 65, loss = 0.55557076\n",
            "Iteration 66, loss = 0.55544684\n",
            "Iteration 67, loss = 0.55535016\n",
            "Iteration 68, loss = 0.55520181\n",
            "Iteration 69, loss = 0.55506680\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 70, loss = 0.55496549\n",
            "Iteration 71, loss = 0.55492873\n",
            "Iteration 72, loss = 0.55490898\n",
            "Iteration 73, loss = 0.55487960\n",
            "Iteration 74, loss = 0.55486221\n",
            "Iteration 75, loss = 0.55483592\n",
            "Iteration 76, loss = 0.55480562\n",
            "Iteration 77, loss = 0.55479141\n",
            "Iteration 78, loss = 0.55476286\n",
            "Iteration 79, loss = 0.55474630\n",
            "Iteration 80, loss = 0.55471688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.61958302\n",
            "Iteration 3, loss = 0.59554933\n",
            "Iteration 4, loss = 0.58237844\n",
            "Iteration 5, loss = 0.57881861\n",
            "Iteration 6, loss = 0.57618169\n",
            "Iteration 7, loss = 0.56824367\n",
            "Iteration 8, loss = 0.56460084\n",
            "Iteration 9, loss = 0.56258919\n",
            "Iteration 10, loss = 0.56262593\n",
            "Iteration 11, loss = 0.56104175\n",
            "Iteration 12, loss = 0.56049901\n",
            "Iteration 13, loss = 0.56139908\n",
            "Iteration 14, loss = 0.56106416\n",
            "Iteration 15, loss = 0.56037879\n",
            "Iteration 16, loss = 0.56141660\n",
            "Iteration 17, loss = 0.56130322\n",
            "Iteration 18, loss = 0.56085376\n",
            "Iteration 19, loss = 0.56142214\n",
            "Iteration 20, loss = 0.56068394\n",
            "Iteration 21, loss = 0.56224242\n",
            "Iteration 22, loss = 0.56024198\n",
            "Iteration 23, loss = 0.56396376\n",
            "Iteration 24, loss = 0.56168780\n",
            "Iteration 25, loss = 0.56029630\n",
            "Iteration 26, loss = 0.56094702\n",
            "Iteration 27, loss = 0.55932005\n",
            "Iteration 28, loss = 0.56047379\n",
            "Iteration 29, loss = 0.55860407\n",
            "Iteration 30, loss = 0.55978371\n",
            "Iteration 31, loss = 0.55994268\n",
            "Iteration 32, loss = 0.55962277\n",
            "Iteration 33, loss = 0.55931138\n",
            "Iteration 34, loss = 0.55809896\n",
            "Iteration 35, loss = 0.55868156\n",
            "Iteration 36, loss = 0.56959990\n",
            "Iteration 37, loss = 0.57199433\n",
            "Iteration 38, loss = 0.56536054\n",
            "Iteration 39, loss = 0.56075438\n",
            "Iteration 40, loss = 0.55626333\n",
            "Iteration 41, loss = 0.55648271\n",
            "Iteration 42, loss = 0.55514709\n",
            "Iteration 43, loss = 0.65271295\n",
            "Iteration 44, loss = 0.60430708\n",
            "Iteration 45, loss = 0.58362626\n",
            "Iteration 46, loss = 0.58220326\n",
            "Iteration 47, loss = 0.57622289\n",
            "Iteration 48, loss = 0.56835240\n",
            "Iteration 49, loss = 0.56437418\n",
            "Iteration 50, loss = 0.56388276\n",
            "Iteration 51, loss = 0.56271774\n",
            "Iteration 52, loss = 0.56105947\n",
            "Iteration 53, loss = 0.56328869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.51183899\n",
            "Iteration 55, loss = 0.48881346\n",
            "Iteration 56, loss = 0.49708913\n",
            "Iteration 57, loss = 0.51344259\n",
            "Iteration 58, loss = 0.51229963\n",
            "Iteration 59, loss = 0.50966656\n",
            "Iteration 60, loss = 0.50779314\n",
            "Iteration 61, loss = 0.50888812\n",
            "Iteration 62, loss = 0.50709894\n",
            "Iteration 63, loss = 0.50809854\n",
            "Iteration 64, loss = 0.51131805\n",
            "Iteration 65, loss = 0.50776579\n",
            "Iteration 66, loss = 0.50830516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.41440248\n",
            "Iteration 68, loss = 0.39976442\n",
            "Iteration 69, loss = 0.38715540\n",
            "Iteration 70, loss = 0.37595397\n",
            "Iteration 71, loss = 0.36515952\n",
            "Iteration 72, loss = 0.35547997\n",
            "Iteration 73, loss = 0.34682257\n",
            "Iteration 74, loss = 0.33870599\n",
            "Iteration 75, loss = 0.33162276\n",
            "Iteration 76, loss = 0.32521483\n",
            "Iteration 77, loss = 0.31902673\n",
            "Iteration 78, loss = 0.31367794\n",
            "Iteration 79, loss = 0.30916489\n",
            "Iteration 80, loss = 0.30461216\n",
            "Iteration 81, loss = 0.30076926\n",
            "Iteration 82, loss = 0.29663889\n",
            "Iteration 83, loss = 0.29340004\n",
            "Iteration 84, loss = 0.29040988\n",
            "Iteration 85, loss = 0.28764039\n",
            "Iteration 86, loss = 0.28518831\n",
            "Iteration 87, loss = 0.28339623\n",
            "Iteration 88, loss = 0.28104708\n",
            "Iteration 89, loss = 0.27951758\n",
            "Iteration 90, loss = 0.27748561\n",
            "Iteration 91, loss = 0.27587667\n",
            "Iteration 92, loss = 0.27384994\n",
            "Iteration 93, loss = 0.27322386\n",
            "Iteration 94, loss = 0.27155339\n",
            "Iteration 95, loss = 0.27171250\n",
            "Iteration 96, loss = 0.27015045\n",
            "Iteration 97, loss = 0.27009265\n",
            "Iteration 98, loss = 0.26814553\n",
            "Iteration 99, loss = 0.26731496\n",
            "Iteration 100, loss = 0.26634400\n",
            "Iteration 101, loss = 0.26843261\n",
            "Iteration 102, loss = 0.26752169\n",
            "Iteration 103, loss = 0.26446501\n",
            "Iteration 104, loss = 0.26773178\n",
            "Iteration 105, loss = 0.26579340\n",
            "Iteration 106, loss = 0.26384593\n",
            "Iteration 107, loss = 0.26492010\n",
            "Iteration 108, loss = 0.26891866\n",
            "Iteration 109, loss = 0.26477152\n",
            "Iteration 110, loss = 0.26633159\n",
            "Iteration 111, loss = 0.26217391\n",
            "Iteration 112, loss = 0.26495046\n",
            "Iteration 113, loss = 0.26388760\n",
            "Iteration 114, loss = 0.26756748\n",
            "Iteration 115, loss = 0.26380104\n",
            "Iteration 116, loss = 0.26601272\n",
            "Iteration 117, loss = 0.27049086\n",
            "Iteration 118, loss = 0.26943868\n",
            "Iteration 119, loss = 0.26463791\n",
            "Iteration 120, loss = 0.26492494\n",
            "Iteration 121, loss = 0.27026180\n",
            "Iteration 122, loss = 0.27842393\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 123, loss = 0.25079270\n",
            "Iteration 124, loss = 0.25077834\n",
            "Iteration 125, loss = 0.25068581\n",
            "Iteration 126, loss = 0.25066821\n",
            "Iteration 127, loss = 0.25066562\n",
            "Iteration 128, loss = 0.25057741\n",
            "Iteration 129, loss = 0.25051081\n",
            "Iteration 130, loss = 0.25045603\n",
            "Iteration 131, loss = 0.25043475\n",
            "Iteration 132, loss = 0.25040423\n",
            "Iteration 133, loss = 0.25028593\n",
            "Iteration 134, loss = 0.25025118\n",
            "Iteration 135, loss = 0.25027974\n",
            "Iteration 136, loss = 0.25019776\n",
            "Iteration 137, loss = 0.25015548\n",
            "Iteration 138, loss = 0.25016777\n",
            "Iteration 139, loss = 0.25006649\n",
            "Iteration 140, loss = 0.25002571\n",
            "Iteration 141, loss = 0.24996186\n",
            "Iteration 142, loss = 0.24999927\n",
            "Iteration 143, loss = 0.24995810\n",
            "Iteration 144, loss = 0.24986844\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 145, loss = 0.24958334\n",
            "Iteration 146, loss = 0.24958202\n",
            "Iteration 147, loss = 0.24959568\n",
            "Iteration 148, loss = 0.24955202\n",
            "Iteration 149, loss = 0.24956712\n",
            "Iteration 150, loss = 0.24954664\n",
            "Iteration 151, loss = 0.24956385\n",
            "Iteration 152, loss = 0.24955486\n",
            "Iteration 153, loss = 0.24953156\n",
            "Iteration 154, loss = 0.24952220\n",
            "Iteration 155, loss = 0.24952313\n",
            "Iteration 156, loss = 0.24949434\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 157, loss = 0.24942987\n",
            "Iteration 158, loss = 0.24941521\n",
            "Iteration 159, loss = 0.24940644\n",
            "Iteration 160, loss = 0.24941599\n",
            "Iteration 161, loss = 0.24941027\n",
            "Iteration 162, loss = 0.24939723\n",
            "Iteration 163, loss = 0.24940889\n",
            "Iteration 164, loss = 0.24941770\n",
            "Iteration 165, loss = 0.24939817\n",
            "Iteration 166, loss = 0.24940171\n",
            "Iteration 167, loss = 0.24939795\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63825303\n",
            "Iteration 3, loss = 0.60993665\n",
            "Iteration 4, loss = 0.59344664\n",
            "Iteration 5, loss = 0.58211095\n",
            "Iteration 6, loss = 0.58267427\n",
            "Iteration 7, loss = 0.60277774\n",
            "Iteration 8, loss = 0.58145914\n",
            "Iteration 9, loss = 0.57342366\n",
            "Iteration 10, loss = 0.56852692\n",
            "Iteration 11, loss = 0.56849183\n",
            "Iteration 12, loss = 0.56709489\n",
            "Iteration 13, loss = 0.56685645\n",
            "Iteration 14, loss = 0.56540471\n",
            "Iteration 15, loss = 0.56582198\n",
            "Iteration 16, loss = 0.56657797\n",
            "Iteration 17, loss = 0.56611347\n",
            "Iteration 18, loss = 0.56675471\n",
            "Iteration 19, loss = 0.56751143\n",
            "Iteration 20, loss = 0.56850077\n",
            "Iteration 21, loss = 0.56540993\n",
            "Iteration 22, loss = 0.56780259\n",
            "Iteration 23, loss = 0.56757399\n",
            "Iteration 24, loss = 0.56735168\n",
            "Iteration 25, loss = 0.56877868\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.51650774\n",
            "Iteration 27, loss = 0.49606815\n",
            "Iteration 28, loss = 0.51114609\n",
            "Iteration 29, loss = 0.51968221\n",
            "Iteration 30, loss = 0.51960814\n",
            "Iteration 31, loss = 0.52193248\n",
            "Iteration 32, loss = 0.51723726\n",
            "Iteration 33, loss = 0.52009401\n",
            "Iteration 34, loss = 0.51623937\n",
            "Iteration 35, loss = 0.51906804\n",
            "Iteration 36, loss = 0.51755713\n",
            "Iteration 37, loss = 0.51439733\n",
            "Iteration 38, loss = 0.51953489\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.43357047\n",
            "Iteration 40, loss = 0.42146540\n",
            "Iteration 41, loss = 0.41027919\n",
            "Iteration 42, loss = 0.39999086\n",
            "Iteration 43, loss = 0.39064993\n",
            "Iteration 44, loss = 0.38223599\n",
            "Iteration 45, loss = 0.37433899\n",
            "Iteration 46, loss = 0.36747520\n",
            "Iteration 47, loss = 0.36125883\n",
            "Iteration 48, loss = 0.35575034\n",
            "Iteration 49, loss = 0.35118911\n",
            "Iteration 50, loss = 0.34652051\n",
            "Iteration 51, loss = 0.34313955\n",
            "Iteration 52, loss = 0.33934542\n",
            "Iteration 53, loss = 0.33644936\n",
            "Iteration 54, loss = 0.33402567\n",
            "Iteration 55, loss = 0.33151179\n",
            "Iteration 56, loss = 0.32978919\n",
            "Iteration 57, loss = 0.32762650\n",
            "Iteration 58, loss = 0.32640356\n",
            "Iteration 59, loss = 0.32568699\n",
            "Iteration 60, loss = 0.32430196\n",
            "Iteration 61, loss = 0.32469028\n",
            "Iteration 62, loss = 0.32433259\n",
            "Iteration 63, loss = 0.32371814\n",
            "Iteration 64, loss = 0.32216484\n",
            "Iteration 65, loss = 0.32080482\n",
            "Iteration 66, loss = 0.32341935\n",
            "Iteration 67, loss = 0.32121617\n",
            "Iteration 68, loss = 0.32168428\n",
            "Iteration 69, loss = 0.32177878\n",
            "Iteration 70, loss = 0.32019043\n",
            "Iteration 71, loss = 0.32263015\n",
            "Iteration 72, loss = 0.32151380\n",
            "Iteration 73, loss = 0.32095457\n",
            "Iteration 74, loss = 0.32480792\n",
            "Iteration 75, loss = 0.32367707\n",
            "Iteration 76, loss = 0.32919471\n",
            "Iteration 77, loss = 0.32722050\n",
            "Iteration 78, loss = 0.32063316\n",
            "Iteration 79, loss = 0.32225621\n",
            "Iteration 80, loss = 0.32547668\n",
            "Iteration 81, loss = 0.32544485\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 82, loss = 0.29063579\n",
            "Iteration 83, loss = 0.29028873\n",
            "Iteration 84, loss = 0.29005470\n",
            "Iteration 85, loss = 0.28979112\n",
            "Iteration 86, loss = 0.28961946\n",
            "Iteration 87, loss = 0.28951648\n",
            "Iteration 88, loss = 0.28930570\n",
            "Iteration 89, loss = 0.28915704\n",
            "Iteration 90, loss = 0.28903591\n",
            "Iteration 91, loss = 0.28887308\n",
            "Iteration 92, loss = 0.28869317\n",
            "Iteration 93, loss = 0.28853098\n",
            "Iteration 94, loss = 0.28841888\n",
            "Iteration 95, loss = 0.28825274\n",
            "Iteration 96, loss = 0.28813638\n",
            "Iteration 97, loss = 0.28796328\n",
            "Iteration 98, loss = 0.28780374\n",
            "Iteration 99, loss = 0.28766544\n",
            "Iteration 100, loss = 0.28758472\n",
            "Iteration 101, loss = 0.28737594\n",
            "Iteration 102, loss = 0.28723802\n",
            "Iteration 103, loss = 0.28712540\n",
            "Iteration 104, loss = 0.28693401\n",
            "Iteration 105, loss = 0.28679949\n",
            "Iteration 106, loss = 0.28665517\n",
            "Iteration 107, loss = 0.28658082\n",
            "Iteration 108, loss = 0.28637245\n",
            "Iteration 109, loss = 0.28617821\n",
            "Iteration 110, loss = 0.28614136\n",
            "Iteration 111, loss = 0.28599594\n",
            "Iteration 112, loss = 0.28583787\n",
            "Iteration 113, loss = 0.28573690\n",
            "Iteration 114, loss = 0.28560459\n",
            "Iteration 115, loss = 0.28546450\n",
            "Iteration 116, loss = 0.28535782\n",
            "Iteration 117, loss = 0.28527131\n",
            "Iteration 118, loss = 0.28514665\n",
            "Iteration 119, loss = 0.28496574\n",
            "Iteration 120, loss = 0.28485158\n",
            "Iteration 121, loss = 0.28475149\n",
            "Iteration 122, loss = 0.28462337\n",
            "Iteration 123, loss = 0.28445781\n",
            "Iteration 124, loss = 0.28438152\n",
            "Iteration 125, loss = 0.28426950\n",
            "Iteration 126, loss = 0.28411531\n",
            "Iteration 127, loss = 0.28404612\n",
            "Iteration 128, loss = 0.28394992\n",
            "Iteration 129, loss = 0.28384081\n",
            "Iteration 130, loss = 0.28369322\n",
            "Iteration 131, loss = 0.28360831\n",
            "Iteration 132, loss = 0.28351545\n",
            "Iteration 133, loss = 0.28335869\n",
            "Iteration 134, loss = 0.28323024\n",
            "Iteration 135, loss = 0.28311634\n",
            "Iteration 136, loss = 0.28301765\n",
            "Iteration 137, loss = 0.28291989\n",
            "Iteration 138, loss = 0.28275656\n",
            "Iteration 139, loss = 0.28264566\n",
            "Iteration 140, loss = 0.28258355\n",
            "Iteration 141, loss = 0.28241593\n",
            "Iteration 142, loss = 0.28235204\n",
            "Iteration 143, loss = 0.28223567\n",
            "Iteration 144, loss = 0.28212228\n",
            "Iteration 145, loss = 0.28204276\n",
            "Iteration 146, loss = 0.28198597\n",
            "Iteration 147, loss = 0.28188255\n",
            "Iteration 148, loss = 0.28174745\n",
            "Iteration 149, loss = 0.28162528\n",
            "Iteration 150, loss = 0.28154975\n",
            "Iteration 151, loss = 0.28143909\n",
            "Iteration 152, loss = 0.28133668\n",
            "Iteration 153, loss = 0.28126389\n",
            "Iteration 154, loss = 0.28108802\n",
            "Iteration 155, loss = 0.28108404\n",
            "Iteration 156, loss = 0.28092253\n",
            "Iteration 157, loss = 0.28086396\n",
            "Iteration 158, loss = 0.28080124\n",
            "Iteration 159, loss = 0.28070796\n",
            "Iteration 160, loss = 0.28057820\n",
            "Iteration 161, loss = 0.28052754\n",
            "Iteration 162, loss = 0.28043026\n",
            "Iteration 163, loss = 0.28028601\n",
            "Iteration 164, loss = 0.28019093\n",
            "Iteration 165, loss = 0.28014283\n",
            "Iteration 166, loss = 0.28001684\n",
            "Iteration 167, loss = 0.27998009\n",
            "Iteration 168, loss = 0.27984997\n",
            "Iteration 169, loss = 0.27978300\n",
            "Iteration 170, loss = 0.27967258\n",
            "Iteration 171, loss = 0.27957133\n",
            "Iteration 172, loss = 0.27951719\n",
            "Iteration 173, loss = 0.27938863\n",
            "Iteration 174, loss = 0.27935048\n",
            "Iteration 175, loss = 0.27922483\n",
            "Iteration 176, loss = 0.27920378\n",
            "Iteration 177, loss = 0.27903886\n",
            "Iteration 178, loss = 0.27903064\n",
            "Iteration 179, loss = 0.27895926\n",
            "Iteration 180, loss = 0.27878836\n",
            "Iteration 181, loss = 0.27873458\n",
            "Iteration 182, loss = 0.27869134\n",
            "Iteration 183, loss = 0.27853153\n",
            "Iteration 184, loss = 0.27852369\n",
            "Iteration 185, loss = 0.27842037\n",
            "Iteration 186, loss = 0.27837020\n",
            "Iteration 187, loss = 0.27821563\n",
            "Iteration 188, loss = 0.27819020\n",
            "Iteration 189, loss = 0.27805706\n",
            "Iteration 190, loss = 0.27802063\n",
            "Iteration 191, loss = 0.27801974\n",
            "Iteration 192, loss = 0.27779225\n",
            "Iteration 193, loss = 0.27777612\n",
            "Iteration 194, loss = 0.27771968\n",
            "Iteration 195, loss = 0.27764836\n",
            "Iteration 196, loss = 0.27758886\n",
            "Iteration 197, loss = 0.27750861\n",
            "Iteration 198, loss = 0.27738320\n",
            "Iteration 199, loss = 0.27733773\n",
            "Iteration 200, loss = 0.27723993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63301450\n",
            "Iteration 3, loss = 0.60530044\n",
            "Iteration 4, loss = 0.58830206\n",
            "Iteration 5, loss = 0.57733613\n",
            "Iteration 6, loss = 0.57184580\n",
            "Iteration 7, loss = 0.56623749\n",
            "Iteration 8, loss = 0.56603912\n",
            "Iteration 9, loss = 0.56412225\n",
            "Iteration 10, loss = 0.56456131\n",
            "Iteration 11, loss = 0.56542195\n",
            "Iteration 12, loss = 0.56302920\n",
            "Iteration 13, loss = 0.56265588\n",
            "Iteration 14, loss = 0.56354068\n",
            "Iteration 15, loss = 0.56134234\n",
            "Iteration 16, loss = 0.56173917\n",
            "Iteration 17, loss = 0.56282876\n",
            "Iteration 18, loss = 0.56222300\n",
            "Iteration 19, loss = 0.56203909\n",
            "Iteration 20, loss = 0.56735637\n",
            "Iteration 21, loss = 0.61510507\n",
            "Iteration 22, loss = 0.59243325\n",
            "Iteration 23, loss = 0.57301499\n",
            "Iteration 24, loss = 0.56642109\n",
            "Iteration 25, loss = 0.56049489\n",
            "Iteration 26, loss = 0.56074044\n",
            "Iteration 27, loss = 0.59960381\n",
            "Iteration 28, loss = 0.57194215\n",
            "Iteration 29, loss = 0.56251031\n",
            "Iteration 30, loss = 0.55740885\n",
            "Iteration 31, loss = 0.55594270\n",
            "Iteration 32, loss = 0.55453755\n",
            "Iteration 33, loss = 0.56600008\n",
            "Iteration 34, loss = 0.58210473\n",
            "Iteration 35, loss = 0.56529598\n",
            "Iteration 36, loss = 0.59445887\n",
            "Iteration 37, loss = 0.57255161\n",
            "Iteration 38, loss = 0.56137574\n",
            "Iteration 39, loss = 0.55767916\n",
            "Iteration 40, loss = 0.55341613\n",
            "Iteration 41, loss = 0.55385244\n",
            "Iteration 42, loss = 0.55242767\n",
            "Iteration 43, loss = 0.55290229\n",
            "Iteration 44, loss = 0.55289095\n",
            "Iteration 45, loss = 0.56270441\n",
            "Iteration 46, loss = 0.55921929\n",
            "Iteration 47, loss = 0.57839011\n",
            "Iteration 48, loss = 0.61358336\n",
            "Iteration 49, loss = 0.57242524\n",
            "Iteration 50, loss = 0.56171090\n",
            "Iteration 51, loss = 0.55683194\n",
            "Iteration 52, loss = 0.55364513\n",
            "Iteration 53, loss = 0.56106469\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.53359878\n",
            "Iteration 55, loss = 0.51865497\n",
            "Iteration 56, loss = 0.50873503\n",
            "Iteration 57, loss = 0.51534328\n",
            "Iteration 58, loss = 0.51753749\n",
            "Iteration 59, loss = 0.51481190\n",
            "Iteration 60, loss = 0.51519793\n",
            "Iteration 61, loss = 0.51120305\n",
            "Iteration 62, loss = 0.50646405\n",
            "Iteration 63, loss = 0.50829652\n",
            "Iteration 64, loss = 0.50519486\n",
            "Iteration 65, loss = 0.50430442\n",
            "Iteration 66, loss = 0.50590902\n",
            "Iteration 67, loss = 0.50716190\n",
            "Iteration 68, loss = 0.50208582\n",
            "Iteration 69, loss = 0.50503858\n",
            "Iteration 70, loss = 0.49793149\n",
            "Iteration 71, loss = 0.50768696\n",
            "Iteration 72, loss = 0.50092148\n",
            "Iteration 73, loss = 0.49928875\n",
            "Iteration 74, loss = 0.50205638\n",
            "Iteration 75, loss = 0.50412484\n",
            "Iteration 76, loss = 0.50594223\n",
            "Iteration 77, loss = 0.50161915\n",
            "Iteration 78, loss = 0.50350448\n",
            "Iteration 79, loss = 0.50037811\n",
            "Iteration 80, loss = 0.50467926\n",
            "Iteration 81, loss = 0.50476816\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 82, loss = 0.39837648\n",
            "Iteration 83, loss = 0.38291647\n",
            "Iteration 84, loss = 0.36974057\n",
            "Iteration 85, loss = 0.35798783\n",
            "Iteration 86, loss = 0.34752506\n",
            "Iteration 87, loss = 0.33834111\n",
            "Iteration 88, loss = 0.32990545\n",
            "Iteration 89, loss = 0.32264632\n",
            "Iteration 90, loss = 0.31586645\n",
            "Iteration 91, loss = 0.31018664\n",
            "Iteration 92, loss = 0.30512843\n",
            "Iteration 93, loss = 0.30032850\n",
            "Iteration 94, loss = 0.29591947\n",
            "Iteration 95, loss = 0.29244482\n",
            "Iteration 96, loss = 0.28875370\n",
            "Iteration 97, loss = 0.28593906\n",
            "Iteration 98, loss = 0.28300824\n",
            "Iteration 99, loss = 0.28045473\n",
            "Iteration 100, loss = 0.27840307\n",
            "Iteration 101, loss = 0.27635858\n",
            "Iteration 102, loss = 0.27438088\n",
            "Iteration 103, loss = 0.27280157\n",
            "Iteration 104, loss = 0.27129498\n",
            "Iteration 105, loss = 0.26964069\n",
            "Iteration 106, loss = 0.26842019\n",
            "Iteration 107, loss = 0.26673562\n",
            "Iteration 108, loss = 0.26623402\n",
            "Iteration 109, loss = 0.26539611\n",
            "Iteration 110, loss = 0.26417071\n",
            "Iteration 111, loss = 0.26298210\n",
            "Iteration 112, loss = 0.26272597\n",
            "Iteration 113, loss = 0.26175966\n",
            "Iteration 114, loss = 0.26016286\n",
            "Iteration 115, loss = 0.26055451\n",
            "Iteration 116, loss = 0.25972689\n",
            "Iteration 117, loss = 0.25958387\n",
            "Iteration 118, loss = 0.25903191\n",
            "Iteration 119, loss = 0.25843397\n",
            "Iteration 120, loss = 0.25911034\n",
            "Iteration 121, loss = 0.25758796\n",
            "Iteration 122, loss = 0.25774699\n",
            "Iteration 123, loss = 0.25712367\n",
            "Iteration 124, loss = 0.25703380\n",
            "Iteration 125, loss = 0.25738757\n",
            "Iteration 126, loss = 0.25708190\n",
            "Iteration 127, loss = 0.25601381\n",
            "Iteration 128, loss = 0.25575179\n",
            "Iteration 129, loss = 0.25654705\n",
            "Iteration 130, loss = 0.25587575\n",
            "Iteration 131, loss = 0.25533696\n",
            "Iteration 132, loss = 0.25644249\n",
            "Iteration 133, loss = 0.25575965\n",
            "Iteration 134, loss = 0.25501033\n",
            "Iteration 135, loss = 0.25522480\n",
            "Iteration 136, loss = 0.25657811\n",
            "Iteration 137, loss = 0.25463735\n",
            "Iteration 138, loss = 0.25500364\n",
            "Iteration 139, loss = 0.25885435\n",
            "Iteration 140, loss = 0.25638451\n",
            "Iteration 141, loss = 0.26042573\n",
            "Iteration 142, loss = 0.25624824\n",
            "Iteration 143, loss = 0.25595753\n",
            "Iteration 144, loss = 0.25939134\n",
            "Iteration 145, loss = 0.25423905\n",
            "Iteration 146, loss = 0.25657112\n",
            "Iteration 147, loss = 0.25450245\n",
            "Iteration 148, loss = 0.25608592\n",
            "Iteration 149, loss = 0.26073035\n",
            "Iteration 150, loss = 0.26220426\n",
            "Iteration 151, loss = 0.25848118\n",
            "Iteration 152, loss = 0.27521850\n",
            "Iteration 153, loss = 0.25528878\n",
            "Iteration 154, loss = 0.26112929\n",
            "Iteration 155, loss = 0.26321036\n",
            "Iteration 156, loss = 0.26409506\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 157, loss = 0.25148786\n",
            "Iteration 158, loss = 0.24663724\n",
            "Iteration 159, loss = 0.24662416\n",
            "Iteration 160, loss = 0.24654539\n",
            "Iteration 161, loss = 0.24654320\n",
            "Iteration 162, loss = 0.24653178\n",
            "Iteration 163, loss = 0.24644876\n",
            "Iteration 164, loss = 0.24646703\n",
            "Iteration 165, loss = 0.24643581\n",
            "Iteration 166, loss = 0.24644764\n",
            "Iteration 167, loss = 0.24642076\n",
            "Iteration 168, loss = 0.24637620\n",
            "Iteration 169, loss = 0.24636925\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 170, loss = 0.24608791\n",
            "Iteration 171, loss = 0.24609835\n",
            "Iteration 172, loss = 0.24608710\n",
            "Iteration 173, loss = 0.24608397\n",
            "Iteration 174, loss = 0.24606539\n",
            "Iteration 175, loss = 0.24608455\n",
            "Iteration 176, loss = 0.24607061\n",
            "Iteration 177, loss = 0.24606502\n",
            "Iteration 178, loss = 0.24605288\n",
            "Iteration 179, loss = 0.24604763\n",
            "Iteration 180, loss = 0.24603825\n",
            "Iteration 181, loss = 0.24604330\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 182, loss = 0.24596995\n",
            "Iteration 183, loss = 0.24595974\n",
            "Iteration 184, loss = 0.24597081\n",
            "Iteration 185, loss = 0.24596450\n",
            "Iteration 186, loss = 0.24595926\n",
            "Iteration 187, loss = 0.24596259\n",
            "Iteration 188, loss = 0.24596463\n",
            "Iteration 189, loss = 0.24595835\n",
            "Iteration 190, loss = 0.24595862\n",
            "Iteration 191, loss = 0.24596085\n",
            "Iteration 192, loss = 0.24595983\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = 0.88071253\n",
            "Iteration 2, loss = 0.62897736\n",
            "Iteration 3, loss = 0.60310083\n",
            "Iteration 4, loss = 0.58661429\n",
            "Iteration 5, loss = 0.57757340\n",
            "Iteration 6, loss = 0.57108794\n",
            "Iteration 7, loss = 0.56701592\n",
            "Iteration 8, loss = 0.56512594\n",
            "Iteration 9, loss = 0.56341025\n",
            "Iteration 10, loss = 0.57201084\n",
            "Iteration 11, loss = 0.56849922\n",
            "Iteration 12, loss = 0.56802151\n",
            "Iteration 13, loss = 0.56843759\n",
            "Iteration 14, loss = 0.56719191\n",
            "Iteration 15, loss = 0.56761791\n",
            "Iteration 16, loss = 0.56782474\n",
            "Iteration 17, loss = 0.56766904\n",
            "Iteration 18, loss = 0.56692249\n",
            "Iteration 19, loss = 0.56716478\n",
            "Iteration 20, loss = 0.57001859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 21, loss = 0.51813447\n",
            "Iteration 22, loss = 0.49465377\n",
            "Iteration 23, loss = 0.51012231\n",
            "Iteration 24, loss = 0.51702411\n",
            "Iteration 25, loss = 0.51999283\n",
            "Iteration 26, loss = 0.51763692\n",
            "Iteration 27, loss = 0.51811164\n",
            "Iteration 28, loss = 0.51994773\n",
            "Iteration 29, loss = 0.51848829\n",
            "Iteration 30, loss = 0.51357307\n",
            "Iteration 31, loss = 0.51697509\n",
            "Iteration 32, loss = 0.51692374\n",
            "Iteration 33, loss = 0.51815709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 34, loss = 0.43217789\n",
            "Iteration 35, loss = 0.41994471\n",
            "Iteration 36, loss = 0.40875253\n",
            "Iteration 37, loss = 0.39858502\n",
            "Iteration 38, loss = 0.38903437\n",
            "Iteration 39, loss = 0.38055378\n",
            "Iteration 40, loss = 0.37297441\n",
            "Iteration 41, loss = 0.36596519\n",
            "Iteration 42, loss = 0.35981776\n",
            "Iteration 43, loss = 0.35424979\n",
            "Iteration 44, loss = 0.34918283\n",
            "Iteration 45, loss = 0.34513727\n",
            "Iteration 46, loss = 0.34101304\n",
            "Iteration 47, loss = 0.33834257\n",
            "Iteration 48, loss = 0.33439728\n",
            "Iteration 49, loss = 0.33195713\n",
            "Iteration 50, loss = 0.33039933\n",
            "Iteration 51, loss = 0.32874168\n",
            "Iteration 52, loss = 0.32686382\n",
            "Iteration 53, loss = 0.32476812\n",
            "Iteration 54, loss = 0.32213504\n",
            "Iteration 55, loss = 0.32295429\n",
            "Iteration 56, loss = 0.32201948\n",
            "Iteration 57, loss = 0.32151874\n",
            "Iteration 58, loss = 0.32076458\n",
            "Iteration 59, loss = 0.31955990\n",
            "Iteration 60, loss = 0.32183610\n",
            "Iteration 61, loss = 0.32171366\n",
            "Iteration 62, loss = 0.32056355\n",
            "Iteration 63, loss = 0.32157654\n",
            "Iteration 64, loss = 0.31993786\n",
            "Iteration 65, loss = 0.32040890\n",
            "Iteration 66, loss = 0.32168056\n",
            "Iteration 67, loss = 0.32443942\n",
            "Iteration 68, loss = 0.32310207\n",
            "Iteration 69, loss = 0.31946741\n",
            "Iteration 70, loss = 0.32352387\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 71, loss = 0.29368791\n",
            "Iteration 72, loss = 0.29298795\n",
            "Iteration 73, loss = 0.29276192\n",
            "Iteration 74, loss = 0.29257329\n",
            "Iteration 75, loss = 0.29229411\n",
            "Iteration 76, loss = 0.29211042\n",
            "Iteration 77, loss = 0.29191453\n",
            "Iteration 78, loss = 0.29177996\n",
            "Iteration 79, loss = 0.29149623\n",
            "Iteration 80, loss = 0.29137937\n",
            "Iteration 81, loss = 0.29114686\n",
            "Iteration 82, loss = 0.29103995\n",
            "Iteration 83, loss = 0.29082954\n",
            "Iteration 84, loss = 0.29066893\n",
            "Iteration 85, loss = 0.29049758\n",
            "Iteration 86, loss = 0.29028093\n",
            "Iteration 87, loss = 0.29014248\n",
            "Iteration 88, loss = 0.28988890\n",
            "Iteration 89, loss = 0.28978969\n",
            "Iteration 90, loss = 0.28962939\n",
            "Iteration 91, loss = 0.28943453\n",
            "Iteration 92, loss = 0.28931579\n",
            "Iteration 93, loss = 0.28909130\n",
            "Iteration 94, loss = 0.28895628\n",
            "Iteration 95, loss = 0.28882709\n",
            "Iteration 96, loss = 0.28861784\n",
            "Iteration 97, loss = 0.28851792\n",
            "Iteration 98, loss = 0.28829191\n",
            "Iteration 99, loss = 0.28819311\n",
            "Iteration 100, loss = 0.28799411\n",
            "Iteration 101, loss = 0.28787802\n",
            "Iteration 102, loss = 0.28769283\n",
            "Iteration 103, loss = 0.28757128\n",
            "Iteration 104, loss = 0.28737712\n",
            "Iteration 105, loss = 0.28725514\n",
            "Iteration 106, loss = 0.28706135\n",
            "Iteration 107, loss = 0.28691016\n",
            "Iteration 108, loss = 0.28677696\n",
            "Iteration 109, loss = 0.28667409\n",
            "Iteration 110, loss = 0.28648160\n",
            "Iteration 111, loss = 0.28635383\n",
            "Iteration 112, loss = 0.28624072\n",
            "Iteration 113, loss = 0.28609738\n",
            "Iteration 114, loss = 0.28594549\n",
            "Iteration 115, loss = 0.28577783\n",
            "Iteration 116, loss = 0.28565916\n",
            "Iteration 117, loss = 0.28558423\n",
            "Iteration 118, loss = 0.28544244\n",
            "Iteration 119, loss = 0.28528620\n",
            "Iteration 120, loss = 0.28515120\n",
            "Iteration 121, loss = 0.28505337\n",
            "Iteration 122, loss = 0.28488944\n",
            "Iteration 123, loss = 0.28473931\n",
            "Iteration 124, loss = 0.28463134\n",
            "Iteration 125, loss = 0.28447531\n",
            "Iteration 126, loss = 0.28435335\n",
            "Iteration 127, loss = 0.28427280\n",
            "Iteration 128, loss = 0.28414721\n",
            "Iteration 129, loss = 0.28398855\n",
            "Iteration 130, loss = 0.28389770\n",
            "Iteration 131, loss = 0.28371579\n",
            "Iteration 132, loss = 0.28364641\n",
            "Iteration 133, loss = 0.28353348\n",
            "Iteration 134, loss = 0.28340357\n",
            "Iteration 135, loss = 0.28328045\n",
            "Iteration 136, loss = 0.28314664\n",
            "Iteration 137, loss = 0.28307428\n",
            "Iteration 138, loss = 0.28290087\n",
            "Iteration 139, loss = 0.28276347\n",
            "Iteration 140, loss = 0.28269668\n",
            "Iteration 141, loss = 0.28252601\n",
            "Iteration 142, loss = 0.28249163\n",
            "Iteration 143, loss = 0.28238084\n",
            "Iteration 144, loss = 0.28236707\n",
            "Iteration 145, loss = 0.28212773\n",
            "Iteration 146, loss = 0.28203438\n",
            "Iteration 147, loss = 0.28190466\n",
            "Iteration 148, loss = 0.28184090\n",
            "Iteration 149, loss = 0.28167881\n",
            "Iteration 150, loss = 0.28157460\n",
            "Iteration 151, loss = 0.28149609\n",
            "Iteration 152, loss = 0.28139700\n",
            "Iteration 153, loss = 0.28119027\n",
            "Iteration 154, loss = 0.28113556\n",
            "Iteration 155, loss = 0.28099004\n",
            "Iteration 156, loss = 0.28098715\n",
            "Iteration 157, loss = 0.28093322\n",
            "Iteration 158, loss = 0.28077391\n",
            "Iteration 159, loss = 0.28052634\n",
            "Iteration 160, loss = 0.28052214\n",
            "Iteration 161, loss = 0.28040069\n",
            "Iteration 162, loss = 0.28026030\n",
            "Iteration 163, loss = 0.28023659\n",
            "Iteration 164, loss = 0.28009022\n",
            "Iteration 165, loss = 0.28003019\n",
            "Iteration 166, loss = 0.27990249\n",
            "Iteration 167, loss = 0.27984018\n",
            "Iteration 168, loss = 0.27974655\n",
            "Iteration 169, loss = 0.27959863\n",
            "Iteration 170, loss = 0.27953997\n",
            "Iteration 171, loss = 0.27954282\n",
            "Iteration 172, loss = 0.27937858\n",
            "Iteration 173, loss = 0.27922476\n",
            "Iteration 174, loss = 0.27917383\n",
            "Iteration 175, loss = 0.27903957\n",
            "Iteration 176, loss = 0.27908816\n",
            "Iteration 177, loss = 0.27890434\n",
            "Iteration 178, loss = 0.27888054\n",
            "Iteration 179, loss = 0.27866633\n",
            "Iteration 180, loss = 0.27866845\n",
            "Iteration 181, loss = 0.27860102\n",
            "Iteration 182, loss = 0.27842085\n",
            "Iteration 183, loss = 0.27839511\n",
            "Iteration 184, loss = 0.27831601\n",
            "Iteration 185, loss = 0.27814918\n",
            "Iteration 186, loss = 0.27810230\n",
            "Iteration 187, loss = 0.27806544\n",
            "Iteration 188, loss = 0.27795465\n",
            "Iteration 189, loss = 0.27791878\n",
            "Iteration 190, loss = 0.27780012\n",
            "Iteration 191, loss = 0.27768786\n",
            "Iteration 192, loss = 0.27766638\n",
            "Iteration 193, loss = 0.27752780\n",
            "Iteration 194, loss = 0.27747738\n",
            "Iteration 195, loss = 0.27743192\n",
            "Iteration 196, loss = 0.27732490\n",
            "Iteration 197, loss = 0.27722382\n",
            "Iteration 198, loss = 0.27716791\n",
            "Iteration 199, loss = 0.27704578\n",
            "Iteration 200, loss = 0.27695812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.89105991\n",
            "Iteration 2, loss = 0.62986577\n",
            "Iteration 3, loss = 0.60329997\n",
            "Iteration 4, loss = 0.58731488\n",
            "Iteration 5, loss = 0.57866711\n",
            "Iteration 6, loss = 0.57195557\n",
            "Iteration 7, loss = 0.56683507\n",
            "Iteration 8, loss = 0.57025061\n",
            "Iteration 9, loss = 0.57233175\n",
            "Iteration 10, loss = 0.56952762\n",
            "Iteration 11, loss = 0.56968542\n",
            "Iteration 12, loss = 0.56810532\n",
            "Iteration 13, loss = 0.56664096\n",
            "Iteration 14, loss = 0.56801038\n",
            "Iteration 15, loss = 0.56680843\n",
            "Iteration 16, loss = 0.56845696\n",
            "Iteration 17, loss = 0.56866570\n",
            "Iteration 18, loss = 0.56825063\n",
            "Iteration 19, loss = 0.56851638\n",
            "Iteration 20, loss = 0.56744022\n",
            "Iteration 21, loss = 0.56914090\n",
            "Iteration 22, loss = 0.56898329\n",
            "Iteration 23, loss = 0.56914415\n",
            "Iteration 24, loss = 0.56896703\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 25, loss = 0.51643576\n",
            "Iteration 26, loss = 0.49651104\n",
            "Iteration 27, loss = 0.51186726\n",
            "Iteration 28, loss = 0.52003615\n",
            "Iteration 29, loss = 0.51895149\n",
            "Iteration 30, loss = 0.52009083\n",
            "Iteration 31, loss = 0.51717149\n",
            "Iteration 32, loss = 0.51637666\n",
            "Iteration 33, loss = 0.51933212\n",
            "Iteration 34, loss = 0.51598054\n",
            "Iteration 35, loss = 0.51262240\n",
            "Iteration 36, loss = 0.52386662\n",
            "Iteration 37, loss = 0.51615194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 38, loss = 0.43236314\n",
            "Iteration 39, loss = 0.41978305\n",
            "Iteration 40, loss = 0.40867078\n",
            "Iteration 41, loss = 0.39829776\n",
            "Iteration 42, loss = 0.38900742\n",
            "Iteration 43, loss = 0.38033528\n",
            "Iteration 44, loss = 0.37246246\n",
            "Iteration 45, loss = 0.36563418\n",
            "Iteration 46, loss = 0.35948183\n",
            "Iteration 47, loss = 0.35365961\n",
            "Iteration 48, loss = 0.34901013\n",
            "Iteration 49, loss = 0.34463641\n",
            "Iteration 50, loss = 0.34092487\n",
            "Iteration 51, loss = 0.33752384\n",
            "Iteration 52, loss = 0.33479064\n",
            "Iteration 53, loss = 0.33128834\n",
            "Iteration 54, loss = 0.32992678\n",
            "Iteration 55, loss = 0.32865047\n",
            "Iteration 56, loss = 0.32619466\n",
            "Iteration 57, loss = 0.32464836\n",
            "Iteration 58, loss = 0.32456751\n",
            "Iteration 59, loss = 0.32279637\n",
            "Iteration 60, loss = 0.32194065\n",
            "Iteration 61, loss = 0.31990425\n",
            "Iteration 62, loss = 0.32198057\n",
            "Iteration 63, loss = 0.32280858\n",
            "Iteration 64, loss = 0.32046293\n",
            "Iteration 65, loss = 0.32278339\n",
            "Iteration 66, loss = 0.31989788\n",
            "Iteration 67, loss = 0.31865842\n",
            "Iteration 68, loss = 0.32119125\n",
            "Iteration 69, loss = 0.31869070\n",
            "Iteration 70, loss = 0.32075690\n",
            "Iteration 71, loss = 0.32027521\n",
            "Iteration 72, loss = 0.32650285\n",
            "Iteration 73, loss = 0.31961833\n",
            "Iteration 74, loss = 0.32374757\n",
            "Iteration 75, loss = 0.32061485\n",
            "Iteration 76, loss = 0.31839356\n",
            "Iteration 77, loss = 0.32561645\n",
            "Iteration 78, loss = 0.32043338\n",
            "Iteration 79, loss = 0.32331821\n",
            "Iteration 80, loss = 0.32311803\n",
            "Iteration 81, loss = 0.32001595\n",
            "Iteration 82, loss = 0.31764482\n",
            "Iteration 83, loss = 0.32315377\n",
            "Iteration 84, loss = 0.32173474\n",
            "Iteration 85, loss = 0.32003630\n",
            "Iteration 86, loss = 0.32291129\n",
            "Iteration 87, loss = 0.32392176\n",
            "Iteration 88, loss = 0.31749583\n",
            "Iteration 89, loss = 0.32141258\n",
            "Iteration 90, loss = 0.32324838\n",
            "Iteration 91, loss = 0.32278161\n",
            "Iteration 92, loss = 0.32091291\n",
            "Iteration 93, loss = 0.31747837\n",
            "Iteration 94, loss = 0.31744722\n",
            "Iteration 95, loss = 0.32019072\n",
            "Iteration 96, loss = 0.31922866\n",
            "Iteration 97, loss = 0.32379863\n",
            "Iteration 98, loss = 0.31257608\n",
            "Iteration 99, loss = 0.31910713\n",
            "Iteration 100, loss = 0.31425417\n",
            "Iteration 101, loss = 0.31815596\n",
            "Iteration 102, loss = 0.31924441\n",
            "Iteration 103, loss = 0.31335050\n",
            "Iteration 104, loss = 0.31397372\n",
            "Iteration 105, loss = 0.31589607\n",
            "Iteration 106, loss = 0.31804106\n",
            "Iteration 107, loss = 0.32065784\n",
            "Iteration 108, loss = 0.31703009\n",
            "Iteration 109, loss = 0.31986127\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 110, loss = 0.27762814\n",
            "Iteration 111, loss = 0.27719801\n",
            "Iteration 112, loss = 0.27708757\n",
            "Iteration 113, loss = 0.27692936\n",
            "Iteration 114, loss = 0.27675994\n",
            "Iteration 115, loss = 0.27668377\n",
            "Iteration 116, loss = 0.27657405\n",
            "Iteration 117, loss = 0.27650798\n",
            "Iteration 118, loss = 0.27644397\n",
            "Iteration 119, loss = 0.27635124\n",
            "Iteration 120, loss = 0.27626208\n",
            "Iteration 121, loss = 0.27618100\n",
            "Iteration 122, loss = 0.27613638\n",
            "Iteration 123, loss = 0.27602693\n",
            "Iteration 124, loss = 0.27590941\n",
            "Iteration 125, loss = 0.27582034\n",
            "Iteration 126, loss = 0.27577031\n",
            "Iteration 127, loss = 0.27569980\n",
            "Iteration 128, loss = 0.27564039\n",
            "Iteration 129, loss = 0.27547669\n",
            "Iteration 130, loss = 0.27542676\n",
            "Iteration 131, loss = 0.27533740\n",
            "Iteration 132, loss = 0.27529984\n",
            "Iteration 133, loss = 0.27521234\n",
            "Iteration 134, loss = 0.27510387\n",
            "Iteration 135, loss = 0.27503434\n",
            "Iteration 136, loss = 0.27497393\n",
            "Iteration 137, loss = 0.27487319\n",
            "Iteration 138, loss = 0.27480836\n",
            "Iteration 139, loss = 0.27472830\n",
            "Iteration 140, loss = 0.27470294\n",
            "Iteration 141, loss = 0.27461194\n",
            "Iteration 142, loss = 0.27457449\n",
            "Iteration 143, loss = 0.27446887\n",
            "Iteration 144, loss = 0.27442305\n",
            "Iteration 145, loss = 0.27427968\n",
            "Iteration 146, loss = 0.27423842\n",
            "Iteration 147, loss = 0.27420395\n",
            "Iteration 148, loss = 0.27408733\n",
            "Iteration 149, loss = 0.27405835\n",
            "Iteration 150, loss = 0.27404895\n",
            "Iteration 151, loss = 0.27390230\n",
            "Iteration 152, loss = 0.27381780\n",
            "Iteration 153, loss = 0.27373868\n",
            "Iteration 154, loss = 0.27374412\n",
            "Iteration 155, loss = 0.27364668\n",
            "Iteration 156, loss = 0.27351271\n",
            "Iteration 157, loss = 0.27350069\n",
            "Iteration 158, loss = 0.27343364\n",
            "Iteration 159, loss = 0.27336700\n",
            "Iteration 160, loss = 0.27331796\n",
            "Iteration 161, loss = 0.27326928\n",
            "Iteration 162, loss = 0.27311210\n",
            "Iteration 163, loss = 0.27306400\n",
            "Iteration 164, loss = 0.27304399\n",
            "Iteration 165, loss = 0.27301432\n",
            "Iteration 166, loss = 0.27292222\n",
            "Iteration 167, loss = 0.27284963\n",
            "Iteration 168, loss = 0.27277557\n",
            "Iteration 169, loss = 0.27270871\n",
            "Iteration 170, loss = 0.27271998\n",
            "Iteration 171, loss = 0.27263914\n",
            "Iteration 172, loss = 0.27253959\n",
            "Iteration 173, loss = 0.27244669\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 174, loss = 0.27215795\n",
            "Iteration 175, loss = 0.27212943\n",
            "Iteration 176, loss = 0.27213989\n",
            "Iteration 177, loss = 0.27210644\n",
            "Iteration 178, loss = 0.27208617\n",
            "Iteration 179, loss = 0.27208723\n",
            "Iteration 180, loss = 0.27205003\n",
            "Iteration 181, loss = 0.27205446\n",
            "Iteration 182, loss = 0.27203094\n",
            "Iteration 183, loss = 0.27201906\n",
            "Iteration 184, loss = 0.27200637\n",
            "Iteration 185, loss = 0.27200746\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 186, loss = 0.27189926\n",
            "Iteration 187, loss = 0.27189542\n",
            "Iteration 188, loss = 0.27189072\n",
            "Iteration 189, loss = 0.27190384\n",
            "Iteration 190, loss = 0.27188836\n",
            "Iteration 191, loss = 0.27188729\n",
            "Iteration 192, loss = 0.27189835\n",
            "Iteration 193, loss = 0.27187417\n",
            "Iteration 194, loss = 0.27187989\n",
            "Iteration 195, loss = 0.27188136\n",
            "Iteration 196, loss = 0.27187559\n",
            "Iteration 197, loss = 0.27187454\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.64529176\n",
            "Iteration 3, loss = 0.61091980\n",
            "Iteration 4, loss = 0.59042592\n",
            "Iteration 5, loss = 0.57925752\n",
            "Iteration 6, loss = 0.57080979\n",
            "Iteration 7, loss = 0.56570816\n",
            "Iteration 8, loss = 0.56412886\n",
            "Iteration 9, loss = 0.56210259\n",
            "Iteration 10, loss = 0.56341747\n",
            "Iteration 11, loss = 0.56221901\n",
            "Iteration 12, loss = 0.56148117\n",
            "Iteration 13, loss = 0.56234033\n",
            "Iteration 14, loss = 0.56080207\n",
            "Iteration 15, loss = 0.56285663\n",
            "Iteration 16, loss = 0.56219308\n",
            "Iteration 17, loss = 0.56313072\n",
            "Iteration 18, loss = 0.56694718\n",
            "Iteration 19, loss = 0.56300118\n",
            "Iteration 20, loss = 0.56412930\n",
            "Iteration 21, loss = 0.56497295\n",
            "Iteration 22, loss = 0.56477352\n",
            "Iteration 23, loss = 0.56494845\n",
            "Iteration 24, loss = 0.56499098\n",
            "Iteration 25, loss = 0.56333070\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 26, loss = 0.50003022\n",
            "Iteration 27, loss = 0.47588834\n",
            "Iteration 28, loss = 0.50350757\n",
            "Iteration 29, loss = 0.50808274\n",
            "Iteration 30, loss = 0.50976976\n",
            "Iteration 31, loss = 0.50725329\n",
            "Iteration 32, loss = 0.50425275\n",
            "Iteration 33, loss = 0.50710596\n",
            "Iteration 34, loss = 0.50694952\n",
            "Iteration 35, loss = 0.50527125\n",
            "Iteration 36, loss = 0.50748438\n",
            "Iteration 37, loss = 0.50724987\n",
            "Iteration 38, loss = 0.50440843\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 39, loss = 0.40733729\n",
            "Iteration 40, loss = 0.39083841\n",
            "Iteration 41, loss = 0.37811092\n",
            "Iteration 42, loss = 0.36617225\n",
            "Iteration 43, loss = 0.35568190\n",
            "Iteration 44, loss = 0.34585695\n",
            "Iteration 45, loss = 0.33726545\n",
            "Iteration 46, loss = 0.32972342\n",
            "Iteration 47, loss = 0.32298345\n",
            "Iteration 48, loss = 0.31671908\n",
            "Iteration 49, loss = 0.31102282\n",
            "Iteration 50, loss = 0.30581266\n",
            "Iteration 51, loss = 0.30116585\n",
            "Iteration 52, loss = 0.29675925\n",
            "Iteration 53, loss = 0.29326644\n",
            "Iteration 54, loss = 0.28983612\n",
            "Iteration 55, loss = 0.28679184\n",
            "Iteration 56, loss = 0.28416952\n",
            "Iteration 57, loss = 0.28124256\n",
            "Iteration 58, loss = 0.27941533\n",
            "Iteration 59, loss = 0.27713399\n",
            "Iteration 60, loss = 0.27508803\n",
            "Iteration 61, loss = 0.27368375\n",
            "Iteration 62, loss = 0.27238714\n",
            "Iteration 63, loss = 0.27025525\n",
            "Iteration 64, loss = 0.26918784\n",
            "Iteration 65, loss = 0.26796527\n",
            "Iteration 66, loss = 0.26656597\n",
            "Iteration 67, loss = 0.26697214\n",
            "Iteration 68, loss = 0.26517273\n",
            "Iteration 69, loss = 0.26417695\n",
            "Iteration 70, loss = 0.26379575\n",
            "Iteration 71, loss = 0.26319864\n",
            "Iteration 72, loss = 0.26211903\n",
            "Iteration 73, loss = 0.26068526\n",
            "Iteration 74, loss = 0.26081745\n",
            "Iteration 75, loss = 0.26032611\n",
            "Iteration 76, loss = 0.26060188\n",
            "Iteration 77, loss = 0.26080928\n",
            "Iteration 78, loss = 0.26084604\n",
            "Iteration 79, loss = 0.25945862\n",
            "Iteration 80, loss = 0.26174121\n",
            "Iteration 81, loss = 0.25941311\n",
            "Iteration 82, loss = 0.25802350\n",
            "Iteration 83, loss = 0.26095908\n",
            "Iteration 84, loss = 0.26285934\n",
            "Iteration 85, loss = 0.25758931\n",
            "Iteration 86, loss = 0.25829873\n",
            "Iteration 87, loss = 0.26047641\n",
            "Iteration 88, loss = 0.25953012\n",
            "Iteration 89, loss = 0.25961315\n",
            "Iteration 90, loss = 0.26003270\n",
            "Iteration 91, loss = 0.26130154\n",
            "Iteration 92, loss = 0.25946505\n",
            "Iteration 93, loss = 0.26628435\n",
            "Iteration 94, loss = 0.26034532\n",
            "Iteration 95, loss = 0.25995437\n",
            "Iteration 96, loss = 0.26871997\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 97, loss = 0.24835367\n",
            "Iteration 98, loss = 0.24834171\n",
            "Iteration 99, loss = 0.24825822\n",
            "Iteration 100, loss = 0.24819840\n",
            "Iteration 101, loss = 0.24827435\n",
            "Iteration 102, loss = 0.24813880\n",
            "Iteration 103, loss = 0.24813496\n",
            "Iteration 104, loss = 0.24807692\n",
            "Iteration 105, loss = 0.24803826\n",
            "Iteration 106, loss = 0.24802896\n",
            "Iteration 107, loss = 0.24793576\n",
            "Iteration 108, loss = 0.24793945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 109, loss = 0.24766575\n",
            "Iteration 110, loss = 0.24767300\n",
            "Iteration 111, loss = 0.24765665\n",
            "Iteration 112, loss = 0.24764705\n",
            "Iteration 113, loss = 0.24765529\n",
            "Iteration 114, loss = 0.24763422\n",
            "Iteration 115, loss = 0.24762802\n",
            "Iteration 116, loss = 0.24761076\n",
            "Iteration 117, loss = 0.24759552\n",
            "Iteration 118, loss = 0.24757240\n",
            "Iteration 119, loss = 0.24759497\n",
            "Iteration 120, loss = 0.24758571\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 121, loss = 0.24748767\n",
            "Iteration 122, loss = 0.24750142\n",
            "Iteration 123, loss = 0.24750921\n",
            "Iteration 124, loss = 0.24750031\n",
            "Iteration 125, loss = 0.24749125\n",
            "Iteration 126, loss = 0.24749913\n",
            "Iteration 127, loss = 0.24749123\n",
            "Iteration 128, loss = 0.24748734\n",
            "Iteration 129, loss = 0.24749143\n",
            "Iteration 130, loss = 0.24749358\n",
            "Iteration 131, loss = 0.24748179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.63373754\n",
            "Iteration 3, loss = 0.60485039\n",
            "Iteration 4, loss = 0.58551977\n",
            "Iteration 5, loss = 0.57557939\n",
            "Iteration 6, loss = 0.56799543\n",
            "Iteration 7, loss = 0.56383663\n",
            "Iteration 8, loss = 0.56389989\n",
            "Iteration 9, loss = 0.56224028\n",
            "Iteration 10, loss = 0.56104353\n",
            "Iteration 11, loss = 0.56079440\n",
            "Iteration 12, loss = 0.56369445\n",
            "Iteration 13, loss = 0.56250302\n",
            "Iteration 14, loss = 0.56147705\n",
            "Iteration 15, loss = 0.56152666\n",
            "Iteration 16, loss = 0.56141915\n",
            "Iteration 17, loss = 0.56191602\n",
            "Iteration 18, loss = 0.56308182\n",
            "Iteration 19, loss = 0.56159736\n",
            "Iteration 20, loss = 0.56207526\n",
            "Iteration 21, loss = 0.56268644\n",
            "Iteration 22, loss = 0.56111395\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 23, loss = 0.50514900\n",
            "Iteration 24, loss = 0.48069587\n",
            "Iteration 25, loss = 0.49275451\n",
            "Iteration 26, loss = 0.52511628\n",
            "Iteration 27, loss = 0.50452032\n",
            "Iteration 28, loss = 0.51180377\n",
            "Iteration 29, loss = 0.51050782\n",
            "Iteration 30, loss = 0.51053348\n",
            "Iteration 31, loss = 0.50971095\n",
            "Iteration 32, loss = 0.50575843\n",
            "Iteration 33, loss = 0.50711733\n",
            "Iteration 34, loss = 0.50426130\n",
            "Iteration 35, loss = 0.51061658\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 36, loss = 0.40968356\n",
            "Iteration 37, loss = 0.39545473\n",
            "Iteration 38, loss = 0.38262707\n",
            "Iteration 39, loss = 0.37087167\n",
            "Iteration 40, loss = 0.36037139\n",
            "Iteration 41, loss = 0.35099845\n",
            "Iteration 42, loss = 0.34235952\n",
            "Iteration 43, loss = 0.33491307\n",
            "Iteration 44, loss = 0.32733520\n",
            "Iteration 45, loss = 0.32125776\n",
            "Iteration 46, loss = 0.31553284\n",
            "Iteration 47, loss = 0.31029425\n",
            "Iteration 48, loss = 0.30565189\n",
            "Iteration 49, loss = 0.30154646\n",
            "Iteration 50, loss = 0.29743635\n",
            "Iteration 51, loss = 0.29431878\n",
            "Iteration 52, loss = 0.29111640\n",
            "Iteration 53, loss = 0.28774492\n",
            "Iteration 54, loss = 0.28580283\n",
            "Iteration 55, loss = 0.28287563\n",
            "Iteration 56, loss = 0.28084815\n",
            "Iteration 57, loss = 0.27937308\n",
            "Iteration 58, loss = 0.27777452\n",
            "Iteration 59, loss = 0.27627619\n",
            "Iteration 60, loss = 0.27335886\n",
            "Iteration 61, loss = 0.27321420\n",
            "Iteration 62, loss = 0.27211658\n",
            "Iteration 63, loss = 0.27164683\n",
            "Iteration 64, loss = 0.26907033\n",
            "Iteration 65, loss = 0.26931515\n",
            "Iteration 66, loss = 0.26752239\n",
            "Iteration 67, loss = 0.26768042\n",
            "Iteration 68, loss = 0.26813611\n",
            "Iteration 69, loss = 0.26543754\n",
            "Iteration 70, loss = 0.26652730\n",
            "Iteration 71, loss = 0.26524426\n",
            "Iteration 72, loss = 0.26412623\n",
            "Iteration 73, loss = 0.26353494\n",
            "Iteration 74, loss = 0.26412578\n",
            "Iteration 75, loss = 0.26696404\n",
            "Iteration 76, loss = 0.26318918\n",
            "Iteration 77, loss = 0.26325682\n",
            "Iteration 78, loss = 0.26233342\n",
            "Iteration 79, loss = 0.26236900\n",
            "Iteration 80, loss = 0.26394699\n",
            "Iteration 81, loss = 0.26735177\n",
            "Iteration 82, loss = 0.26154891\n",
            "Iteration 83, loss = 0.26436239\n",
            "Iteration 84, loss = 0.26299840\n",
            "Iteration 85, loss = 0.26302797\n",
            "Iteration 86, loss = 0.26546182\n",
            "Iteration 87, loss = 0.26613874\n",
            "Iteration 88, loss = 0.26372231\n",
            "Iteration 89, loss = 0.26611539\n",
            "Iteration 90, loss = 0.26538162\n",
            "Iteration 91, loss = 0.27427610\n",
            "Iteration 92, loss = 0.26846638\n",
            "Iteration 93, loss = 0.26239516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 94, loss = 0.25052748\n",
            "Iteration 95, loss = 0.25052291\n",
            "Iteration 96, loss = 0.25042646\n",
            "Iteration 97, loss = 0.25043253\n",
            "Iteration 98, loss = 0.25036570\n",
            "Iteration 99, loss = 0.25029330\n",
            "Iteration 100, loss = 0.25033220\n",
            "Iteration 101, loss = 0.25024884\n",
            "Iteration 102, loss = 0.25019313\n",
            "Iteration 103, loss = 0.25010540\n",
            "Iteration 104, loss = 0.25017260\n",
            "Iteration 105, loss = 0.25011697\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 106, loss = 0.24982596\n",
            "Iteration 107, loss = 0.24977625\n",
            "Iteration 108, loss = 0.24976495\n",
            "Iteration 109, loss = 0.24974395\n",
            "Iteration 110, loss = 0.24973020\n",
            "Iteration 111, loss = 0.24978323\n",
            "Iteration 112, loss = 0.24974774\n",
            "Iteration 113, loss = 0.24975571\n",
            "Iteration 114, loss = 0.24973330\n",
            "Iteration 115, loss = 0.24973163\n",
            "Iteration 116, loss = 0.24970863\n",
            "Iteration 117, loss = 0.24968064\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 118, loss = 0.24961870\n",
            "Iteration 119, loss = 0.24961352\n",
            "Iteration 120, loss = 0.24961834\n",
            "Iteration 121, loss = 0.24960505\n",
            "Iteration 122, loss = 0.24960749\n",
            "Iteration 123, loss = 0.24960570\n",
            "Iteration 124, loss = 0.24960742\n",
            "Iteration 125, loss = 0.24960674\n",
            "Iteration 126, loss = 0.24959797\n",
            "Iteration 127, loss = 0.24960991\n",
            "Iteration 128, loss = 0.24959460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 15.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.66655609\n",
            "Iteration 3, loss = 0.65577691\n",
            "Iteration 4, loss = 0.64556226\n",
            "Iteration 5, loss = 0.63806090\n",
            "Iteration 6, loss = 0.63192875\n",
            "Iteration 7, loss = 0.62080010\n",
            "Iteration 8, loss = 0.61275122\n",
            "Iteration 9, loss = 0.60931539\n",
            "Iteration 10, loss = 0.60466990\n",
            "Iteration 11, loss = 0.59873828\n",
            "Iteration 12, loss = 0.59696000\n",
            "Iteration 13, loss = 0.59167156\n",
            "Iteration 14, loss = 0.58757609\n",
            "Iteration 15, loss = 0.58653126\n",
            "Iteration 16, loss = 0.58144963\n",
            "Iteration 17, loss = 0.57865904\n",
            "Iteration 18, loss = 0.57660119\n",
            "Iteration 19, loss = 0.57383104\n",
            "Iteration 20, loss = 0.57166831\n",
            "Iteration 21, loss = 0.57356007\n",
            "Iteration 22, loss = 0.56709654\n",
            "Iteration 23, loss = 0.57158969\n",
            "Iteration 24, loss = 0.57065194\n",
            "Iteration 25, loss = 0.56914509\n",
            "Iteration 26, loss = 0.56797990\n",
            "Iteration 27, loss = 0.56645184\n",
            "Iteration 28, loss = 0.56440646\n",
            "Iteration 29, loss = 0.56709540\n",
            "Iteration 30, loss = 0.56389396\n",
            "Iteration 31, loss = 0.56708416\n",
            "Iteration 32, loss = 0.56208678\n",
            "Iteration 33, loss = 0.56288371\n",
            "Iteration 34, loss = 0.56269054\n",
            "Iteration 35, loss = 0.56461113\n",
            "Iteration 36, loss = 0.56369787\n",
            "Iteration 37, loss = 0.56208300\n",
            "Iteration 38, loss = 0.56456178\n",
            "Iteration 39, loss = 0.56296260\n",
            "Iteration 40, loss = 0.56484733\n",
            "Iteration 41, loss = 0.56383464\n",
            "Iteration 42, loss = 0.56076596\n",
            "Iteration 43, loss = 0.56194552\n",
            "Iteration 44, loss = 0.56783999\n",
            "Iteration 45, loss = 0.56348409\n",
            "Iteration 46, loss = 0.56219953\n",
            "Iteration 47, loss = 0.56885639\n",
            "Iteration 48, loss = 0.56792314\n",
            "Iteration 49, loss = 0.57183048\n",
            "Iteration 50, loss = 0.56680396\n",
            "Iteration 51, loss = 0.56984295\n",
            "Iteration 52, loss = 0.56574329\n",
            "Iteration 53, loss = 0.56639436\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 54, loss = 0.52612613\n",
            "Iteration 55, loss = 0.51409480\n",
            "Iteration 56, loss = 0.50724462\n",
            "Iteration 57, loss = 0.50064111\n",
            "Iteration 58, loss = 0.49490524\n",
            "Iteration 59, loss = 0.48789622\n",
            "Iteration 60, loss = 0.48222279\n",
            "Iteration 61, loss = 0.48160957\n",
            "Iteration 62, loss = 0.48483779\n",
            "Iteration 63, loss = 0.49500001\n",
            "Iteration 64, loss = 0.49978179\n",
            "Iteration 65, loss = 0.49479233\n",
            "Iteration 66, loss = 0.50283312\n",
            "Iteration 67, loss = 0.50333344\n",
            "Iteration 68, loss = 0.51830718\n",
            "Iteration 69, loss = 0.51447109\n",
            "Iteration 70, loss = 0.50492402\n",
            "Iteration 71, loss = 0.50507857\n",
            "Iteration 72, loss = 0.50251733\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 73, loss = 0.43868150\n",
            "Iteration 74, loss = 0.43451647\n",
            "Iteration 75, loss = 0.43242107\n",
            "Iteration 76, loss = 0.42976997\n",
            "Iteration 77, loss = 0.42781244\n",
            "Iteration 78, loss = 0.42552277\n",
            "Iteration 79, loss = 0.42306509\n",
            "Iteration 80, loss = 0.42115349\n",
            "Iteration 81, loss = 0.41909879\n",
            "Iteration 82, loss = 0.41665698\n",
            "Iteration 83, loss = 0.41444989\n",
            "Iteration 84, loss = 0.41251677\n",
            "Iteration 85, loss = 0.41047269\n",
            "Iteration 86, loss = 0.40809702\n",
            "Iteration 87, loss = 0.40633858\n",
            "Iteration 88, loss = 0.40410232\n",
            "Iteration 89, loss = 0.40219084\n",
            "Iteration 90, loss = 0.40022191\n",
            "Iteration 91, loss = 0.39837876\n",
            "Iteration 92, loss = 0.39644412\n",
            "Iteration 93, loss = 0.39430541\n",
            "Iteration 94, loss = 0.39279465\n",
            "Iteration 95, loss = 0.39067965\n",
            "Iteration 96, loss = 0.38805765\n",
            "Iteration 97, loss = 0.38751915\n",
            "Iteration 98, loss = 0.38518180\n",
            "Iteration 99, loss = 0.38306455\n",
            "Iteration 100, loss = 0.38128511\n",
            "Iteration 101, loss = 0.37903177\n",
            "Iteration 102, loss = 0.37753787\n",
            "Iteration 103, loss = 0.37606471\n",
            "Iteration 104, loss = 0.37414615\n",
            "Iteration 105, loss = 0.37286785\n",
            "Iteration 106, loss = 0.37105806\n",
            "Iteration 107, loss = 0.36925295\n",
            "Iteration 108, loss = 0.36731939\n",
            "Iteration 109, loss = 0.36632801\n",
            "Iteration 110, loss = 0.36432419\n",
            "Iteration 111, loss = 0.36280519\n",
            "Iteration 112, loss = 0.36171376\n",
            "Iteration 113, loss = 0.35999749\n",
            "Iteration 114, loss = 0.35806627\n",
            "Iteration 115, loss = 0.35689434\n",
            "Iteration 116, loss = 0.35525675\n",
            "Iteration 117, loss = 0.35375412\n",
            "Iteration 118, loss = 0.35178826\n",
            "Iteration 119, loss = 0.35051263\n",
            "Iteration 120, loss = 0.34942420\n",
            "Iteration 121, loss = 0.34820079\n",
            "Iteration 122, loss = 0.34676878\n",
            "Iteration 123, loss = 0.34566282\n",
            "Iteration 124, loss = 0.34447727\n",
            "Iteration 125, loss = 0.34273222\n",
            "Iteration 126, loss = 0.34170979\n",
            "Iteration 127, loss = 0.33998057\n",
            "Iteration 128, loss = 0.33976928\n",
            "Iteration 129, loss = 0.33853255\n",
            "Iteration 130, loss = 0.33707131\n",
            "Iteration 131, loss = 0.33447639\n",
            "Iteration 132, loss = 0.33487582\n",
            "Iteration 133, loss = 0.33360735\n",
            "Iteration 134, loss = 0.33281553\n",
            "Iteration 135, loss = 0.33054701\n",
            "Iteration 136, loss = 0.32912584\n",
            "Iteration 137, loss = 0.32914054\n",
            "Iteration 138, loss = 0.32794199\n",
            "Iteration 139, loss = 0.32653942\n",
            "Iteration 140, loss = 0.32566845\n",
            "Iteration 141, loss = 0.32379041\n",
            "Iteration 142, loss = 0.32348889\n",
            "Iteration 143, loss = 0.32225965\n",
            "Iteration 144, loss = 0.32125592\n",
            "Iteration 145, loss = 0.32096506\n",
            "Iteration 146, loss = 0.31912637\n",
            "Iteration 147, loss = 0.31875701\n",
            "Iteration 148, loss = 0.31855828\n",
            "Iteration 149, loss = 0.31644853\n",
            "Iteration 150, loss = 0.31523890\n",
            "Iteration 151, loss = 0.31375733\n",
            "Iteration 152, loss = 0.31654012\n",
            "Iteration 153, loss = 0.31396845\n",
            "Iteration 154, loss = 0.31289704\n",
            "Iteration 155, loss = 0.31113331\n",
            "Iteration 156, loss = 0.31185071\n",
            "Iteration 157, loss = 0.30950625\n",
            "Iteration 158, loss = 0.31173522\n",
            "Iteration 159, loss = 0.30924358\n",
            "Iteration 160, loss = 0.30868992\n",
            "Iteration 161, loss = 0.30798068\n",
            "Iteration 162, loss = 0.30686987\n",
            "Iteration 163, loss = 0.30543832\n",
            "Iteration 164, loss = 0.30675513\n",
            "Iteration 165, loss = 0.30534385\n",
            "Iteration 166, loss = 0.30393151\n",
            "Iteration 167, loss = 0.30285960\n",
            "Iteration 168, loss = 0.30276197\n",
            "Iteration 169, loss = 0.30255278\n",
            "Iteration 170, loss = 0.30174195\n",
            "Iteration 171, loss = 0.30129416\n",
            "Iteration 172, loss = 0.30028647\n",
            "Iteration 173, loss = 0.30044119\n",
            "Iteration 174, loss = 0.30013577\n",
            "Iteration 175, loss = 0.29815861\n",
            "Iteration 176, loss = 0.29803409\n",
            "Iteration 177, loss = 0.29834594\n",
            "Iteration 178, loss = 0.29752750\n",
            "Iteration 179, loss = 0.29677694\n",
            "Iteration 180, loss = 0.29780622\n",
            "Iteration 181, loss = 0.29505320\n",
            "Iteration 182, loss = 0.29629666\n",
            "Iteration 183, loss = 0.29484498\n",
            "Iteration 184, loss = 0.29686501\n",
            "Iteration 185, loss = 0.29600247\n",
            "Iteration 186, loss = 0.29289282\n",
            "Iteration 187, loss = 0.29376250\n",
            "Iteration 188, loss = 0.29390391\n",
            "Iteration 189, loss = 0.29242043\n",
            "Iteration 190, loss = 0.29099987\n",
            "Iteration 191, loss = 0.29186658\n",
            "Iteration 192, loss = 0.29041865\n",
            "Iteration 193, loss = 0.29148637\n",
            "Iteration 194, loss = 0.29128266\n",
            "Iteration 195, loss = 0.28872770\n",
            "Iteration 196, loss = 0.29013957\n",
            "Iteration 197, loss = 0.28875835\n",
            "Iteration 198, loss = 0.29011155\n",
            "Iteration 199, loss = 0.28695161\n",
            "Iteration 200, loss = 0.28769265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67460511\n",
            "Iteration 3, loss = 0.66308999\n",
            "Iteration 4, loss = 0.64664045\n",
            "Iteration 5, loss = 0.63519234\n",
            "Iteration 6, loss = 0.63309806\n",
            "Iteration 7, loss = 0.62049472\n",
            "Iteration 8, loss = 0.61736213\n",
            "Iteration 9, loss = 0.60688670\n",
            "Iteration 10, loss = 0.60280900\n",
            "Iteration 11, loss = 0.59971065\n",
            "Iteration 12, loss = 0.59017165\n",
            "Iteration 13, loss = 0.59061522\n",
            "Iteration 14, loss = 0.58897765\n",
            "Iteration 15, loss = 0.59394482\n",
            "Iteration 16, loss = 0.59015936\n",
            "Iteration 17, loss = 0.58536627\n",
            "Iteration 18, loss = 0.58329460\n",
            "Iteration 19, loss = 0.57659069\n",
            "Iteration 20, loss = 0.57425166\n",
            "Iteration 21, loss = 0.57886229\n",
            "Iteration 22, loss = 0.56828276\n",
            "Iteration 23, loss = 0.57135801\n",
            "Iteration 24, loss = 0.56896422\n",
            "Iteration 25, loss = 0.57159594\n",
            "Iteration 26, loss = 0.57312583\n",
            "Iteration 27, loss = 0.56830742\n",
            "Iteration 28, loss = 0.57151283\n",
            "Iteration 29, loss = 0.56892974\n",
            "Iteration 30, loss = 0.56673620\n",
            "Iteration 31, loss = 0.56702268\n",
            "Iteration 32, loss = 0.56436491\n",
            "Iteration 33, loss = 0.56583537\n",
            "Iteration 34, loss = 0.56043963\n",
            "Iteration 35, loss = 0.56776593\n",
            "Iteration 36, loss = 0.56379278\n",
            "Iteration 37, loss = 0.56182623\n",
            "Iteration 38, loss = 0.56202773\n",
            "Iteration 39, loss = 0.56474628\n",
            "Iteration 40, loss = 0.56066209\n",
            "Iteration 41, loss = 0.56073887\n",
            "Iteration 42, loss = 0.56263566\n",
            "Iteration 43, loss = 0.55997495\n",
            "Iteration 44, loss = 0.56026274\n",
            "Iteration 45, loss = 0.56190787\n",
            "Iteration 46, loss = 0.56397790\n",
            "Iteration 47, loss = 0.56040845\n",
            "Iteration 48, loss = 0.56046492\n",
            "Iteration 49, loss = 0.56197728\n",
            "Iteration 50, loss = 0.55967602\n",
            "Iteration 51, loss = 0.55938236\n",
            "Iteration 52, loss = 0.56385233\n",
            "Iteration 53, loss = 0.56580091\n",
            "Iteration 54, loss = 0.56014459\n",
            "Iteration 55, loss = 0.56366361\n",
            "Iteration 56, loss = 0.56604740\n",
            "Iteration 57, loss = 0.55799346\n",
            "Iteration 58, loss = 0.56271543\n",
            "Iteration 59, loss = 0.55890855\n",
            "Iteration 60, loss = 0.56221162\n",
            "Iteration 61, loss = 0.56218489\n",
            "Iteration 62, loss = 0.56225189\n",
            "Iteration 63, loss = 0.56431185\n",
            "Iteration 64, loss = 0.56037984\n",
            "Iteration 65, loss = 0.55805964\n",
            "Iteration 66, loss = 0.56220237\n",
            "Iteration 67, loss = 0.57180486\n",
            "Iteration 68, loss = 0.56866259\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 69, loss = 0.52633218\n",
            "Iteration 70, loss = 0.51197562\n",
            "Iteration 71, loss = 0.50379059\n",
            "Iteration 72, loss = 0.49748199\n",
            "Iteration 73, loss = 0.48803836\n",
            "Iteration 74, loss = 0.48334958\n",
            "Iteration 75, loss = 0.48140210\n",
            "Iteration 76, loss = 0.48310656\n",
            "Iteration 77, loss = 0.50196442\n",
            "Iteration 78, loss = 0.50459105\n",
            "Iteration 79, loss = 0.51724066\n",
            "Iteration 80, loss = 0.50157779\n",
            "Iteration 81, loss = 0.50238764\n",
            "Iteration 82, loss = 0.50740517\n",
            "Iteration 83, loss = 0.51135660\n",
            "Iteration 84, loss = 0.50302153\n",
            "Iteration 85, loss = 0.51782053\n",
            "Iteration 86, loss = 0.52439846\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 87, loss = 0.43968703\n",
            "Iteration 88, loss = 0.43687872\n",
            "Iteration 89, loss = 0.43392007\n",
            "Iteration 90, loss = 0.43098696\n",
            "Iteration 91, loss = 0.42849159\n",
            "Iteration 92, loss = 0.42568180\n",
            "Iteration 93, loss = 0.42342843\n",
            "Iteration 94, loss = 0.42031559\n",
            "Iteration 95, loss = 0.41807881\n",
            "Iteration 96, loss = 0.41534538\n",
            "Iteration 97, loss = 0.41253936\n",
            "Iteration 98, loss = 0.41008135\n",
            "Iteration 99, loss = 0.40782400\n",
            "Iteration 100, loss = 0.40533755\n",
            "Iteration 101, loss = 0.40247562\n",
            "Iteration 102, loss = 0.40026649\n",
            "Iteration 103, loss = 0.39751473\n",
            "Iteration 104, loss = 0.39528267\n",
            "Iteration 105, loss = 0.39318291\n",
            "Iteration 106, loss = 0.39077690\n",
            "Iteration 107, loss = 0.38867465\n",
            "Iteration 108, loss = 0.38605499\n",
            "Iteration 109, loss = 0.38397443\n",
            "Iteration 110, loss = 0.38187495\n",
            "Iteration 111, loss = 0.37919943\n",
            "Iteration 112, loss = 0.37762428\n",
            "Iteration 113, loss = 0.37515271\n",
            "Iteration 114, loss = 0.37304855\n",
            "Iteration 115, loss = 0.37095989\n",
            "Iteration 116, loss = 0.36881551\n",
            "Iteration 117, loss = 0.36671368\n",
            "Iteration 118, loss = 0.36522196\n",
            "Iteration 119, loss = 0.36318028\n",
            "Iteration 120, loss = 0.36108971\n",
            "Iteration 121, loss = 0.35948275\n",
            "Iteration 122, loss = 0.35755135\n",
            "Iteration 123, loss = 0.35593852\n",
            "Iteration 124, loss = 0.35406364\n",
            "Iteration 125, loss = 0.35300349\n",
            "Iteration 126, loss = 0.35046788\n",
            "Iteration 127, loss = 0.34964986\n",
            "Iteration 128, loss = 0.34780967\n",
            "Iteration 129, loss = 0.34619947\n",
            "Iteration 130, loss = 0.34463845\n",
            "Iteration 131, loss = 0.34287160\n",
            "Iteration 132, loss = 0.34186519\n",
            "Iteration 133, loss = 0.33994714\n",
            "Iteration 134, loss = 0.33817293\n",
            "Iteration 135, loss = 0.33695454\n",
            "Iteration 136, loss = 0.33469509\n",
            "Iteration 137, loss = 0.33317807\n",
            "Iteration 138, loss = 0.33199888\n",
            "Iteration 139, loss = 0.33060712\n",
            "Iteration 140, loss = 0.32947995\n",
            "Iteration 141, loss = 0.32807914\n",
            "Iteration 142, loss = 0.32718700\n",
            "Iteration 143, loss = 0.32722515\n",
            "Iteration 144, loss = 0.32483125\n",
            "Iteration 145, loss = 0.32362514\n",
            "Iteration 146, loss = 0.32427390\n",
            "Iteration 147, loss = 0.32078809\n",
            "Iteration 148, loss = 0.32061340\n",
            "Iteration 149, loss = 0.31960473\n",
            "Iteration 150, loss = 0.31924843\n",
            "Iteration 151, loss = 0.31662756\n",
            "Iteration 152, loss = 0.31559804\n",
            "Iteration 153, loss = 0.31406326\n",
            "Iteration 154, loss = 0.31471367\n",
            "Iteration 155, loss = 0.31298782\n",
            "Iteration 156, loss = 0.31254449\n",
            "Iteration 157, loss = 0.31135204\n",
            "Iteration 158, loss = 0.30897156\n",
            "Iteration 159, loss = 0.30986880\n",
            "Iteration 160, loss = 0.30862656\n",
            "Iteration 161, loss = 0.30675978\n",
            "Iteration 162, loss = 0.30599809\n",
            "Iteration 163, loss = 0.30562449\n",
            "Iteration 164, loss = 0.30550989\n",
            "Iteration 165, loss = 0.30363935\n",
            "Iteration 166, loss = 0.30364882\n",
            "Iteration 167, loss = 0.30343977\n",
            "Iteration 168, loss = 0.30227128\n",
            "Iteration 169, loss = 0.30354698\n",
            "Iteration 170, loss = 0.30001403\n",
            "Iteration 171, loss = 0.29891503\n",
            "Iteration 172, loss = 0.30072893\n",
            "Iteration 173, loss = 0.29805033\n",
            "Iteration 174, loss = 0.29803855\n",
            "Iteration 175, loss = 0.29827443\n",
            "Iteration 176, loss = 0.29706346\n",
            "Iteration 177, loss = 0.29670403\n",
            "Iteration 178, loss = 0.29692798\n",
            "Iteration 179, loss = 0.29463696\n",
            "Iteration 180, loss = 0.29485291\n",
            "Iteration 181, loss = 0.29506568\n",
            "Iteration 182, loss = 0.29495517\n",
            "Iteration 183, loss = 0.29280641\n",
            "Iteration 184, loss = 0.29351072\n",
            "Iteration 185, loss = 0.29141235\n",
            "Iteration 186, loss = 0.29356868\n",
            "Iteration 187, loss = 0.28971393\n",
            "Iteration 188, loss = 0.29210844\n",
            "Iteration 189, loss = 0.29341507\n",
            "Iteration 190, loss = 0.29042022\n",
            "Iteration 191, loss = 0.29214958\n",
            "Iteration 192, loss = 0.29294042\n",
            "Iteration 193, loss = 0.29089566\n",
            "Iteration 194, loss = 0.28949212\n",
            "Iteration 195, loss = 0.28874260\n",
            "Iteration 196, loss = 0.28929627\n",
            "Iteration 197, loss = 0.28916138\n",
            "Iteration 198, loss = 0.29099801\n",
            "Iteration 199, loss = 0.28758267\n",
            "Iteration 200, loss = 0.29199578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67774005\n",
            "Iteration 3, loss = 0.66432882\n",
            "Iteration 4, loss = 0.65009565\n",
            "Iteration 5, loss = 0.63589014\n",
            "Iteration 6, loss = 0.62749342\n",
            "Iteration 7, loss = 0.61808323\n",
            "Iteration 8, loss = 0.61444962\n",
            "Iteration 9, loss = 0.60675425\n",
            "Iteration 10, loss = 0.59884021\n",
            "Iteration 11, loss = 0.59474953\n",
            "Iteration 12, loss = 0.59101524\n",
            "Iteration 13, loss = 0.58903744\n",
            "Iteration 14, loss = 0.58634728\n",
            "Iteration 15, loss = 0.58201052\n",
            "Iteration 16, loss = 0.57730991\n",
            "Iteration 17, loss = 0.57667930\n",
            "Iteration 18, loss = 0.57149651\n",
            "Iteration 19, loss = 0.57447441\n",
            "Iteration 20, loss = 0.56679784\n",
            "Iteration 21, loss = 0.56913584\n",
            "Iteration 22, loss = 0.57036589\n",
            "Iteration 23, loss = 0.56360901\n",
            "Iteration 24, loss = 0.56358428\n",
            "Iteration 25, loss = 0.56697704\n",
            "Iteration 26, loss = 0.56519663\n",
            "Iteration 27, loss = 0.56065874\n",
            "Iteration 28, loss = 0.56332837\n",
            "Iteration 29, loss = 0.56214645\n",
            "Iteration 30, loss = 0.56587242\n",
            "Iteration 31, loss = 0.56219807\n",
            "Iteration 32, loss = 0.56432526\n",
            "Iteration 33, loss = 0.56351663\n",
            "Iteration 34, loss = 0.56123843\n",
            "Iteration 35, loss = 0.56082311\n",
            "Iteration 36, loss = 0.56029815\n",
            "Iteration 37, loss = 0.55843326\n",
            "Iteration 38, loss = 0.56221118\n",
            "Iteration 39, loss = 0.56048536\n",
            "Iteration 40, loss = 0.56174763\n",
            "Iteration 41, loss = 0.56561136\n",
            "Iteration 42, loss = 0.56144895\n",
            "Iteration 43, loss = 0.56166215\n",
            "Iteration 44, loss = 0.56513558\n",
            "Iteration 45, loss = 0.56019172\n",
            "Iteration 46, loss = 0.55856847\n",
            "Iteration 47, loss = 0.56509200\n",
            "Iteration 48, loss = 0.56055148\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 49, loss = 0.51974959\n",
            "Iteration 50, loss = 0.50787948\n",
            "Iteration 51, loss = 0.49979081\n",
            "Iteration 52, loss = 0.49249950\n",
            "Iteration 53, loss = 0.48554712\n",
            "Iteration 54, loss = 0.48034874\n",
            "Iteration 55, loss = 0.47596342\n",
            "Iteration 56, loss = 0.47445102\n",
            "Iteration 57, loss = 0.48012900\n",
            "Iteration 58, loss = 0.50720621\n",
            "Iteration 59, loss = 0.49790972\n",
            "Iteration 60, loss = 0.50586754\n",
            "Iteration 61, loss = 0.50783938\n",
            "Iteration 62, loss = 0.50729829\n",
            "Iteration 63, loss = 0.49463776\n",
            "Iteration 64, loss = 0.50109325\n",
            "Iteration 65, loss = 0.50868765\n",
            "Iteration 66, loss = 0.50889006\n",
            "Iteration 67, loss = 0.49291243\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 68, loss = 0.43073156\n",
            "Iteration 69, loss = 0.42788825\n",
            "Iteration 70, loss = 0.42552846\n",
            "Iteration 71, loss = 0.42263813\n",
            "Iteration 72, loss = 0.41995163\n",
            "Iteration 73, loss = 0.41721701\n",
            "Iteration 74, loss = 0.41503534\n",
            "Iteration 75, loss = 0.41214384\n",
            "Iteration 76, loss = 0.40984653\n",
            "Iteration 77, loss = 0.40720397\n",
            "Iteration 78, loss = 0.40462101\n",
            "Iteration 79, loss = 0.40248931\n",
            "Iteration 80, loss = 0.39965089\n",
            "Iteration 81, loss = 0.39733304\n",
            "Iteration 82, loss = 0.39540309\n",
            "Iteration 83, loss = 0.39309556\n",
            "Iteration 84, loss = 0.39082217\n",
            "Iteration 85, loss = 0.38832211\n",
            "Iteration 86, loss = 0.38572554\n",
            "Iteration 87, loss = 0.38380273\n",
            "Iteration 88, loss = 0.38155508\n",
            "Iteration 89, loss = 0.37969720\n",
            "Iteration 90, loss = 0.37733753\n",
            "Iteration 91, loss = 0.37550322\n",
            "Iteration 92, loss = 0.37339499\n",
            "Iteration 93, loss = 0.37110737\n",
            "Iteration 94, loss = 0.36920815\n",
            "Iteration 95, loss = 0.36712989\n",
            "Iteration 96, loss = 0.36480385\n",
            "Iteration 97, loss = 0.36341772\n",
            "Iteration 98, loss = 0.36099458\n",
            "Iteration 99, loss = 0.35931540\n",
            "Iteration 100, loss = 0.35734661\n",
            "Iteration 101, loss = 0.35624932\n",
            "Iteration 102, loss = 0.35465930\n",
            "Iteration 103, loss = 0.35229602\n",
            "Iteration 104, loss = 0.35072527\n",
            "Iteration 105, loss = 0.34931765\n",
            "Iteration 106, loss = 0.34701618\n",
            "Iteration 107, loss = 0.34576682\n",
            "Iteration 108, loss = 0.34416737\n",
            "Iteration 109, loss = 0.34271756\n",
            "Iteration 110, loss = 0.34126055\n",
            "Iteration 111, loss = 0.33921875\n",
            "Iteration 112, loss = 0.33778779\n",
            "Iteration 113, loss = 0.33620889\n",
            "Iteration 114, loss = 0.33480487\n",
            "Iteration 115, loss = 0.33395731\n",
            "Iteration 116, loss = 0.33240375\n",
            "Iteration 117, loss = 0.33101148\n",
            "Iteration 118, loss = 0.32970751\n",
            "Iteration 119, loss = 0.32820742\n",
            "Iteration 120, loss = 0.32731577\n",
            "Iteration 121, loss = 0.32522894\n",
            "Iteration 122, loss = 0.32396301\n",
            "Iteration 123, loss = 0.32404609\n",
            "Iteration 124, loss = 0.32129750\n",
            "Iteration 125, loss = 0.32032026\n",
            "Iteration 126, loss = 0.31905610\n",
            "Iteration 127, loss = 0.31905254\n",
            "Iteration 128, loss = 0.31700619\n",
            "Iteration 129, loss = 0.31692486\n",
            "Iteration 130, loss = 0.31693367\n",
            "Iteration 131, loss = 0.31424489\n",
            "Iteration 132, loss = 0.31402472\n",
            "Iteration 133, loss = 0.31209144\n",
            "Iteration 134, loss = 0.31167538\n",
            "Iteration 135, loss = 0.31024186\n",
            "Iteration 136, loss = 0.30945151\n",
            "Iteration 137, loss = 0.30952479\n",
            "Iteration 138, loss = 0.30797429\n",
            "Iteration 139, loss = 0.30800874\n",
            "Iteration 140, loss = 0.30563902\n",
            "Iteration 141, loss = 0.30677020\n",
            "Iteration 142, loss = 0.30525937\n",
            "Iteration 143, loss = 0.30658464\n",
            "Iteration 144, loss = 0.30225210\n",
            "Iteration 145, loss = 0.30227445\n",
            "Iteration 146, loss = 0.30294290\n",
            "Iteration 147, loss = 0.30119285\n",
            "Iteration 148, loss = 0.29792455\n",
            "Iteration 149, loss = 0.29919960\n",
            "Iteration 150, loss = 0.29806814\n",
            "Iteration 151, loss = 0.29671465\n",
            "Iteration 152, loss = 0.29842980\n",
            "Iteration 153, loss = 0.29612915\n",
            "Iteration 154, loss = 0.29420508\n",
            "Iteration 155, loss = 0.29469961\n",
            "Iteration 156, loss = 0.29565762\n",
            "Iteration 157, loss = 0.29478843\n",
            "Iteration 158, loss = 0.29340527\n",
            "Iteration 159, loss = 0.29373000\n",
            "Iteration 160, loss = 0.29286478\n",
            "Iteration 161, loss = 0.29176100\n",
            "Iteration 162, loss = 0.29373449\n",
            "Iteration 163, loss = 0.29150145\n",
            "Iteration 164, loss = 0.29284213\n",
            "Iteration 165, loss = 0.29256231\n",
            "Iteration 166, loss = 0.28917998\n",
            "Iteration 167, loss = 0.29029050\n",
            "Iteration 168, loss = 0.29063283\n",
            "Iteration 169, loss = 0.28700519\n",
            "Iteration 170, loss = 0.29095667\n",
            "Iteration 171, loss = 0.28711381\n",
            "Iteration 172, loss = 0.28528898\n",
            "Iteration 173, loss = 0.28696867\n",
            "Iteration 174, loss = 0.28868807\n",
            "Iteration 175, loss = 0.28593624\n",
            "Iteration 176, loss = 0.28818731\n",
            "Iteration 177, loss = 0.28515295\n",
            "Iteration 178, loss = 0.28447539\n",
            "Iteration 179, loss = 0.28644644\n",
            "Iteration 180, loss = 0.28223565\n",
            "Iteration 181, loss = 0.28997737\n",
            "Iteration 182, loss = 0.28809261\n",
            "Iteration 183, loss = 0.28118264\n",
            "Iteration 184, loss = 0.28338598\n",
            "Iteration 185, loss = 0.28719281\n",
            "Iteration 186, loss = 0.28335860\n",
            "Iteration 187, loss = 0.28056947\n",
            "Iteration 188, loss = 0.28196235\n",
            "Iteration 189, loss = 0.28544784\n",
            "Iteration 190, loss = 0.28513278\n",
            "Iteration 191, loss = 0.28630108\n",
            "Iteration 192, loss = 0.28323247\n",
            "Iteration 193, loss = 0.27982108\n",
            "Iteration 194, loss = 0.28265685\n",
            "Iteration 195, loss = 0.28512783\n",
            "Iteration 196, loss = 0.28090241\n",
            "Iteration 197, loss = 0.28002812\n",
            "Iteration 198, loss = 0.28972478\n",
            "Iteration 199, loss = 0.28717441\n",
            "Iteration 200, loss = 0.28110196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67475372\n",
            "Iteration 3, loss = 0.66018694\n",
            "Iteration 4, loss = 0.64857883\n",
            "Iteration 5, loss = 0.63710158\n",
            "Iteration 6, loss = 0.63249273\n",
            "Iteration 7, loss = 0.62231835\n",
            "Iteration 8, loss = 0.61365503\n",
            "Iteration 9, loss = 0.60512197\n",
            "Iteration 10, loss = 0.60131758\n",
            "Iteration 11, loss = 0.59456583\n",
            "Iteration 12, loss = 0.59186644\n",
            "Iteration 13, loss = 0.58742796\n",
            "Iteration 14, loss = 0.58364990\n",
            "Iteration 15, loss = 0.58150854\n",
            "Iteration 16, loss = 0.57832881\n",
            "Iteration 17, loss = 0.57688743\n",
            "Iteration 18, loss = 0.57723755\n",
            "Iteration 19, loss = 0.57352980\n",
            "Iteration 20, loss = 0.57191673\n",
            "Iteration 21, loss = 0.56704393\n",
            "Iteration 22, loss = 0.56995194\n",
            "Iteration 23, loss = 0.57068953\n",
            "Iteration 24, loss = 0.56468673\n",
            "Iteration 25, loss = 0.56720818\n",
            "Iteration 26, loss = 0.56358069\n",
            "Iteration 27, loss = 0.56499371\n",
            "Iteration 28, loss = 0.56297322\n",
            "Iteration 29, loss = 0.56287983\n",
            "Iteration 30, loss = 0.56211526\n",
            "Iteration 31, loss = 0.56508427\n",
            "Iteration 32, loss = 0.56416329\n",
            "Iteration 33, loss = 0.56335062\n",
            "Iteration 34, loss = 0.55957207\n",
            "Iteration 35, loss = 0.56162610\n",
            "Iteration 36, loss = 0.56086026\n",
            "Iteration 37, loss = 0.56599057\n",
            "Iteration 38, loss = 0.55971322\n",
            "Iteration 39, loss = 0.56049973\n",
            "Iteration 40, loss = 0.56150146\n",
            "Iteration 41, loss = 0.56112836\n",
            "Iteration 42, loss = 0.56113932\n",
            "Iteration 43, loss = 0.56241017\n",
            "Iteration 44, loss = 0.56266586\n",
            "Iteration 45, loss = 0.55980765\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 46, loss = 0.52324797\n",
            "Iteration 47, loss = 0.51281714\n",
            "Iteration 48, loss = 0.50769858\n",
            "Iteration 49, loss = 0.50030461\n",
            "Iteration 50, loss = 0.49522388\n",
            "Iteration 51, loss = 0.49059670\n",
            "Iteration 52, loss = 0.48339453\n",
            "Iteration 53, loss = 0.48721645\n",
            "Iteration 54, loss = 0.47954879\n",
            "Iteration 55, loss = 0.48218570\n",
            "Iteration 56, loss = 0.49829479\n",
            "Iteration 57, loss = 0.49859675\n",
            "Iteration 58, loss = 0.49428599\n",
            "Iteration 59, loss = 0.51877818\n",
            "Iteration 60, loss = 0.50631033\n",
            "Iteration 61, loss = 0.51357003\n",
            "Iteration 62, loss = 0.51202626\n",
            "Iteration 63, loss = 0.50492518\n",
            "Iteration 64, loss = 0.51506610\n",
            "Iteration 65, loss = 0.50911519\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 66, loss = 0.44069528\n",
            "Iteration 67, loss = 0.43853334\n",
            "Iteration 68, loss = 0.43616739\n",
            "Iteration 69, loss = 0.43425793\n",
            "Iteration 70, loss = 0.43213029\n",
            "Iteration 71, loss = 0.42999141\n",
            "Iteration 72, loss = 0.42792643\n",
            "Iteration 73, loss = 0.42625235\n",
            "Iteration 74, loss = 0.42421471\n",
            "Iteration 75, loss = 0.42208635\n",
            "Iteration 76, loss = 0.42016564\n",
            "Iteration 77, loss = 0.41823344\n",
            "Iteration 78, loss = 0.41594506\n",
            "Iteration 79, loss = 0.41407171\n",
            "Iteration 80, loss = 0.41218304\n",
            "Iteration 81, loss = 0.41030331\n",
            "Iteration 82, loss = 0.40840516\n",
            "Iteration 83, loss = 0.40652316\n",
            "Iteration 84, loss = 0.40478175\n",
            "Iteration 85, loss = 0.40276845\n",
            "Iteration 86, loss = 0.40143659\n",
            "Iteration 87, loss = 0.39914192\n",
            "Iteration 88, loss = 0.39791230\n",
            "Iteration 89, loss = 0.39563755\n",
            "Iteration 90, loss = 0.39380484\n",
            "Iteration 91, loss = 0.39217135\n",
            "Iteration 92, loss = 0.39134353\n",
            "Iteration 93, loss = 0.38884601\n",
            "Iteration 94, loss = 0.38769146\n",
            "Iteration 95, loss = 0.38598989\n",
            "Iteration 96, loss = 0.38401087\n",
            "Iteration 97, loss = 0.38221252\n",
            "Iteration 98, loss = 0.38121594\n",
            "Iteration 99, loss = 0.37993209\n",
            "Iteration 100, loss = 0.37768558\n",
            "Iteration 101, loss = 0.37624978\n",
            "Iteration 102, loss = 0.37459535\n",
            "Iteration 103, loss = 0.37325695\n",
            "Iteration 104, loss = 0.37250773\n",
            "Iteration 105, loss = 0.37129420\n",
            "Iteration 106, loss = 0.36915826\n",
            "Iteration 107, loss = 0.36795220\n",
            "Iteration 108, loss = 0.36668335\n",
            "Iteration 109, loss = 0.36513738\n",
            "Iteration 110, loss = 0.36446644\n",
            "Iteration 111, loss = 0.36310908\n",
            "Iteration 112, loss = 0.36154758\n",
            "Iteration 113, loss = 0.35951494\n",
            "Iteration 114, loss = 0.35954101\n",
            "Iteration 115, loss = 0.35693971\n",
            "Iteration 116, loss = 0.35578248\n",
            "Iteration 117, loss = 0.35541949\n",
            "Iteration 118, loss = 0.35353822\n",
            "Iteration 119, loss = 0.35291969\n",
            "Iteration 120, loss = 0.35110880\n",
            "Iteration 121, loss = 0.35105989\n",
            "Iteration 122, loss = 0.34998644\n",
            "Iteration 123, loss = 0.34834809\n",
            "Iteration 124, loss = 0.34700911\n",
            "Iteration 125, loss = 0.34714091\n",
            "Iteration 126, loss = 0.34480174\n",
            "Iteration 127, loss = 0.34568093\n",
            "Iteration 128, loss = 0.34451658\n",
            "Iteration 129, loss = 0.34505411\n",
            "Iteration 130, loss = 0.34193761\n",
            "Iteration 131, loss = 0.34135238\n",
            "Iteration 132, loss = 0.34073840\n",
            "Iteration 133, loss = 0.33936914\n",
            "Iteration 134, loss = 0.33891716\n",
            "Iteration 135, loss = 0.33879281\n",
            "Iteration 136, loss = 0.33833116\n",
            "Iteration 137, loss = 0.33698841\n",
            "Iteration 138, loss = 0.33548264\n",
            "Iteration 139, loss = 0.33617895\n",
            "Iteration 140, loss = 0.33643197\n",
            "Iteration 141, loss = 0.33345620\n",
            "Iteration 142, loss = 0.33450629\n",
            "Iteration 143, loss = 0.33222136\n",
            "Iteration 144, loss = 0.33304847\n",
            "Iteration 145, loss = 0.33280970\n",
            "Iteration 146, loss = 0.33030391\n",
            "Iteration 147, loss = 0.33000668\n",
            "Iteration 148, loss = 0.32943813\n",
            "Iteration 149, loss = 0.32856061\n",
            "Iteration 150, loss = 0.32717694\n",
            "Iteration 151, loss = 0.32702364\n",
            "Iteration 152, loss = 0.32914148\n",
            "Iteration 153, loss = 0.32718179\n",
            "Iteration 154, loss = 0.32742446\n",
            "Iteration 155, loss = 0.32745905\n",
            "Iteration 156, loss = 0.32622936\n",
            "Iteration 157, loss = 0.32440503\n",
            "Iteration 158, loss = 0.32152138\n",
            "Iteration 159, loss = 0.32378721\n",
            "Iteration 160, loss = 0.32648985\n",
            "Iteration 161, loss = 0.32442628\n",
            "Iteration 162, loss = 0.32414127\n",
            "Iteration 163, loss = 0.32518378\n",
            "Iteration 164, loss = 0.32216110\n",
            "Iteration 165, loss = 0.32340723\n",
            "Iteration 166, loss = 0.32205445\n",
            "Iteration 167, loss = 0.32464667\n",
            "Iteration 168, loss = 0.32116657\n",
            "Iteration 169, loss = 0.32053819\n",
            "Iteration 170, loss = 0.31865928\n",
            "Iteration 171, loss = 0.32032877\n",
            "Iteration 172, loss = 0.31981401\n",
            "Iteration 173, loss = 0.31820818\n",
            "Iteration 174, loss = 0.32299261\n",
            "Iteration 175, loss = 0.32069792\n",
            "Iteration 176, loss = 0.31726840\n",
            "Iteration 177, loss = 0.30466612\n",
            "Iteration 178, loss = 0.29885123\n",
            "Iteration 179, loss = 0.29788697\n",
            "Iteration 180, loss = 0.29893547\n",
            "Iteration 181, loss = 0.29663471\n",
            "Iteration 182, loss = 0.30165272\n",
            "Iteration 183, loss = 0.30112384\n",
            "Iteration 184, loss = 0.29859447\n",
            "Iteration 185, loss = 0.29479919\n",
            "Iteration 186, loss = 0.29410720\n",
            "Iteration 187, loss = 0.29594264\n",
            "Iteration 188, loss = 0.29839531\n",
            "Iteration 189, loss = 0.29429402\n",
            "Iteration 190, loss = 0.29368685\n",
            "Iteration 191, loss = 0.29256857\n",
            "Iteration 192, loss = 0.29824279\n",
            "Iteration 193, loss = 0.29551959\n",
            "Iteration 194, loss = 0.29340173\n",
            "Iteration 195, loss = 0.29631383\n",
            "Iteration 196, loss = 0.29269044\n",
            "Iteration 197, loss = 0.30079397\n",
            "Iteration 198, loss = 0.29156451\n",
            "Iteration 199, loss = 0.29363080\n",
            "Iteration 200, loss = 0.30230103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.67008396\n",
            "Iteration 3, loss = 0.65615821\n",
            "Iteration 4, loss = 0.64610676\n",
            "Iteration 5, loss = 0.63516783\n",
            "Iteration 6, loss = 0.62992261\n",
            "Iteration 7, loss = 0.61654665\n",
            "Iteration 8, loss = 0.61126682\n",
            "Iteration 9, loss = 0.60654415\n",
            "Iteration 10, loss = 0.59910380\n",
            "Iteration 11, loss = 0.59613264\n",
            "Iteration 12, loss = 0.59283394\n",
            "Iteration 13, loss = 0.58417951\n",
            "Iteration 14, loss = 0.58454766\n",
            "Iteration 15, loss = 0.58265118\n",
            "Iteration 16, loss = 0.57600145\n",
            "Iteration 17, loss = 0.57851008\n",
            "Iteration 18, loss = 0.57415848\n",
            "Iteration 19, loss = 0.56943307\n",
            "Iteration 20, loss = 0.57277524\n",
            "Iteration 21, loss = 0.56750974\n",
            "Iteration 22, loss = 0.56707309\n",
            "Iteration 23, loss = 0.56820808\n",
            "Iteration 24, loss = 0.56536399\n",
            "Iteration 25, loss = 0.56665761\n",
            "Iteration 26, loss = 0.56525290\n",
            "Iteration 27, loss = 0.56542659\n",
            "Iteration 28, loss = 0.56402335\n",
            "Iteration 29, loss = 0.56143513\n",
            "Iteration 30, loss = 0.56961034\n",
            "Iteration 31, loss = 0.57821446\n",
            "Iteration 32, loss = 0.57240565\n",
            "Iteration 33, loss = 0.57017979\n",
            "Iteration 34, loss = 0.56902096\n",
            "Iteration 35, loss = 0.56697245\n",
            "Iteration 36, loss = 0.56703836\n",
            "Iteration 37, loss = 0.56489427\n",
            "Iteration 38, loss = 0.56665554\n",
            "Iteration 39, loss = 0.56344781\n",
            "Iteration 40, loss = 0.56012295\n",
            "Iteration 41, loss = 0.56635514\n",
            "Iteration 42, loss = 0.56225677\n",
            "Iteration 43, loss = 0.56225474\n",
            "Iteration 44, loss = 0.55949021\n",
            "Iteration 45, loss = 0.55939171\n",
            "Iteration 46, loss = 0.55938843\n",
            "Iteration 47, loss = 0.55986843\n",
            "Iteration 48, loss = 0.57645212\n",
            "Iteration 49, loss = 0.57749887\n",
            "Iteration 50, loss = 0.57085361\n",
            "Iteration 51, loss = 0.56724217\n",
            "Iteration 52, loss = 0.56804795\n",
            "Iteration 53, loss = 0.56865144\n",
            "Iteration 54, loss = 0.56387583\n",
            "Iteration 55, loss = 0.56259742\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 56, loss = 0.52944946\n",
            "Iteration 57, loss = 0.52058131\n",
            "Iteration 58, loss = 0.51423537\n",
            "Iteration 59, loss = 0.51005643\n",
            "Iteration 60, loss = 0.50384052\n",
            "Iteration 61, loss = 0.49840004\n",
            "Iteration 62, loss = 0.49505624\n",
            "Iteration 63, loss = 0.48889893\n",
            "Iteration 64, loss = 0.48471640\n",
            "Iteration 65, loss = 0.49440965\n",
            "Iteration 66, loss = 0.50112463\n",
            "Iteration 67, loss = 0.50693691\n",
            "Iteration 68, loss = 0.50351417\n",
            "Iteration 69, loss = 0.51975244\n",
            "Iteration 70, loss = 0.51190803\n",
            "Iteration 71, loss = 0.50200952\n",
            "Iteration 72, loss = 0.52132205\n",
            "Iteration 73, loss = 0.50346342\n",
            "Iteration 74, loss = 0.51138930\n",
            "Iteration 75, loss = 0.50819415\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 76, loss = 0.46286072\n",
            "Iteration 77, loss = 0.44423042\n",
            "Iteration 78, loss = 0.44187852\n",
            "Iteration 79, loss = 0.43993354\n",
            "Iteration 80, loss = 0.43740529\n",
            "Iteration 81, loss = 0.43499625\n",
            "Iteration 82, loss = 0.43286212\n",
            "Iteration 83, loss = 0.43080170\n",
            "Iteration 84, loss = 0.42857168\n",
            "Iteration 85, loss = 0.42595458\n",
            "Iteration 86, loss = 0.42368193\n",
            "Iteration 87, loss = 0.42220218\n",
            "Iteration 88, loss = 0.41989524\n",
            "Iteration 89, loss = 0.41757353\n",
            "Iteration 90, loss = 0.41562982\n",
            "Iteration 91, loss = 0.41336201\n",
            "Iteration 92, loss = 0.41108548\n",
            "Iteration 93, loss = 0.40874080\n",
            "Iteration 94, loss = 0.40684464\n",
            "Iteration 95, loss = 0.40466937\n",
            "Iteration 96, loss = 0.40231336\n",
            "Iteration 97, loss = 0.40060554\n",
            "Iteration 98, loss = 0.39861104\n",
            "Iteration 99, loss = 0.39636581\n",
            "Iteration 100, loss = 0.39402303\n",
            "Iteration 101, loss = 0.39248486\n",
            "Iteration 102, loss = 0.39047758\n",
            "Iteration 103, loss = 0.38829330\n",
            "Iteration 104, loss = 0.38672616\n",
            "Iteration 105, loss = 0.38469875\n",
            "Iteration 106, loss = 0.38239461\n",
            "Iteration 107, loss = 0.38094816\n",
            "Iteration 108, loss = 0.37850158\n",
            "Iteration 109, loss = 0.37679810\n",
            "Iteration 110, loss = 0.37557012\n",
            "Iteration 111, loss = 0.37309723\n",
            "Iteration 112, loss = 0.37194823\n",
            "Iteration 113, loss = 0.37016447\n",
            "Iteration 114, loss = 0.36794247\n",
            "Iteration 115, loss = 0.36669998\n",
            "Iteration 116, loss = 0.36429728\n",
            "Iteration 117, loss = 0.36268291\n",
            "Iteration 118, loss = 0.36122337\n",
            "Iteration 119, loss = 0.35997122\n",
            "Iteration 120, loss = 0.35781778\n",
            "Iteration 121, loss = 0.35615231\n",
            "Iteration 122, loss = 0.35541309\n",
            "Iteration 123, loss = 0.35380028\n",
            "Iteration 124, loss = 0.35151409\n",
            "Iteration 125, loss = 0.35089548\n",
            "Iteration 126, loss = 0.34907050\n",
            "Iteration 127, loss = 0.34856772\n",
            "Iteration 128, loss = 0.34568392\n",
            "Iteration 129, loss = 0.34404167\n",
            "Iteration 130, loss = 0.34291750\n",
            "Iteration 131, loss = 0.34127889\n",
            "Iteration 132, loss = 0.34021844\n",
            "Iteration 133, loss = 0.33898037\n",
            "Iteration 134, loss = 0.33723412\n",
            "Iteration 135, loss = 0.33644949\n",
            "Iteration 136, loss = 0.33400724\n",
            "Iteration 137, loss = 0.33338013\n",
            "Iteration 138, loss = 0.33266658\n",
            "Iteration 139, loss = 0.33096789\n",
            "Iteration 140, loss = 0.32941612\n",
            "Iteration 141, loss = 0.32864084\n",
            "Iteration 142, loss = 0.32766410\n",
            "Iteration 143, loss = 0.32713336\n",
            "Iteration 144, loss = 0.32506607\n",
            "Iteration 145, loss = 0.32354573\n",
            "Iteration 146, loss = 0.32377553\n",
            "Iteration 147, loss = 0.32183632\n",
            "Iteration 148, loss = 0.32081360\n",
            "Iteration 149, loss = 0.32025853\n",
            "Iteration 150, loss = 0.32019465\n",
            "Iteration 151, loss = 0.31919230\n",
            "Iteration 152, loss = 0.31692400\n",
            "Iteration 153, loss = 0.31561011\n",
            "Iteration 154, loss = 0.31439863\n",
            "Iteration 155, loss = 0.31425432\n",
            "Iteration 156, loss = 0.31351503\n",
            "Iteration 157, loss = 0.31284603\n",
            "Iteration 158, loss = 0.31116741\n",
            "Iteration 159, loss = 0.31115527\n",
            "Iteration 160, loss = 0.30964690\n",
            "Iteration 161, loss = 0.30938709\n",
            "Iteration 162, loss = 0.30861325\n",
            "Iteration 163, loss = 0.30810729\n",
            "Iteration 164, loss = 0.30730285\n",
            "Iteration 165, loss = 0.30525895\n",
            "Iteration 166, loss = 0.30408173\n",
            "Iteration 167, loss = 0.30754674\n",
            "Iteration 168, loss = 0.30333708\n",
            "Iteration 169, loss = 0.30164427\n",
            "Iteration 170, loss = 0.30592318\n",
            "Iteration 171, loss = 0.30270895\n",
            "Iteration 172, loss = 0.30309881\n",
            "Iteration 173, loss = 0.29963856\n",
            "Iteration 174, loss = 0.30334578\n",
            "Iteration 175, loss = 0.29871652\n",
            "Iteration 176, loss = 0.29851274\n",
            "Iteration 177, loss = 0.29694235\n",
            "Iteration 178, loss = 0.29889350\n",
            "Iteration 179, loss = 0.29999786\n",
            "Iteration 180, loss = 0.29985672\n",
            "Iteration 181, loss = 0.29524329\n",
            "Iteration 182, loss = 0.29517403\n",
            "Iteration 183, loss = 0.29479530\n",
            "Iteration 184, loss = 0.29586112\n",
            "Iteration 185, loss = 0.29577978\n",
            "Iteration 186, loss = 0.29466451\n",
            "Iteration 187, loss = 0.29378844\n",
            "Iteration 188, loss = 0.29256694\n",
            "Iteration 189, loss = 0.29620471\n",
            "Iteration 190, loss = 0.29660525\n",
            "Iteration 191, loss = 0.29493426\n",
            "Iteration 192, loss = 0.29488867\n",
            "Iteration 193, loss = 0.29358014\n",
            "Iteration 194, loss = 0.28937182\n",
            "Iteration 195, loss = 0.28850738\n",
            "Iteration 196, loss = 0.29284954\n",
            "Iteration 197, loss = 0.29451067\n",
            "Iteration 198, loss = 0.28971095\n",
            "Iteration 199, loss = 0.29510372\n",
            "Iteration 200, loss = 0.29307893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.69681152\n",
            "Iteration 3, loss = 0.67757991\n",
            "Iteration 4, loss = 0.68924390\n",
            "Iteration 5, loss = 0.66742143\n",
            "Iteration 6, loss = 0.65423033\n",
            "Iteration 7, loss = 0.64260170\n",
            "Iteration 8, loss = 0.63141292\n",
            "Iteration 9, loss = 0.62295780\n",
            "Iteration 10, loss = 0.61594003\n",
            "Iteration 11, loss = 0.61024611\n",
            "Iteration 12, loss = 0.60565032\n",
            "Iteration 13, loss = 0.60325074\n",
            "Iteration 14, loss = 0.59635736\n",
            "Iteration 15, loss = 0.59432515\n",
            "Iteration 16, loss = 0.58806417\n",
            "Iteration 17, loss = 0.58687877\n",
            "Iteration 18, loss = 0.58430558\n",
            "Iteration 19, loss = 0.58325009\n",
            "Iteration 20, loss = 0.58124654\n",
            "Iteration 21, loss = 0.58232581\n",
            "Iteration 22, loss = 0.57495969\n",
            "Iteration 23, loss = 0.57661643\n",
            "Iteration 24, loss = 0.57441219\n",
            "Iteration 25, loss = 0.66404628\n",
            "Iteration 26, loss = 0.61968106\n",
            "Iteration 27, loss = 0.60619007\n",
            "Iteration 28, loss = 0.59997678\n",
            "Iteration 29, loss = 0.59362926\n",
            "Iteration 30, loss = 0.59011935\n",
            "Iteration 31, loss = 0.58468666\n",
            "Iteration 32, loss = 0.58310772\n",
            "Iteration 33, loss = 0.58207913\n",
            "Iteration 34, loss = 0.57970172\n",
            "Iteration 35, loss = 0.57915465\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 36, loss = 0.56355076\n",
            "Iteration 37, loss = 0.56177737\n",
            "Iteration 38, loss = 0.56026766\n",
            "Iteration 39, loss = 0.55835609\n",
            "Iteration 40, loss = 0.55741327\n",
            "Iteration 41, loss = 0.55626589\n",
            "Iteration 42, loss = 0.55423050\n",
            "Iteration 43, loss = 0.55275152\n",
            "Iteration 44, loss = 0.55073512\n",
            "Iteration 45, loss = 0.54967848\n",
            "Iteration 46, loss = 0.54694700\n",
            "Iteration 47, loss = 0.54421945\n",
            "Iteration 48, loss = 0.54302959\n",
            "Iteration 49, loss = 0.54121430\n",
            "Iteration 50, loss = 0.53769395\n",
            "Iteration 51, loss = 0.53636197\n",
            "Iteration 52, loss = 0.53439835\n",
            "Iteration 53, loss = 0.53408632\n",
            "Iteration 54, loss = 0.53175168\n",
            "Iteration 55, loss = 0.52948404\n",
            "Iteration 56, loss = 0.53591790\n",
            "Iteration 57, loss = 0.53534201\n",
            "Iteration 58, loss = 0.53585079\n",
            "Iteration 59, loss = 0.53405045\n",
            "Iteration 60, loss = 0.53524130\n",
            "Iteration 61, loss = 0.53479859\n",
            "Iteration 62, loss = 0.53966509\n",
            "Iteration 63, loss = 0.53685968\n",
            "Iteration 64, loss = 0.53979570\n",
            "Iteration 65, loss = 0.53499355\n",
            "Iteration 66, loss = 0.53407475\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 67, loss = 0.50147380\n",
            "Iteration 68, loss = 0.49009391\n",
            "Iteration 69, loss = 0.48838722\n",
            "Iteration 70, loss = 0.48694309\n",
            "Iteration 71, loss = 0.48553609\n",
            "Iteration 72, loss = 0.48414228\n",
            "Iteration 73, loss = 0.48291154\n",
            "Iteration 74, loss = 0.48101220\n",
            "Iteration 75, loss = 0.47942347\n",
            "Iteration 76, loss = 0.47770817\n",
            "Iteration 77, loss = 0.47694876\n",
            "Iteration 78, loss = 0.47470209\n",
            "Iteration 79, loss = 0.47337386\n",
            "Iteration 80, loss = 0.47199330\n",
            "Iteration 81, loss = 0.47055657\n",
            "Iteration 82, loss = 0.46853871\n",
            "Iteration 83, loss = 0.46702088\n",
            "Iteration 84, loss = 0.46563488\n",
            "Iteration 85, loss = 0.46445278\n",
            "Iteration 86, loss = 0.46265181\n",
            "Iteration 87, loss = 0.46093210\n",
            "Iteration 88, loss = 0.45961653\n",
            "Iteration 89, loss = 0.45816942\n",
            "Iteration 90, loss = 0.45662971\n",
            "Iteration 91, loss = 0.45479789\n",
            "Iteration 92, loss = 0.45326794\n",
            "Iteration 93, loss = 0.45155947\n",
            "Iteration 94, loss = 0.45087587\n",
            "Iteration 95, loss = 0.44929593\n",
            "Iteration 96, loss = 0.44784515\n",
            "Iteration 97, loss = 0.44645634\n",
            "Iteration 98, loss = 0.44480805\n",
            "Iteration 99, loss = 0.44288350\n",
            "Iteration 100, loss = 0.44121701\n",
            "Iteration 101, loss = 0.44053236\n",
            "Iteration 102, loss = 0.43883715\n",
            "Iteration 103, loss = 0.43750770\n",
            "Iteration 104, loss = 0.43563439\n",
            "Iteration 105, loss = 0.43407377\n",
            "Iteration 106, loss = 0.43364749\n",
            "Iteration 107, loss = 0.43176900\n",
            "Iteration 108, loss = 0.42995249\n",
            "Iteration 109, loss = 0.42988434\n",
            "Iteration 110, loss = 0.42743905\n",
            "Iteration 111, loss = 0.42694352\n",
            "Iteration 112, loss = 0.42485802\n",
            "Iteration 113, loss = 0.42377057\n",
            "Iteration 114, loss = 0.42218454\n",
            "Iteration 115, loss = 0.42296008\n",
            "Iteration 116, loss = 0.41968198\n",
            "Iteration 117, loss = 0.41746009\n",
            "Iteration 118, loss = 0.41810684\n",
            "Iteration 119, loss = 0.41713821\n",
            "Iteration 120, loss = 0.41655434\n",
            "Iteration 121, loss = 0.41285310\n",
            "Iteration 122, loss = 0.41391661\n",
            "Iteration 123, loss = 0.41123673\n",
            "Iteration 124, loss = 0.41007655\n",
            "Iteration 125, loss = 0.40920057\n",
            "Iteration 126, loss = 0.41010881\n",
            "Iteration 127, loss = 0.40755434\n",
            "Iteration 128, loss = 0.40932848\n",
            "Iteration 129, loss = 0.40689893\n",
            "Iteration 130, loss = 0.40420572\n",
            "Iteration 131, loss = 0.40655037\n",
            "Iteration 132, loss = 0.40429956\n",
            "Iteration 133, loss = 0.40394464\n",
            "Iteration 134, loss = 0.40460551\n",
            "Iteration 135, loss = 0.40068659\n",
            "Iteration 136, loss = 0.40025164\n",
            "Iteration 137, loss = 0.40093243\n",
            "Iteration 138, loss = 0.40176300\n",
            "Iteration 139, loss = 0.39955710\n",
            "Iteration 140, loss = 0.40059671\n",
            "Iteration 141, loss = 0.39574710\n",
            "Iteration 142, loss = 0.39840369\n",
            "Iteration 143, loss = 0.39978624\n",
            "Iteration 144, loss = 0.39587222\n",
            "Iteration 145, loss = 0.39576838\n",
            "Iteration 146, loss = 0.39947868\n",
            "Iteration 147, loss = 0.39835437\n",
            "Iteration 148, loss = 0.39973329\n",
            "Iteration 149, loss = 0.39592521\n",
            "Iteration 150, loss = 0.39237592\n",
            "Iteration 151, loss = 0.39155118\n",
            "Iteration 152, loss = 0.39182382\n",
            "Iteration 153, loss = 0.39587276\n",
            "Iteration 154, loss = 0.38992232\n",
            "Iteration 155, loss = 0.39393665\n",
            "Iteration 156, loss = 0.39450716\n",
            "Iteration 157, loss = 0.39383808\n",
            "Iteration 158, loss = 0.39102668\n",
            "Iteration 159, loss = 0.38324245\n",
            "Iteration 160, loss = 0.38929013\n",
            "Iteration 161, loss = 0.39628066\n",
            "Iteration 162, loss = 0.39350548\n",
            "Iteration 163, loss = 0.39117460\n",
            "Iteration 164, loss = 0.39753409\n",
            "Iteration 165, loss = 0.38783369\n",
            "Iteration 166, loss = 0.39046771\n",
            "Iteration 167, loss = 0.38741804\n",
            "Iteration 168, loss = 0.38657256\n",
            "Iteration 169, loss = 0.38621614\n",
            "Iteration 170, loss = 0.38498967\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 171, loss = 0.35969984\n",
            "Iteration 172, loss = 0.35922359\n",
            "Iteration 173, loss = 0.35917758\n",
            "Iteration 174, loss = 0.35894400\n",
            "Iteration 175, loss = 0.35860644\n",
            "Iteration 176, loss = 0.35844982\n",
            "Iteration 177, loss = 0.35836040\n",
            "Iteration 178, loss = 0.35793598\n",
            "Iteration 179, loss = 0.35774653\n",
            "Iteration 180, loss = 0.35769897\n",
            "Iteration 181, loss = 0.35758146\n",
            "Iteration 182, loss = 0.35725983\n",
            "Iteration 183, loss = 0.35708459\n",
            "Iteration 184, loss = 0.35715779\n",
            "Iteration 185, loss = 0.35695774\n",
            "Iteration 186, loss = 0.35659095\n",
            "Iteration 187, loss = 0.35642076\n",
            "Iteration 188, loss = 0.35643750\n",
            "Iteration 189, loss = 0.35623843\n",
            "Iteration 190, loss = 0.35616913\n",
            "Iteration 191, loss = 0.35590180\n",
            "Iteration 192, loss = 0.35574535\n",
            "Iteration 193, loss = 0.35552591\n",
            "Iteration 194, loss = 0.35544903\n",
            "Iteration 195, loss = 0.35532173\n",
            "Iteration 196, loss = 0.35519032\n",
            "Iteration 197, loss = 0.35492941\n",
            "Iteration 198, loss = 0.35476600\n",
            "Iteration 199, loss = 0.35456869\n",
            "Iteration 200, loss = 0.35435055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.72121533\n",
            "Iteration 3, loss = 0.70241792\n",
            "Iteration 4, loss = 0.68730693\n",
            "Iteration 5, loss = 0.67451190\n",
            "Iteration 6, loss = 0.66135952\n",
            "Iteration 7, loss = 0.65081293\n",
            "Iteration 8, loss = 0.63989281\n",
            "Iteration 9, loss = 0.63214957\n",
            "Iteration 10, loss = 0.62363391\n",
            "Iteration 11, loss = 0.63872162\n",
            "Iteration 12, loss = 0.64935293\n",
            "Iteration 13, loss = 0.64993290\n",
            "Iteration 14, loss = 0.64440422\n",
            "Iteration 15, loss = 0.64010864\n",
            "Iteration 16, loss = 0.63639176\n",
            "Iteration 17, loss = 0.63306922\n",
            "Iteration 18, loss = 0.63030226\n",
            "Iteration 19, loss = 0.62802954\n",
            "Iteration 20, loss = 0.62172370\n",
            "Iteration 21, loss = 0.61739557\n",
            "Iteration 22, loss = 0.60862585\n",
            "Iteration 23, loss = 0.59903485\n",
            "Iteration 24, loss = 0.62376871\n",
            "Iteration 25, loss = 0.62010652\n",
            "Iteration 26, loss = 0.61825584\n",
            "Iteration 27, loss = 0.61643364\n",
            "Iteration 28, loss = 0.61533361\n",
            "Iteration 29, loss = 0.61440157\n",
            "Iteration 30, loss = 0.61298540\n",
            "Iteration 31, loss = 0.61260100\n",
            "Iteration 32, loss = 0.61128504\n",
            "Iteration 33, loss = 0.61067517\n",
            "Iteration 34, loss = 0.61050870\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 35, loss = 0.60979906\n",
            "Iteration 36, loss = 0.60873998\n",
            "Iteration 37, loss = 0.60861007\n",
            "Iteration 38, loss = 0.60849768\n",
            "Iteration 39, loss = 0.60857040\n",
            "Iteration 40, loss = 0.60829263\n",
            "Iteration 41, loss = 0.60834942\n",
            "Iteration 42, loss = 0.60821679\n",
            "Iteration 43, loss = 0.60807927\n",
            "Iteration 44, loss = 0.60771437\n",
            "Iteration 45, loss = 0.60764615\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 46, loss = 0.60740415\n",
            "Iteration 47, loss = 0.60741974\n",
            "Iteration 48, loss = 0.60737503\n",
            "Iteration 49, loss = 0.60752009\n",
            "Iteration 50, loss = 0.60739627\n",
            "Iteration 51, loss = 0.60727051\n",
            "Iteration 52, loss = 0.60729798\n",
            "Iteration 53, loss = 0.60727146\n",
            "Iteration 54, loss = 0.60736925\n",
            "Iteration 55, loss = 0.60726808\n",
            "Iteration 56, loss = 0.60722402\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 57, loss = 0.60713208\n",
            "Iteration 58, loss = 0.60713114\n",
            "Iteration 59, loss = 0.60712443\n",
            "Iteration 60, loss = 0.60711284\n",
            "Iteration 61, loss = 0.60711034\n",
            "Iteration 62, loss = 0.60709360\n",
            "Iteration 63, loss = 0.60710004\n",
            "Iteration 64, loss = 0.60708459\n",
            "Iteration 65, loss = 0.60708146\n",
            "Iteration 66, loss = 0.60709360\n",
            "Iteration 67, loss = 0.60708138\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 68, loss = 0.60706284\n",
            "Iteration 69, loss = 0.60705450\n",
            "Iteration 70, loss = 0.60705493\n",
            "Iteration 71, loss = 0.60705097\n",
            "Iteration 72, loss = 0.60705525\n",
            "Iteration 73, loss = 0.60705408\n",
            "Iteration 74, loss = 0.60705033\n",
            "Iteration 75, loss = 0.60704984\n",
            "Iteration 76, loss = 0.60704969\n",
            "Iteration 77, loss = 0.60704959\n",
            "Iteration 78, loss = 0.60704859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 79, loss = 0.60704030\n",
            "Iteration 80, loss = 0.60704089\n",
            "Iteration 81, loss = 0.60703994\n",
            "Iteration 82, loss = 0.60704004\n",
            "Iteration 83, loss = 0.60703984\n",
            "Iteration 84, loss = 0.60703970\n",
            "Iteration 85, loss = 0.60703914\n",
            "Iteration 86, loss = 0.60703894\n",
            "Iteration 87, loss = 0.60703876\n",
            "Iteration 88, loss = 0.60703883\n",
            "Iteration 89, loss = 0.60703856\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.70161667\n",
            "Iteration 3, loss = 0.67842866\n",
            "Iteration 4, loss = 0.66226735\n",
            "Iteration 5, loss = 0.64751569\n",
            "Iteration 6, loss = 0.63870016\n",
            "Iteration 7, loss = 0.63076054\n",
            "Iteration 8, loss = 0.62194378\n",
            "Iteration 9, loss = 0.61443419\n",
            "Iteration 10, loss = 0.60962683\n",
            "Iteration 11, loss = 0.60255648\n",
            "Iteration 12, loss = 0.59940048\n",
            "Iteration 13, loss = 0.59245604\n",
            "Iteration 14, loss = 0.58857286\n",
            "Iteration 15, loss = 0.58847731\n",
            "Iteration 16, loss = 0.58958743\n",
            "Iteration 17, loss = 0.57982054\n",
            "Iteration 18, loss = 0.57871848\n",
            "Iteration 19, loss = 0.57347520\n",
            "Iteration 20, loss = 0.57652382\n",
            "Iteration 21, loss = 0.57249111\n",
            "Iteration 22, loss = 0.57096113\n",
            "Iteration 23, loss = 0.57073039\n",
            "Iteration 24, loss = 0.57078379\n",
            "Iteration 25, loss = 0.56926746\n",
            "Iteration 26, loss = 0.56569598\n",
            "Iteration 27, loss = 0.56774062\n",
            "Iteration 28, loss = 0.56812502\n",
            "Iteration 29, loss = 0.56609636\n",
            "Iteration 30, loss = 0.59308937\n",
            "Iteration 31, loss = 0.58627447\n",
            "Iteration 32, loss = 0.57732805\n",
            "Iteration 33, loss = 0.57710166\n",
            "Iteration 34, loss = 0.57486821\n",
            "Iteration 35, loss = 0.57027751\n",
            "Iteration 36, loss = 0.56896386\n",
            "Iteration 37, loss = 0.57263553\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 38, loss = 0.54955256\n",
            "Iteration 39, loss = 0.54136056\n",
            "Iteration 40, loss = 0.53816947\n",
            "Iteration 41, loss = 0.53527234\n",
            "Iteration 42, loss = 0.53320494\n",
            "Iteration 43, loss = 0.53146867\n",
            "Iteration 44, loss = 0.52745032\n",
            "Iteration 45, loss = 0.52491271\n",
            "Iteration 46, loss = 0.52191633\n",
            "Iteration 47, loss = 0.51944986\n",
            "Iteration 48, loss = 0.51594810\n",
            "Iteration 49, loss = 0.51624436\n",
            "Iteration 50, loss = 0.51187474\n",
            "Iteration 51, loss = 0.51295745\n",
            "Iteration 52, loss = 0.51443501\n",
            "Iteration 53, loss = 0.52392344\n",
            "Iteration 54, loss = 0.51281788\n",
            "Iteration 55, loss = 0.52278649\n",
            "Iteration 56, loss = 0.51932792\n",
            "Iteration 57, loss = 0.52237158\n",
            "Iteration 58, loss = 0.52032631\n",
            "Iteration 59, loss = 0.52614982\n",
            "Iteration 60, loss = 0.52422318\n",
            "Iteration 61, loss = 0.51877992\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 62, loss = 0.47586949\n",
            "Iteration 63, loss = 0.47125735\n",
            "Iteration 64, loss = 0.46969800\n",
            "Iteration 65, loss = 0.46814720\n",
            "Iteration 66, loss = 0.46671641\n",
            "Iteration 67, loss = 0.46504404\n",
            "Iteration 68, loss = 0.46336050\n",
            "Iteration 69, loss = 0.46175951\n",
            "Iteration 70, loss = 0.46014004\n",
            "Iteration 71, loss = 0.45797309\n",
            "Iteration 72, loss = 0.45655538\n",
            "Iteration 73, loss = 0.45475478\n",
            "Iteration 74, loss = 0.45351957\n",
            "Iteration 75, loss = 0.45169615\n",
            "Iteration 76, loss = 0.44950015\n",
            "Iteration 77, loss = 0.44786538\n",
            "Iteration 78, loss = 0.44699484\n",
            "Iteration 79, loss = 0.44512976\n",
            "Iteration 80, loss = 0.44353510\n",
            "Iteration 81, loss = 0.44126325\n",
            "Iteration 82, loss = 0.44013301\n",
            "Iteration 83, loss = 0.43827277\n",
            "Iteration 84, loss = 0.43731001\n",
            "Iteration 85, loss = 0.43580214\n",
            "Iteration 86, loss = 0.43386913\n",
            "Iteration 87, loss = 0.43221326\n",
            "Iteration 88, loss = 0.43040095\n",
            "Iteration 89, loss = 0.42913629\n",
            "Iteration 90, loss = 0.42766093\n",
            "Iteration 91, loss = 0.42694018\n",
            "Iteration 92, loss = 0.42479906\n",
            "Iteration 93, loss = 0.42301944\n",
            "Iteration 94, loss = 0.42183579\n",
            "Iteration 95, loss = 0.42023434\n",
            "Iteration 96, loss = 0.41901006\n",
            "Iteration 97, loss = 0.41689519\n",
            "Iteration 98, loss = 0.41573129\n",
            "Iteration 99, loss = 0.41406156\n",
            "Iteration 100, loss = 0.41340092\n",
            "Iteration 101, loss = 0.41222005\n",
            "Iteration 102, loss = 0.41074601\n",
            "Iteration 103, loss = 0.40875615\n",
            "Iteration 104, loss = 0.40742464\n",
            "Iteration 105, loss = 0.40624799\n",
            "Iteration 106, loss = 0.40598502\n",
            "Iteration 107, loss = 0.40294653\n",
            "Iteration 108, loss = 0.40381813\n",
            "Iteration 109, loss = 0.40116654\n",
            "Iteration 110, loss = 0.40031070\n",
            "Iteration 111, loss = 0.39898125\n",
            "Iteration 112, loss = 0.39788343\n",
            "Iteration 113, loss = 0.39739987\n",
            "Iteration 114, loss = 0.39526717\n",
            "Iteration 115, loss = 0.39518568\n",
            "Iteration 116, loss = 0.39377376\n",
            "Iteration 117, loss = 0.39181411\n",
            "Iteration 118, loss = 0.39274110\n",
            "Iteration 119, loss = 0.39132149\n",
            "Iteration 120, loss = 0.38843191\n",
            "Iteration 121, loss = 0.38775851\n",
            "Iteration 122, loss = 0.38973582\n",
            "Iteration 123, loss = 0.38512538\n",
            "Iteration 124, loss = 0.38424880\n",
            "Iteration 125, loss = 0.38311262\n",
            "Iteration 126, loss = 0.38429371\n",
            "Iteration 127, loss = 0.38247021\n",
            "Iteration 128, loss = 0.38238422\n",
            "Iteration 129, loss = 0.38320000\n",
            "Iteration 130, loss = 0.37883601\n",
            "Iteration 131, loss = 0.38175525\n",
            "Iteration 132, loss = 0.37954523\n",
            "Iteration 133, loss = 0.37895230\n",
            "Iteration 134, loss = 0.37846151\n",
            "Iteration 135, loss = 0.37885575\n",
            "Iteration 136, loss = 0.37652375\n",
            "Iteration 137, loss = 0.37835142\n",
            "Iteration 138, loss = 0.37639500\n",
            "Iteration 139, loss = 0.37722598\n",
            "Iteration 140, loss = 0.37877689\n",
            "Iteration 141, loss = 0.37606352\n",
            "Iteration 142, loss = 0.37305499\n",
            "Iteration 143, loss = 0.37563760\n",
            "Iteration 144, loss = 0.37253484\n",
            "Iteration 145, loss = 0.37629768\n",
            "Iteration 146, loss = 0.37025384\n",
            "Iteration 147, loss = 0.37224823\n",
            "Iteration 148, loss = 0.37267681\n",
            "Iteration 149, loss = 0.36907545\n",
            "Iteration 150, loss = 0.37018488\n",
            "Iteration 151, loss = 0.36838103\n",
            "Iteration 152, loss = 0.37034816\n",
            "Iteration 153, loss = 0.36772791\n",
            "Iteration 154, loss = 0.37132971\n",
            "Iteration 155, loss = 0.36711693\n",
            "Iteration 156, loss = 0.36859542\n",
            "Iteration 157, loss = 0.36194028\n",
            "Iteration 158, loss = 0.36648886\n",
            "Iteration 159, loss = 0.37286356\n",
            "Iteration 160, loss = 0.36912866\n",
            "Iteration 161, loss = 0.37462925\n",
            "Iteration 162, loss = 0.37048688\n",
            "Iteration 163, loss = 0.37676505\n",
            "Iteration 164, loss = 0.36825994\n",
            "Iteration 165, loss = 0.37184265\n",
            "Iteration 166, loss = 0.36883750\n",
            "Iteration 167, loss = 0.36287134\n",
            "Iteration 168, loss = 0.36164641\n",
            "Iteration 169, loss = 0.37046537\n",
            "Iteration 170, loss = 0.36546929\n",
            "Iteration 171, loss = 0.36568483\n",
            "Iteration 172, loss = 0.36880618\n",
            "Iteration 173, loss = 0.36856397\n",
            "Iteration 174, loss = 0.37379360\n",
            "Iteration 175, loss = 0.37610091\n",
            "Iteration 176, loss = 0.37405250\n",
            "Iteration 177, loss = 0.35884324\n",
            "Iteration 178, loss = 0.35956650\n",
            "Iteration 179, loss = 0.36816712\n",
            "Iteration 180, loss = 0.35979584\n",
            "Iteration 181, loss = 0.36580349\n",
            "Iteration 182, loss = 0.36572120\n",
            "Iteration 183, loss = 0.36278644\n",
            "Iteration 184, loss = 0.36612887\n",
            "Iteration 185, loss = 0.36330549\n",
            "Iteration 186, loss = 0.36221186\n",
            "Iteration 187, loss = 0.36037097\n",
            "Iteration 188, loss = 0.36601842\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 189, loss = 0.33089931\n",
            "Iteration 190, loss = 0.33063771\n",
            "Iteration 191, loss = 0.33021471\n",
            "Iteration 192, loss = 0.33017886\n",
            "Iteration 193, loss = 0.33011295\n",
            "Iteration 194, loss = 0.32980321\n",
            "Iteration 195, loss = 0.32976362\n",
            "Iteration 196, loss = 0.32962546\n",
            "Iteration 197, loss = 0.32949747\n",
            "Iteration 198, loss = 0.32937906\n",
            "Iteration 199, loss = 0.32937827\n",
            "Iteration 200, loss = 0.32923668\n",
            "Iteration 1, loss = 1.95942080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.69519835\n",
            "Iteration 3, loss = 0.68200649\n",
            "Iteration 4, loss = 0.66564131\n",
            "Iteration 5, loss = 0.65489499\n",
            "Iteration 6, loss = 0.64256846\n",
            "Iteration 7, loss = 0.63352096\n",
            "Iteration 8, loss = 0.62511872\n",
            "Iteration 9, loss = 0.61757435\n",
            "Iteration 10, loss = 0.61250576\n",
            "Iteration 11, loss = 0.60594342\n",
            "Iteration 12, loss = 0.60621531\n",
            "Iteration 13, loss = 0.59648932\n",
            "Iteration 14, loss = 0.59373566\n",
            "Iteration 15, loss = 0.59043621\n",
            "Iteration 16, loss = 0.58752984\n",
            "Iteration 17, loss = 0.58573526\n",
            "Iteration 18, loss = 0.58043628\n",
            "Iteration 19, loss = 0.58231876\n",
            "Iteration 20, loss = 0.57803404\n",
            "Iteration 21, loss = 0.58219758\n",
            "Iteration 22, loss = 0.57617450\n",
            "Iteration 23, loss = 0.57357025\n",
            "Iteration 24, loss = 0.57264067\n",
            "Iteration 25, loss = 0.57322370\n",
            "Iteration 26, loss = 0.57376533\n",
            "Iteration 27, loss = 0.56909149\n",
            "Iteration 28, loss = 0.56978847\n",
            "Iteration 29, loss = 0.57237329\n",
            "Iteration 30, loss = 0.56905798\n",
            "Iteration 31, loss = 0.56840539\n",
            "Iteration 32, loss = 0.57061181\n",
            "Iteration 33, loss = 0.56751525\n",
            "Iteration 34, loss = 0.56694335\n",
            "Iteration 35, loss = 0.56835618\n",
            "Iteration 36, loss = 0.56878485\n",
            "Iteration 37, loss = 0.56888181\n",
            "Iteration 38, loss = 0.56697984\n",
            "Iteration 39, loss = 0.56689225\n",
            "Iteration 40, loss = 0.56952091\n",
            "Iteration 41, loss = 0.56630265\n",
            "Iteration 42, loss = 0.56628832\n",
            "Iteration 43, loss = 0.56635730\n",
            "Iteration 44, loss = 0.56847048\n",
            "Iteration 45, loss = 0.56688850\n",
            "Iteration 46, loss = 0.56472402\n",
            "Iteration 47, loss = 0.57039138\n",
            "Iteration 48, loss = 0.56128951\n",
            "Iteration 49, loss = 0.57018396\n",
            "Iteration 50, loss = 0.57128385\n",
            "Iteration 51, loss = 0.56272404\n",
            "Iteration 52, loss = 0.56940873\n",
            "Iteration 53, loss = 0.56536907\n",
            "Iteration 54, loss = 0.56684950\n",
            "Iteration 55, loss = 0.56501561\n",
            "Iteration 56, loss = 0.63734296\n",
            "Iteration 57, loss = 0.61742360\n",
            "Iteration 58, loss = 0.60288391\n",
            "Iteration 59, loss = 0.59766738\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 60, loss = 0.57815652\n",
            "Iteration 61, loss = 0.57532562\n",
            "Iteration 62, loss = 0.57263179\n",
            "Iteration 63, loss = 0.57110658\n",
            "Iteration 64, loss = 0.56927529\n",
            "Iteration 65, loss = 0.56782928\n",
            "Iteration 66, loss = 0.56569753\n",
            "Iteration 67, loss = 0.56403228\n",
            "Iteration 68, loss = 0.56230196\n",
            "Iteration 69, loss = 0.56052438\n",
            "Iteration 70, loss = 0.55946647\n",
            "Iteration 71, loss = 0.55733843\n",
            "Iteration 72, loss = 0.55579081\n",
            "Iteration 73, loss = 0.55263765\n",
            "Iteration 74, loss = 0.55134729\n",
            "Iteration 75, loss = 0.54975959\n",
            "Iteration 76, loss = 0.54864670\n",
            "Iteration 77, loss = 0.54589389\n",
            "Iteration 78, loss = 0.54343008\n",
            "Iteration 79, loss = 0.54128051\n",
            "Iteration 80, loss = 0.53933784\n",
            "Iteration 81, loss = 0.54527085\n",
            "Iteration 82, loss = 0.54210200\n",
            "Iteration 83, loss = 0.54095184\n",
            "Iteration 84, loss = 0.53895935\n",
            "Iteration 85, loss = 0.53781518\n",
            "Iteration 86, loss = 0.54387932\n",
            "Iteration 87, loss = 0.53987201\n",
            "Iteration 88, loss = 0.53885677\n",
            "Iteration 89, loss = 0.53965543\n",
            "Iteration 90, loss = 0.53845369\n",
            "Iteration 91, loss = 0.53841981\n",
            "Iteration 92, loss = 0.54149443\n",
            "Iteration 93, loss = 0.53936895\n",
            "Iteration 94, loss = 0.53999132\n",
            "Iteration 95, loss = 0.53671066\n",
            "Iteration 96, loss = 0.53485822\n",
            "Iteration 97, loss = 0.53701720\n",
            "Iteration 98, loss = 0.54293935\n",
            "Iteration 99, loss = 0.53124793\n",
            "Iteration 100, loss = 0.53322300\n",
            "Iteration 101, loss = 0.53195405\n",
            "Iteration 102, loss = 0.53299795\n",
            "Iteration 103, loss = 0.53689222\n",
            "Iteration 104, loss = 0.53364547\n",
            "Iteration 105, loss = 0.52399893\n",
            "Iteration 106, loss = 0.53133325\n",
            "Iteration 107, loss = 0.53182375\n",
            "Iteration 108, loss = 0.52671432\n",
            "Iteration 109, loss = 0.52715116\n",
            "Iteration 110, loss = 0.53826366\n",
            "Iteration 111, loss = 0.52595338\n",
            "Iteration 112, loss = 0.52256246\n",
            "Iteration 113, loss = 0.52919066\n",
            "Iteration 114, loss = 0.52754614\n",
            "Iteration 115, loss = 0.52960750\n",
            "Iteration 116, loss = 0.52400565\n",
            "Iteration 117, loss = 0.52491727\n",
            "Iteration 118, loss = 0.53203729\n",
            "Iteration 119, loss = 0.52706399\n",
            "Iteration 120, loss = 0.51983228\n",
            "Iteration 121, loss = 0.52846990\n",
            "Iteration 122, loss = 0.52406346\n",
            "Iteration 123, loss = 0.52287828\n",
            "Iteration 124, loss = 0.53524034\n",
            "Iteration 125, loss = 0.52369393\n",
            "Iteration 126, loss = 0.52246339\n",
            "Iteration 127, loss = 0.51606350\n",
            "Iteration 128, loss = 0.52748431\n",
            "Iteration 129, loss = 0.51947671\n",
            "Iteration 130, loss = 0.53186038\n",
            "Iteration 131, loss = 0.51481515\n",
            "Iteration 132, loss = 0.52890987\n",
            "Iteration 133, loss = 0.52723106\n",
            "Iteration 134, loss = 0.50871708\n",
            "Iteration 135, loss = 0.52712197\n",
            "Iteration 136, loss = 0.51415726\n",
            "Iteration 137, loss = 0.53099060\n",
            "Iteration 138, loss = 0.52150728\n",
            "Iteration 139, loss = 0.52576787\n",
            "Iteration 140, loss = 0.51585522\n",
            "Iteration 141, loss = 0.51871234\n",
            "Iteration 142, loss = 0.52449223\n",
            "Iteration 143, loss = 0.53014762\n",
            "Iteration 144, loss = 0.51473184\n",
            "Iteration 145, loss = 0.51928967\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 146, loss = 0.44851935\n",
            "Iteration 147, loss = 0.44541821\n",
            "Iteration 148, loss = 0.44236694\n",
            "Iteration 149, loss = 0.44013284\n",
            "Iteration 150, loss = 0.43741749\n",
            "Iteration 151, loss = 0.43455573\n",
            "Iteration 152, loss = 0.43189099\n",
            "Iteration 153, loss = 0.42910343\n",
            "Iteration 154, loss = 0.42642658\n",
            "Iteration 155, loss = 0.42427253\n",
            "Iteration 156, loss = 0.42191972\n",
            "Iteration 157, loss = 0.42015312\n",
            "Iteration 158, loss = 0.41653999\n",
            "Iteration 159, loss = 0.41477350\n",
            "Iteration 160, loss = 0.41249326\n",
            "Iteration 161, loss = 0.41006419\n",
            "Iteration 162, loss = 0.40782270\n",
            "Iteration 163, loss = 0.40572497\n",
            "Iteration 164, loss = 0.40353705\n",
            "Iteration 165, loss = 0.40161430\n",
            "Iteration 166, loss = 0.39950540\n",
            "Iteration 167, loss = 0.39751864\n",
            "Iteration 168, loss = 0.39543484\n",
            "Iteration 169, loss = 0.39350748\n",
            "Iteration 170, loss = 0.39168330\n",
            "Iteration 171, loss = 0.39052200\n",
            "Iteration 172, loss = 0.38770075\n",
            "Iteration 173, loss = 0.38629933\n",
            "Iteration 174, loss = 0.38443426\n",
            "Iteration 175, loss = 0.38308200\n",
            "Iteration 176, loss = 0.38090943\n",
            "Iteration 177, loss = 0.37949874\n",
            "Iteration 178, loss = 0.37777717\n",
            "Iteration 179, loss = 0.37669348\n",
            "Iteration 180, loss = 0.37533815\n",
            "Iteration 181, loss = 0.37344393\n",
            "Iteration 182, loss = 0.37225502\n",
            "Iteration 183, loss = 0.37099634\n",
            "Iteration 184, loss = 0.36923722\n",
            "Iteration 185, loss = 0.36826050\n",
            "Iteration 186, loss = 0.36734651\n",
            "Iteration 187, loss = 0.36640835\n",
            "Iteration 188, loss = 0.36461321\n",
            "Iteration 189, loss = 0.36250371\n",
            "Iteration 190, loss = 0.36084537\n",
            "Iteration 191, loss = 0.36053252\n",
            "Iteration 192, loss = 0.35930305\n",
            "Iteration 193, loss = 0.35848790\n",
            "Iteration 194, loss = 0.35737931\n",
            "Iteration 195, loss = 0.35763104\n",
            "Iteration 196, loss = 0.35560986\n",
            "Iteration 197, loss = 0.35534454\n",
            "Iteration 198, loss = 0.35392157\n",
            "Iteration 199, loss = 0.35308198\n",
            "Iteration 200, loss = 0.35179903\n",
            "Iteration 1, loss = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2, loss = 0.70820908\n",
            "Iteration 3, loss = 0.68830817\n",
            "Iteration 4, loss = 0.67210867\n",
            "Iteration 5, loss = 0.65859005\n",
            "Iteration 6, loss = 0.64964791\n",
            "Iteration 7, loss = 0.63598252\n",
            "Iteration 8, loss = 0.63003523\n",
            "Iteration 9, loss = 0.62200064\n",
            "Iteration 10, loss = 0.61601736\n",
            "Iteration 11, loss = 0.61035621\n",
            "Iteration 12, loss = 0.60228191\n",
            "Iteration 13, loss = 0.59844359\n",
            "Iteration 14, loss = 0.59534947\n",
            "Iteration 15, loss = 0.58821903\n",
            "Iteration 16, loss = 0.59114184\n",
            "Iteration 17, loss = 0.58829679\n",
            "Iteration 18, loss = 0.58462037\n",
            "Iteration 19, loss = 0.57975322\n",
            "Iteration 20, loss = 0.57859603\n",
            "Iteration 21, loss = 0.57673181\n",
            "Iteration 22, loss = 0.57587942\n",
            "Iteration 23, loss = 0.57369107\n",
            "Iteration 24, loss = 0.57297372\n",
            "Iteration 25, loss = 0.57311891\n",
            "Iteration 26, loss = 0.57192875\n",
            "Iteration 27, loss = 0.56937295\n",
            "Iteration 28, loss = 0.57128982\n",
            "Iteration 29, loss = 0.56876906\n",
            "Iteration 30, loss = 0.56660435\n",
            "Iteration 31, loss = 0.56828304\n",
            "Iteration 32, loss = 0.57111173\n",
            "Iteration 33, loss = 0.56795752\n",
            "Iteration 34, loss = 0.56627265\n",
            "Iteration 35, loss = 0.56821416\n",
            "Iteration 36, loss = 0.56676647\n",
            "Iteration 37, loss = 0.56751156\n",
            "Iteration 38, loss = 0.56889848\n",
            "Iteration 39, loss = 0.56600645\n",
            "Iteration 40, loss = 0.56532393\n",
            "Iteration 41, loss = 0.56952291\n",
            "Iteration 42, loss = 0.56773876\n",
            "Iteration 43, loss = 0.56669814\n",
            "Iteration 44, loss = 0.56801396\n",
            "Iteration 45, loss = 0.56404803\n",
            "Iteration 46, loss = 0.56550777\n",
            "Iteration 47, loss = 0.56327952\n",
            "Iteration 48, loss = 0.56728091\n",
            "Iteration 49, loss = 0.56337744\n",
            "Iteration 50, loss = 0.56617959\n",
            "Iteration 51, loss = 0.57071557\n",
            "Iteration 52, loss = 0.57151108\n",
            "Iteration 53, loss = 0.56587807\n",
            "Iteration 54, loss = 0.56769345\n",
            "Iteration 55, loss = 0.56635793\n",
            "Iteration 56, loss = 0.56530252\n",
            "Iteration 57, loss = 0.56629727\n",
            "Iteration 58, loss = 0.57121724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 59, loss = 0.52679331\n",
            "Iteration 60, loss = 0.51353372\n",
            "Iteration 61, loss = 0.50447476\n",
            "Iteration 62, loss = 0.49573797\n",
            "Iteration 63, loss = 0.49010139\n",
            "Iteration 64, loss = 0.48213745\n",
            "Iteration 65, loss = 0.47887259\n",
            "Iteration 66, loss = 0.48733863\n",
            "Iteration 67, loss = 0.48542379\n",
            "Iteration 68, loss = 0.49527897\n",
            "Iteration 69, loss = 0.51259652\n",
            "Iteration 70, loss = 0.50808399\n",
            "Iteration 71, loss = 0.52352400\n",
            "Iteration 72, loss = 0.50313104\n",
            "Iteration 73, loss = 0.50288599\n",
            "Iteration 74, loss = 0.52082706\n",
            "Iteration 75, loss = 0.50876668\n",
            "Iteration 76, loss = 0.51684536\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 77, loss = 0.45065266\n",
            "Iteration 78, loss = 0.43457034\n",
            "Iteration 79, loss = 0.43160469\n",
            "Iteration 80, loss = 0.42875589\n",
            "Iteration 81, loss = 0.42577630\n",
            "Iteration 82, loss = 0.42284793\n",
            "Iteration 83, loss = 0.42018874\n",
            "Iteration 84, loss = 0.41737777\n",
            "Iteration 85, loss = 0.41461477\n",
            "Iteration 86, loss = 0.41245752\n",
            "Iteration 87, loss = 0.40895043\n",
            "Iteration 88, loss = 0.40663482\n",
            "Iteration 89, loss = 0.40395233\n",
            "Iteration 90, loss = 0.40161882\n",
            "Iteration 91, loss = 0.39932779\n",
            "Iteration 92, loss = 0.39636221\n",
            "Iteration 93, loss = 0.39382520\n",
            "Iteration 94, loss = 0.39149701\n",
            "Iteration 95, loss = 0.38901859\n",
            "Iteration 96, loss = 0.38703437\n",
            "Iteration 97, loss = 0.38490630\n",
            "Iteration 98, loss = 0.38237389\n",
            "Iteration 99, loss = 0.38048762\n",
            "Iteration 100, loss = 0.37783673\n",
            "Iteration 101, loss = 0.37539193\n",
            "Iteration 102, loss = 0.37312784\n",
            "Iteration 103, loss = 0.37097135\n",
            "Iteration 104, loss = 0.36959738\n",
            "Iteration 105, loss = 0.36722382\n",
            "Iteration 106, loss = 0.36490613\n",
            "Iteration 107, loss = 0.36290892\n",
            "Iteration 108, loss = 0.36078282\n",
            "Iteration 109, loss = 0.35892265\n",
            "Iteration 110, loss = 0.35687154\n",
            "Iteration 111, loss = 0.35571274\n",
            "Iteration 112, loss = 0.35398775\n",
            "Iteration 113, loss = 0.35191027\n",
            "Iteration 114, loss = 0.35022346\n",
            "Iteration 115, loss = 0.34791411\n",
            "Iteration 116, loss = 0.34660748\n",
            "Iteration 117, loss = 0.34488938\n",
            "Iteration 118, loss = 0.34371093\n",
            "Iteration 119, loss = 0.34221298\n",
            "Iteration 120, loss = 0.34150110\n",
            "Iteration 121, loss = 0.33844107\n",
            "Iteration 122, loss = 0.33681238\n",
            "Iteration 123, loss = 0.33607165\n",
            "Iteration 124, loss = 0.33498448\n",
            "Iteration 125, loss = 0.33276961\n",
            "Iteration 126, loss = 0.33157714\n",
            "Iteration 127, loss = 0.32976790\n",
            "Iteration 128, loss = 0.32886002\n",
            "Iteration 129, loss = 0.32597783\n",
            "Iteration 130, loss = 0.32545466\n",
            "Iteration 131, loss = 0.32451492\n",
            "Iteration 132, loss = 0.32291407\n",
            "Iteration 133, loss = 0.32284796\n",
            "Iteration 134, loss = 0.32143445\n",
            "Iteration 135, loss = 0.32098747\n",
            "Iteration 136, loss = 0.31811020\n",
            "Iteration 137, loss = 0.31786088\n",
            "Iteration 138, loss = 0.31680446\n",
            "Iteration 139, loss = 0.31449449\n",
            "Iteration 140, loss = 0.31361971\n",
            "Iteration 141, loss = 0.31404046\n",
            "Iteration 142, loss = 0.31195653\n",
            "Iteration 143, loss = 0.31134531\n",
            "Iteration 144, loss = 0.31100331\n",
            "Iteration 145, loss = 0.30886987\n",
            "Iteration 146, loss = 0.30861906\n",
            "Iteration 147, loss = 0.30760702\n",
            "Iteration 148, loss = 0.30649664\n",
            "Iteration 149, loss = 0.30465831\n",
            "Iteration 150, loss = 0.30527697\n",
            "Iteration 151, loss = 0.30494346\n",
            "Iteration 152, loss = 0.30304411\n",
            "Iteration 153, loss = 0.30324019\n",
            "Iteration 154, loss = 0.30012693\n",
            "Iteration 155, loss = 0.29980334\n",
            "Iteration 156, loss = 0.29844869\n",
            "Iteration 157, loss = 0.29817271\n",
            "Iteration 158, loss = 0.29761106\n",
            "Iteration 159, loss = 0.29796785\n",
            "Iteration 160, loss = 0.29686934\n",
            "Iteration 161, loss = 0.29812547\n",
            "Iteration 162, loss = 0.29518105\n",
            "Iteration 163, loss = 0.29557413\n",
            "Iteration 164, loss = 0.29411450\n",
            "Iteration 165, loss = 0.29401242\n",
            "Iteration 166, loss = 0.29616737\n",
            "Iteration 167, loss = 0.29332938\n",
            "Iteration 168, loss = 0.29196608\n",
            "Iteration 169, loss = 0.29519169\n",
            "Iteration 170, loss = 0.29113296\n",
            "Iteration 171, loss = 0.29186748\n",
            "Iteration 172, loss = 0.29363489\n",
            "Iteration 173, loss = 0.28968138\n",
            "Iteration 174, loss = 0.29271178\n",
            "Iteration 175, loss = 0.28858362\n",
            "Iteration 176, loss = 0.29526039\n",
            "Iteration 177, loss = 0.29233487\n",
            "Iteration 178, loss = 0.29449085\n",
            "Iteration 179, loss = 0.28986851\n",
            "Iteration 180, loss = 0.28775771\n",
            "Iteration 181, loss = 0.28768640\n",
            "Iteration 182, loss = 0.29376620\n",
            "Iteration 183, loss = 0.28695209\n",
            "Iteration 184, loss = 0.28722768\n",
            "Iteration 185, loss = 0.28875875\n",
            "Iteration 186, loss = 0.28419580\n",
            "Iteration 187, loss = 0.28732610\n",
            "Iteration 188, loss = 0.28104724\n",
            "Iteration 189, loss = 0.29248011\n",
            "Iteration 190, loss = 0.28648233\n",
            "Iteration 191, loss = 0.28516260\n",
            "Iteration 192, loss = 0.28186923\n",
            "Iteration 193, loss = 0.28456604\n",
            "Iteration 194, loss = 0.27960173\n",
            "Iteration 195, loss = 0.28293033\n",
            "Iteration 196, loss = 0.28195940\n",
            "Iteration 197, loss = 0.28466907\n",
            "Iteration 198, loss = 0.28597483\n",
            "Iteration 199, loss = 0.28663065\n",
            "Iteration 200, loss = 0.28139040\n",
            "----------------------------------\n",
            "[[33420   307]\n",
            " [ 1277 15642]]\n",
            "----------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     33727\n",
            "           1       0.98      0.92      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.96     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n",
            "Training Accuracy (Shuffle Split) : 99.701% (7.386%)\n",
            "Prediction Accuracy (Shuffle Split) : 98.644% (8.753%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.2min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXhDR-cYkQ29",
        "colab_type": "code",
        "outputId": "18d5ef7d-44fc-48fd-c9ee-e78f4e8de47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"----------------------------------\")\n",
        "print(confusion_matrix(y_test,y_te_pred))\n",
        "print(\"----------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "[[33420   307]\n",
            " [ 1277 15642]]\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg99i8EmvEb_",
        "colab_type": "code",
        "outputId": "15a795b1-3226-4bda-fa71-d76bc1290aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "training_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[99.73344741345268,\n",
              " 99.67092273265763,\n",
              " 99.84204291167566,\n",
              " 99.76635514018692,\n",
              " 99.6807950506779,\n",
              " 99.70712123206529,\n",
              " 99.75648282216665,\n",
              " 100.0,\n",
              " 99.62814268790312,\n",
              " 99.70053968671844]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VosBD6LZvHl_",
        "colab_type": "code",
        "outputId": "6046c762-6af4-45ef-8ecc-d7e08118bbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "testing_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[98.9864420165855,\n",
              " 98.94695274450442,\n",
              " 99.09174674213506,\n",
              " 98.52573384230617,\n",
              " 99.06542056074767,\n",
              " 99.10490983282875,\n",
              " 98.85481110964854,\n",
              " 100.0,\n",
              " 98.9337896538107,\n",
              " 98.64420165854942]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9cTDZuikdoZ",
        "colab_type": "code",
        "outputId": "748da3bc-cdb7-4f0d-93a0-424e91edace9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print(classification_report(y_test,y_te_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     33727\n",
            "           1       0.98      0.92      0.95     16919\n",
            "\n",
            "    accuracy                           0.97     50646\n",
            "   macro avg       0.97      0.96      0.96     50646\n",
            "weighted avg       0.97      0.97      0.97     50646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7g494fKE0ra",
        "colab_type": "code",
        "outputId": "79b940a4-1c5e-411c-e71d-3037ce6f80ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "iterations = list(range(epochs))\n",
        "plt.plot(iterations, training_accuracy, label='Train')\n",
        "plt.plot(iterations, testing_accuracy, label='Test')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}